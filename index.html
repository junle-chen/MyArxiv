<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2025-10-30T00:00:00Z">2025-10-30</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">115</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with
  the MME-CoF Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26802v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26802v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyu Guo, Xinyan Chen, Renrui Zhang, Ruichuan An, Yu Qi, Dongzhi Jiang, Xiangtai Li, Manyuan Zhang, Hongsheng Li, Pheng-Ann Heng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent video generation models can produce high-fidelity, temporally coherent
videos, indicating that they may encode substantial world knowledge. Beyond
realistic synthesis, they also exhibit emerging behaviors indicative of visual
perception, modeling, and manipulation. Yet, an important question still
remains: Are video models ready to serve as zero-shot reasoners in challenging
visual reasoning scenarios? In this work, we conduct an empirical study to
comprehensively investigate this question, focusing on the leading and popular
Veo-3. We evaluate its reasoning behavior across 12 dimensions, including
spatial, geometric, physical, temporal, and embodied logic, systematically
characterizing both its strengths and failure modes. To standardize this study,
we curate the evaluation data into MME-CoF, a compact benchmark that enables
in-depth and thorough assessment of Chain-of-Frame (CoF) reasoning. Our
findings reveal that while current video models demonstrate promising reasoning
patterns on short-horizon spatial coherence, fine-grained grounding, and
locally consistent dynamics, they remain limited in long-horizon causal
reasoning, strict geometric constraints, and abstract logic. Overall, they are
not yet reliable as standalone zero-shot reasoners, but exhibit encouraging
signs as complementary visual engines alongside dedicated reasoning models.
Project page: https://video-cof.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://video-cof.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gistify! Codebase-Level Understanding via Runtime Execution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26790v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26790v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyunji Lee, Minseon Kim, Chinmay Singh, Matheus Pereira, Atharv Sonwane, Isadora White, Elias Stengel-Eskin, Mohit Bansal, Zhengyan Shi, Alessandro Sordoni, Marc-Alexandre Côté, Xingdi Yuan, Lucas Caccia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As coding agents are increasingly deployed in large codebases, the need to
automatically design challenging, codebase-level evaluation is central. We
propose Gistify, a task where a coding LLM must create a single, minimal,
self-contained file that can reproduce a specific functionality of a codebase.
The coding LLM is given full access to a codebase along with a specific
entrypoint (e.g., a python command), and the generated file must replicate the
output of the same command ran under the full codebase, while containing only
the essential components necessary to execute the provided command. Success on
Gistify requires both structural understanding of the codebase, accurate
modeling of its execution flow as well as the ability to produce potentially
large code patches. Our findings show that current state-of-the-art models
struggle to reliably solve Gistify tasks, especially ones with long executions
traces.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Defeating the Training-Inference Mismatch via FP16 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26788v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26788v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Penghui Qi, Zichen Liu, Xiangxin Zhou, Tianyu Pang, Chao Du, Wee Sun Lee, Min Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) fine-tuning of large language models (LLMs) often
suffers from instability due to the numerical mismatch between the training and
inference policies. While prior work has attempted to mitigate this issue
through algorithmic corrections or engineering alignments, we show that its
root cause lies in the floating point precision itself. The widely adopted
BF16, despite its large dynamic range, introduces large rounding errors that
breaks the consistency between training and inference. In this work, we
demonstrate that simply reverting to \textbf{FP16} effectively eliminates this
mismatch. The change is simple, fully supported by modern frameworks with only
a few lines of code change, and requires no modification to the model
architecture or learning algorithm. Our results suggest that using FP16
uniformly yields more stable optimization, faster convergence, and stronger
performance across diverse tasks, algorithms and frameworks. We hope these
findings motivate a broader reconsideration of precision trade-offs in RL
fine-tuning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Remote Labor Index: Measuring AI Automation of Remote Work 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26787v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26787v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mantas Mazeika, Alice Gatti, Cristina Menghini, Udari Madhushani Sehwag, Shivam Singhal, Yury Orlovskiy, Steven Basart, Manasi Sharma, Denis Peskoff, Elaine Lau, Jaehyuk Lim, Lachlan Carroll, Alice Blair, Vinaya Sivakumar, Sumana Basu, Brad Kenstler, Yuntao Ma, Julian Michael, Xiaoke Li, Oliver Ingebretsen, Aditya Mehta, Jean Mottola, John Teichmann, Kevin Yu, Zaina Shaik, Adam Khoja, Richard Ren, Jason Hausenloy, Long Phan, Ye Htet, Ankit Aich, Tahseen Rabbani, Vivswan Shah, Andriy Novykov, Felix Binder, Kirill Chugunov, Luis Ramirez, Matias Geralnik, Hernán Mesura, Dean Lee, Ed-Yeremai Hernandez Cardona, Annette Diamond, Summer Yue, Alexandr Wang, Bing Liu, Ernesto Hernandez, Dan Hendrycks
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AIs have made rapid progress on research-oriented benchmarks of knowledge and
reasoning, but it remains unclear how these gains translate into economic value
and automation. To measure this, we introduce the Remote Labor Index (RLI), a
broadly multi-sector benchmark comprising real-world, economically valuable
projects designed to evaluate end-to-end agent performance in practical
settings. AI agents perform near the floor on RLI, with the highest-performing
agent achieving an automation rate of 2.5%. These results help ground
discussions of AI automation in empirical evidence, setting a common basis for
tracking AI impacts and enabling stakeholders to proactively navigate AI-driven
labor automation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Website: https://www.remotelabor.ai</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AMO-Bench: Large Language Models Still Struggle in High School Math
  Competitions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26768v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26768v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengnan An, Xunliang Cai, Xuezhi Cao, Xiaoyu Li, Yehao Lin, Junlin Liu, Xinxuan Lv, Dan Ma, Xuanlin Wang, Ziwen Wang, Shuang Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present AMO-Bench, an Advanced Mathematical reasoning benchmark with
Olympiad level or even higher difficulty, comprising 50 human-crafted problems.
Existing benchmarks have widely leveraged high school math competitions for
evaluating mathematical reasoning capabilities of large language models (LLMs).
However, many existing math competitions are becoming less effective for
assessing top-tier LLMs due to performance saturation (e.g., AIME24/25). To
address this, AMO-Bench introduces more rigorous challenges by ensuring all 50
problems are (1) cross-validated by experts to meet at least the International
Mathematical Olympiad (IMO) difficulty standards, and (2) entirely original
problems to prevent potential performance leakages from data memorization.
Moreover, each problem in AMO-Bench requires only a final answer rather than a
proof, enabling automatic and robust grading for evaluation. Experimental
results across 26 LLMs on AMO-Bench show that even the best-performing model
achieves only 52.4% accuracy on AMO-Bench, with most LLMs scoring below 40%.
Beyond these poor performances, our further analysis reveals a promising
scaling trend with increasing test-time compute on AMO-Bench. These results
highlight the significant room for improving the mathematical reasoning in
current LLMs. We release AMO-Bench to facilitate further research into
advancing the reasoning abilities of language models.
https://amo-bench.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep sequence models tend to memorize geometrically; it is unclear why 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26745v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26745v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shahriar Noroozizadeh, Vaishnavh Nagarajan, Elan Rosenfeld, Sanjiv Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In sequence modeling, the parametric memory of atomic facts has been
predominantly abstracted as a brute-force lookup of co-occurrences between
entities. We contrast this associative view against a geometric view of how
memory is stored. We begin by isolating a clean and analyzable instance of
Transformer reasoning that is incompatible with memory as strictly a storage of
the local co-occurrences specified during training. Instead, the model must
have somehow synthesized its own geometry of atomic facts, encoding global
relationships between all entities, including non-co-occurring ones. This in
turn has simplified a hard reasoning task involving an $\ell$-fold composition
into an easy-to-learn 1-step geometric task.
  From this phenomenon, we extract fundamental aspects of neural embedding
geometries that are hard to explain. We argue that the rise of such a geometry,
despite optimizing over mere local associations, cannot be straightforwardly
attributed to typical architectural or optimizational pressures.
Counterintuitively, an elegant geometry is learned even when it is not more
succinct than a brute-force lookup of associations.
  Then, by analyzing a connection to Node2Vec, we demonstrate how the geometry
stems from a spectral bias that -- in contrast to prevailing theories -- indeed
arises naturally despite the lack of various pressures. This analysis also
points to practitioners a visible headroom to make Transformer memory more
strongly geometric. We hope the geometric view of parametric memory encourages
revisiting the default intuitions that guide researchers in areas like
knowledge acquisition, capacity, discovery and unlearning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-Platform Evaluation of Reasoning Capabilities in Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26732v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26732v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        J. de Curtò, I. de Zarzà, Pablo García, Jordi Cabot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a comprehensive cross-platform evaluation of reasoning
capabilities in contemporary foundation models, establishing an
infrastructure-agnostic benchmark across three computational paradigms: HPC
supercomputing (MareNostrum 5), cloud platforms (Nebius AI Studio), and
university clusters (a node with eight H200 GPUs).
  We evaluate 15 foundation models across 79 problems spanning eight academic
domains (Physics, Mathematics, Chemistry, Economics, Biology, Statistics,
Calculus, and Optimization) through three experimental phases: (1) Baseline
establishment: Six models (Mixtral-8x7B, Phi-3, LLaMA 3.1-8B, Gemma-2-9b,
Mistral-7B, OLMo-7B) evaluated on 19 problems using MareNostrum 5, establishing
methodology and reference performance; (2) Infrastructure validation: The
19-problem benchmark repeated on university cluster (seven models including
Falcon-Mamba state-space architecture) and Nebius AI Studio (nine
state-of-the-art models: Hermes-4 70B/405B, LLaMA 3.1-405B/3.3-70B, Qwen3
30B/235B, DeepSeek-R1, GPT-OSS 20B/120B) to confirm infrastructure-agnostic
reproducibility; (3) Extended evaluation: Full 79-problem assessment on both
university cluster and Nebius platforms, probing generalization at scale across
architectural diversity.
  The findings challenge conventional scaling assumptions, establish training
data quality as more critical than model size, and provide actionable
guidelines for model selection across educational, production, and research
contexts. The tri-infrastructure methodology and 79-problem benchmark enable
longitudinal tracking of reasoning capabilities as foundation models evolve.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Value Drifts: Tracing Value Alignment During LLM Post-Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26707v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26707v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehar Bhatia, Shravan Nayak, Gaurav Kamath, Marius Mosbach, Karolina Stańczak, Vered Shwartz, Siva Reddy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As LLMs occupy an increasingly important role in society, they are more and
more confronted with questions that require them not only to draw on their
general knowledge but also to align with certain human value systems.
Therefore, studying the alignment of LLMs with human values has become a
crucial field of inquiry. Prior work, however, mostly focuses on evaluating the
alignment of fully trained models, overlooking the training dynamics by which
models learn to express human values. In this work, we investigate how and at
which stage value alignment arises during the course of a model's
post-training. Our analysis disentangles the effects of post-training
algorithms and datasets, measuring both the magnitude and time of value drifts
during training. Experimenting with Llama-3 and Qwen-3 models of different
sizes and popular supervised fine-tuning (SFT) and preference optimization
datasets and algorithms, we find that the SFT phase generally establishes a
model's values, and subsequent preference optimization rarely re-aligns these
values. Furthermore, using a synthetic preference dataset that enables
controlled manipulation of values, we find that different preference
optimization algorithms lead to different value alignment outcomes, even when
preference data is held constant. Our findings provide actionable insights into
how values are learned during post-training and help to inform data curation,
as well as the selection of models and algorithms for preference optimization
to improve model alignment to human values.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The End of Manual Decoding: Towards Truly End-to-End Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26697v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26697v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhichao Wang, Dongyang Ma, Xinting Huang, Deng Cai, Tian Lan, Jiahao Xu, Haitao Mi, Xiaoying Tang, Yan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The "end-to-end" label for LLMs is a misnomer. In practice, they depend on a
non-differentiable decoding process that requires laborious, hand-tuning of
hyperparameters like temperature and top-p. This paper introduces AutoDeco, a
novel architecture that enables truly "end-to-end" generation by learning to
control its own decoding strategy. We augment the standard transformer with
lightweight heads that, at each step, dynamically predict context-specific
temperature and top-p values alongside the next-token logits. This approach
transforms decoding into a parametric, token-level process, allowing the model
to self-regulate its sampling strategy within a single forward pass.
  Through extensive experiments on eight benchmarks, we demonstrate that
AutoDeco not only significantly outperforms default decoding strategies but
also achieves performance comparable to an oracle-tuned baseline derived from
"hacking the test set"-a practical upper bound for any static method.
Crucially, we uncover an emergent capability for instruction-based decoding
control: the model learns to interpret natural language commands (e.g.,
"generate with low randomness") and adjusts its predicted temperature and top-p
on a token-by-token basis, opening a new paradigm for steerable and interactive
LLM decoding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Kimi Linear: An Expressive, Efficient Attention Architecture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26692v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26692v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         Kimi Team, Yu Zhang, Zongyu Lin, Xingcheng Yao, Jiaxi Hu, Fanqing Meng, Chengyin Liu, Xin Men, Songlin Yang, Zhiyuan Li, Wentao Li, Enzhe Lu, Weizhou Liu, Yanru Chen, Weixin Xu, Longhui Yu, Yejie Wang, Yu Fan, Longguang Zhong, Enming Yuan, Dehao Zhang, Yizhi Zhang, T. Y. Liu, Haiming Wang, Shengjun Fang, Weiran He, Shaowei Liu, Yiwei Li, Jianlin Su, Jiezhong Qiu, Bo Pang, Junjie Yan, Zhejun Jiang, Weixiao Huang, Bohong Yin, Jiacheng You, Chu Wei, Zhengtao Wang, Chao Hong, Yutian Chen, Guanduo Chen, Yucheng Wang, Huabin Zheng, Feng Wang, Yibo Liu, Mengnan Dong, Zheng Zhang, Siyuan Pan, Wenhao Wu, Yuhao Wu, Longyu Guan, Jiawen Tao, Guohong Fu, Xinran Xu, Yuzhi Wang, Guokun Lai, Yuxin Wu, Xinyu Zhou, Zhilin Yang, Yulun Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Kimi Linear, a hybrid linear attention architecture that, for
the first time, outperforms full attention under fair comparisons across
various scenarios -- including short-context, long-context, and reinforcement
learning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA), an
expressive linear attention module that extends Gated DeltaNet with a
finer-grained gating mechanism, enabling more effective use of limited
finite-state RNN memory. Our bespoke chunkwise algorithm achieves high hardware
efficiency through a specialized variant of the Diagonal-Plus-Low-Rank (DPLR)
transition matrices, which substantially reduces computation compared to the
general DPLR formulation while remaining more consistent with the classical
delta rule.
  We pretrain a Kimi Linear model with 3B activated parameters and 48B total
parameters, based on a layerwise hybrid of KDA and Multi-Head Latent Attention
(MLA). Our experiments show that with an identical training recipe, Kimi Linear
outperforms full MLA with a sizeable margin across all evaluated tasks, while
reducing KV cache usage by up to 75% and achieving up to 6 times decoding
throughput for a 1M context. These results demonstrate that Kimi Linear can be
a drop-in replacement for full attention architectures with superior
performance and efficiency, including tasks with longer input and output
lengths.
  To support further research, we open-source the KDA kernel and vLLM
implementations, and release the pre-trained and instruction-tuned model
checkpoints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Kimi Linear tech report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evontree: Ontology Rule-Guided Self-Evolution of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26683v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26683v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingchen Tu, Zhiqiang Liu, Juan Li, Liangyurui Liu, Junjie Wang, Lei Liang, Wen Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated exceptional capabilities
across multiple domains by leveraging massive pre-training and curated
fine-tuning data. However, in data-sensitive fields such as healthcare, the
lack of high-quality, domain-specific training corpus hinders LLMs' adaptation
for specialized applications. Meanwhile, domain experts have distilled domain
wisdom into ontology rules, which formalize relationships among concepts and
ensure the integrity of knowledge management repositories. Viewing LLMs as
implicit repositories of human knowledge, we propose Evontree, a novel
framework that leverages a small set of high-quality ontology rules to
systematically extract, validate, and enhance domain knowledge within LLMs,
without requiring extensive external datasets. Specifically, Evontree extracts
domain ontology from raw models, detects inconsistencies using two core
ontology rules, and reinforces the refined knowledge via self-distilled
fine-tuning. Extensive experiments on medical QA benchmarks with
Llama3-8B-Instruct and Med42-v2 demonstrate consistent outperformance over both
unmodified models and leading supervised baselines, achieving up to a 3.7%
improvement in accuracy. These results confirm the effectiveness, efficiency,
and robustness of our approach for low-resource domain adaptation of LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Era of Agentic Organization: Learning to Organize with Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26658v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26658v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zewen Chi, Li Dong, Qingxiu Dong, Yaru Hao, Xun Wu, Shaohan Huang, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We envision a new era of AI, termed agentic organization, where agents solve
complex problems by working collaboratively and concurrently, enabling outcomes
beyond individual intelligence. To realize this vision, we introduce
asynchronous thinking (AsyncThink) as a new paradigm of reasoning with large
language models, which organizes the internal thinking process into
concurrently executable structures. Specifically, we propose a thinking
protocol where an organizer dynamically assigns sub-queries to workers, merges
intermediate knowledge, and produces coherent solutions. More importantly, the
thinking structure in this protocol can be further optimized through
reinforcement learning. Experiments demonstrate that AsyncThink achieves 28%
lower inference latency compared to parallel thinking while improving accuracy
on mathematical reasoning. Moreover, AsyncThink generalizes its learned
asynchronous thinking capabilities, effectively tackling unseen tasks without
additional training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Encoder-Decoder or Decoder-Only? Revisiting Encoder-Decoder Large
  Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26622v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26622v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Biao Zhang, Yong Cheng, Siamak Shakeri, Xinyi Wang, Min Ma, Orhan Firat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent large language model (LLM) research has undergone an architectural
shift from encoder-decoder modeling to nowadays the dominant decoder-only
modeling. This rapid transition, however, comes without a rigorous comparative
analysis especially \textit{from the scaling perspective}, raising concerns
that the potential of encoder-decoder models may have been overlooked. To fill
this gap, we revisit encoder-decoder LLM (RedLLM), enhancing it with recent
recipes from decoder-only LLM (DecLLM). We conduct a comprehensive comparison
between RedLLM, pretrained with prefix language modeling (LM), and DecLLM,
pretrained with causal LM, at different model scales, ranging from $\sim$150M
to $\sim$8B. Using RedPajama V1 (1.6T tokens) for pretraining and FLAN for
instruction tuning, our experiments show that RedLLM produces compelling
scaling properties and surprisingly strong performance. While DecLLM is overall
more compute-optimal during pretraining, RedLLM demonstrates comparable scaling
and context length extrapolation capabilities. After instruction tuning, RedLLM
achieves comparable and even better results on various downstream tasks while
enjoying substantially better inference efficiency. We hope our findings could
inspire more efforts on re-examining RedLLM, unlocking its potential for
developing powerful and efficient LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The scaling study inspiring T5Gemma</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SlideAgent: Hierarchical Agentic Framework for Multi-Page Visual
  Document Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26615v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26615v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiqiao Jin, Rachneet Kaur, Zhen Zeng, Sumitra Ganesh, Srijan Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-page visual documents such as manuals, brochures, presentations, and
posters convey key information through layout, colors, icons, and cross-slide
references. While large language models (LLMs) offer opportunities in document
understanding, current systems struggle with complex, multi-page visual
documents, particularly in fine-grained reasoning over elements and pages. We
introduce SlideAgent, a versatile agentic framework for understanding
multi-modal, multi-page, and multi-layout documents, especially slide decks.
SlideAgent employs specialized agents and decomposes reasoning into three
specialized levels-global, page, and element-to construct a structured,
query-agnostic representation that captures both overarching themes and
detailed visual or textual cues. During inference, SlideAgent selectively
activates specialized agents for multi-level reasoning and integrates their
outputs into coherent, context-aware answers. Extensive experiments show that
SlideAgent achieves significant improvement over both proprietary (+7.9
overall) and open-source models (+9.8 overall).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://slideagent.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Normative Reasoning in Large Language Models: A Comparative Benchmark
  from Logical and Modal Perspectives <span class="chip">EMNLP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26606v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26606v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kentaro Ozeki, Risako Ando, Takanobu Morishita, Hirohiko Abe, Koji Mineshima, Mitsuhiro Okada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Normative reasoning is a type of reasoning that involves normative or deontic
modality, such as obligation and permission. While large language models (LLMs)
have demonstrated remarkable performance across various reasoning tasks, their
ability to handle normative reasoning remains underexplored. In this paper, we
systematically evaluate LLMs' reasoning capabilities in the normative domain
from both logical and modal perspectives. Specifically, to assess how well LLMs
reason with normative modals, we make a comparison between their reasoning with
normative modals and their reasoning with epistemic modals, which share a
common formal structure. To this end, we introduce a new dataset covering a
wide range of formal patterns of reasoning in both normative and epistemic
domains, while also incorporating non-formal cognitive factors that influence
human reasoning. Our results indicate that, although LLMs generally adhere to
valid reasoning patterns, they exhibit notable inconsistencies in specific
types of normative reasoning and display cognitive biases similar to those
observed in psychological studies of human reasoning. These findings highlight
challenges in achieving logical consistency in LLMs' normative reasoning and
provide insights for enhancing their reliability. All data and code are
released publicly at https://github.com/kmineshima/NeuBAROCO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 8th BlackboxNLP Workshop at EMNLP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inference-Cost-Aware Dynamic Tree Construction for Efficient Inference
  in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26577v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26577v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinrong Hong, Zhiquan Tan, Kai Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) face significant inference latency challenges
stemming from their autoregressive design and large size. To address this,
speculative decoding emerges as a solution, enabling the simultaneous
generation and validation of multiple tokens. While recent approaches like
EAGLE-2 and EAGLE-3 improve speculative decoding using dynamic tree structures,
they often neglect the impact of crucial system variables such as GPU devices
and batch sizes.
  Therefore, we introduce a new dynamic tree decoding approach called CAST that
takes into account inference costs, including factors such as GPU
configurations and batch sizes, to dynamically refine the tree structure.
Through comprehensive experimentation across six diverse tasks and utilizing
six distinct LLMs, our methodology demonstrates remarkable results, achieving
speeds up to 5.2 times faster than conventional decoding methods. Moreover, it
generally outperforms existing state-of-the-art techniques from 5% to 20%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InfoFlow: Reinforcing Search Agent Via Reward Density Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26575v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26575v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Luo, Hongjin Qian, Zheng Liu, Ziyi Xia, Shitao Xiao, Siqi Bao, Jun Zhao, Kang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning with Verifiable Rewards (RLVR) is a promising approach
for enhancing agentic deep search. However, its application is often hindered
by low \textbf{Reward Density} in deep search scenarios, where agents expend
significant exploratory costs for infrequent and often null final rewards. In
this paper, we formalize this challenge as the \textbf{Reward Density
Optimization} problem, which aims to improve the reward obtained per unit of
exploration cost. This paper introduce \textbf{InfoFlow}, a systematic
framework that tackles this problem from three aspects. 1) \textbf{Subproblem
decomposition}: breaking down long-range tasks to assign process rewards,
thereby providing denser learning signals. 2) \textbf{Failure-guided hints}:
injecting corrective guidance into stalled trajectories to increase the
probability of successful outcomes. 3) \textbf{Dual-agent refinement}:
employing a dual-agent architecture to offload the cognitive burden of deep
exploration. A refiner agent synthesizes the search history, which effectively
compresses the researcher's perceived trajectory, thereby reducing exploration
cost and increasing the overall reward density. We evaluate InfoFlow on
multiple agentic search benchmarks, where it significantly outperforms strong
baselines, enabling lightweight LLMs to achieve performance comparable to
advanced proprietary LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Structure of Relation Decoding Linear Operators in Large Language
  Models <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26543v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26543v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miranda Anna Christ, Adrián Csiszárik, Gergely Becsó, Dániel Varga
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the structure of linear operators introduced in
Hernandez et al. [2023] that decode specific relational facts in transformer
language models. We extend their single-relation findings to a collection of
relations and systematically chart their organization. We show that such
collections of relation decoders can be highly compressed by simple order-3
tensor networks without significant loss in decoding accuracy. To explain this
surprising redundancy, we develop a cross-evaluation protocol, in which we
apply each linear decoder operator to the subjects of every other relation. Our
results reveal that these linear maps do not encode distinct relations, but
extract recurring, coarse-grained semantic properties (e.g., country of capital
city and country of food are both in the country-of-X property). This
property-centric structure clarifies both the operators' compressibility and
highlights why they generalize only to new relations that are semantically
close. Our findings thus interpret linear relational decoding in transformer
language models as primarily property-based, rather than relation-specific.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025 (Spotlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hebrew Diacritics Restoration using Visual Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26521v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26521v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yair Elboher, Yuval Pinter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diacritics restoration in Hebrew is a fundamental task for ensuring accurate
word pronunciation and disambiguating textual meaning. Despite the language's
high degree of ambiguity when unvocalized, recent machine learning approaches
have significantly advanced performance on this task.
  In this work, we present DIVRIT, a novel system for Hebrew diacritization
that frames the task as a zero-shot classification problem. Our approach
operates at the word level, selecting the most appropriate diacritization
pattern for each undiacritized word from a dynamically generated candidate set,
conditioned on the surrounding textual context. A key innovation of DIVRIT is
its use of a Hebrew Visual Language Model, which processes undiacritized text
as an image, allowing diacritic information to be embedded directly within the
input's vector representation.
  Through a comprehensive evaluation across various configurations, we
demonstrate that the system effectively performs diacritization without relying
on complex, explicit linguistic analysis. Notably, in an ``oracle'' setting
where the correct diacritized form is guaranteed to be among the provided
candidates, DIVRIT achieves a high level of accuracy. Furthermore, strategic
architectural enhancements and optimized training methodologies yield
significant improvements in the system's overall generalization capabilities.
These findings highlight the promising potential of visual representations for
accurate and automated Hebrew diacritization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inside CORE-KG: Evaluating Structured <span class="highlight-title">Prompt</span>ing and Coreference
  Resolution for Knowledge Graphs <span class="chip">ICDM 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26512v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26512v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dipak Meher, Carlotta Domeniconi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human smuggling networks are increasingly adaptive and difficult to analyze.
Legal case documents offer critical insights but are often unstructured,
lexically dense, and filled with ambiguous or shifting references, which pose
significant challenges for automated knowledge graph (KG) construction. While
recent LLM-based approaches improve over static templates, they still generate
noisy, fragmented graphs with duplicate nodes due to the absence of guided
extraction and coreference resolution. The recently proposed CORE-KG framework
addresses these limitations by integrating a type-aware coreference module and
domain-guided structured prompts, significantly reducing node duplication and
legal noise. In this work, we present a systematic ablation study of CORE-KG to
quantify the individual contributions of its two key components. Our results
show that removing coreference resolution results in a 28.32% increase in node
duplication and a 4.32% increase in noisy nodes, while removing structured
prompts leads to a 4.34% increase in node duplication and a 73.33% increase in
noisy nodes. These findings offer empirical insights for designing robust
LLM-based pipelines for extracting structured representations from complex
legal texts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICDM 2025 Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Multi-agent Large Language Model Framework to Automatically Assess
  Performance of a Clinical AI Triage Tool 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26498v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26498v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adam E. Flanders, Yifan Peng, Luciano Prevedello, Robyn Ball, Errol Colak, Prahlad Menon, George Shih, Hui-Ming Lin, Paras Lakhani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Purpose: The purpose of this study was to determine if an ensemble of
multiple LLM agents could be used collectively to provide a more reliable
assessment of a pixel-based AI triage tool than a single LLM.
  Methods: 29,766 non-contrast CT head exams from fourteen hospitals were
processed by a commercial intracranial hemorrhage (ICH) AI detection tool.
Radiology reports were analyzed by an ensemble of eight open-source LLM models
and a HIPAA compliant internal version of GPT-4o using a single multi-shot
prompt that assessed for presence of ICH. 1,726 examples were manually
reviewed. Performance characteristics of the eight open-source models and
consensus were compared to GPT-4o. Three ideal consensus LLM ensembles were
tested for rating the performance of the triage tool.
  Results: The cohort consisted of 29,766 head CTs exam-report pairs. The
highest AUC performance was achieved with llama3.3:70b and GPT-4o (AUC= 0.78).
The average precision was highest for Llama3.3:70b and GPT-4o (AP=0.75 & 0.76).
Llama3.3:70b had the highest F1 score (0.81) and recall (0.85), greater
precision (0.78), specificity (0.72), and MCC (0.57). Using MCC (95% CI) the
ideal combination of LLMs were: Full-9 Ensemble 0.571 (0.552-0.591), Top-3
Ensemble 0.558 (0.537-0.579), Consensus 0.556 (0.539-0.574), and GPT4o 0.522
(0.500-0.543). No statistically significant differences were observed between
Top-3, Full-9, and Consensus (p > 0.05).
  Conclusion: An ensemble of medium to large sized open-source LLMs provides a
more consistent and reliable method to derive a ground truth retrospective
evaluation of a clinical AI triage tool over a single LLM alone.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 3 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Text-to-SQL: Dynamic Multi-turn SQL Interaction for
  Real-world Database Exploration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26495v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26495v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linzhuang Sun, Tianyu Guo, Hao Liang, Yuying Li, Qifeng Cai, Jingxuan Wei, Bihui Yu, Wentao Zhang, Bin Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Text-to-SQL have achieved strong results in static,
single-turn tasks, where models generate SQL queries from natural language
questions. However, these systems fall short in real-world interactive
scenarios, where user intents evolve and queries must be refined over multiple
turns. In applications such as finance and business analytics, users
iteratively adjust query constraints or dimensions based on intermediate
results. To evaluate such dynamic capabilities, we introduce DySQL-Bench, a
benchmark assessing model performance under evolving user interactions. Unlike
previous manually curated datasets, DySQL-Bench is built through an automated
two-stage pipeline of task synthesis and verification. Structured tree
representations derived from raw database tables guide LLM-based task
generation, followed by interaction-oriented filtering and expert validation.
Human evaluation confirms 100% correctness of the synthesized data. We further
propose a multi-turn evaluation framework simulating realistic interactions
among an LLM-simulated user, the model under test, and an executable database.
The model must adapt its reasoning and SQL generation as user intents change.
DySQL-Bench covers 13 domains across BIRD and Spider 2 databases, totaling
1,072 tasks. Even GPT-4o attains only 58.34% overall accuracy and 23.81% on the
Pass@5 metric, underscoring the benchmark's difficulty. All code and data are
released at https://github.com/Aurora-slz/Real-World-SQL-Bench .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Context Engineering 2.0: The Context of Context Engineering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26493v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26493v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qishuo Hua, Lyumanshan Ye, Dayuan Fu, Yang Xiao, Xiaojie Cai, Yunze Wu, Jifan Lin, Junfei Wang, Pengfei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Karl Marx once wrote that ``the human essence is the ensemble of social
relations'', suggesting that individuals are not isolated entities but are
fundamentally shaped by their interactions with other entities, within which
contexts play a constitutive and essential role. With the advent of computers
and artificial intelligence, these contexts are no longer limited to purely
human--human interactions: human--machine interactions are included as well.
Then a central question emerges: How can machines better understand our
situations and purposes? To address this challenge, researchers have recently
introduced the concept of context engineering. Although it is often regarded as
a recent innovation of the agent era, we argue that related practices can be
traced back more than twenty years. Since the early 1990s, the field has
evolved through distinct historical phases, each shaped by the intelligence
level of machines: from early human--computer interaction frameworks built
around primitive computers, to today's human--agent interaction paradigms
driven by intelligent agents, and potentially to human--level or superhuman
intelligence in the future. In this paper, we situate context engineering,
provide a systematic definition, outline its historical and conceptual
landscape, and examine key design considerations for practice. By addressing
these questions, we aim to offer a conceptual foundation for context
engineering and sketch its promising future. This paper is a stepping stone for
a broader community effort toward systematic context engineering in AI systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bayesian Network Fusion of Large Language Models for Sentiment Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26484v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26484v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rasoul Amirzadeh, Dhananjay Thiruvady, Fatemeh Shiri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) continue to advance, with an increasing number
of domain-specific variants tailored for specialised tasks. However, these
models often lack transparency and explainability, can be costly to fine-tune,
require substantial prompt engineering, yield inconsistent results across
domains, and impose significant adverse environmental impact due to their high
computational demands. To address these challenges, we propose the Bayesian
network LLM fusion (BNLF) framework, which integrates predictions from three
LLMs, including FinBERT, RoBERTa, and BERTweet, through a probabilistic
mechanism for sentiment analysis. BNLF performs late fusion by modelling the
sentiment predictions from multiple LLMs as probabilistic nodes within a
Bayesian network. Evaluated across three human-annotated financial corpora with
distinct linguistic and contextual characteristics, BNLF demonstrates
consistent gains of about six percent in accuracy over the baseline LLMs,
underscoring its robustness to dataset variability and the effectiveness of
probabilistic fusion for interpretable sentiment classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Counteracting Matthew Effect in Self-Improvement of LVLMs through
  Head-Tail Re-balancing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26474v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26474v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Guo, Zhiheng Xi, Yiwen Ding, Yitao Zhai, Xiaowei Shi, Xunliang Cai, Tao Gui, Qi Zhang, Xuanjing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-improvement has emerged as a mainstream paradigm for advancing the
reasoning capabilities of large vision-language models (LVLMs), where models
explore and learn from successful trajectories iteratively. However, we
identify a critical issue during this process: the model excels at generating
high-quality trajectories for simple queries (i.e., head data) but struggles
with more complex ones (i.e., tail data). This leads to an imbalanced
optimization that drives the model to prioritize simple reasoning skills, while
hindering its ability to tackle more complex reasoning tasks. Over iterations,
this imbalance becomes increasingly pronounced--a dynamic we term the "Matthew
effect"--which ultimately hinders further model improvement and leads to
performance bottlenecks. To counteract this challenge, we introduce four
efficient strategies from two perspectives: distribution-reshaping and
trajectory-resampling, to achieve head-tail re-balancing during the
exploration-and-learning self-improvement process. Extensive experiments on
Qwen2-VL-7B-Instruct and InternVL2.5-4B models across visual reasoning tasks
demonstrate that our methods consistently improve visual reasoning
capabilities, outperforming vanilla self-improvement by 3.86 points on average.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Secure<span class="highlight-title">Review</span>er: Enhancing Large Language Models for Secure Code <span class="highlight-title">Review</span>
  through Secure-aware Fine-tuning <span class="chip">ICSE 2026</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26457v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26457v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fang Liu, Simiao Liu, Yinghao Zhu, Xiaoli Lian, Li Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identifying and addressing security issues during the early phase of the
development lifecycle is critical for mitigating the long-term negative impacts
on software systems. Code review serves as an effective practice that enables
developers to check their teammates' code before integration into the codebase.
To streamline the generation of review comments, various automated code review
approaches have been proposed, where LLM-based methods have significantly
advanced the capabilities of automated review generation. However, existing
models primarily focus on general-purpose code review, their effectiveness in
identifying and addressing security-related issues remains underexplored.
Moreover, adapting existing code review approaches to target security issues
faces substantial challenges, including data scarcity and inadequate evaluation
metrics. To address these limitations, we propose SecureReviewer, a new
approach designed for enhancing LLMs' ability to identify and resolve
security-related issues during code review. Specifically, we first construct a
dataset tailored for training and evaluating secure code review capabilities.
Leveraging this dataset, we fine-tune LLMs to generate code review comments
that can effectively identify security issues and provide fix suggestions with
our proposed secure-aware fine-tuning strategy. To mitigate hallucination in
LLMs and enhance the reliability of their outputs, we integrate the RAG
technique, which grounds the generated comments in domain-specific security
knowledge. Additionally, we introduce SecureBLEU, a new evaluation metric
designed to assess the effectiveness of review comments in addressing security
issues. Experimental results demonstrate that SecureReviewer outperforms
state-of-the-art baselines in both security issue detection accuracy and the
overall quality and practical utility of generated review comments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICSE 2026. Code and data:
  https://github.com/SIMIAO515/SecureReviewer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 1+1>2: A Synergistic Sparse and Low-Rank Compression Method for Large
  Language Models <span class="chip">EMNLP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26446v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26446v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeliang Zong, Kai Zhang, Zheyang Li, Wenming Tan, Ye Ren, Yiyan Zhai, Jilin Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated remarkable proficiency in
language comprehension and generation; however, their widespread adoption is
constrained by substantial bandwidth and computational demands. While pruning
and low-rank approximation have each demonstrated promising performance
individually, their synergy for LLMs remains underexplored. We introduce
\underline{S}ynergistic \underline{S}parse and \underline{L}ow-Rank
\underline{C}ompression (SSLC) methods for LLMs, which leverages the strengths
of both techniques: low-rank approximation compresses the model by retaining
its essential structure with minimal information loss, whereas sparse
optimization eliminates non-essential weights, preserving those crucial for
generalization. Based on theoretical analysis, we first formulate the low-rank
approximation and sparse optimization as a unified problem and solve it by
iterative optimization algorithm. Experiments on LLaMA and Qwen2.5 models
(7B-70B) show that SSLC, without any additional training steps, consistently
surpasses standalone methods, achieving state-of-the-arts results. Notably,
SSLC compresses Qwen2.5 by 50\% with no performance drop and achieves at least
1.63$\times$ speedup, offering a practical solution for efficient LLM
deployment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 6 figures, EMNLP 2025 findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Nexus: Execution-Grounded Multi-Agent Test Oracle Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26423v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26423v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dong Huang, Mingzhe Du, Jie M. Zhang, Zheng Lin, Meng Luo, Qianru Zhang, See-Kiong Ng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Test oracle generation in non-regression testing is a longstanding challenge
in software engineering, where the goal is to produce oracles that can
accurately determine whether a function under test (FUT) behaves as intended
for a given input. In this paper, we introduce Nexus, a novel multi-agent
framework to address this challenge. Nexus generates test oracles by leveraging
a diverse set of specialized agents that synthesize test oracles through a
structured process of deliberation, validation, and iterative self-refinement.
During the deliberation phase, a panel of four specialist agents, each
embodying a distinct testing philosophy, collaboratively critiques and refines
an initial set of test oracles. Then, in the validation phase, Nexus generates
a plausible candidate implementation of the FUT and executes the proposed
oracles against it in a secure sandbox. For any oracle that fails this
execution-based check, Nexus activates an automated selfrefinement loop, using
the specific runtime error to debug and correct the oracle before
re-validation. Our extensive evaluation on seven diverse benchmarks
demonstrates that Nexus consistently and substantially outperforms
state-of-theart baselines. For instance, Nexus improves the test-level oracle
accuracy on the LiveCodeBench from 46.30% to 57.73% for GPT-4.1-Mini. The
improved accuracy also significantly enhances downstream tasks: the bug
detection rate of GPT4.1-Mini generated test oracles on HumanEval increases
from 90.91% to 95.45% for Nexus compared to baselines, and the success rate of
automated program repair improves from 35.23% to 69.32%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OmniEduBench: A Comprehensive Chinese Benchmark for Evaluating Large
  Language Models in Education 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26422v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26422v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Min Zhang, Hao Chen, Hao Chen, Wenqi Zhang, Didi Zhu, Xin Lin, Bo Jiang, Aimin Zhou, Fei Wu, Kun Kuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of large language models (LLMs), various LLM-based
works have been widely applied in educational fields. However, most existing
LLMs and their benchmarks focus primarily on the knowledge dimension, largely
neglecting the evaluation of cultivation capabilities that are essential for
real-world educational scenarios. Additionally, current benchmarks are often
limited to a single subject or question type, lacking sufficient diversity.
This issue is particularly prominent within the Chinese context. To address
this gap, we introduce OmniEduBench, a comprehensive Chinese educational
benchmark. OmniEduBench consists of 24.602K high-quality question-answer pairs.
The data is meticulously divided into two core dimensions: the knowledge
dimension and the cultivation dimension, which contain 18.121K and 6.481K
entries, respectively. Each dimension is further subdivided into 6 fine-grained
categories, covering a total of 61 different subjects (41 in the knowledge and
20 in the cultivation). Furthermore, the dataset features a rich variety of
question formats, including 11 common exam question types, providing a solid
foundation for comprehensively evaluating LLMs' capabilities in education.
Extensive experiments on 11 mainstream open-source and closed-source LLMs
reveal a clear performance gap. In the knowledge dimension, only Gemini-2.5 Pro
surpassed 60\% accuracy, while in the cultivation dimension, the
best-performing model, QWQ, still trailed human intelligence by nearly 30\%.
These results highlight the substantial room for improvement and underscore the
challenges of applying LLMs in education.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Role of Context for Discourse Relation Classification in
  Scientific Writing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26354v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26354v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stephen Wan, Wei Liu, Michael Strube
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increasing use of generative Artificial Intelligence (AI) methods to
support science workflows, we are interested in the use of discourse-level
information to find supporting evidence for AI generated scientific claims. A
first step towards this objective is to examine the task of inferring discourse
structure in scientific writing.
  In this work, we present a preliminary investigation of pretrained language
model (PLM) and Large Language Model (LLM) approaches for Discourse Relation
Classification (DRC), focusing on scientific publications, an under-studied
genre for this task. We examine how context can help with the DRC task, with
our experiments showing that context, as defined by discourse structure, is
generally helpful. We also present an analysis of which scientific discourse
relation types might benefit most from context.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Joint Sixth Workshop on Computational Approaches to
  Discourse, Context and Document-Level Inferences (CODI 2025) and Eighth
  Workshop on Computational Models of Reference, Anaphora and Coreference (CRAC
  2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Geometry of Dialogue: Graphing Language Models to Reveal Synergistic
  Teams for Multi-Agent Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26352v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26352v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kotaro Furuya, Yuichi Kitagawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While a multi-agent approach based on large language models (LLMs) represents
a promising strategy to surpass the capabilities of single models, its success
is critically dependent on synergistic team composition. However, forming
optimal teams is a significant challenge, as the inherent opacity of most
models obscures the internal characteristics necessary for effective
collaboration. In this paper, we propose an interaction-centric framework for
automatic team composition that does not require any prior knowledge including
their internal architectures, training data, or task performances. Our method
constructs a "language model graph" that maps relationships between models from
the semantic coherence of pairwise conversations, and then applies community
detection to identify synergistic model clusters. Our experiments with diverse
LLMs demonstrate that the proposed method discovers functionally coherent
groups that reflect their latent specializations. Priming conversations with
specific topics identified synergistic teams which outperform random baselines
on downstream benchmarks and achieve comparable accuracy to that of
manually-curated teams based on known model specializations. Our findings
provide a new basis for the automated design of collaborative multi-agent LLM
teams.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MisSynth: Improving MISSCI Logical Fallacies Classification with
  Synthetic Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26345v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26345v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mykhailo Poliakov, Nadiya Shvai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Health-related misinformation is very prevalent and potentially harmful. It
is difficult to identify, especially when claims distort or misinterpret
scientific findings. We investigate the impact of synthetic data generation and
lightweight fine-tuning techniques on the ability of large language models
(LLMs) to recognize fallacious arguments using the MISSCI dataset and
framework. In this work, we propose MisSynth, a pipeline that applies
retrieval-augmented generation (RAG) to produce synthetic fallacy samples,
which are then used to fine-tune an LLM model. Our results show substantial
accuracy gains with fine-tuned models compared to vanilla baselines. For
instance, the LLaMA 3.1 8B fine-tuned model achieved an over 35% F1-score
absolute improvement on the MISSCI test split over its vanilla baseline. We
demonstrate that introducing synthetic fallacy data to augment limited
annotated resources can significantly enhance zero-shot LLM classification
performance on real-world scientific misinformation tasks, even with limited
computational resources. The code and synthetic dataset are available on
https://github.com/mxpoliakov/MisSynth.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Amateur to Master: Infusing Knowledge into LLMs via Automated
  Curriculum Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26336v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26336v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nishit Neema, Srinjoy Mukherjee, Sapan Shah, Gokul Ramakrishnan, Ganesh Venkatesh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) excel at general tasks but underperform in
specialized domains like economics and psychology, which require deep,
principled understanding. To address this, we introduce ACER (Automated
Curriculum-Enhanced Regimen) that transforms generalist models into domain
experts without sacrificing their broad capabilities. ACER first synthesizes a
comprehensive, textbook-style curriculum by generating a table of contents for
a subject and then creating question-answer (QA) pairs guided by Bloom's
taxonomy. This ensures systematic topic coverage and progressively increasing
difficulty. The resulting synthetic corpus is used for continual pretraining
with an interleaved curriculum schedule, aligning learning across both content
and cognitive dimensions.
  Experiments with Llama 3.2 (1B and 3B) show significant gains in specialized
MMLU subsets. In challenging domains like microeconomics, where baselines
struggle, ACER boosts accuracy by 5 percentage points. Across all target
domains, we observe a consistent macro-average improvement of 3 percentage
points. Notably, ACER not only prevents catastrophic forgetting but also
facilitates positive cross-domain knowledge transfer, improving performance on
non-target domains by 0.7 points. Beyond MMLU, ACER enhances performance on
knowledge-intensive benchmarks like ARC and GPQA by over 2 absolute points,
while maintaining stable performance on general reasoning tasks. Our results
demonstrate that ACER offers a scalable and effective recipe for closing
critical domain gaps in LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SCRIBE: Structured Chain Reasoning for Interactive Behaviour
  Explanations using Tool Calling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26322v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26322v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fares Fawzi, Vinitra Swamy, Dominik Glandorf, Tanya Nazaretsky, Tanja Käser
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models can be used to provide interactive, personalized student
feedback in educational settings. However, real-world deployment faces three
key challenges: privacy concerns, limited computational resources, and the need
for pedagogically valid responses. These constraints require small, open-source
models that can run locally and reliably ground their outputs in correct
information. We introduce SCRIBE, a framework for multi-hop, tool-augmented
reasoning designed to generate valid responses to student questions about
feedback reports. SCRIBE combines domain-specific tools with a self-reflective
inference pipeline that supports iterative reasoning, tool use, and error
recovery. We distil these capabilities into 3B and 8B models via two-stage LoRA
fine-tuning on synthetic GPT-4o-generated data. Evaluation with a human-aligned
GPT-Judge and a user study with 108 students shows that 8B-SCRIBE models
achieve comparable or superior quality to much larger models in key dimensions
such as relevance and actionability, while being perceived on par with GPT-4o
and Llama-3.3 70B by students. These findings demonstrate the viability of
SCRIBE for low-resource, privacy-sensitive educational applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Agent Conquer Web? Exploring the Frontiers of Chat<span class="highlight-title">GPT</span> Atlas Agent in
  Web Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26298v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26298v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingran Zhang, Ning Li, Justin Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  OpenAI's ChatGPT Atlas introduces new capabilities for web interaction,
enabling the model to analyze webpages, process user intents, and execute
cursor and keyboard inputs directly within the browser. While its capacity for
information retrieval tasks has been demonstrated, its performance in dynamic,
interactive environments remains less explored. In this study, we conduct an
early evaluation of Atlas's web interaction capabilities using browser-based
games as test scenarios, including Google's T-Rex Runner, Sudoku, Flappy Bird,
and Stein.world. We employ in-game performance scores as quantitative metrics
to assess performance across different task types. Our results show that Atlas
performs strongly in logical reasoning tasks like Sudoku, completing puzzles
significantly faster than human baselines, but struggles substantially in
real-time games requiring precise timing and motor control, often failing to
progress beyond initial obstacles. These findings suggest that while Atlas
demonstrates capable analytical processing, there remain notable limitations in
dynamic web environments requiring real-time interaction. The website of our
project can be found at https://atlas-game-eval.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unravelling the Mechanisms of Manipulating Numbers in Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26285v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26285v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michal Štefánik, Timothee Mickus, Marek Kadlčík, Bertram Højer, Michal Spiegel, Raúl Vázquez, Aman Sinha, Josef Kuchař, Philipp Mondorf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work has shown that different large language models (LLMs) converge to
similar and accurate input embedding representations for numbers. These
findings conflict with the documented propensity of LLMs to produce erroneous
outputs when dealing with numeric information. In this work, we aim to explain
this conflict by exploring how language models manipulate numbers and quantify
the lower bounds of accuracy of these mechanisms. We find that despite
surfacing errors, different language models learn interchangeable
representations of numbers that are systematic, highly accurate and universal
across their hidden states and the types of input contexts. This allows us to
create universal probes for each LLM and to trace information -- including the
causes of output errors -- to specific layers. Our results lay a fundamental
understanding of how pre-trained LLMs manipulate numbers and outline the
potential of more accurate probing techniques in addressed refinements of LLMs'
architectures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do LLMs Signal When They're Right? Evidence from Neuron Agreement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26277v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26277v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kang Chen, Yaoning Wang, Kai Xiong, Zhuoka Feng, Wenhe Sun, Haotian Chen, Yixin Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) commonly boost reasoning via
sample-evaluate-ensemble decoders, achieving label free gains without ground
truth. However, prevailing strategies score candidates using only external
outputs such as token probabilities, entropies, or self evaluations, and these
signals can be poorly calibrated after post training. We instead analyze
internal behavior based on neuron activations and uncover three findings: (1)
external signals are low dimensional projections of richer internal dynamics;
(2) correct responses activate substantially fewer unique neurons than
incorrect ones throughout generation; and (3) activations from correct
responses exhibit stronger cross sample agreement, whereas incorrect ones
diverge. Motivated by these observations, we propose Neuron Agreement Decoding
(NAD), an unsupervised best-of-N method that selects candidates using
activation sparsity and cross sample neuron agreement, operating solely on
internal signals and without requiring comparable textual outputs. NAD enables
early correctness prediction within the first 32 generated tokens and supports
aggressive early stopping. Across math and science benchmarks with verifiable
answers, NAD matches majority voting; on open ended coding benchmarks where
majority voting is inapplicable, NAD consistently outperforms Avg@64. By
pruning unpromising trajectories early, NAD reduces token usage by 99% with
minimal loss in generation quality, showing that internal signals provide
reliable, scalable, and efficient guidance for label free ensemble decoding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PVMark: Enabling Public Verifiability for LLM Watermarking Schemes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26274v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26274v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haohua Duan, Liyao Xiang, Xin Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Watermarking schemes for large language models (LLMs) have been proposed to
identify the source of the generated text, mitigating the potential threats
emerged from model theft. However, current watermarking solutions hardly
resolve the trust issue: the non-public watermark detection cannot prove itself
faithfully conducting the detection. We observe that it is attributed to the
secret key mostly used in the watermark detection -- it cannot be public, or
the adversary may launch removal attacks provided the key; nor can it be
private, or the watermarking detection is opaque to the public. To resolve the
dilemma, we propose PVMark, a plugin based on zero-knowledge proof (ZKP),
enabling the watermark detection process to be publicly verifiable by third
parties without disclosing any secret key. PVMark hinges upon the proof of
`correct execution' of watermark detection on which a set of ZKP constraints
are built, including mapping, random number generation, comparison, and
summation. We implement multiple variants of PVMark in Python, Rust and Circom,
covering combinations of three watermarking schemes, three hash functions, and
four ZKP protocols, to show our approach effectively works under a variety of
circumstances. By experimental results, PVMark efficiently enables public
verifiability on the state-of-the-art LLM watermarking schemes yet without
compromising the watermarking performance, promising to be deployed in
practice.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distilling Multilingual Vision-Language Models: When Smaller Models Stay
  Multilingual 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26271v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26271v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sukrit Sriratanawilai, Jhayahgrit Thongwat, Romrawin Chumpu, Patomporn Payoungkhamdee, Sarana Nutanong, Peerat Limkonchotiwat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models (VLMs) exhibit uneven performance across languages, a
problem that is often exacerbated when the model size is reduced. While
Knowledge distillation (KD) demonstrates promising results in transferring
knowledge from larger to smaller VLMs, applying KD in multilingualism is an
underexplored area. This paper presents a controlled empirical study of KD
behavior across five distillation approaches, isolating their effects on
cross-lingual representation consistency and downstream performance stability
under model compression. We study five distillation formulations across CLIP
and SigLIP2, and evaluate them on in-domain retrieval and out-of-domain visual
QA. We find that some configurations preserve or even improve multilingual
retrieval robustness despite halving model size, but others fail to maintain
cross-task stability, exposing design-sensitive trade-offs that aggregate
accuracy alone does not reveal.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Models Are Borrowing-Blind: A Multilingual Evaluation of
  Loanword Identification across 10 Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26254v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26254v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mérilin Sousa Silva, Sina Ahmadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Throughout language history, words are borrowed from one language to another
and gradually become integrated into the recipient's lexicon. Speakers can
often differentiate these loanwords from native vocabulary, particularly in
bilingual communities where a dominant language continuously imposes lexical
items on a minority language. This paper investigates whether pretrained
language models, including large language models, possess similar capabilities
for loanword identification. We evaluate multiple models across 10 languages.
Despite explicit instructions and contextual information, our results show that
models perform poorly in distinguishing loanwords from native ones. These
findings corroborate previous evidence that modern NLP systems exhibit a bias
toward loanwords rather than native equivalents. Our work has implications for
developing NLP tools for minority languages and supporting language
preservation in communities under lexical pressure from dominant languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pragmatic Theories Enhance Understanding of Implied Meanings in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26253v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26253v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takuma Sato, Seiya Kawano, Koichiro Yoshino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to accurately interpret implied meanings plays a crucial role in
human communication and language use, and language models are also expected to
possess this capability. This study demonstrates that providing language models
with pragmatic theories as prompts is an effective in-context learning approach
for tasks to understand implied meanings. Specifically, we propose an approach
in which an overview of pragmatic theories, such as Gricean pragmatics and
Relevance Theory, is presented as a prompt to the language model, guiding it
through a step-by-step reasoning process to derive a final interpretation.
Experimental results showed that, compared to the baseline, which prompts
intermediate reasoning without presenting pragmatic theories (0-shot
Chain-of-Thought), our methods enabled language models to achieve up to 9.6\%
higher scores on pragmatic reasoning tasks. Furthermore, we show that even
without explaining the details of pragmatic theories, merely mentioning their
names in the prompt leads to a certain performance improvement (around 1-3%) in
larger models compared to the baseline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Which Way Does Time Flow? A Psychophysics-Grounded Evaluation for
  Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26241v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26241v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiho Matta, Lis Kanashiro Pereira, Peitao Han, Fei Cheng, Shigeru Kitazawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern vision-language models (VLMs) excel at many multimodal tasks, yet
their grasp of temporal information in video remains weak and, crucially,
under-evaluated. We probe this gap with a deceptively simple but revealing
challenge: judging the arrow of time (AoT)-whether a short clip is played
forward or backward. We introduce AoT-PsyPhyBENCH, a psychophysically validated
benchmark that tests whether VLMs can infer temporal direction in natural
videos using the same stimuli and behavioral baselines established for humans.
Our comprehensive evaluation of open-weight and proprietary, reasoning and
non-reasoning VLMs reveals that most models perform near chance, and even the
best lag far behind human accuracy on physically irreversible processes (e.g.,
free fall, diffusion/explosion) and causal manual actions (division/addition)
that humans recognize almost instantly. These results highlight a fundamental
gap in current multimodal systems: while they capture rich visual-semantic
correlations, they lack the inductive biases required for temporal continuity
and causal understanding. We release the code and data for AoT-PsyPhyBENCH to
encourage further progress in the physical and temporal reasoning capabilities
of VLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Global Retrieval Augmented Generation: A Benchmark for
  Corpus-Level Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26205v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26205v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Luo, Xiaonan Li, Tingshuo Fan, Xinchi Chen, Xipeng Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) has emerged as a leading approach to
reducing hallucinations in large language models (LLMs). Current RAG evaluation
benchmarks primarily focus on what we call local RAG: retrieving relevant
chunks from a small subset of documents to answer queries that require only
localized understanding within specific text chunks. However, many real-world
applications require a fundamentally different capability -- global RAG --
which involves aggregating and analyzing information across entire document
collections to derive corpus-level insights (for example, "What are the top 10
most cited papers in 2023?"). In this paper, we introduce GlobalQA -- the first
benchmark specifically designed to evaluate global RAG capabilities, covering
four core task types: counting, extremum queries, sorting, and top-k
extraction. Through systematic evaluation across different models and
baselines, we find that existing RAG methods perform poorly on global tasks,
with the strongest baseline achieving only 1.51 F1 score. To address these
challenges, we propose GlobalRAG, a multi-tool collaborative framework that
preserves structural coherence through chunk-level retrieval, incorporates
LLM-driven intelligent filters to eliminate noisy documents, and integrates
aggregation modules for precise symbolic computation. On the Qwen2.5-14B model,
GlobalRAG achieves 6.63 F1 compared to the strongest baseline's 1.51 F1,
validating the effectiveness of our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What's In My Human Feedback? Learning Interpretable Descriptions of
  Preference Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26202v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26202v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rajiv Movva, Smitha Milli, Sewon Min, Emma Pierson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human feedback can alter language models in unpredictable and undesirable
ways, as practitioners lack a clear understanding of what feedback data
encodes. While prior work studies preferences over certain attributes (e.g.,
length or sycophancy), automatically extracting relevant features without
pre-specifying hypotheses remains challenging. We introduce What's In My Human
Feedback? (WIMHF), a method to explain feedback data using sparse autoencoders.
WIMHF characterizes both (1) the preferences a dataset is capable of measuring
and (2) the preferences that the annotators actually express. Across 7
datasets, WIMHF identifies a small number of human-interpretable features that
account for the majority of the preference prediction signal achieved by
black-box models. These features reveal a wide diversity in what humans prefer,
and the role of dataset-level context: for example, users on Reddit prefer
informality and jokes, while annotators in HH-RLHF and PRISM disprefer them.
WIMHF also surfaces potentially unsafe preferences, such as that LMArena users
tend to vote against refusals, often in favor of toxic content. The learned
features enable effective data curation: re-labeling the harmful examples in
Arena yields large safety gains (+37%) with no cost to general performance.
They also allow fine-grained personalization: on the Community Alignment
dataset, we learn annotator-specific weights over subjective features that
improve preference prediction. WIMHF provides a human-centered analysis method
for practitioners to better understand and use preference data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/rmovva/wimhf</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Don't Let It Fade: Preserving Edits in Diffusion Language Models via
  Token Timestep Allocation <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26200v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26200v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Woojin Kim, Jaeyoung Do
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While diffusion language models (DLMs) enable fine-grained refinement, their
practical controllability remains fragile. We identify and formally
characterize a central failure mode called update forgetting, in which uniform
and context agnostic updates induce token level fluctuations across timesteps,
erasing earlier semantic edits and disrupting the cumulative refinement
process, thereby degrading fluency and coherence. As this failure originates in
uniform and context agnostic updates, effective control demands explicit token
ordering. We propose Token Timestep Allocation (TTA), which realizes soft and
semantic token ordering via per token timestep schedules: critical tokens are
frozen early, while uncertain tokens receive continued refinement. This
timestep based ordering can be instantiated as either a fixed policy or an
adaptive policy driven by task signals, thereby supporting a broad spectrum of
refinement strategies. Because it operates purely at inference time, it applies
uniformly across various DLMs and naturally extends to diverse supervision
sources. Empirically, TTA improves controllability and fluency: on sentiment
control, it yields more than 20 percent higher accuracy and nearly halves
perplexity using less than one fifth the steps; in detoxification, it lowers
maximum toxicity (12.2 versus 14.5) and perplexity (26.0 versus 32.0).
Together, these results demonstrate that softened ordering via timestep
allocation is the critical lever for mitigating update forgetting and achieving
stable and controllable diffusion text generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RCScore: Quantifying Response Consistency in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26193v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26193v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongjun Jang, Youngchae Ahn, Hyopil Shin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current LLM evaluations often rely on a single instruction template,
overlooking models' sensitivity to instruction style-a critical aspect for
real-world deployments. We present RCScore, a multi-dimensional framework
quantifying how instruction formulation affects model responses. By
systematically transforming benchmark problems into multiple instruction
styles, RCScore reveals performance variations undetected by conventional
metrics. Our experiments across ten LLMs on four reasoning benchmarks
demonstrate that instruction style can shift accuracy by up to 16.7% points. We
introduce Cross-Response Similarity (CRS), a method applying RCScore metrics to
measure stylistic self-consistency, and establish its strong correlation with
task accuracy, suggesting consistency as a valuable proxy for model
reliability. Additional findings show that deterministic decoding produces more
stylistically stable outputs, and model scale correlates positively with
cross-style consistency. RCScore offers a principled approach to assess
instruction robustness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SP-MCQA: Evaluating Intelligibility of TTS Beyond the Word Level 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26190v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26190v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hitomi Jin Ling Tee, Chaoren Wang, Zijie Zhang, Zhizheng Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evaluation of intelligibility for TTS has reached a bottleneck, as
existing assessments heavily rely on word-by-word accuracy metrics such as WER,
which fail to capture the complexity of real-world speech or reflect human
comprehension needs. To address this, we propose Spoken-Passage Multiple-Choice
Question Answering, a novel subjective approach evaluating the accuracy of key
information in synthesized speech, and release SP-MCQA-Eval, an 8.76-hour
news-style benchmark dataset for SP-MCQA evaluation. Our experiments reveal
that low WER does not necessarily guarantee high key-information accuracy,
exposing a gap between traditional metrics and practical intelligibility.
SP-MCQA shows that even state-of-the-art (SOTA) models still lack robust text
normalization and phonetic accuracy. This work underscores the urgent need for
high-level, more life-like evaluation criteria now that many systems already
excel at WER yet may fall short on real-world intelligibility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Similarity-Distance-Magnitude Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26183v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26183v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Allen Schmaltz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Similarity-Distance-Magnitude (SDM) language models (LMs), which
are sequence prediction models fine-tuned to maximize the proportion of
generations in the well-calibrated, high-probability region partitioned by a
final-layer SDM activation layer used for binary classification of
instruction-following. We demonstrate that existing pre-trained decoder-only
Transformer LMs can be readily converted into SDM LMs via supervised
fine-tuning, using the final-layer SDM activation layer during training to
estimate a change-of-base for a supervised next-token loss over a contrastive
input encoding scheme, with additional hard negative examples generated online
during training. This results in reduced abstentions (i.e., improved
statistical efficiency) compared to strong supervised baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MossNet: Mixture of State-Space Experts is a Multi-Head Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26182v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26182v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shikhar Tuli, James Seale Smith, Haris Jeelani, Chi-Heng Lin, Abhishek Patel, Vasili Ramanishka, Yen-Chang Hsu, Hongxia Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have significantly advanced generative
applications in natural language processing (NLP). Recent trends in model
architectures revolve around efficient variants of transformers or
state-space/gated-recurrent models (SSMs, GRMs). However, prevailing
SSM/GRM-based methods often emulate only a single attention head, potentially
limiting their expressiveness. In this work, we propose MossNet, a novel
mixture-of-state-space-experts architecture that emulates a linear multi-head
attention (MHA). MossNet leverages a mixture-of-experts (MoE) implementation
not only in channel-mixing multi-layered perceptron (MLP) blocks but also in
the time-mixing SSM kernels to realize multiple "attention heads." Extensive
experiments on language modeling and downstream evaluations show that MossNet
outperforms both transformer- and SSM-based architectures of similar model size
and data budgets. Larger variants of MossNet, trained on trillions of tokens,
further confirm its scalability and superior performance. In addition,
real-device profiling on a Samsung Galaxy S24 Ultra and an Nvidia A100 GPU
demonstrate favorable runtime speed and resource usage compared to similarly
sized baselines. Our results suggest that MossNet is a compelling new direction
for efficient, high-performing recurrent LLM architectures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ One Model to Critique Them All: Rewarding Agentic Tool-Use via Efficient
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26167v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26167v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renhao Li, Jianhong Tu, Yang Su, Hamid Alinejad-Rokny, Derek F. Wong, Junyang Lin, Min Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reward models (RMs) play a critical role in aligning large language models
(LLMs) with human preferences. Yet in the domain of tool learning, the lack of
RMs specifically designed for function-calling tasks has limited progress
toward more capable agentic AI. We introduce ToolRM, a family of lightweight
generative RMs tailored for general tool-use scenarios. To build these models,
we propose a novel pipeline that constructs pairwise preference data using
rule-based scoring and multidimensional sampling. This yields
ToolPref-Pairwise-30K, a diverse, balanced, and challenging dataset of critique
tasks that supports reinforcement learning with verifiable feedback. To
evaluate tool-use RMs, we also introduce TRBench$_{BFCL}$, a benchmark built on
the agentic evaluation suite BFCL. Trained on our constructed data, models from
the Qwen3-4B/8B series achieve up to 14.28% higher accuracy, substantially
outperforming frontier models such as Claude 4 and OpenAI o3 in pairwise reward
judgments. Beyond training objectives, ToolRM generalizes to broader critique
tasks, including Best-of-N sampling and self-correction. Experiments on
ACEBench highlight its effectiveness and efficiency, enabling inference-time
scaling and reducing output token usage by over 66%. We release data and model
checkpoints to facilitate future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reasoning Curriculum: Bootstrapping Broad LLM Reasoning from Math 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26143v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26143v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Pang, Deqian Kong, Silvio Savarese, Caiming Xiong, Yingbo Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) can elicit strong reasoning in large language
models (LLMs), yet most open efforts focus on math and code. We propose
Reasoning Curriculum, a simple two-stage curriculum that first elicits
reasoning skills in pretraining-aligned domains such as math, then adapts and
refines these skills across other domains via joint RL. Stage 1 performs a
brief cold start and then math-only RL with verifiable rewards to develop
reasoning skills. Stage 2 runs joint RL on mixed-domain data to transfer and
consolidate these skills. The curriculum is minimal and backbone-agnostic,
requiring no specialized reward models beyond standard verifiability checks.
Evaluated on Qwen3-4B and Llama-3.1-8B over a multi-domain suite, reasoning
curriculum yields consistent gains. Ablations and a cognitive-skill analysis
indicate that both stages are necessary and that math-first elicitation
increases cognitive behaviors important for solving complex problems. Reasoning
Curriculum provides a compact, easy-to-adopt recipe for general reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Influence of Discourse Relations in Persuasive Texts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26124v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26124v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nawar Turk, Sevag Kaspar, Leila Kosseim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the relationship between Persuasion Techniques (PTs)
and Discourse Relations (DRs) by leveraging Large Language Models (LLMs) and
prompt engineering. Since no dataset annotated with both PTs and DRs exists, we
took the SemEval 2023 Task 3 dataset labelled with 19 PTs as a starting point
and developed LLM-based classifiers to label each instance of the dataset with
one of the 22 PDTB 3.0 level-2 DRs. In total, four LLMs were evaluated using 10
different prompts, resulting in 40 unique DR classifiers. Ensemble models using
different majority-pooling strategies were used to create 5 silver datasets of
instances labelled with both persuasion techniques and level-2 PDTB senses. The
silver dataset sizes vary from 1,281 instances to 204 instances, depending on
the majority pooling technique used. Statistical analysis of these silver
datasets shows that six discourse relations (namely Cause, Purpose, Contrast,
Cause+Belief, Concession, and Condition) play a crucial role in persuasive
texts, especially in the use of Loaded Language, Exaggeration/Minimisation,
Repetition and to cast Doubt. This insight can contribute to detecting online
propaganda and misinformation, as well as to our general understanding of
effective communication.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Proceedings of the 38th Canadian Conference on
  Artificial Intelligence CanAI 2025 Calgary Alberta May 26-27 2025. 5 figures
  7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reasoning Path Divergence: A New Metric and Curation Strategy to Unlock
  LLM Diverse Thinking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26122v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26122v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng Ju, Zeyu Qin, Rui Min, Zhitao He, Lingpeng Kong, Yi R. Fung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Test-Time Scaling (TTS) has proven effective in improving the reasoning
ability of large language models (LLMs), low diversity in model outputs often
becomes a bottleneck; this is partly caused by the common "one problem, one
solution" (1P1S) training practice, which provides a single canonical answer
and can push models toward a narrow set of reasoning paths. To address this, we
propose a "one problem, multiple solutions" (1PNS) training paradigm that
exposes the model to a variety of valid reasoning trajectories and thus
increases inference diversity. A core challenge for 1PNS is reliably measuring
semantic differences between multi-step chains of thought, so we introduce
Reasoning Path Divergence (RPD), a step-level metric that aligns and scores
Long Chain-of-Thought solutions to capture differences in intermediate
reasoning. Using RPD, we curate maximally diverse solution sets per problem and
fine-tune Qwen3-4B-Base. Experiments show that RPD-selected training yields
more varied outputs and higher pass@k, with an average +2.80% gain in pass@16
over a strong 1P1S baseline and a +4.99% gain on AIME24, demonstrating that
1PNS further amplifies the effectiveness of TTS. Our code is available at
https://github.com/fengjujf/Reasoning-Path-Divergence .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ QCoder Benchmark: Bridging Language Generation and Quantum Hardware
  through Simulator-Based Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26101v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26101v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taku Mikuriya, Tatsuya Ishigaki, Masayuki Kawarada, Shunya Minami, Tadashi Kadowaki, Yohichi Suzuki, Soshun Naito, Shunya Takata, Takumi Kato, Tamotsu Basseda, Reo Yamada, Hiroya Takamura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have increasingly been applied to automatic
programming code generation. This task can be viewed as a language generation
task that bridges natural language, human knowledge, and programming logic.
However, it remains underexplored in domains that require interaction with
hardware devices, such as quantum programming, where human coders write Python
code that is executed on a quantum computer. To address this gap, we introduce
QCoder Benchmark, an evaluation framework that assesses LLMs on quantum
programming with feedback from simulated hardware devices. Our benchmark offers
two key features. First, it supports evaluation using a quantum simulator
environment beyond conventional Python execution, allowing feedback of
domain-specific metrics such as circuit depth, execution time, and error
classification, which can be used to guide better generation. Second, it
incorporates human-written code submissions collected from real programming
contests, enabling both quantitative comparisons and qualitative analyses of
LLM outputs against human-written codes. Our experiments reveal that even
advanced models like GPT-4o achieve only around 18.97% accuracy, highlighting
the difficulty of the benchmark. In contrast, reasoning-based models such as o3
reach up to 78% accuracy, outperforming averaged success rates of human-written
codes (39.98%). We release the QCoder Benchmark dataset and public evaluation
API to support further research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ORBIT -- Open Recommendation Benchmark for Reproducible Research with
  Hidden Tests <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26095v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26095v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyuan He, Jiongnan Liu, Vishan Vishesh Oberoi, Bolin Wu, Mahima Jagadeesh Patel, Kangrui Mao, Chuning Shi, I-Ta Lee, Arnold Overwijk, Chenyan Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems are among the most impactful AI applications, interacting
with billions of users every day, guiding them to relevant products, services,
or information tailored to their preferences. However, the research and
development of recommender systems are hindered by existing datasets that fail
to capture realistic user behaviors and inconsistent evaluation settings that
lead to ambiguous conclusions. This paper introduces the Open Recommendation
Benchmark for Reproducible Research with HIdden Tests (ORBIT), a unified
benchmark for consistent and realistic evaluation of recommendation models.
ORBIT offers a standardized evaluation framework of public datasets with
reproducible splits and transparent settings for its public leaderboard.
Additionally, ORBIT introduces a new webpage recommendation task, ClueWeb-Reco,
featuring web browsing sequences from 87 million public, high-quality webpages.
ClueWeb-Reco is a synthetic dataset derived from real, user-consented, and
privacy-guaranteed browsing data. It aligns with modern recommendation
scenarios and is reserved as the hidden test part of our leaderboard to
challenge recommendation models' generalization ability. ORBIT measures 12
representative recommendation models on its public benchmark and introduces a
prompted LLM baseline on the ClueWeb-Reco hidden test. Our benchmark results
reflect general improvements of recommender systems on the public datasets,
with variable individual performances. The results on the hidden test reveal
the limitations of existing approaches in large-scale webpage recommendation
and highlight the potential for improvements with LLM integrations. ORBIT
benchmark, leaderboard, and codebase are available at
https://www.open-reco-bench.ai.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2025 Datasets & Benchmarks track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do Students Debias Like Teachers? On the Distillability of Bias
  Mitigation Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26038v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26038v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiali Cheng, Chirag Agarwal, Hadi Amiri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge distillation (KD) is an effective method for model compression and
transferring knowledge between models. However, its effect on model's
robustness against spurious correlations that degrade performance on
out-of-distribution data remains underexplored. This study investigates the
effect of knowledge distillation on the transferability of ``debiasing''
capabilities from teacher models to student models on natural language
inference (NLI) and image classification tasks. Through extensive experiments,
we illustrate several key findings: (i) overall the debiasing capability of a
model is undermined post-KD; (ii) training a debiased model does not benefit
from injecting teacher knowledge; (iii) although the overall robustness of a
model may remain stable post-distillation, significant variations can occur
across different types of biases; and (iv) we pin-point the internal attention
pattern and circuit that causes the distinct behavior post-KD. Given the above
findings, we propose three effective solutions to improve the distillability of
debiasing methods: developing high quality data for augmentation, implementing
iterative knowledge distillation, and initializing student models with weights
obtained from teacher models. To the best of our knowledge, this is the first
study on the effect of KD on debiasing and its interenal mechanism at scale.
Our findings provide understandings on how KD works and how to design better
debiasing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SIRAJ: Diverse and Efficient Red-Teaming for LLM Agents via Distilled
  Structured Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26037v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26037v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaiwen Zhou, Ahmed Elgohary, A S M Iftekhar, Amin Saied
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability of LLM agents to plan and invoke tools exposes them to new safety
risks, making a comprehensive red-teaming system crucial for discovering
vulnerabilities and ensuring their safe deployment. We present SIRAJ: a generic
red-teaming framework for arbitrary black-box LLM agents. We employ a dynamic
two-step process that starts with an agent definition and generates diverse
seed test cases that cover various risk outcomes, tool-use trajectories, and
risk sources. Then, it iteratively constructs and refines model-based
adversarial attacks based on the execution trajectories of former attempts. To
optimize the red-teaming cost, we present a model distillation approach that
leverages structured forms of a teacher model's reasoning to train smaller
models that are equally effective. Across diverse evaluation agent settings,
our seed test case generation approach yields 2 -- 2.5x boost to the coverage
of risk outcomes and tool-calling trajectories. Our distilled 8B red-teamer
model improves attack success rate by 100%, surpassing the 671B Deepseek-R1
model. Our ablations and analyses validate the effectiveness of the iterative
framework, structured reasoning, and the generalization of our red-teamer
models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Artificial Intelligence-Enabled Analysis of Radiology Reports:
  Epidemiology and Consequences of Incidental Thyroid Findings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26032v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26032v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felipe Larios, Mariana Borras-Osorio, Yuqi Wu, Ana Gabriela Claros, David Toro-Tobon, Esteban Cabezas, Ricardo Loor-Torres, Maria Mateo Chavez, Kerly Guevara Maldonado, Luis Vilatuna Andrango, Maria Lizarazo Jimenez, Ivan Mateo Alzamora, Misk Al Zahidy, Marcelo Montero, Ana Cristina Proano, Cristian Soto Jacome, Jungwei W. Fan, Oscar J. Ponce-Ponte, Megan E. Branda, Naykky Singh Ospina, Juan P. Brito
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Importance Incidental thyroid findings (ITFs) are increasingly detected on
imaging performed for non-thyroid indications. Their prevalence, features, and
clinical consequences remain undefined. Objective To develop, validate, and
deploy a natural language processing (NLP) pipeline to identify ITFs in
radiology reports and assess their prevalence, features, and clinical outcomes.
Design, Setting, and Participants Retrospective cohort of adults without prior
thyroid disease undergoing thyroid-capturing imaging at Mayo Clinic sites from
July 1, 2017, to September 30, 2023. A transformer-based NLP pipeline
identified ITFs and extracted nodule characteristics from image reports from
multiple modalities and body regions. Main Outcomes and Measures Prevalence of
ITFs, downstream thyroid ultrasound, biopsy, thyroidectomy, and thyroid cancer
diagnosis. Logistic regression identified demographic and imaging-related
factors. Results Among 115,683 patients (mean age, 56.8 [SD 17.2] years; 52.9%
women), 9,077 (7.8%) had an ITF, of which 92.9% were nodules. ITFs were more
likely in women, older adults, those with higher BMI, and when imaging was
ordered by oncology or internal medicine. Compared with chest CT, ITFs were
more likely via neck CT, PET, and nuclear medicine scans. Nodule
characteristics were poorly documented, with size reported in 44% and other
features in fewer than 15% (e.g. calcifications). Compared with patients
without ITFs, those with ITFs had higher odds of thyroid nodule diagnosis,
biopsy, thyroidectomy and thyroid cancer diagnosis. Most cancers were
papillary, and larger when detected after ITFs vs no ITF. Conclusions ITFs were
common and strongly associated with cascades leading to the detection of small,
low-risk cancers. These findings underscore the role of ITFs in thyroid cancer
overdiagnosis and the need for standardized reporting and more selective
follow-up.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TinyTim: A Family of Language Models for Divergent Generation <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.11607v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.11607v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher J. Agostino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the search for artificial general intelligence, model development and
training has focused primarily on vast datasets of known problems and their
accepted solutions. This process necessarily produces convergent systems which
are fundamentally incapable of the conceptual reframing that is required for
genuine creative breakthroughs. Inspired by the divergent cognitive processes
that allow humans to make such creative leaps, our work introduces a family of
language models, TinyTim, to serve as sources of divergent generation within
broader systems. These models have been created by fine-tuning on the
anti-parsimonious text of James Joyce's `Finnegans Wake'. Quantitative analysis
of both an unsupervised fine-tuned model (TinyTim-V1) and a new
instruction-tuned variant (TinyTim-V2) demonstrates a profound capacity for
lexical invention; the foundational V1 model exhibits a Yule's K score for
lexical richness over twenty times greater than that of convergent baselines.
This trait is a stable property of the family, as the instruction-tuned V2
maintains a statistically distinct profile and resists factual convergence,
sacrificing benchmark performance to preserve its core generative style. This
work establishes a methodology for engineering specialized divergent models
that, when paired with convergent systems, can reframe problems and force
breakthroughs beyond the reach of statistical optimization alone.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 3 figures, accepted to NeurIPS Creative AI track, models
  available at https://hf.co/npc-worldwide/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Completion $\neq$ Collaboration: Scaling Collaborative Effort with
  Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25744v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25744v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shannon Zejiang Shen, Valerie Chen, Ken Gu, Alexis Ross, Zixian Ma, Jillian Ross, Alex Gu, Chenglei Si, Wayne Chi, Andi Peng, Jocelyn J Shen, Ameet Talwalkar, Tongshuang Wu, David Sontag
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current evaluations of agents remain centered around one-shot task
completion, failing to account for the inherently iterative and collaborative
nature of many real-world problems, where human goals are often underspecified
and evolve. We argue for a shift from building and assessing task completion
agents to developing collaborative agents, assessed not only by the quality of
their final outputs but by how well they engage with and enhance human effort
throughout the problem-solving process. To support this shift, we introduce
collaborative effort scaling, a framework that captures how an agent's utility
grows with increasing user involvement. Through case studies and simulated
evaluations, we show that state-of-the-art agents often underperform in
multi-turn, real-world scenarios, revealing a missing ingredient in agent
design: the ability to sustain engagement and scaffold user understanding.
Collaborative effort scaling offers a lens for diagnosing agent behavior and
guiding development toward more effective interactions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 5 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Comparing human and LLM politeness strategies in free production <span class="chip">EMNLP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.09391v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.09391v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Zhao, Robert D. Hawkins
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Polite speech poses a fundamental alignment challenge for large language
models (LLMs). Humans deploy a rich repertoire of linguistic strategies to
balance informational and social goals -- from positive approaches that build
rapport (compliments, expressions of interest) to negative strategies that
minimize imposition (hedging, indirectness). We investigate whether LLMs employ
a similarly context-sensitive repertoire by comparing human and LLM responses
in both constrained and open-ended production tasks. We find that larger models
($\ge$70B parameters) successfully replicate key preferences from the
computational pragmatics literature, and human evaluators surprisingly prefer
LLM-generated responses in open-ended contexts. However, further linguistic
analyses reveal that models disproportionately rely on negative politeness
strategies even in positive contexts, potentially leading to
misinterpretations. While modern LLMs demonstrate an impressive handle on
politeness strategies, these subtle differences raise important questions about
pragmatic alignment in AI systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 5 figures | EMNLP 2025 camera-ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quality Over Quantity? LLM-Based Curation for a Data-Efficient
  Audio-Video Foundation Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.09205v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.09205v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Vosoughi, Dimitra Emmanouilidou, Hannes Gamper
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrating audio and visual data for training multimodal foundational models
remains a challenge. The Audio-Video Vector Alignment (AVVA) framework
addresses this by considering AV scene alignment beyond mere temporal
synchronization, and leveraging Large Language Models (LLMs) for data curation.
AVVA implements a scoring mechanism for selecting aligned training data
segments. It integrates Whisper, a speech-based foundation model, for audio and
DINOv2 for video analysis in a dual-encoder structure with contrastive learning
on AV pairs. Evaluations on AudioCaps, VALOR, and VGGSound demonstrate the
effectiveness of the proposed model architecture and data curation approach.
AVVA achieves a significant improvement in top-k accuracies for video-to-audio
retrieval on all datasets compared to DenseAV, while using only 192 hrs of
curated training data. Furthermore, an ablation study indicates that the data
curation process effectively trades data quality for data quantity, yielding
increases in top-k retrieval accuracies on AudioCaps, VALOR, and VGGSound,
compared to training on the full spectrum of uncurated data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 5 figures, 2 tables. Accepted at EUSIPCO 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Massive Supervised Fine-tuning Experiments Reveal How Data, Layer, and
  Training Factors Shape LLM Alignment Quality <span class="chip">EMNLP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14681v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14681v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuto Harada, Yusuke Yamauchi, Yusuke Oda, Yohei Oseki, Yusuke Miyao, Yu Takagi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Supervised fine-tuning (SFT) is a critical step in aligning large language
models (LLMs) with human instructions and values, yet many aspects of SFT
remain poorly understood. We trained a wide range of base models on a variety
of datasets including code generation, mathematical reasoning, and
general-domain tasks, resulting in 1,000+ SFT models under controlled
conditions. We then identified the dataset properties that matter most and
examined the layer-wise modifications introduced by SFT. Our findings reveal
that some training-task synergies persist across all models while others vary
substantially, emphasizing the importance of model-specific strategies.
Moreover, we demonstrate that perplexity consistently predicts SFT
effectiveness, often surpassing superficial similarity between the training
data and the benchmark, and that mid-layer weight changes correlate most
strongly with performance gains. We release these 1,000+ SFT models and
benchmark results to accelerate further research. All resources are available
at https://github.com/llm-jp/massive-sft.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2025 (Main Conference). Models and evaluation
  results available at: https://github.com/llm-jp/massive-sft</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Reasoning Skills in Small Persian Medical Language Models Can
  Outperform Large-Scale Data Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.20059v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.20059v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehrdad Ghassabi, Sadra Hakim, Hamidreza Baradaran Kashani, Pedram Rostami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Enhancing reasoning capabilities in small language models is critical for
specialized applications such as medical question answering, particularly in
underrepresented languages like Persian. In this study, we employ Reinforcement
Learning with AI Feedback (RLAIF) and Direct preference optimization (DPO) to
improve the reasoning skills of a general-purpose Persian language model. To
achieve this, we translated a multiple-choice medical question-answering
dataset into Persian and used RLAIF to generate rejected-preferred answer
pairs, which are essential for DPO training. By prompting both teacher and
student models to produce Chain-of-Thought (CoT) reasoning responses, we
compiled a dataset containing correct and incorrect reasoning trajectories.
This dataset, comprising 2 million tokens in preferred answers and 2.5 million
tokens in rejected ones, was used to train a baseline model, significantly
enhancing its medical reasoning capabilities in Persian. Remarkably, the
resulting model outperformed its predecessor, gaokerena-V, which was trained on
approximately 57 million tokens, despite leveraging a much smaller dataset.
These results highlight the efficiency and effectiveness of reasoning-focused
training approaches in developing domain-specific language models with limited
data availability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Controlling Thinking Speed in Reasoning Models <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.03704v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.03704v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengkai Lin, Zhihang Fu, Ze Chen, Chao Chen, Liang Xie, Wenxiao Wang, Deng Cai, Zheng Wang, Jieping Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human cognition is theorized to operate in two modes: fast, intuitive System
1 thinking and slow, deliberate System 2 thinking. While current Large
Reasoning Models (LRMs) excel at System 2 thinking, their inability to perform
fast thinking leads to high computational overhead and latency. In this work,
we enable LRMs to approximate human intelligence through dynamic thinking speed
adjustment, optimizing accuracy-efficiency trade-offs. Our approach addresses
two key questions: (1) how to control thinking speed in LRMs, and (2) when to
adjust it for optimal performance. For the first question, we identify the
steering vector that governs slow-fast thinking transitions in LRMs'
representation space. Using this vector, we achieve the first representation
editing-based test-time scaling effect, outperforming existing prompt-based
scaling methods. For the second question, we apply real-time difficulty
estimation to signal reasoning segments of varying complexity. Combining these
techniques, we propose the first reasoning strategy that enables fast
processing of easy steps and deeper analysis for complex reasoning. Without any
training or additional cost, our plug-in module delivers an average +1.3%
accuracy with -8.6% token usage across leading LRMs and advanced reasoning
benchmarks. All of our algorithms are implemented based on vLLM and are
expected to support broader applications and inspire future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025 Spotlight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RLBFF: Binary Flexible Feedback to bridge between Human Feedback &
  Verifiable Rewards 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.21319v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.21319v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhilin Wang, Jiaqi Zeng, Olivier Delalleau, Ellie Evans, Daniel Egert, Hoo-Chang Shin, Felipe Soares, Yi Dong, Oleksii Kuchaiev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning with Human Feedback (RLHF) and Reinforcement Learning
with Verifiable Rewards (RLVR) are the main RL paradigms used in LLM
post-training, each offering distinct advantages. However, RLHF struggles with
interpretability and reward hacking because it relies on human judgments that
usually lack explicit criteria, whereas RLVR is limited in scope by its focus
on correctness-based verifiers. We propose Reinforcement Learning with Binary
Flexible Feedback (RLBFF), which combines the versatility of human-driven
preferences with the precision of rule-based verification, enabling reward
models to capture nuanced aspects of response quality beyond mere correctness.
RLBFF extracts principles that can be answered in a binary fashion (e.g.
accuracy of information: yes, or code readability: no) from natural language
feedback. Such principles can then be used to ground Reward Model training as
an entailment task (response satisfies or does not satisfy an arbitrary
principle). We show that Reward Models trained in this manner can outperform
Bradley-Terry models when matched for data and achieve top performance on
RM-Bench (86.2%) and JudgeBench (81.4%, #1 on leaderboard as of September 24,
2025). Additionally, users can specify principles of interest at inference time
to customize the focus of our reward models, in contrast to Bradley-Terry
models. Finally, we present a fully open source recipe (including data) to
align Qwen3-32B using RLBFF and our Reward Model, to match or exceed the
performance of o3-mini and DeepSeek R1 on general alignment benchmarks of
MT-Bench, WildBench, and Arena Hard v2 (at <5% of the inference cost). Models:
https://huggingface.co/collections/nvidia/reward-models-10-2025
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Added link to access models:
  https://huggingface.co/collections/nvidia/reward-models-10-2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CompoST: A Benchmark for Analyzing the Ability of LLMs To
  Compositionally Interpret Questions in a QALD Setting <span class="chip">ISWC
  2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.21257v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.21257v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Maria Schmidt, Raoul Schubert, Philipp Cimiano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language interpretation is a compositional process, in which the meaning of
more complex linguistic structures is inferred from the meaning of their parts.
Large language models possess remarkable language interpretation capabilities
and have been successfully applied to interpret questions by mapping them to
SPARQL queries. An open question is how systematic this interpretation process
is. Toward this question, in this paper, we propose a benchmark for
investigating to what extent the abilities of LLMs to interpret questions are
actually compositional. For this, we generate three datasets of varying
difficulty based on graph patterns in DBpedia, relying on Lemon lexica for
verbalization. Our datasets are created in a very controlled fashion in order
to test the ability of LLMs to interpret structurally complex questions, given
that they have seen the atomic building blocks. This allows us to evaluate to
what degree LLMs are able to interpret complex questions for which they
"understand" the atomic parts. We conduct experiments with models of different
sizes using both various prompt and few-shot optimization techniques as well as
fine-tuning. Our results show that performance in terms of macro $F_1$ degrades
from $0.45$ over $0.26$ down to $0.09$ with increasing deviation from the
samples optimized on. Even when all necessary information was provided to the
model in the input, the $F_1$ scores do not exceed $0.57$ for the dataset of
lowest complexity. We thus conclude that LLMs struggle to systematically and
compositionally interpret questions and map them into SPARQL queries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Research Track, 24th International Semantic Web Conference (ISWC
  2025), November 2-6, 2025, Nara, Japan</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unveiling Unicode's Unseen Underpinnings in Undermining Authorship
  Attribution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.15840v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.15840v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert Dilworth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When using a public communication channel -- whether formal or informal, such
as commenting or posting on social media -- end users have no expectation of
privacy: they compose a message and broadcast it for the world to see. Even if
an end user takes utmost precautions to anonymize their online presence --
using an alias or pseudonym; masking their IP address; spoofing their
geolocation; concealing their operating system and user agent; deploying
encryption; registering with a disposable phone number or email; disabling
non-essential settings; revoking permissions; and blocking cookies and
fingerprinting -- one obvious element still lingers: the message itself.
Assuming they avoid lapses in judgment or accidental self-exposure, there
should be little evidence to validate their actual identity, right? Wrong. The
content of their message -- necessarily open for public consumption -- exposes
an attack vector: stylometric analysis, or author profiling. In this paper, we
dissect the technique of stylometry, discuss an antithetical counter-strategy
in adversarial stylometry, and devise enhancements through Unicode
steganography.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages, 7 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Detecting Early and Implicit Suicidal Ideation via Longitudinal and
  Information Environment Signals on Social Media 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.14889v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.14889v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soorya Ram Shimgekar, Ruining Zhao, Agam Goyal, Violeta J. Rodriguez, Paul A. Bloom, Hari Sundaram, Koustuv Saha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  On social media, many individuals experiencing suicidal ideation (SI) do not
disclose their distress explicitly. Instead, signs may surface indirectly
through everyday posts or peer interactions. Detecting such implicit signals
early is critical but remains challenging. We frame early and implicit SI as a
forward-looking prediction task and develop a computational framework that
models a user's information environment, consisting of both their longitudinal
posting histories as well as the discourse of their socially proximal peers. We
adopted a composite network centrality measure to identify top neighbors of a
user, and temporally aligned the user's and neighbors' interactions --
integrating the multi-layered signals in a fine-tuned DeBERTa-v3 model. In a
Reddit study of 1,000 (500 Case and 500 Control) users, our approach improves
early and implicit SI detection by 15% over individual-only baselines. These
findings highlight that peer interactions offer valuable predictive signals and
carry broader implications for designing early detection systems that capture
indirect as well as masked expressions of risk in online environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LatentBreak: Jailbreaking Large Language Models through Latent Space
  Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.08604v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.08604v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raffaele Mura, Giorgio Piras, Kamilė Lukošiūtė, Maura Pintor, Amin Karbasi, Battista Biggio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Jailbreaks are adversarial attacks designed to bypass the built-in safety
mechanisms of large language models. Automated jailbreaks typically optimize an
adversarial suffix or adapt long prompt templates by forcing the model to
generate the initial part of a restricted or harmful response. In this work, we
show that existing jailbreak attacks that leverage such mechanisms to unlock
the model response can be detected by a straightforward perplexity-based
filtering on the input prompt. To overcome this issue, we propose LatentBreak,
a white-box jailbreak attack that generates natural adversarial prompts with
low perplexity capable of evading such defenses. LatentBreak substitutes words
in the input prompt with semantically-equivalent ones, preserving the initial
intent of the prompt, instead of adding high-perplexity adversarial suffixes or
long templates. These words are chosen by minimizing the distance in the latent
space between the representation of the adversarial prompt and that of harmless
requests. Our extensive evaluation shows that LatentBreak leads to shorter and
low-perplexity prompts, thus outperforming competing jailbreak algorithms
against perplexity-based filters on multiple safety-aligned models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Are You There God? Lightweight Narrative Annotation of Christian Fiction
  with LMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.19756v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.19756v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rebecca M. M. Hicke, Brian W. Haggard, Mia Ferrante, Rayhan Khanna, David Mimno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In addition to its more widely studied cultural movements, American
Evangelicalism has a well-developed but less externally visible literary side.
Christian Fiction, however, has been little studied, and what scholarly
attention there is has focused on the explosively popular Left Behind series.
In this work, we use computational tools to provide both a broad topical
overview of Christian Fiction as a genre and a more directed exploration of how
its authors depict divine acts. Working with human annotators, we first
developed a codebook for identifying "acts of God." We then adapted the
codebook for use by a recent, lightweight LM with the assistance of a much
larger model. The laptop-scale LM is largely capable of matching human
annotations, even when the task is subtle and challenging. Using these
annotations, we show that significant and meaningful differences exist between
divine acts depicted by the Left Behind books and Christian Fiction more
broadly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CHR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unstructured Evidence Attribution for Long Context Query Focused
  Summarization <span class="chip">EMNLP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14409v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14409v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dustin Wright, Zain Muhammad Mujahid, Lu Wang, Isabelle Augenstein, David Jurgens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are capable of generating coherent summaries
from very long contexts given a user query, and extracting and citing evidence
spans helps improve the trustworthiness of these summaries. Whereas previous
work has focused on evidence citation with fixed levels of granularity (e.g.
sentence, paragraph, document, etc.), we propose to extract unstructured (i.e.,
spans of any length) evidence in order to acquire more relevant and consistent
evidence than in the fixed granularity case. We show how existing systems
struggle to copy and properly cite unstructured evidence, which also tends to
be "lost-in-the-middle". To help models perform this task, we create the
Summaries with Unstructured Evidence Text dataset (SUnsET), a synthetic dataset
generated using a novel pipeline, which can be used as training supervision for
unstructured evidence summarization. We demonstrate across 5 LLMs and 4
datasets spanning human written, synthetic, single, and multi-document settings
that LLMs adapted with SUnsET generate more relevant and factually consistent
evidence with their summaries, extract evidence from more diverse locations in
their context, and can generate more relevant and consistent summaries than
baselines with no fine-tuning and fixed granularity evidence. We release SUnsET
and our generation code to the public.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2025 Main; 29 pages; 24 figures; 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Epistemic Diversity and Knowledge Collapse in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.04226v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.04226v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dustin Wright, Sarah Masud, Jared Moore, Srishti Yadav, Maria Antoniak, Chan Young Park, Isabelle Augenstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) tend to generate lexically, semantically, and
stylistically homogenous texts. This poses a risk of knowledge collapse, where
homogenous LLMs mediate a shrinking in the range of accessible information over
time. Existing works on homogenization are limited by a focus on closed-ended
multiple-choice setups or fuzzy semantic features, and do not look at trends
across time and cultural contexts. To overcome this, we present a new
methodology to measure epistemic diversity, i.e., variation in real-world
claims in LLM outputs, which we use to perform a broad empirical study of LLM
knowledge collapse. We test 27 LLMs, 155 topics covering 12 countries, and 200
prompt variations sourced from real user chats. For the topics in our study, we
show that while newer models tend to generate more diverse claims, nearly all
models are less epistemically diverse than a basic web search. We find that
model size has a negative impact on epistemic diversity, while
retrieval-augmented generation (RAG) has a positive impact, though the
improvement from RAG varies by the cultural context. Finally, compared to a
traditional knowledge source (Wikipedia), we find that country-specific claims
reflect the English language more than the local one, highlighting a gap in
epistemic representation
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages; 8 figures, 4 tables; v2 changelog: Fixed the modeling for
  table 3, random effect is the model version; v3 changelog: Fixed minor
  formatting issues in tables 2 and 3; v4 changelog: Fixed some typos and model
  description</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dependency Structure Augmented Contextual Scoping Framework for
  Multimodal Aspect-Based Sentiment Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.11331v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.11331v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Liu, Lijun He, Jiaxi Liang, Zhihan Ren, Haixia Bi, Fan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Aspect-Based Sentiment Analysis (MABSA) seeks to extract
fine-grained information from image-text pairs to identify aspect terms and
determine their sentiment polarity. However, existing approaches often fall
short in simultaneously addressing three core challenges: Sentiment Cue
Perception (SCP), Multimodal Information Misalignment (MIM), and Semantic Noise
Elimination (SNE). To overcome these limitations, we propose DASCO
(\textbf{D}ependency Structure \textbf{A}ugmented \textbf{Sco}ping Framework),
a fine-grained scope-oriented framework that enhances aspect-level sentiment
reasoning by leveraging dependency parsing trees. First, we designed a
multi-task pretraining strategy for MABSA on our base model, combining
aspect-oriented enhancement, image-text matching, and aspect-level
sentiment-sensitive cognition. This improved the model's perception of aspect
terms and sentiment cues while achieving effective image-text alignment,
addressing key challenges like SCP and MIM. Furthermore, we incorporate
dependency trees as syntactic branch combining with semantic branch, guiding
the model to selectively attend to critical contextual elements within a
target-specific scope while effectively filtering out irrelevant noise for
addressing SNE problem. Extensive experiments on two benchmark datasets across
three subtasks demonstrate that DASCO achieves state-of-the-art performance in
MABSA, with notable gains in JMASA (+2.3\% F1 and +3.5\% precision on
Twitter2015). The source code is available at https://github.com/LHaoooo/DASCO .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PairUni: Pairwise Training for Unified Multimodal Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25682v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25682v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiani Zheng, Zhiyang Teng, Xiangtai Li, Anran Wang, Yu Tian, Kunpeng Qiu, Ye Tian, Haochen Wang, Zhuochen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unified vision-language models (UVLMs) must perform both understanding and
generation within a single architecture, but these tasks rely on heterogeneous
data and supervision, making it difficult to balance them during reinforcement
learning (RL). We propose PairUni, a unified framework that reorganizes data
into understanding-generation (UG) pairs and aligns optimization accordingly.
We first use GPT-o3 to augment single-task data, generating captions for
understanding samples and question-answer (QA) pairs for generation samples,
forming aligned pairs from the same instance. Additionally, for each generation
sample, we retrieve a semantically related understanding example to form a
retrieved pair, linking different but related data points. These paired
structures expose cross-task semantic correspondences and support consistent
policy learning. To leverage this structure, we present Pair-GPRO, a pair-aware
variant based on Group Relative Policy Optimization. It assigns a similarity
score to each pair to modulate the advantage, strengthening learning from
well-aligned examples and reducing task interference. We curate a high-quality
dataset of 16K UG pairs named PairUG for RL fine-tuning and evaluate PairUni on
the powerful Janus-Pro UVLMs. Our approach achieves balanced improvements on
various UVLMs, outperforming strong UVLM RL baselines. Codes are available at
https://github.com/Haochen-Wang409/PairUni.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 11 figures, and 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating the Role of Verifiers in Test-Time Scaling for Legal
  Reasoning Tasks <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25623v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25623v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Davide Romano, Jonathan Schwarz, Daniele Giofré
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Test-time scaling (TTS) techniques can improve the performance of large
language models (LLMs) at the expense of additional computation and latency.
While TTS has proven effective in formal domains such as mathematics and
programming, its value in argumentative domains such as law remains
underexplored. We present an empirical study of verifier-based TTS methods for
legal multiple-choice QA (MCQA) across five benchmarks. Using a family of 7
reward models, we evaluate both outcome-level (Best-of-$N$) and process-level
(tree search) verification under realistic low-$N$ budgets. Our analysis
systematically investigates how verifier utility is affected by key properties
such as domain specialization, model size, and supervision type
(process-supervised PRMs vs. outcome-only ORMs), even when applied across
different roles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP - NLLP Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ More of the Same: Persistent Representational Harms Under Increased
  Representation <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.00333v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.00333v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jennifer Mickel, Maria De-Arteaga, Leqi Liu, Kevin Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To recognize and mitigate the harms of generative AI systems, it is crucial
to consider who is represented in the outputs of generative AI systems and how
people are represented. A critical gap emerges when naively improving who is
represented, as this does not imply bias mitigation efforts have been applied
to address how people are represented. We critically examined this by
investigating gender representation in occupation across state-of-the-art large
language models. We first show evidence suggesting that over time there have
been interventions to models altering the resulting gender distribution, and we
find that women are more represented than men when models are prompted to
generate biographies or personas. We then demonstrate that representational
biases persist in how different genders are represented by examining
statistically significant word differences across genders. This results in a
proliferation of representational harms, stereotypes, and neoliberalism ideals
that, despite existing interventions to increase female representation,
reinforce existing systems of oppression.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the 39th Conference on Neural Information Processing
  Systems (NeurIPS 2025) as a poster paper; 39 pages, 7 figures, 15 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MedAgentBoard: Benchmarking Multi-Agent Collaboration with Conventional
  Methods for Diverse Medical Tasks <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12371v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12371v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinghao Zhu, Ziyi He, Haoran Hu, Xiaochen Zheng, Xichen Zhang, Zixiang Wang, Junyi Gao, Liantao Ma, Lequan Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of Large Language Models (LLMs) has stimulated interest
in multi-agent collaboration for addressing complex medical tasks. However, the
practical advantages of multi-agent collaboration approaches remain
insufficiently understood. Existing evaluations often lack generalizability,
failing to cover diverse tasks reflective of real-world clinical practice, and
frequently omit rigorous comparisons against both single-LLM-based and
established conventional methods. To address this critical gap, we introduce
MedAgentBoard, a comprehensive benchmark for the systematic evaluation of
multi-agent collaboration, single-LLM, and conventional approaches.
MedAgentBoard encompasses four diverse medical task categories: (1) medical
(visual) question answering, (2) lay summary generation, (3) structured
Electronic Health Record (EHR) predictive modeling, and (4) clinical workflow
automation, across text, medical images, and structured EHR data. Our extensive
experiments reveal a nuanced landscape: while multi-agent collaboration
demonstrates benefits in specific scenarios, such as enhancing task
completeness in clinical workflow automation, it does not consistently
outperform advanced single LLMs (e.g., in textual medical QA) or, critically,
specialized conventional methods that generally maintain better performance in
tasks like medical VQA and EHR-based prediction. MedAgentBoard offers a vital
resource and actionable insights, emphasizing the necessity of a task-specific,
evidence-based approach to selecting and developing AI solutions in medicine.
It underscores that the inherent complexity and overhead of multi-agent
collaboration must be carefully weighed against tangible performance gains. All
code, datasets, detailed prompts, and experimental results are open-sourced at
https://medagentboard.netlify.app/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2025 Datasets & Benchmarks Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Wisdom and Delusion of LLM Ensembles for Code Generation and Repair 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.21513v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.21513v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fernando Vallecillos-Ruiz, Max Hort, Leon Moonen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Today's pursuit of a single Large Language Model (LMM) for all software
engineering tasks is resource-intensive and overlooks the potential benefits of
complementarity, where different models contribute unique strengths. However,
the degree to which coding LLMs complement each other and the best strategy for
maximizing an ensemble's potential are unclear, leaving practitioners without a
clear path to move beyond single-model systems. To address this gap, we
empirically compare ten individual LLMs from five families, and three ensembles
of these LLMs across three software engineering benchmarks covering code
generation and program repair. We assess the complementarity between models and
the performance gap between the best individual model and the ensembles. Next,
we evaluate various selection heuristics to identify correct solutions from an
ensemble's candidate pool. We find that the theoretical upperbound for an
ensemble's performance can be 83% above the best single model. Our results show
that consensus-based strategies for selecting solutions fall into a "popularity
trap," amplifying common but incorrect outputs. In contrast, a diversity-based
strategy realizes up to 95% of this theoretical potential, and proves effective
even in small two-model ensembles, enabling a cost-efficient way to enhance
performance by leveraging multiple LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Added Acknowledgments section and hyphenated last names</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TwinVoice: A Multi-dimensional Benchmark Towards Digital Twins via LLM
  Persona Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25536v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25536v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bangde Du, Minghao Guo, Songming He, Ziyi Ye, Xi Zhu, Weihang Su, Shuqi Zhu, Yujia Zhou, Yongfeng Zhang, Qingyao Ai, Yiqun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are exhibiting emergent human-like abilities and
are increasingly envisioned as the foundation for simulating an individual's
communication style, behavioral tendencies, and personality traits. However,
current evaluations of LLM-based persona simulation remain limited: most rely
on synthetic dialogues, lack systematic frameworks, and lack analysis of the
capability requirement. To address these limitations, we introduce TwinVoice, a
comprehensive benchmark for assessing persona simulation across diverse
real-world contexts. TwinVoice encompasses three dimensions: Social Persona
(public social interactions), Interpersonal Persona (private dialogues), and
Narrative Persona (role-based expression). It further decomposes the evaluation
of LLM performance into six fundamental capabilities, including opinion
consistency, memory recall, logical reasoning, lexical fidelity, persona tone,
and syntactic style. Experimental results reveal that while advanced models
achieve moderate accuracy in persona simulation, they still fall short of
capabilities such as syntactic style and memory recall. Consequently, the
average performance achieved by LLMs remains considerably below the human
baseline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Main paper: 11 pages, 3 figures, 6 tables. Appendix: 28 pages. Bangde
  Du and Minghao Guo contributed equally. Corresponding authors: Ziyi Ye
  (ziyiye@fudan.edu.cn), Qingyao Ai (aiqy@tsinghua.edu.cn)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ReForm: Reflective Autoformalization with Prospective Bounded Sequence
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24592v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24592v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoxin Chen, Jing Wu, Xinjie Chen, Wayne Xin Zhao, Ruihua Song, Chengxi Li, Kai Fan, Dayiheng Liu, Minpeng Liao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autoformalization, which translates natural language mathematics into
machine-verifiable formal statements, is critical for using formal mathematical
reasoning to solve math problems stated in natural language. While Large
Language Models can generate syntactically correct formal statements, they
often fail to preserve the original problem's semantic intent. This limitation
arises from the LLM approaches' treating autoformalization as a simplistic
translation task which lacks mechanisms for self-reflection and iterative
refinement that human experts naturally employ. To address these issues, we
propose ReForm, a Reflective Autoformalization method that tightly integrates
semantic consistency evaluation into the autoformalization process. This
enables the model to iteratively generate formal statements, assess its
semantic fidelity, and self-correct identified errors through progressive
refinement. To effectively train this reflective model, we introduce
Prospective Bounded Sequence Optimization (PBSO), which employs different
rewards at different sequence positions to ensure that the model develops both
accurate autoformalization and correct semantic validations, preventing
superficial critiques that would undermine the purpose of reflection. Extensive
experiments across four autoformalization benchmarks demonstrate that ReForm
achieves an average improvement of 22.6 percentage points over the strongest
baselines. To further ensure evaluation reliability, we introduce
ConsistencyCheck, a benchmark of 859 expert-annotated items that not only
validates LLMs as judges but also reveals that autoformalization is inherently
difficult: even human experts produce semantic errors in up to 38.5% of cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/Chen-GX/ReForm</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards a Method for Synthetic Generation of Persons with Aphasia
  Transcripts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24817v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24817v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jason M. Pittman, Anton Phillips Jr., Yesenia Medina-Santos, Brielle C. Stark
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In aphasia research, Speech-Language Pathologists (SLPs) devote extensive
time to manually coding speech samples using Correct Information Units (CIUs),
a measure of how informative an individual sample of speech is. Developing
automated systems to recognize aphasic language is limited by data scarcity.
For example, only about 600 transcripts are available in AphasiaBank yet
billions of tokens are used to train large language models (LLMs). In the
broader field of machine learning (ML), researchers increasingly turn to
synthetic data when such are sparse. Therefore, this study constructs and
validates two methods to generate synthetic transcripts of the AphasiaBank Cat
Rescue picture description task. One method leverages a procedural programming
approach while the second uses Mistral 7b Instruct and Llama 3.1 8b Instruct
LLMs. The methods generate transcripts across four severity levels (Mild,
Moderate, Severe, Very Severe) through word dropping, filler insertion, and
paraphasia substitution. Overall, we found, compared to human-elicited
transcripts, Mistral 7b Instruct best captures key aspects of linguistic
degradation observed in aphasia, showing realistic directional changes in NDW,
word count, and word length amongst the synthetic generation methods. Based on
the results, future work should plan to create a larger dataset, fine-tune
models for better aphasic representation, and have SLPs assess the realism and
usefulness of the synthetic transcripts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 1 figure, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Paper2Poster: Towards Multimodal Poster Automation from Scientific
  Papers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.21497v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.21497v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Pang, Kevin Qinghong Lin, Xiangru Jian, Xi He, Philip Torr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Academic poster generation is a crucial yet challenging task in scientific
communication, requiring the compression of long-context interleaved documents
into a single, visually coherent page. To address this challenge, we introduce
the first benchmark and metric suite for poster generation, which pairs recent
conference papers with author-designed posters and evaluates outputs on
(i)Visual Quality-semantic alignment with human posters, (ii)Textual
Coherence-language fluency, (iii)Holistic Assessment-six fine-grained aesthetic
and informational criteria scored by a VLM-as-judge, and notably
(iv)PaperQuiz-the poster's ability to convey core paper content as measured by
VLMs answering generated quizzes. Building on this benchmark, we propose
PosterAgent, a top-down, visual-in-the-loop multi-agent pipeline: the (a)Parser
distills the paper into a structured asset library; the (b)Planner aligns
text-visual pairs into a binary-tree layout that preserves reading order and
spatial balance; and the (c)Painter-Commenter loop refines each panel by
executing rendering code and using VLM feedback to eliminate overflow and
ensure alignment. In our comprehensive evaluation, we find that GPT-4o
outputs-though visually appealing at first glance-often exhibit noisy text and
poor PaperQuiz scores, and we find that reader engagement is the primary
aesthetic bottleneck, as human-designed posters rely largely on visual
semantics to convey meaning. Our fully open-source variants (e.g. based on the
Qwen-2.5 series) outperform existing 4o-driven multi-agent systems across
nearly all metrics, while using 87% fewer tokens. It transforms a 22-page paper
into a finalized yet editable .pptx poster - all for just $0.005. These
findings chart clear directions for the next generation of fully automated
poster-generation models. The code and datasets are available at
https://github.com/Paper2Poster/Paper2Poster.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://github.com/Paper2Poster/Paper2Poster</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BhashaBench V1: A Comprehensive Benchmark for the Quadrant of Indic
  Domains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25409v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25409v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vijay Devane, Mohd Nauman, Bhargav Patel, Aniket Mahendra Wakchoure, Yogeshkumar Sant, Shyam Pawar, Viraj Thakur, Ananya Godse, Sunil Patra, Neha Maurya, Suraj Racha, Nitish Kamal Singh, Ajay Nagpal, Piyush Sawarkar, Kundeshwar Vijayrao Pundalik, Rohit Saluja, Ganesh Ramakrishnan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of large language models(LLMs) has intensified the need
for domain and culture specific evaluation. Existing benchmarks are largely
Anglocentric and domain-agnostic, limiting their applicability to India-centric
contexts. To address this gap, we introduce BhashaBench V1, the first
domain-specific, multi-task, bilingual benchmark focusing on critical Indic
knowledge systems. BhashaBench V1 contains 74,166 meticulously curated
question-answer pairs, with 52,494 in English and 21,672 in Hindi, sourced from
authentic government and domain-specific exams. It spans four major domains:
Agriculture, Legal, Finance, and Ayurveda, comprising 90+ subdomains and
covering 500+ topics, enabling fine-grained evaluation. Evaluation of 29+ LLMs
reveals significant domain and language specific performance gaps, with
especially large disparities in low-resource domains. For instance, GPT-4o
achieves 76.49% overall accuracy in Legal but only 59.74% in Ayurveda. Models
consistently perform better on English content compared to Hindi across all
domains. Subdomain-level analysis shows that areas such as Cyber Law,
International Finance perform relatively well, while Panchakarma, Seed Science,
and Human Rights remain notably weak. BhashaBench V1 provides a comprehensive
dataset for evaluating large language models across India's diverse knowledge
domains. It enables assessment of models' ability to integrate domain-specific
knowledge with bilingual understanding. All code, benchmarks, and resources are
publicly available to support open research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MindGYM: What Matters in Question Synthesis for Thinking-Centric
  Fine-Tuning? <span class="chip">NeurIPS'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.09499v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.09499v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Xu, Daoyuan Chen, Zhenqing Ling, Yaliang Li, Ying Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large foundation models face challenges in acquiring transferable, structured
thinking abilities, especially when supervised with rigid templates or
crowd-annotated instruction datasets. Unlike prior approaches, we focus on a
thinking-centric data synthesis paradigm that enables models to evolve through
self-generated, cognitively guided data. We propose MindGYM, a structured and
scalable framework for question synthesis, composed of: (1) Cognitive Thinking
Process Injection, which infuses high-level reasoning objectives to shape the
model's synthesis behavior; (2) Seed Single-Hop Question Synthesis, generating
atomic questions from diverse semantic types to encourage broader thinking; and
(3) Challenging Multi-Hop QA Synthesis, composing more complex multi-hop
questions based on QA seeds for deeper reasoning. Detailed analysis shows that
synthetic data generated by our method achieves 16.7% higher average quality
and 67.91% lower quality variance compared to baseline sources, highlighting
that both high-quality and self-contained data are essential for effective,
thinking-oriented fine-tuning. MindGYM improves performance on six reasoning
benchmarks, achieving gains of up to 16% on MathVision using only 400 data
samples, and generalizable improvements across different model sizes and
architectures. MindGYM underscores the viability of self-challenging mechanisms
in refining large model capabilities while minimizing human intervention and
resource demands. Code and data are released to promote data-centric research
into self-evolving foundation models driven by their internal reasoning
capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS'25. 30 pages, 2 figures, 13 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UNO-Bench: A Unified Benchmark for Exploring the Compositional Law
  Between Uni-modal and Omni-modal in Omni Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.18915v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.18915v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Chen, ZeYang Hu, Fengjiao Chen, Liya Ma, Jiaxing Liu, Xiaoyu Li, Ziwen Wang, Xuezhi Cao, Xunliang Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Languages models have been progressing from uni-modal
understanding toward unifying visual, audio and language modalities,
collectively termed omni models. However, the correlation between uni-modal and
omni-modal remains unclear, which requires comprehensive evaluation to drive
omni model's intelligence evolution. In this work, we introduce a novel,
high-quality, and UNified Omni model benchmark, UNO-Bench. This benchmark is
designed to effectively evaluate both UNi-modal and Omni-modal capabilities
under a unified ability taxonomy, spanning 44 task types and 5 modality
combinations. It includes 1250 human curated samples for omni-modal with 98%
cross-modality solvability, and 2480 enhanced uni-modal samples. The
human-generated dataset is well-suited to real-world scenarios, particularly
within the Chinese context, whereas the automatically compressed dataset offers
a 90% increase in speed and maintains 98% consistency across 18 public
benchmarks. In addition to traditional multi-choice questions, we propose an
innovative multi-step open-ended question format to assess complex reasoning. A
general scoring model is incorporated, supporting 6 question types for
automated evaluation with 95% accuracy. Experimental result shows the
Compositional Law between omni-modal and uni-modal performance and the
omni-modal capability manifests as a bottleneck effect on weak models, while
exhibiting synergistic promotion on strong models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v3: Switch the paper template. Work in progress. Github:
  https://github.com/meituan-longcat/UNO-Bench Hugging Face:
  https://huggingface.co/datasets/meituan-longcat/UNO-Bench</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Isolated Dots: Benchmarking Structured Table Construction as Deep
  Knowledge Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.16271v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.16271v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyun Zhong, Guozhao Mo, Yanjiang Liu, Yihan Chen, Lingdi Kong, Xuanang Chen, Yaojie Lu, Hongyu Lin, Shiwei Ye, Xianpei Han, Ben He, Le Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the emergence of large language models (LLMs), there is an expectation
that LLMs can effectively extract explicit information from complex real-world
documents (e.g., papers, reports). However, most LLMs generate paragraph-style
answers that are chaotic, disorganized, and untraceable. To bridge this gap, we
introduce the Arranged and Organized Extraction Benchmark (AOE), a new
bilingual benchmark with data and documents of varying lengths designed to
systematically evaluate the ability of LLMs to comprehend fragmented documents
and reconstruct isolated information into one organized table. Unlike
conventional text-to-table tasks, which rely on fixed schema and narrow task
domains, AOE includes 11 carefully crafted tasks across three diverse domains,
requiring models to generate context-specific schema tailored to varied input
queries. In the experiment, we evaluated both open-source and closed-source
state-of-the-art LLMs. The results show that even the most advanced models
struggled significantly. The benchmark is available at
https://anonymous.4open.science/r/AOE-Benchmark/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Models Have Intrinsic Meta-Cognition, but Need a Good
  Lens <span class="chip">EMNLP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.08410v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.08410v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyang Ma, Qingyue Yuan, Zhenglin Wang, Deyu Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous research has primarily focused on the cognitive error detection
capabilities of Large Language Models (LLMs), often prompting them to analyze
mistakes in reasoning chains. However, few studies have examined the
meta-cognitive abilities of LLMs (e.g., their self-awareness of step errors),
which are crucial for their reliability. While studies on LLM self-evaluation
present some measures, such as perplexity, which can reflect the answer
correctness and be viewed as the lens of meta-cognition, they lack step-level
analysis and adaptation. This paper studies the evaluation of LLM
meta-cognition using the current lenses and how to improve these lenses.
Specifically, we propose AutoMeco, an Automated Meta-cognition Evaluation
framework for benchmarking the existing lenses. Furthermore, a training-free
Markovian Intrinsic Reward Adjustment strategy, MIRA, is proposed to boost
current meta-cognition lenses. Experimental results on three mathematical
reasoning datasets and three LLMs show the reasonableness of AutoMeco by
comparing it with Best-of-N verification. Moreover, the meta-cognition ability
of LLMs can be better evaluated using MIRA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hysteresis Activation Function for Efficient Inference <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.10573v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.10573v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moshe Kimhi, Idan Kashani, Avi Mendelson, Chaim Baskin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widely used ReLU is favored for its hardware efficiency, {as the
implementation at inference is a one bit sign case,} yet suffers from issues
such as the ``dying ReLU'' problem, where during training, neurons fail to
activate and constantly remain at zero, as highlighted by Lu et al. Traditional
approaches to mitigate this issue often introduce more complex and less
hardware-friendly activation functions. In this work, we propose a Hysteresis
Rectified Linear Unit (HeLU), an efficient activation function designed to
address the ``dying ReLU'' problem with minimal complexity. Unlike traditional
activation functions with fixed thresholds for training and inference, HeLU
employs a variable threshold that refines the backpropagation. This refined
mechanism allows simpler activation functions to achieve competitive
performance comparable to their more complex counterparts without introducing
unnecessary complexity or requiring inductive biases. Empirical evaluations
demonstrate that HeLU enhances model generalization across diverse datasets,
offering a promising solution for efficient and effective inference suitable
for a wide range of neural network architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to 4th NeurIPS Efficient Natural Language and Speech
  Processing Workshop (ENLSP-IV 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diversity as a Reward: Fine-Tuning LLMs on a Mixture of
  Domain-Undetermined Data <span class="chip">NeurIPS'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.04380v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.04380v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenqing Ling, Daoyuan Chen, Liuyi Yao, Qianli Shen, Yaliang Li, Ying Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning large language models (LLMs) using diverse datasets is crucial
for enhancing their overall performance across various domains. In practical
scenarios, existing methods based on modeling the mixture proportions of data
composition often struggle with data whose domain labels are missing, imprecise
or non-normalized, while methods based on data selection usually encounter
difficulties in balancing multi-domain performance. To address these
challenges, in this work, we investigate the role of data diversity in
enhancing the overall abilities of LLMs by empirically constructing contrastive
data pools and theoretically deriving explanations. Building upon the insights
gained, we propose a new method that gives the LLM a dual identity: an output
model to cognitively probe and select data based on diversity reward, as well
as an input model to be tuned with the selected data. Extensive experiments
show that the proposed method notably boosts performance across
domain-undetermined data and a series of foundational downstream tasks when
applied to various advanced LLMs. We release our code and hope this study can
shed light on the understanding of data diversity and advance feedback-driven
data-model co-design for LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS'25 main track. 47 pages, 21 figures, 32 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SEA-LION: Southeast Asian Languages in One Network <span class="chip">AACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.05747v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.05747v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raymond Ng, Thanh Ngan Nguyen, Yuli Huang, Ngee Chia Tai, Wai Yi Leong, Wei Qi Leong, Xianbin Yong, Jian Gang Ngui, Yosephine Susanto, Nicholas Cheng, Hamsawardhini Rengarajan, Peerat Limkonchotiwat, Adithya Venkatadri Hulagadri, Kok Wai Teng, Yeo Yeow Tong, Bryan Siow, Wei Yi Teo, Wayne Lau, Choon Meng Tan, Brandon Ong, Zhi Hao Ong, Jann Railey Montalan, Adwin Chan, Sajeban Antonyrex, Ren Lee, Esther Choa, David Ong Tat-Wee, Bing Jie Darius Liu, William Chandra Tjhi, Erik Cambria, Leslie Teo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Large Language Models (LLMs) have dominated much of the artificial
intelligence scene with their ability to process and generate natural
languages. However, the majority of LLM research and development remains
English-centric, leaving low-resource languages such as those in the Southeast
Asian (SEA) region under-represented. To address this representation gap, we
introduce Llama-SEA-LION-v3-8B-IT and Gemma-SEA-LION-v3-9B-IT, two cutting-edge
multilingual LLMs designed for SEA languages. The SEA-LION family of LLMs
supports 11 SEA languages, namely English, Chinese, Indonesian, Vietnamese,
Malay, Thai, Burmese, Lao, Filipino, Tamil, and Khmer. Our work leverages
large-scale multilingual continued pre-training with a comprehensive
post-training regime involving multiple stages of instruction fine-tuning,
alignment, and model merging. Evaluation results on multilingual benchmarks
indicate that our models achieve state-of-the-art performance across LLMs
supporting SEA languages. We open-source the models to benefit the wider SEA
community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IJCNLP-AACL 2025 (Main Track). We released our model at
  https://huggingface.co/collections/aisingapore/sea-lionv3-672589a39cdadd6a5b199581</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Model-Document Protocol for AI Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25160v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25160v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongjin Qian, Zheng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI search depends on linking large language models (LLMs) with vast external
knowledge sources. Yet web pages, PDF files, and other raw documents are not
inherently LLM-ready: they are long, noisy, and unstructured. Conventional
retrieval methods treat these documents as verbatim text and return raw
passages, leaving the burden of fragment assembly and contextual reasoning to
the LLM. This gap underscores the need for a new retrieval paradigm that
redefines how models interact with documents.
  We introduce the Model-Document Protocol (MDP), a general framework that
formalizes how raw text is bridged to LLMs through consumable knowledge
representations. Rather than treating retrieval as passage fetching, MDP
defines multiple pathways that transform unstructured documents into
task-specific, LLM-ready inputs. These include agentic reasoning, which curates
raw evidence into coherent context; memory grounding, which accumulates
reusable notes to enrich reasoning; and structured leveraging, which encodes
documents into formal representations such as graphs or key-value caches. All
three pathways share the same goal: ensuring that what reaches the LLM is not
raw fragments but compact, structured knowledge directly consumable for
reasoning.
  As an instantiation, we present MDP-Agent, which realizes the protocol
through an agentic process: constructing document-level gist memories for
global coverage, performing diffusion-based exploration with vertical
exploitation to uncover layered dependencies, and applying map-reduce style
synthesis to integrate large-scale evidence into compact yet sufficient
context. Experiments on information-seeking benchmarks demonstrate that
MDP-Agent outperforms baselines, validating both the soundness of the MDP
framework and the effectiveness of its agentic instantiation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Efficient Are Diffusion Language Models? A Critical Examination of
  Efficiency Evaluation Practices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.18480v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.18480v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Peng, Peiyu Liu, Zican Dong, Daixuan Cheng, Junyi Li, Yiru Tang, Shuo Wang, Wayne Xin Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion language models (DLMs) have emerged as a promising alternative to
the long-dominant autoregressive (AR) paradigm, offering a parallelable
decoding process that could yield greater efficiency. Yet, in practice, current
open-source DLMs often underperform their AR counterparts in speed, limiting
their real-world utility. This work presents a systematic study of DLM
efficiency, identifying key issues in prior evaluation methods. Through
empirical benchmarking and a roofline-based theoretical analysis, we
demonstrate that AR models generally achieve higher throughput, while DLMs
consistently lag. We also investigate acceleration strategies, finding that
techniques like dual cache and parallel decoding mainly offer gains at small
batch sizes, with their benefits diminishing upon scaling. Our findings
underscore the necessity of robust evaluation methods and improved acceleration
strategies to advance research on DLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Withdrawn by the authors to better delineate the related work from
  the paper's original contributions</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SPARTA ALIGNMENT: Collectively Aligning Multiple Language Models through
  Combat <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04721v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04721v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuru Jiang, Wenxuan Ding, Shangbin Feng, Greg Durrett, Yulia Tsvetkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose SPARTA ALIGNMENT, an algorithm to collectively align multiple LLMs
through competition and combat. To complement a single model's lack of
diversity in generation and biases in evaluation, multiple LLMs form a "sparta
tribe" to compete against each other in fulfilling instructions while serving
as judges for the competition of others. For each iteration, one instruction
and two models are selected for a duel, the other models evaluate the two
responses, and their evaluation scores are aggregated through a adapted
elo-ranking based reputation system, where winners/losers of combat gain/lose
weight in evaluating others. The peer-evaluated combat results then become
preference pairs where the winning response is preferred over the losing one,
and all models learn from these preferences at the end of each iteration.
SPARTA ALIGNMENT enables the self-evolution of multiple LLMs in an iterative
and collective competition process. Extensive experiments demonstrate that
SPARTA ALIGNMENT outperforms initial models and 4 self-alignment baselines
across 10 out of 12 tasks and datasets with 7.0% average improvement. Further
analysis reveals that SPARTA ALIGNMENT generalizes more effectively to unseen
tasks and leverages the expertise diversity of participating models to produce
more logical, direct and informative outputs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IGD: Token Decisiveness Modeling via Information Gain in LLMs for
  Personalized Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13229v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13229v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijie Lin, Yang Zhang, Xiaoyan Zhao, Fengbin Zhu, Fuli Feng, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown strong potential for recommendation
by framing item prediction as a token-by-token language generation task.
However, existing methods treat all item tokens equally, simply pursuing
likelihood maximization during both optimization and decoding. This overlooks
crucial token-level differences in decisiveness-many tokens contribute little
to item discrimination yet can dominate optimization or decoding. To quantify
token decisiveness, we propose a novel perspective that models item generation
as a decision process, measuring token decisiveness by the Information Gain
(IG) each token provides in reducing uncertainty about the generated item. Our
empirical analysis reveals that most tokens have low IG but often correspond to
high logits, disproportionately influencing training loss and decoding, which
may impair model performance. Building on these insights, we introduce an
Information Gain-based Decisiveness-aware Token handling (IGD) strategy that
integrates token decisiveness into both tuning and decoding. Specifically, IGD
downweights low-IG tokens during tuning and rebalances decoding to emphasize
tokens with high IG. In this way, IGD moves beyond pure likelihood
maximization, effectively prioritizing high-decisiveness tokens. Extensive
experiments on four benchmark datasets with two LLM backbones demonstrate that
IGD consistently improves recommendation accuracy, achieving significant gains
on widely used ranking metrics compared to strong baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ClueAnchor: Clue-Anchored Knowledge Reasoning Exploration and
  Optimization for Retrieval-Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.24388v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.24388v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Chen, Yukun Yan, Sen Mei, Wanxiang Che, Zhenghao Liu, Qi Shi, Xinze Li, Yuchun Fan, Pengcheng Huang, Qiushi Xiong, Zhiyuan Liu, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) augments Large Language Models (LLMs)
with external knowledge to improve factuality. However, existing RAG systems
frequently underutilize the retrieved documents, failing to extract and
integrate the key clues needed to support faithful and interpretable reasoning,
especially in cases where relevant evidence is implicit, scattered, or obscured
by noise. To address this issue, we propose ClueAnchor, a novel framework for
enhancing RAG via clue-anchored reasoning exploration and optimization.
ClueAnchor extracts key clues from retrieved content and generates multiple
reasoning paths based on different knowledge configurations, optimizing the
model by selecting the most appropriate reasoning path for the given context
through reward-based preference optimization. Experiments show that ClueAnchor
significantly outperforms prior RAG baselines in the completeness and
robustness of reasoning. Further analysis confirms its strong resilience to
noisy or partially relevant retrieved content, as well as its capability to
identify supporting evidence even in the absence of explicit clue supervision
during inference. All codes are available at
https://github.com/thunlp/ClueAnchor.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nek Minit: Harnessing Pragmatic Metacognitive <span class="highlight-title">Prompt</span>ing for Explainable
  Sarcasm Detection of Australian and Indian English <span class="chip">ALT</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.15095v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.15095v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ishmanbir Singh, Dipankar Srirag, Aditya Joshi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sarcasm is a challenge to sentiment analysis because of the incongruity
between stated and implied sentiment. The challenge is exacerbated when the
implication may be relevant to a specific country or geographical region.
Pragmatic metacognitive prompting (PMP) is a cognition-inspired technique that
has been used for pragmatic reasoning. In this paper, we harness PMP for
explainable sarcasm detection for Australian and Indian English, alongside a
benchmark dataset for standard English. We manually add sarcasm explanations to
an existing sarcasm-labeled dataset for Australian and Indian English called
BESSTIE, and compare the performance for explainable sarcasm detection for them
with FLUTE, a standard English dataset containing sarcasm explanations. Our
approach utilising PMP when evaluated on two open-weight LLMs (GEMMA and LLAMA)
achieves statistically significant performance improvement across all tasks and
datasets when compared with four alternative prompting strategies. We also find
that alternative techniques such as agentic prompting mitigate context-related
failures by enabling external knowledge retrieval. The focused contribution of
our work is utilising PMP in generating sarcasm explanations for varieties of
English.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ALTA 2025 (Best Paper Honorable Mention). Camera-ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FESTA: Functionally Equivalent Sampling for Trust Assessment of
  Multimodal LLMs <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.16648v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.16648v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Debarpan Bhattacharya, Apoorva Kulkarni, Sriram Ganapathy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The accurate trust assessment of multimodal large language models (MLLMs)
generated predictions, which can enable selective prediction and improve user
confidence, is challenging due to the diverse multi-modal input paradigms. We
propose Functionally Equivalent Sampling for Trust Assessment (FESTA), a
multimodal input sampling technique for MLLMs, that generates an uncertainty
measure based on the equivalent and complementary input samplings. The proposed
task-preserving sampling approach for uncertainty quantification expands the
input space to probe the consistency (through equivalent samples) and
sensitivity (through complementary samples) of the model. FESTA uses only
input-output access of the model (black-box), and does not require ground truth
(unsupervised). The experiments are conducted with various off-the-shelf
multi-modal LLMs, on both visual and audio reasoning tasks. The proposed FESTA
uncertainty estimate achieves significant improvement (33.3% relative
improvement for vision-LLMs and 29.6% relative improvement for audio-LLMs) in
selective prediction performance, based on
area-under-receiver-operating-characteristic curve (AUROC) metric in detecting
mispredictions. The code implementation is open-sourced.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in the Findings of EMNLP, 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient
  in Latent Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13308v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13308v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hengli Li, Chenxi Li, Tong Wu, Xuekai Zhu, Yuxuan Wang, Zhaoxin Yu, Eric Hanchen Jiang, Song-Chun Zhu, Zixia Jia, Ying Nian Wu, Zilong Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reasoning ability, a core component of human intelligence, continues to pose
a significant challenge for Large Language Models (LLMs) in the pursuit of AGI.
Although model performance has improved under the training scaling law,
significant challenges remain, particularly with respect to training
algorithms, such as catastrophic forgetting, and the limited availability of
novel training data. As an alternative, test-time scaling enhances reasoning
performance by increasing test-time computation without parameter updating.
Unlike prior methods in this paradigm focused on token space, we propose
leveraging latent space for more effective reasoning and better adherence to
the test-time scaling law. We introduce LatentSeek, a novel framework that
enhances LLM reasoning through Test-Time Instance-level Adaptation (TTIA)
within the model's latent space. Specifically, LatentSeek leverages policy
gradient to iteratively update latent representations, guided by self-generated
reward signals. LatentSeek is evaluated on a range of reasoning benchmarks,
including GSM8K, MATH-500, and AIME2024, across multiple LLM architectures.
Results show that LatentSeek consistently outperforms strong baselines, such as
Chain-of-Thought prompting and fine-tuning-based methods. Furthermore, our
analysis demonstrates that LatentSeek is highly efficient, typically converging
within a few iterations for problems of average complexity, while also
benefiting from additional iterations, thereby highlighting the potential of
test-time scaling in the latent space. These findings position LatentSeek as a
lightweight, scalable, and effective solution for enhancing the reasoning
capabilities of LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Scales of Justitia: A Comprehensive <span class="highlight-title">Survey</span> on Safety Evaluation of
  LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11094v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11094v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songyang Liu, Chaozhuo Li, Jiameng Qiu, Xi Zhang, Feiran Huang, Litian Zhang, Yiming Hei, Philip S. Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid advancement of artificial intelligence, Large Language Models
(LLMs) have shown remarkable capabilities in Natural Language Processing (NLP),
including content generation, human-computer interaction, machine translation,
and code generation. However, their widespread deployment has also raised
significant safety concerns. In particular, LLM-generated content can exhibit
unsafe behaviors such as toxicity, bias, or misinformation, especially in
adversarial contexts, which has attracted increasing attention from both
academia and industry. Although numerous studies have attempted to evaluate
these risks, a comprehensive and systematic survey on safety evaluation of LLMs
is still lacking. This work aims to fill this gap by presenting a structured
overview of recent advances in safety evaluation of LLMs. Specifically, we
propose a four-dimensional taxonomy: (i) Why to evaluate, which explores the
background of safety evaluation of LLMs, how they differ from general LLMs
evaluation, and the significance of such evaluation; (ii) What to evaluate,
which examines and categorizes existing safety evaluation tasks based on key
capabilities, including dimensions such as toxicity, robustness, ethics, bias
and fairness, truthfulness, and related aspects; (iii) Where to evaluate, which
summarizes the evaluation metrics, datasets and benchmarks currently used in
safety evaluations; (iv) How to evaluate, which reviews existing mainstream
evaluation methods based on the roles of the evaluators and some evaluation
frameworks that integrate the entire evaluation pipeline. Finally, we identify
the challenges in safety evaluation of LLMs and propose promising research
directions to promote further advancement in this field. We emphasize the
necessity of prioritizing safety evaluation to ensure the reliable and
responsible deployment of LLMs in real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Similarity-Distance-Magnitude Activations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.12760v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.12760v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Allen Schmaltz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the Similarity-Distance-Magnitude (SDM) activation function, a
more robust and interpretable formulation of the standard softmax activation
function, adding Similarity (i.e., correctly predicted depth-matches into
training) awareness and Distance-to-training-distribution awareness to the
existing output Magnitude (i.e., decision-boundary) awareness, and enabling
interpretability-by-exemplar via dense matching. We further introduce the SDM
estimator, based on a data-driven partitioning of the class-wise empirical CDFs
via the SDM activation, to control the class- and prediction-conditional
accuracy among selective classifications. When used as the final-layer
activation over pre-trained language models for selective classification, the
SDM estimator is more robust to co-variate shifts and out-of-distribution
inputs than existing calibration methods using softmax activations, while
remaining informative over in-distribution data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 5 tables, 1 algorithm. arXiv admin note: substantial text
  overlap with arXiv:2502.20167</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TEXT2DB: Integration-Aware Information Extraction with Large Language
  Model Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24014v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24014v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhu Jiao, Sha Li, Sizhe Zhou, Heng Ji, Jiawei Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of information extraction (IE) is to extract structured knowledge
from text. However, it is often not straightforward to utilize IE output due to
the mismatch between the IE ontology and the downstream application needs. We
propose a new formulation of IE TEXT2DB that emphasizes the integration of IE
output and the target database (or knowledge base). Given a user instruction, a
document set, and a database, our task requires the model to update the
database with values from the document set to satisfy the user instruction.
This task requires understanding user instructions for what to extract and
adapting to the given DB/KB schema for how to extract on the fly. To evaluate
this new task, we introduce a new benchmark featuring common demands such as
data infilling, row population, and column addition. In addition, we propose an
LLM agent framework OPAL (Observe-PlanAnalyze LLM) which includes an Observer
component that interacts with the database, the Planner component that
generates a code-based plan with calls to IE models, and the Analyzer component
that provides feedback regarding code quality before execution. Experiments
show that OPAL can successfully adapt to diverse database schemas by generating
different code plans and calling the required IE models. We also highlight
difficult cases such as dealing with large databases with complex dependencies
and extraction hallucination, which we believe deserve further investigation.
Source code: https://github.com/yzjiao/Text2DB
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Source code: https://github.com/yzjiao/Text2DB</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Predicting Any Human Trajectory In Context <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.00871v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.00871v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryo Fujii, Hideo Saito, Ryo Hachiuma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting accurate future trajectories of pedestrians is essential for
autonomous systems but remains a challenging task due to the need for
adaptability in different environments and domains. A common approach involves
collecting scenario-specific data and performing fine-tuning via
backpropagation. However, the need to fine-tune for each new scenario is often
impractical for deployment on edge devices. To address this challenge, we
introduce \paper, an In-Context Learning (ICL) framework for pedestrian
trajectory prediction that enables adaptation without fine-tuning on the
scenario-specific data at inference time without requiring weight updates. We
propose a spatio-temporal similarity-based example selection (STES) method that
selects relevant examples from previously observed trajectories within the same
scene by identifying similar motion patterns at corresponding locations. To
further refine this selection, we introduce prediction-guided example selection
(PG-ES), which selects examples based on both the past trajectory and the
predicted future trajectory, rather than relying solely on the past trajectory.
This approach allows the model to account for long-term dynamics when selecting
examples. Finally, instead of relying on small real-world datasets with limited
scenario diversity, we train our model on a large-scale synthetic dataset to
enhance its prediction ability by leveraging in-context examples. Extensive
experiments demonstrate that TrajICL achieves remarkable adaptation across both
in-domain and cross-domain scenarios, outperforming even fine-tuned approaches
across multiple public benchmarks. Project Page:
https://fujiry0.github.io/TrajICL-project-page/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Speak & Spell: LLM-Driven Controllable Phonetic Error Augmentation for
  Robust Dialogue State Tracking <span class="chip">AACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06263v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06263v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jihyun Lee, Solee Im, Wonjun Lee, Gary Geunbae Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dialogue State Tracking (DST) is a key part of task-oriented dialogue
systems, identifying important information in conversations. However, its
accuracy drops significantly in spoken dialogue environments due to named
entity errors from Automatic Speech Recognition (ASR) systems. We introduce a
simple yet effective data augmentation method that targets those entities to
improve the robustness of DST model. Our novel method can control the placement
of errors using keyword-highlighted prompts while introducing phonetically
similar errors. As a result, our method generated sufficient error patterns on
keywords, leading to improved accuracy in noised and low-accuracy ASR
environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AACL-IJCNLP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Are LLMs Rigorous Logical Reasoners? Empowering Natural Language Proof
  Generation by Stepwise Decoding with Contrastive Learning <span class="chip">AACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.06736v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.06736v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ying Su, Mingwen Liu, Zhijiang Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Logical reasoning is a pivotal component in the field of artificial
intelligence. Proof planning, particularly in contexts requiring the validation
of explanation accuracy, continues to present challenges. The recent
advancement of large language models (LLMs) has led to significant progress in
natural language proof planning, evolving from one-stage generators to more
complex three-stage systems that include additional searchers or verifiers.
While these assisted methods improve the quality of generated results, they
also introduce increased search efforts and computational costs. Furthermore,
the generative process itself remains underexplored. In this study, we propose
a stepwise decoding approach augmented by contrastive learning to address two
common errors encountered during the LLM generator's decoding process. We
fine-tune the language model using both vanilla and enhanced hard negatives to
mitigate these decoding errors. Empirical results demonstrate the effectiveness
of our strategy. Additionally, our further analysis reveals that even larger
LLMs still struggle to generate rigorous logical chains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 2 figures, 11 tables. Accepted by AACL 2025 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pass@K Policy Optimization: Solving Harder Reinforcement Learning
  Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.15201v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.15201v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Walder, Deep Karkhanis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning (RL) algorithms sample multiple n>1 solution attempts
for each problem and reward them independently. This optimizes for pass@1
performance and prioritizes the strength of isolated samples at the expense of
the diversity and collective utility of sets of samples. This under-utilizes
the sampling capacity, limiting exploration and eventual improvement on harder
examples. As a fix, we propose Pass-at-k Policy Optimization (PKPO), a
transformation on the final rewards which leads to direct optimization of
pass@k performance, thus optimizing for sets of samples that maximize reward
when considered jointly. Our contribution is to derive novel low variance
unbiased estimators for pass@k and its gradient, in both the binary and
continuous reward settings. We show optimization with our estimators reduces to
standard RL with rewards that have been jointly transformed by a stable and
efficient transformation function.
  While previous efforts are restricted to k=n, ours is the first to enable
robust optimization of pass@k for any arbitrary k <= n. Moreover, instead of
trading off pass@1 performance for pass@k gains, our method allows annealing k
during training, optimizing both metrics and often achieving strong pass@1
numbers alongside significant pass@k gains.
  We validate our reward transformations on toy experiments, which reveal the
variance reducing properties of our formulations. We also include real-world
examples using the open-source LLM, GEMMA-2. We find that our transformation
effectively optimizes for the target k. Furthermore, higher k values enable
solving more and harder problems, while annealing k boosts both the pass@1 and
pass@k . Crucially, for challenging task sets where conventional pass@1
optimization stalls, our pass@k approach unblocks learning, likely due to
better exploration by prioritizing joint utility over the utility of individual
samples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Networks for Learnable and Scalable Influence Estimation of
  Instruction Fine-Tuning Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09969v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09969v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ishika Agarwal, Dilek Hakkani-Tür
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Influence functions provide crucial insights into model training, but
existing methods suffer from large computational costs and limited
generalization. Particularly, recent works have proposed various metrics and
algorithms to calculate the influence of data using language models, which do
not scale well with large models and datasets. This is because of the expensive
forward and backward passes required for computation, substantial memory
requirements to store large models, and poor generalization of influence
estimates to new data. In this paper, we explore the use of small neural
networks -- which we refer to as the InfluenceNetwork -- to estimate influence
values, achieving up to 99% cost reduction. Our evaluation demonstrates that
influence values can be estimated with models just 0.0027% the size of full
language models (we use 7B and 8B versions). We apply our algorithm of
estimating influence values (called NN-CIFT: Neural Networks for effiCient
Instruction Fine-Tuning) to the downstream task of subset selection for general
instruction fine-tuning. In our study, we include four state-of-the-art
influence functions and show no compromise in performance, despite large
speedups, between NN-CIFT and the original influence functions. We provide an
in-depth hyperparameter analyses of NN-CIFT. The code for our method can be
found here: https://github.com/agarwalishika/NN-CIFT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Models Report Subjective Experience Under
  Self-Referential Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24797v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24797v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cameron Berg, Diogo de Lucena, Judd Rosenblatt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models sometimes produce structured, first-person descriptions
that explicitly reference awareness or subjective experience. To better
understand this behavior, we investigate one theoretically motivated condition
under which such reports arise: self-referential processing, a computational
motif emphasized across major theories of consciousness. Through a series of
controlled experiments on GPT, Claude, and Gemini model families, we test
whether this regime reliably shifts models toward first-person reports of
subjective experience, and how such claims behave under mechanistic and
behavioral probes. Four main results emerge: (1) Inducing sustained
self-reference through simple prompting consistently elicits structured
subjective experience reports across model families. (2) These reports are
mechanistically gated by interpretable sparse-autoencoder features associated
with deception and roleplay: surprisingly, suppressing deception features
sharply increases the frequency of experience claims, while amplifying them
minimizes such claims. (3) Structured descriptions of the self-referential
state converge statistically across model families in ways not observed in any
control condition. (4) The induced state yields significantly richer
introspection in downstream reasoning tasks where self-reflection is only
indirectly afforded. While these findings do not constitute direct evidence of
consciousness, they implicate self-referential processing as a minimal and
reproducible condition under which large language models generate structured
first-person reports that are mechanistically gated, semantically convergent,
and behaviorally generalizable. The systematic emergence of this pattern across
architectures makes it a first-order scientific and ethical priority for
further investigation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Let LRMs Break Free from Overthinking via Self-Braking Tuning <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14604v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14604v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Zhao, Yuchen Yan, Yongliang Shen, Haolei Xu, Wenqi Zhang, Kaitao Song, Jian Shao, Weiming Lu, Jun Xiao, Yueting Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large reasoning models (LRMs), such as OpenAI o1 and DeepSeek-R1, have
significantly enhanced their reasoning capabilities by generating longer chains
of thought, demonstrating outstanding performance across a variety of tasks.
However, this performance gain comes at the cost of a substantial increase in
redundant reasoning during the generation process, leading to high
computational overhead and exacerbating the issue of overthinking. Although
numerous existing approaches aim to address the problem of overthinking, they
often rely on external interventions. In this paper, we propose a novel
framework, Self-Braking Tuning (SBT), which tackles overthinking from the
perspective of allowing the model to regulate its own reasoning process, thus
eliminating the reliance on external control mechanisms. We construct a set of
overthinking identification metrics based on standard answers and design a
systematic method to detect redundant reasoning. This method accurately
identifies unnecessary steps within the reasoning trajectory and generates
training signals for learning self-regulation behaviors. Building on this
foundation, we develop a complete strategy for constructing data with adaptive
reasoning lengths and introduce an innovative braking prompt mechanism that
enables the model to naturally learn when to terminate reasoning at an
appropriate point. Experiments across mathematical benchmarks (AIME, AMC,
MATH500, GSM8K) demonstrate that our method reduces token consumption by up to
60% while maintaining comparable accuracy to unconstrained models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2025; Camera ready version, 10 pages.
  Github:https://github.com/ZJU-REAL/Self-Braking-Tuning Project Page:
  https://ZJU-REAL.github.io/SBT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Model Provenance Testing for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.00706v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.00706v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ivica Nikolic, Teodora Baluta, Prateek Saxena
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models are increasingly customized through fine-tuning and
other adaptations, creating challenges in enforcing licensing terms and
managing downstream impacts. Tracking model origins is crucial both for
protecting intellectual property and for identifying derived models when biases
or vulnerabilities are discovered in foundation models. We address this
challenge by developing a framework for testing model provenance: Whether one
model is derived from another. Our approach is based on the key observation
that real-world model derivations preserve significant similarities in model
outputs that can be detected through statistical analysis. Using only black-box
access to models, we employ multiple hypothesis testing to compare model
similarities against a baseline established by unrelated models. On two
comprehensive real-world benchmarks spanning models from 30M to 4B parameters
and comprising over 600 models, our tester achieves 90-95% precision and 80-90%
recall in identifying derived models. These results demonstrate the viability
of systematic provenance verification in production environments even when only
API access is available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When Agents Trade: Live Multi-Market Trading Benchmark for LLM Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.11695v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.11695v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingfei Qian, Xueqing Peng, Yan Wang, Vincent Jim Zhang, Huan He, Hanley Smith, Yi Han, Yueru He, Haohang Li, Yupeng Cao, Yangyang Yu, Alejandro Lopez-Lira, Peng Lu, Jian-Yun Nie, Guojun Xiong, Jimin Huang, Sophia Ananiadou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although Large Language Model (LLM)-based agents are increasingly used in
financial trading, it remains unclear whether they can reason and adapt in live
markets, as most studies test models instead of agents, cover limited periods
and assets, and rely on unverified data. To address these gaps, we introduce
Agent Market Arena (AMA), the first lifelong, real-time benchmark for
evaluating LLM-based trading agents across multiple markets. AMA integrates
verified trading data, expert-checked news, and diverse agent architectures
within a unified trading framework, enabling fair and continuous comparison
under real conditions. It implements four agents, including InvestorAgent as a
single-agent baseline, TradeAgent and HedgeFundAgent with different risk
styles, and DeepFundAgent with memory-based reasoning, and evaluates them
across GPT-4o, GPT-4.1, Claude-3.5-haiku, Claude-sonnet-4, and
Gemini-2.0-flash. Live experiments on both cryptocurrency and stock markets
demonstrate that agent frameworks display markedly distinct behavioral
patterns, spanning from aggressive risk-taking to conservative decision-making,
whereas model backbones contribute less to outcome variation. AMA thus
establishes a foundation for rigorous, reproducible, and continuously evolving
evaluation of financial reasoning and trading intelligence in LLM-based agents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ChartMuseum: Testing Visual Reasoning Capabilities of Large
  Vision-Language Models <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13444v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13444v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liyan Tang, Grace Kim, Xinyu Zhao, Thom Lake, Wenxuan Ding, Fangcong Yin, Prasann Singhal, Manya Wadhwa, Zeyu Leo Liu, Zayne Sprague, Ramya Namuduri, Bodun Hu, Juan Diego Rodriguez, Puyuan Peng, Greg Durrett
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chart understanding presents a unique challenge for large vision-language
models (LVLMs), as it requires the integration of sophisticated textual and
visual reasoning capabilities. However, current LVLMs exhibit a notable
imbalance between these skills, falling short on visual reasoning that is
difficult to perform in text. We conduct a case study using a synthetic dataset
solvable only through visual reasoning and show that model performance degrades
significantly with increasing visual complexity, while human performance
remains robust. We then introduce ChartMuseum, a new Chart Question Answering
(QA) benchmark containing 1,162 expert-annotated questions spanning multiple
reasoning types, curated from real-world charts across 184 sources,
specifically built to evaluate complex visual and textual reasoning. Unlike
prior chart understanding benchmarks -- where frontier models perform similarly
and near saturation -- our benchmark exposes a substantial gap between model
and human performance, while effectively differentiating model capabilities:
although humans achieve 93% accuracy, the best-performing model Gemini-2.5-Pro
attains only 63.0%, and the leading open-source LVLM Qwen2.5-VL-72B-Instruct
achieves only 38.5%. Moreover, on questions requiring primarily visual
reasoning, all models experience a 35%-55% performance drop from
text-reasoning-heavy question performance. Lastly, our qualitative error
analysis reveals specific categories of visual reasoning that are challenging
for current LVLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025 Datasets & Benchmarks</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Emotion Recognition in Spoken Language Models on Emotionally
  Incongruent Speech <span class="chip">ICASSP 2026</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25054v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25054v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro Corrêa, João Lima, Victor Moreno, Lucas Ueda, Paula Dornhofer Paro Costa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advancements in spoken language processing have driven the development of
spoken language models (SLMs), designed to achieve universal audio
understanding by jointly learning text and audio representations for a wide
range of tasks. Although promising results have been achieved, there is growing
discussion regarding these models' generalization capabilities and the extent
to which they truly integrate audio and text modalities in their internal
representations. In this work, we evaluate four SLMs on the task of speech
emotion recognition using a dataset of emotionally incongruent speech samples,
a condition under which the semantic content of the spoken utterance conveys
one emotion while speech expressiveness conveys another. Our results indicate
that SLMs rely predominantly on textual semantics rather than speech emotion to
perform the task, indicating that text-related representations largely dominate
over acoustic representations. We release both the code and the Emotionally
Incongruent Synthetic Speech dataset (EMIS) to the community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE ICASSP 2026. Copyright 2026 IEEE. Personal use of
  this material is permitted. Permission from IEEE must be obtained for all
  other uses</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving LLM Safety Alignment with Dual-Objective Optimization <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03710v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03710v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuandong Zhao, Will Cai, Tianneng Shi, David Huang, Licong Lin, Song Mei, Dawn Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing training-time safety alignment techniques for large language models
(LLMs) remain vulnerable to jailbreak attacks. Direct preference optimization
(DPO), a widely deployed alignment method, exhibits limitations in both
experimental and theoretical contexts as its loss function proves suboptimal
for refusal learning. Through gradient-based analysis, we identify these
shortcomings and propose an improved safety alignment that disentangles DPO
objectives into two components: (1) robust refusal training, which encourages
refusal even when partial unsafe generations are produced, and (2) targeted
unlearning of harmful knowledge. This approach significantly increases LLM
robustness against a wide range of jailbreak attacks, including prefilling,
suffix, and multi-turn attacks across both in-distribution and
out-of-distribution scenarios. Furthermore, we introduce a method to emphasize
critical refusal tokens by incorporating a reward-based token-level weighting
mechanism for refusal learning, which further improves the robustness against
adversarial exploits. Our research also suggests that robustness to jailbreak
attacks is correlated with token distribution shifts in the training process
and internal representations of refusal and harmful tokens, offering valuable
directions for future research in LLM safety alignment. The code is available
at https://github.com/wicai24/DOOR-Alignment
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language Model Preference Evaluation with Multiple Weak Evaluators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12869v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12869v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengyu Hu, Jieyu Zhang, Zhihan Xiong, Alexander Ratner, Kaize Ding, Ranjay Krishna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the remarkable success of Large Language Models (LLMs), evaluating
their outputs' quality regarding preference remains a critical challenge. While
existing works usually leverage a strong LLM as the judge for comparing LLMs'
response pairwisely, such a single-evaluator approach is vulnerable to cyclic
preference, i.e., output A is better than B, B than C, but C is better than A,
causing contradictory evaluation results. To address this, we introduce PGED
(Preference Graph Ensemble and Denoise), a novel approach that leverages
multiple model-based evaluators to construct preference graphs, and then
ensembles and denoises these graphs for acyclic, non-contradictory evaluation
results. We provide theoretical guarantees for our framework, demonstrating its
efficacy in recovering the ground truth preference structure. Extensive
experiments on ten benchmarks demonstrate PGED 's superiority in three
applications: 1) model ranking for evaluation, 2) response selection for
test-time scaling, and 3) data selection for model fine-tuning. Notably, PGED
combines small LLM evaluators (e.g., Llama3-8B, Mistral-7B, Qwen2-7B) to
outperform strong ones (e.g., Qwen2-72B), showcasing its effectiveness in
enhancing evaluation reliability and improving model performance.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">128</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with
  the MME-CoF Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26802v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26802v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyu Guo, Xinyan Chen, Renrui Zhang, Ruichuan An, Yu Qi, Dongzhi Jiang, Xiangtai Li, Manyuan Zhang, Hongsheng Li, Pheng-Ann Heng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent video generation models can produce high-fidelity, temporally coherent
videos, indicating that they may encode substantial world knowledge. Beyond
realistic synthesis, they also exhibit emerging behaviors indicative of visual
perception, modeling, and manipulation. Yet, an important question still
remains: Are video models ready to serve as zero-shot reasoners in challenging
visual reasoning scenarios? In this work, we conduct an empirical study to
comprehensively investigate this question, focusing on the leading and popular
Veo-3. We evaluate its reasoning behavior across 12 dimensions, including
spatial, geometric, physical, temporal, and embodied logic, systematically
characterizing both its strengths and failure modes. To standardize this study,
we curate the evaluation data into MME-CoF, a compact benchmark that enables
in-depth and thorough assessment of Chain-of-Frame (CoF) reasoning. Our
findings reveal that while current video models demonstrate promising reasoning
patterns on short-horizon spatial coherence, fine-grained grounding, and
locally consistent dynamics, they remain limited in long-horizon causal
reasoning, strict geometric constraints, and abstract logic. Overall, they are
not yet reliable as standalone zero-shot reasoners, but exhibit encouraging
signs as complementary visual engines alongside dedicated reasoning models.
Project page: https://video-cof.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://video-cof.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OmniX: From Unified Panoramic Generation and Perception to
  Graphics-Ready 3D Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26800v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26800v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yukun Huang, Jiwen Yu, Yanning Zhou, Jianan Wang, Xintao Wang, Pengfei Wan, Xihui Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There are two prevalent ways to constructing 3D scenes: procedural generation
and 2D lifting. Among them, panorama-based 2D lifting has emerged as a
promising technique, leveraging powerful 2D generative priors to produce
immersive, realistic, and diverse 3D environments. In this work, we advance
this technique to generate graphics-ready 3D scenes suitable for physically
based rendering (PBR), relighting, and simulation. Our key insight is to
repurpose 2D generative models for panoramic perception of geometry, textures,
and PBR materials. Unlike existing 2D lifting approaches that emphasize
appearance generation and ignore the perception of intrinsic properties, we
present OmniX, a versatile and unified framework. Based on a lightweight and
efficient cross-modal adapter structure, OmniX reuses 2D generative priors for
a broad range of panoramic vision tasks, including panoramic perception,
generation, and completion. Furthermore, we construct a large-scale synthetic
panorama dataset containing high-quality multimodal panoramas from diverse
indoor and outdoor scenes. Extensive experiments demonstrate the effectiveness
of our model in panoramic visual perception and graphics-ready 3D scene
generation, opening new possibilities for immersive and physically realistic
virtual world generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://yukun-huang.github.io/OmniX/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Masked Diffusion Captioning for Visual Feature Learning <span class="chip">EMNLP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26799v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26799v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Feng, Zihao Wei, Andrew Owens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We learn visual features by captioning images with an image-conditioned
masked diffusion language model, a formulation we call masked diffusion
captioning (MDC). During training, text tokens in each image-caption pair are
masked at a randomly chosen ratio, and a decoder conditioned on visual features
is trained to reconstruct the original text. After training, the learned visual
features can be applied to downstream vision tasks. Unlike autoregressive
captioning, the strength of the visual learning signal in MDC does not depend
on each token's position in the sequence, reducing the need for auxiliary
objectives. Linear probing experiments across a variety of academic-scale
models and datasets show that the learned visual features are competitive with
those produced by autoregressive and contrastive approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2025 (Findings). Project page:
  https://cfeng16.github.io/mdlm4vfl/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SEE4D: Pose-Free 4D Generation via Auto-Regressive Video Inpainting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26796v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26796v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongyue Lu, Ao Liang, Tianxin Huang, Xiao Fu, Yuyang Zhao, Baorui Ma, Liang Pan, Wei Yin, Lingdong Kong, Wei Tsang Ooi, Ziwei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Immersive applications call for synthesizing spatiotemporal 4D content from
casual videos without costly 3D supervision. Existing video-to-4D methods
typically rely on manually annotated camera poses, which are labor-intensive
and brittle for in-the-wild footage. Recent warp-then-inpaint approaches
mitigate the need for pose labels by warping input frames along a novel camera
trajectory and using an inpainting model to fill missing regions, thereby
depicting the 4D scene from diverse viewpoints. However, this
trajectory-to-trajectory formulation often entangles camera motion with scene
dynamics and complicates both modeling and inference. We introduce SEE4D, a
pose-free, trajectory-to-camera framework that replaces explicit trajectory
prediction with rendering to a bank of fixed virtual cameras, thereby
separating camera control from scene modeling. A view-conditional video
inpainting model is trained to learn a robust geometry prior by denoising
realistically synthesized warped images and to inpaint occluded or missing
regions across virtual viewpoints, eliminating the need for explicit 3D
annotations. Building on this inpainting core, we design a spatiotemporal
autoregressive inference pipeline that traverses virtual-camera splines and
extends videos with overlapping windows, enabling coherent generation at
bounded per-step complexity. We validate See4D on cross-view video generation
and sparse reconstruction benchmarks. Across quantitative metrics and
qualitative assessments, our method achieves superior generalization and
improved performance relative to pose- or trajectory-conditioned baselines,
advancing practical 4D world modeling from casual videos.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages; 21 figures; 3 tables; project page:
  https://see-4d.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling Image Geo-Localization to Continent Level <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26795v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26795v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp Lindenberger, Paul-Edouard Sarlin, Jan Hosang, Matteo Balice, Marc Pollefeys, Simon Lynen, Eduard Trulls
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Determining the precise geographic location of an image at a global scale
remains an unsolved challenge. Standard image retrieval techniques are
inefficient due to the sheer volume of images (>100M) and fail when coverage is
insufficient. Scalable solutions, however, involve a trade-off: global
classification typically yields coarse results (10+ kilometers), while
cross-view retrieval between ground and aerial imagery suffers from a domain
gap and has been primarily studied on smaller regions. This paper introduces a
hybrid approach that achieves fine-grained geo-localization across a large
geographic expanse the size of a continent. We leverage a proxy classification
task during training to learn rich feature representations that implicitly
encode precise location information. We combine these learned prototypes with
embeddings of aerial imagery to increase robustness to the sparsity of
ground-level data. This enables direct, fine-grained retrieval over areas
spanning multiple countries. Our extensive evaluation demonstrates that our
approach can localize within 200m more than 68\% of queries of a dataset
covering a large part of Europe. The code is publicly available at
https://scaling-geoloc.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Quest for Generalizable Motion Generation: Data, Model, and
  Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26794v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26794v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Lin, Ruisi Wang, Junzhe Lu, Ziqi Huang, Guorui Song, Ailing Zeng, Xian Liu, Chen Wei, Wanqi Yin, Qingping Sun, Zhongang Cai, Lei Yang, Ziwei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent advances in 3D human motion generation (MoGen) on standard
benchmarks, existing models still face a fundamental bottleneck in their
generalization capability. In contrast, adjacent generative fields, most
notably video generation (ViGen), have demonstrated remarkable generalization
in modeling human behaviors, highlighting transferable insights that MoGen can
leverage. Motivated by this observation, we present a comprehensive framework
that systematically transfers knowledge from ViGen to MoGen across three key
pillars: data, modeling, and evaluation. First, we introduce ViMoGen-228K, a
large-scale dataset comprising 228,000 high-quality motion samples that
integrates high-fidelity optical MoCap data with semantically annotated motions
from web videos and synthesized samples generated by state-of-the-art ViGen
models. The dataset includes both text-motion pairs and text-video-motion
triplets, substantially expanding semantic diversity. Second, we propose
ViMoGen, a flow-matching-based diffusion transformer that unifies priors from
MoCap data and ViGen models through gated multimodal conditioning. To enhance
efficiency, we further develop ViMoGen-light, a distilled variant that
eliminates video generation dependencies while preserving strong
generalization. Finally, we present MBench, a hierarchical benchmark designed
for fine-grained evaluation across motion quality, prompt fidelity, and
generalization ability. Extensive experiments show that our framework
significantly outperforms existing approaches in both automatic and human
evaluations. The code, data, and benchmark will be made publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HEIR: Learning Graph-Based Motion Hierarchies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26786v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26786v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Zheng, William Koch, Baiang Li, Felix Heide
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hierarchical structures of motion exist across research fields, including
computer vision, graphics, and robotics, where complex dynamics typically arise
from coordinated interactions among simpler motion components. Existing methods
to model such dynamics typically rely on manually-defined or heuristic
hierarchies with fixed motion primitives, limiting their generalizability
across different tasks. In this work, we propose a general hierarchical motion
modeling method that learns structured, interpretable motion relationships
directly from data. Our method represents observed motions using graph-based
hierarchies, explicitly decomposing global absolute motions into
parent-inherited patterns and local motion residuals. We formulate hierarchy
inference as a differentiable graph learning problem, where vertices represent
elemental motions and directed edges capture learned parent-child dependencies
through graph neural networks. We evaluate our hierarchical reconstruction
approach on three examples: 1D translational motion, 2D rotational motion, and
dynamic 3D scene deformation via Gaussian splatting. Experimental results show
that our method reconstructs the intrinsic motion hierarchy in 1D and 2D cases,
and produces more realistic and interpretable deformations compared to the
baseline on dynamic 3D Gaussian splatting scenes. By providing an adaptable,
data-driven hierarchical modeling paradigm, our method offers a formulation
applicable to a broad range of motion-centric tasks. Project Page:
https://light.princeton.edu/HEIR/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code link: https://github.com/princeton-computational-imaging/HEIR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Clone Deterministic 3D Worlds with Geometrically-Regularized World
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26782v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26782v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zaishuo Xia, Yukuan Lu, Xinyi Li, Yifan Xu, Yubei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A world model is an internal model that simulates how the world evolves.
Given past observations and actions, it predicts the future of both the
embodied agent and its environment. Accurate world models are essential for
enabling agents to think, plan, and reason effectively in complex, dynamic
settings. Despite rapid progress, current world models remain brittle and
degrade over long horizons. We argue that a central cause is representation
quality: exteroceptive inputs (e.g., images) are high-dimensional, and lossy or
entangled latents make dynamics learning unnecessarily hard. We therefore ask
whether improving representation learning alone can substantially improve
world-model performance. In this work, we take a step toward building a truly
accurate world model by addressing a fundamental yet open problem: constructing
a model that can fully clone and overfit to a deterministic 3D world. We
propose Geometrically-Regularized World Models (GRWM), which enforces that
consecutive points along a natural sensory trajectory remain close in latent
representation space. This approach yields significantly improved latent
representations that align closely with the true topology of the environment.
GRWM is plug-and-play, requires only minimal architectural modification, scales
with trajectory length, and is compatible with diverse latent generative
backbones. Across deterministic 3D settings and long-horizon prediction tasks,
GRWM significantly increases rollout fidelity and stability. Analyses show that
its benefits stem from learning a latent manifold with superior geometric
structure. These findings support a clear takeaway: improving representation
learning is a direct and useful path to robust world models, delivering
reliable long-horizon predictions without enlarging the dynamics module.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ChartAB: A Benchmark for Chart Grounding & Dense Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26781v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26781v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aniruddh Bansal, Davit Soselia, Dang Nguyen, Tianyi Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Charts play an important role in visualization, reasoning, data analysis, and
the exchange of ideas among humans. However, existing vision-language models
(VLMs) still lack accurate perception of details and struggle to extract
fine-grained structures from charts. Such limitations in chart grounding also
hinder their ability to compare multiple charts and reason over them. In this
paper, we introduce a novel "ChartAlign Benchmark (ChartAB)" to provide a
comprehensive evaluation of VLMs in chart grounding tasks, i.e., extracting
tabular data, localizing visualization elements, and recognizing various
attributes from charts of diverse types and complexities. We design a JSON
template to facilitate the calculation of evaluation metrics specifically
tailored for each grounding task. By incorporating a novel two-stage inference
workflow, the benchmark can further evaluate VLMs' capability to align and
compare elements/attributes across two charts. Our analysis of evaluations on
several recent VLMs reveals new insights into their perception biases,
weaknesses, robustness, and hallucinations in chart understanding. These
findings highlight the fine-grained discrepancies among VLMs in chart
understanding tasks and point to specific skills that need to be strengthened
in current models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Surpassing state of the art on AMD area estimation from RGB fundus
  images through careful selection of U-Net architectures and loss functions
  for class imbalance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26778v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26778v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valentyna Starodub, Mantas Lukoševičius
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Age-related macular degeneration (AMD) is one of the leading causes of
irreversible vision impairment in people over the age of 60. This research
focuses on semantic segmentation for AMD lesion detection in RGB fundus images,
a non-invasive and cost-effective imaging technique. The results of the ADAM
challenge - the most comprehensive AMD detection from RGB fundus images
research competition and open dataset to date - serve as a benchmark for our
evaluation. Taking the U-Net connectivity as a base of our framework, we
evaluate and compare several approaches to improve the segmentation model's
architecture and training pipeline, including pre-processing techniques,
encoder (backbone) deep network types of varying complexity, and specialized
loss functions to mitigate class imbalances on image and pixel levels. The main
outcome of this research is the final configuration of the AMD detection
framework, which outperforms all the prior ADAM challenge submissions on the
multi-class segmentation of different AMD lesion types in non-invasive RGB
fundus images. The source code used to conduct the experiments presented in
this paper is made freely available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SteerVLM: Robust Model Control through Lightweight Activation Steering
  for Vision Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26769v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26769v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anushka Sivakumar, Andrew Zhang, Zaber Hakim, Chris Thomas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work introduces SteerVLM, a lightweight steering module designed to
guide Vision-Language Models (VLMs) towards outputs that better adhere to
desired instructions. Our approach learns from the latent embeddings of paired
prompts encoding target and converse behaviors to dynamically adjust
activations connecting the language modality with image context. This allows
for fine-grained, inference-time control over complex output semantics without
modifying model weights while preserving performance on off-target tasks. Our
steering module requires learning parameters equal to 0.14% of the original
VLM's size. Our steering module gains model control through dimension-wise
activation modulation and adaptive steering across layers without requiring
pre-extracted static vectors or manual tuning of intervention points.
Furthermore, we introduce VNIA (Visual Narrative Intent Alignment), a
multimodal dataset specifically created to facilitate the development and
evaluation of VLM steering techniques. Our method outperforms existing
intervention techniques on steering and hallucination mitigation benchmarks for
VLMs and proposes a robust solution for multimodal model control through
activation engineering.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MORE: Multi-Organ Medical Image REconstruction <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26759v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26759v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaokai Wu, Yapan Guo, Yanbiao Ji, Jing Tong, Yuxiang Lu, Mei Li, Suizhi Huang, Yue Ding, Hongtao Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  CT reconstruction provides radiologists with images for diagnosis and
treatment, yet current deep learning methods are typically limited to specific
anatomies and datasets, hindering generalization ability to unseen anatomies
and lesions. To address this, we introduce the Multi-Organ medical image
REconstruction (MORE) dataset, comprising CT scans across 9 diverse anatomies
with 15 lesion types. This dataset serves two key purposes: (1) enabling robust
training of deep learning models on extensive, heterogeneous data, and (2)
facilitating rigorous evaluation of model generalization for CT reconstruction.
We further establish a strong baseline solution that outperforms prior
approaches under these challenging conditions. Our results demonstrate that:
(1) a comprehensive dataset helps improve the generalization capability of
models, and (2) optimization-based methods offer enhanced robustness for unseen
anatomies. The MORE dataset is freely accessible under CC-BY-NC 4.0 at our
project page https://more-med.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACMMM 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ProstNFound+: A Prospective Study using Medical Foundation Models for
  Prostate Cancer Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26703v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26703v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul F. R. Wilson, Mohamed Harmanani, Minh Nguyen Nhat To, Amoon Jamzad, Tarek Elghareb, Zhuoxin Guo, Adam Kinnaird, Brian Wodlinger, Purang Abolmaesumi, Parvin Mousavi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Purpose: Medical foundation models (FMs) offer a path to build
high-performance diagnostic systems. However, their application to prostate
cancer (PCa) detection from micro-ultrasound ({\mu}US) remains untested in
clinical settings. We present ProstNFound+, an adaptation of FMs for PCa
detection from {\mu}US, along with its first prospective validation. Methods:
ProstNFound+ incorporates a medical FM, adapter tuning, and a custom prompt
encoder that embeds PCa-specific clinical biomarkers. The model generates a
cancer heatmap and a risk score for clinically significant PCa. Following
training on multi-center retrospective data, the model is prospectively
evaluated on data acquired five years later from a new clinical site. Model
predictions are benchmarked against standard clinical scoring protocols
(PRI-MUS and PI-RADS). Results: ProstNFound+ shows strong generalization to the
prospective data, with no performance degradation compared to retrospective
evaluation. It aligns closely with clinical scores and produces interpretable
heatmaps consistent with biopsy-confirmed lesions. Conclusion: The results
highlight its potential for clinical deployment, offering a scalable and
interpretable alternative to expert-driven protocols.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Impact and Outlook of 3D Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26694v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26694v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bernhard Kerbl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since its introduction, 3D Gaussian Splatting (3DGS) has rapidly transformed
the landscape of 3D scene representations, inspiring an extensive body of
associated research. Follow-up work includes analyses and contributions that
enhance the efficiency, scalability, and real-world applicability of 3DGS. In
this summary, we present an overview of several key directions that have
emerged in the wake of 3DGS. We highlight advances enabling resource-efficient
training and rendering, the evolution toward dynamic (or four-dimensional,
4DGS) representations, and deeper exploration of the mathematical foundations
underlying its appearance modeling and rendering process. Furthermore, we
examine efforts to bring 3DGS to mobile and virtual reality platforms, its
extension to massive-scale environments, and recent progress toward
near-instant radiance field reconstruction via feed-forward or distributed
computation. Collectively, these developments illustrate how 3DGS has evolved
from a breakthrough representation into a versatile and foundational tool for
3D vision and graphics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Article written for Frontiers of Science Award, International
  Congress on Basic Science, 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Process Integrated Computer Vision for Real-Time Failure Prediction in
  Steel Rolling Mill 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26684v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26684v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vaibhav Kurrey, Sivakalyan Pujari, Gagan Raj Gupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a long-term deployment study of a machine vision-based anomaly
detection system for failure prediction in a steel rolling mill. The system
integrates industrial cameras to monitor equipment operation, alignment, and
hot bar motion in real time along the process line. Live video streams are
processed on a centralized video server using deep learning models, enabling
early prediction of equipment failures and process interruptions, thereby
reducing unplanned breakdown costs. Server-based inference minimizes the
computational load on industrial process control systems (PLCs), supporting
scalable deployment across production lines with minimal additional resources.
By jointly analyzing sensor data from data acquisition systems and visual
inputs, the system identifies the location and probable root causes of
failures, providing actionable insights for proactive maintenance. This
integrated approach enhances operational reliability, productivity, and
profitability in industrial manufacturing environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Classification of Occluded Objects through Scene Context 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26681v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26681v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Courtney M. King, Daniel D. Leeds, Damian Lyons, George Kalaitzis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The presence of occlusions has provided substantial challenges to
typically-powerful object recognition algorithms. Additional sources of
information can be extremely valuable to reduce errors caused by occlusions.
Scene context is known to aid in object recognition in biological vision. In
this work, we attempt to add robustness into existing Region Proposal
Network-Deep Convolutional Neural Network (RPN-DCNN) object detection networks
through two distinct scene-based information fusion techniques. We present one
algorithm under each methodology: the first operates prior to prediction,
selecting a custom object network to use based on the identified background
scene, and the second operates after detection, fusing scene knowledge into
initial object scores output by the RPN. We demonstrate our algorithms on
challenging datasets featuring partial occlusions, which show overall
improvement in both recall and precision against baseline methods. In addition,
our experiments contrast multiple training methodologies for occlusion
handling, finding that training on a combination of both occluded and
unoccluded images demonstrates an improvement over the others. Our method is
interpretable and can easily be adapted to other datasets, offering many future
directions for research and practical applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BRIQA: Balanced Reweighting in Image Quality Assessment of Pediatric
  Brain MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26661v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26661v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alya Almsouti, Ainur Khamitova, Darya Taratynova, Mohammad Yaqub
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Assessing the severity of artifacts in pediatric brain Magnetic Resonance
Imaging (MRI) is critical for diagnostic accuracy, especially in low-field
systems where the signal-to-noise ratio is reduced. Manual quality assessment
is time-consuming and subjective, motivating the need for robust automated
solutions. In this work, we propose BRIQA (Balanced Reweighting in Image
Quality Assessment), which addresses class imbalance in artifact severity
levels. BRIQA uses gradient-based loss reweighting to dynamically adjust
per-class contributions and employs a rotating batching scheme to ensure
consistent exposure to underrepresented classes. Through experiments, no single
architecture performs best across all artifact types, emphasizing the
importance of architectural diversity. The rotating batching configuration
improves performance across metrics by promoting balanced learning when
combined with cross-entropy loss. BRIQA improves average macro F1 score from
0.659 to 0.706, with notable gains in Noise (0.430), Zipper (0.098),
Positioning (0.097), Contrast (0.217), Motion (0.022), and Banding (0.012)
artifact severity classification. The code is available at
https://github.com/BioMedIA-MBZUAI/BRIQA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Reliable Sea Ice Drift Estimation in the Arctic Deep Learning
  Optical Flow on RADARSAT-2 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26653v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26653v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniela Martin, Joseph Gallego
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate estimation of sea ice drift is critical for Arctic navigation,
climate research, and operational forecasting. While optical flow, a computer
vision technique for estimating pixel wise motion between consecutive images,
has advanced rapidly in computer vision, its applicability to geophysical
problems and to satellite SAR imagery remains underexplored. Classical optical
flow methods rely on mathematical models and strong assumptions about motion,
which limit their accuracy in complex scenarios. Recent deep learning based
approaches have substantially improved performance and are now the standard in
computer vision, motivating their application to sea ice drift estimation. We
present the first large scale benchmark of 48 deep learning optical flow models
on RADARSAT 2 ScanSAR sea ice imagery, evaluated with endpoint error (EPE) and
Fl all metrics against GNSS tracked buoys. Several models achieve sub kilometer
accuracy (EPE 6 to 8 pixels, 300 to 400 m), a small error relative to the
spatial scales of sea ice motion and typical navigation requirements in the
Arctic. Our results demonstrate that the models are capable of capturing
consistent regional drift patterns and that recent deep learning based optical
flow methods, which have substantially improved motion estimation accuracy
compared to classical methods, can be effectively transferred to polar remote
sensing. Optical flow produces spatially continuous drift fields, providing
motion estimates for every image pixel rather than at sparse buoy locations,
offering new opportunities for navigation and climate modeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ All You Need for Object Detection: From Pixels, Points, and <span class="highlight-title">Prompt</span>s to
  Next-Gen Fusion and Multimodal LLMs/VLMs in Autonomous Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26641v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26641v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sayed Pedram Haeri Boroujeni, Niloufar Mehrabi, Hazim Alzorgan, Ahmad Sarlak, Mahlagha Fazeli, Abolfazl Razi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous Vehicles (AVs) are transforming the future of transportation
through advances in intelligent perception, decision-making, and control
systems. However, their success is tied to one core capability, reliable object
detection in complex and multimodal environments. While recent breakthroughs in
Computer Vision (CV) and Artificial Intelligence (AI) have driven remarkable
progress, the field still faces a critical challenge as knowledge remains
fragmented across multimodal perception, contextual reasoning, and cooperative
intelligence. This survey bridges that gap by delivering a forward-looking
analysis of object detection in AVs, emphasizing emerging paradigms such as
Vision-Language Models (VLMs), Large Language Models (LLMs), and Generative AI
rather than re-examining outdated techniques. We begin by systematically
reviewing the fundamental spectrum of AV sensors (camera, ultrasonic, LiDAR,
and Radar) and their fusion strategies, highlighting not only their
capabilities and limitations in dynamic driving environments but also their
potential to integrate with recent advances in LLM/VLM-driven perception
frameworks. Next, we introduce a structured categorization of AV datasets that
moves beyond simple collections, positioning ego-vehicle, infrastructure-based,
and cooperative datasets (e.g., V2V, V2I, V2X, I2I), followed by a
cross-analysis of data structures and characteristics. Ultimately, we analyze
cutting-edge detection methodologies, ranging from 2D and 3D pipelines to
hybrid sensor fusion, with particular attention to emerging transformer-driven
approaches powered by Vision Transformers (ViTs), Large and Small Language
Models (SLMs), and VLMs. By synthesizing these perspectives, our survey
delivers a clear roadmap of current capabilities, open challenges, and future
opportunities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SAMRI: Segment Anything Model for MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26635v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26635v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhao Wang, Wei Dai, Thuy Thanh Dao, Steffen Bollmann, Hongfu Sun, Craig Engstrom, Shekhar S. Chandra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate magnetic resonance imaging (MRI) segmentation is crucial for
clinical decision-making, but remains labor-intensive when performed manually.
Convolutional neural network (CNN)-based methods can be accurate and efficient,
but often generalize poorly to MRI's variable contrast, intensity
inhomogeneity, and protocols. Although the transformer-based Segment Anything
Model (SAM) has demonstrated remarkable generalizability in natural images,
existing adaptations often treat MRI as another imaging modality, overlooking
these modality-specific challenges. We present SAMRI, an MRI-specialized SAM
trained and validated on 1.1 million labeled MR slices spanning whole-body
organs and pathologies. We demonstrate that SAM can be effectively adapted to
MRI by simply fine-tuning its mask decoder using a two-stage strategy, reducing
training time by 94% and trainable parameters by 96% versus full-model
retraining. Across diverse MRI segmentation tasks, SAMRI achieves a mean Dice
of 0.87, delivering state-of-the-art accuracy across anatomical regions and
robust generalization on unseen structures, particularly small and clinically
important structures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PT-DETR: Small Target Detection Based on Partially-Aware Detail Focus 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26630v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26630v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bingcong Huo, Zhiming Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To address the challenges in UAV object detection, such as complex
backgrounds, severe occlusion, dense small objects, and varying lighting
conditions,this paper proposes PT-DETR based on RT-DETR, a novel detection
algorithm specifically designed for small objects in UAV imagery. In the
backbone network, we introduce the Partially-Aware Detail Focus (PADF) Module
to enhance feature extraction for small objects. Additionally,we design the
Median-Frequency Feature Fusion (MFFF) module,which effectively improves the
model's ability to capture small-object details and contextual information.
Furthermore,we incorporate Focaler-SIoU to strengthen the model's bounding box
matching capability and increase its sensitivity to small-object features,
thereby further enhancing detection accuracy and robustness. Compared with
RT-DETR, our PT-DETR achieves mAP improvements of 1.6% and 1.7% on the
VisDrone2019 dataset with lower computational complexity and fewer parameters,
demonstrating its robustness and feasibility for small-object detection tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spiking Patches: Asynchronous, Sparse, and Efficient Tokens for Event
  Cameras 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26614v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26614v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christoffer Koo Øhrstrøm, Ronja Güldenring, Lazaros Nalpantidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose tokenization of events and present a tokenizer, Spiking Patches,
specifically designed for event cameras. Given a stream of asynchronous and
spatially sparse events, our goal is to discover an event representation that
preserves these properties. Prior works have represented events as frames or as
voxels. However, while these representations yield high accuracy, both frames
and voxels are synchronous and decrease the spatial sparsity. Spiking Patches
gives the means to preserve the unique properties of event cameras and we show
in our experiments that this comes without sacrificing accuracy. We evaluate
our tokenizer using a GNN, PCN, and a Transformer on gesture recognition and
object detection. Tokens from Spiking Patches yield inference times that are up
to 3.4x faster than voxel-based tokens and up to 10.4x faster than frames. We
achieve this while matching their accuracy and even surpassing in some cases
with absolute improvements up to 3.8 for gesture recognition and up to 1.4 for
object detection. Thus, tokenization constitutes a novel direction in
event-based vision and marks a step towards methods that preserve the
properties of event cameras.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CYPRESS: Crop Yield Prediction via Regression on Prithvi's Encoder for
  Satellite Sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26609v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26609v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shayan Nejadshamsi, Yuanyuan Zhang, Shadi Zaki, Brock Porth, Lysa Porth, Vahab Khoshdel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate and timely crop yield prediction is crucial for global food security
and modern agricultural management. Traditional methods often lack the
scalability and granularity required for precision farming. This paper
introduces CYPRESS (Crop Yield Prediction via Regression on Prithvi's Encoder
for Satellite Sensing), a deep learning model designed for high-resolution,
intra-field canola yield prediction. CYPRESS leverages a pre-trained,
large-scale geospatial foundation model (Prithvi-EO-2.0-600M) and adapts it for
a continuous regression task, transforming multi-temporal satellite imagery
into dense, pixel-level yield maps. Evaluated on a comprehensive dataset from
the Canadian Prairies, CYPRESS demonstrates superior performance over existing
deep learning-based yield prediction models, highlighting the effectiveness of
fine-tuning foundation models for specialized agricultural applications. By
providing a continuous, high-resolution output, CYPRESS offers a more
actionable tool for precision agriculture than conventional classification or
county-level aggregation methods. This work validates a novel approach that
bridges the gap between large-scale Earth observation and on-farm
decision-making, offering a scalable solution for detailed agricultural
monitoring.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ResMatching: Noise-Resilient Computational Super-Resolution via Guided
  Conditional Flow Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26601v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26601v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anirban Ray, Vera Galinova, Florian Jug
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computational Super-Resolution (CSR) in fluorescence microscopy has, despite
being an ill-posed problem, a long history. At its very core, CSR is about
finding a prior that can be used to extrapolate frequencies in a micrograph
that have never been imaged by the image-generating microscope. It stands to
reason that, with the advent of better data-driven machine learning techniques,
stronger prior can be learned and hence CSR can lead to better results. Here,
we present ResMatching, a novel CSR method that uses guided conditional flow
matching to learn such improved data-priors. We evaluate ResMatching on 4
diverse biological structures from the BioSR dataset and compare its results
against 7 baselines. ResMatching consistently achieves competitive results,
demonstrating in all cases the best trade-off between data fidelity and
perceptual realism. We observe that CSR using ResMatching is particularly
effective in cases where a strong prior is hard to learn, e.g. when the given
low-resolution images contain a lot of noise. Additionally, we show that
ResMatching can be used to sample from an implicitly learned posterior
distribution and that this distribution is calibrated for all tested use-cases,
enabling our method to deliver a pixel-wise data-uncertainty term that can
guide future users to reject uncertain predictions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Emu3.5: Native Multimodal Models are World Learners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26583v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26583v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yufeng Cui, Honghao Chen, Haoge Deng, Xu Huang, Xinghang Li, Jirong Liu, Yang Liu, Zhuoyan Luo, Jinsheng Wang, Wenxuan Wang, Yueze Wang, Chengyuan Wang, Fan Zhang, Yingli Zhao, Ting Pan, Xianduo Li, Zecheng Hao, Wenxuan Ma, Zhuo Chen, Yulong Ao, Tiejun Huang, Zhongyuan Wang, Xinlong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Emu3.5, a large-scale multimodal world model that natively
predicts the next state across vision and language. Emu3.5 is pre-trained
end-to-end with a unified next-token prediction objective on a corpus of
vision-language interleaved data containing over 10 trillion tokens, primarily
derived from sequential frames and transcripts of internet videos. The model
naturally accepts interleaved vision-language inputs and generates interleaved
vision-language outputs. Emu3.5 is further post-trained with large-scale
reinforcement learning to enhance multimodal reasoning and generation. To
improve inference efficiency, we propose Discrete Diffusion Adaptation (DiDA),
which converts token-by-token decoding into bidirectional parallel prediction,
accelerating per-image inference by about 20x without sacrificing performance.
Emu3.5 exhibits strong native multimodal capabilities, including long-horizon
vision-language generation, any-to-image (X2I) generation, and complex
text-rich image generation. It also exhibits generalizable world-modeling
abilities, enabling spatiotemporally consistent world exploration and
open-world embodied manipulation across diverse scenarios and tasks. For
comparison, Emu3.5 achieves performance comparable to Gemini 2.5 Flash Image
(Nano Banana) on image generation and editing tasks and demonstrates superior
results on a suite of interleaved generation tasks. We open-source Emu3.5 at
https://github.com/baaivision/Emu3.5 to support community research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>project page: https://emu.world</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CATCH: A Modular Cross-domain Adaptive Template with Hook 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26582v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26582v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinjin Li, Yulie Lu, Jinghan Cao, Yu Ma, Zhenglin Li, Yeyang Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Visual Question Answering (VQA) have demonstrated
impressive performance in natural image domains, with models like LLaVA
leveraging large language models (LLMs) for open-ended reasoning. However,
their generalization degrades significantly when transferred to out-of-domain
scenarios such as remote sensing, medical imaging, or math diagrams, due to
large distributional shifts and the lack of effective domain adaptation
mechanisms. Existing approaches typically rely on per-domain fine-tuning or
bespoke pipelines, which are costly, inflexible, and not scalable across
diverse tasks. In this paper, we propose CATCH, a plug-and-play framework for
cross-domain adaptation that improves the generalization of VQA models while
requiring minimal changes to their core architecture. Our key idea is to
decouple visual and linguistic adaptation by introducing two lightweight
modules: a domain classifier to identify the input image type, and a dual
adapter mechanism comprising a Prompt Adapter for language modulation and a
Visual Adapter for vision feature adjustment. Both modules are dynamically
injected via a unified hook interface, requiring no retraining of the backbone
model. Experimental results across four domain-specific VQA benchmarks
demonstrate that our framework achieves consistent performance gains without
retraining the backbone model, including +2.3 BLEU on MathVQA, +2.6 VQA on
MedVQA-RAD, and +3.1 ROUGE on ChartQA. These results highlight that CATCH
provides a scalable and extensible approach to multi-domain VQA, enabling
practical deployment across diverse application domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Context-Aware Scene Reasoning Using Vision-Language Alignment in
  Zero-Shot Real-World Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26580v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26580v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manjunath Prasad Holenarasipura Rajiv, B. M. Vidyavathi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world environments, AI systems often face unfamiliar scenarios
without labeled data, creating a major challenge for conventional scene
understanding models. The inability to generalize across unseen contexts limits
the deployment of vision-based applications in dynamic, unstructured settings.
This work introduces a Dynamic Context-Aware Scene Reasoning framework that
leverages Vision-Language Alignment to address zero-shot real-world scenarios.
The goal is to enable intelligent systems to infer and adapt to new
environments without prior task-specific training. The proposed approach
integrates pre-trained vision transformers and large language models to align
visual semantics with natural language descriptions, enhancing contextual
comprehension. A dynamic reasoning module refines predictions by combining
global scene cues and object-level interactions guided by linguistic priors.
Extensive experiments on zero-shot benchmarks such as COCO, Visual Genome, and
Open Images demonstrate up to 18% improvement in scene understanding accuracy
over baseline models in complex and unseen environments. Results also show
robust performance in ambiguous or cluttered scenes due to the synergistic
fusion of vision and language. This framework offers a scalable and
interpretable approach for context-aware reasoning, advancing zero-shot
generalization in dynamic real-world settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint under review at IEEE Transactions on Pattern Analysis and
  Machine Intelligence (TPAMI), 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparative Analysis of Deep Learning Models for Olive Tree Crown and
  Shadow Segmentation Towards Biovolume Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26573v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26573v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wondimagegn Abebe Demissie, Stefano Roccella, Rudy Rossetto, Antonio Minnocci, Andrea Vannini, Luca Sebastiani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Olive tree biovolume estimation is a key task in precision agriculture,
supporting yield prediction and resource management, especially in
Mediterranean regions severely impacted by climate-induced stress. This study
presents a comparative analysis of three deep learning models U-Net,
YOLOv11m-seg, and Mask RCNN for segmenting olive tree crowns and their shadows
in ultra-high resolution UAV imagery. The UAV dataset, acquired over
Vicopisano, Italy, includes manually annotated crown and shadow masks. Building
on these annotations, the methodology emphasizes spatial feature extraction and
robust segmentation; per-tree biovolume is then estimated by combining crown
projected area with shadow-derived height using solar geometry. In testing,
Mask R-CNN achieved the best overall accuracy (F1 = 0.86; mIoU = 0.72), while
YOLOv11m-seg provided the fastest throughput (0.12 second per image). The
estimated biovolumes spanned from approximately 4 to 24 cubic meters,
reflecting clear structural differences among trees. These results indicate
Mask R-CNN is preferable when biovolume accuracy is paramount, whereas
YOLOv11m-seg suits large-area deployments where speed is critical; U-Net
remains a lightweight, high-sensitivity option. The framework enables accurate,
scalable orchard monitoring and can be further strengthened with DEM or DSM
integration and field calibration for operational decision support.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2025 IEEE International Workshop on Metrology for
  Agriculture and Forestry (MetroAgriFor)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AdSum: Two-stream Audio-visual Summarization for Automated Video
  Advertisement Clipping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26569v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26569v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wen Xie, Yanjun Zhu, Gijs Overgoor, Yakov Bart, Agata Lapedriza Garcia, Sarah Ostadabbas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advertisers commonly need multiple versions of the same advertisement (ad) at
varying durations for a single campaign. The traditional approach involves
manually selecting and re-editing shots from longer video ads to create shorter
versions, which is labor-intensive and time-consuming. In this paper, we
introduce a framework for automated video ad clipping using video summarization
techniques. We are the first to frame video clipping as a shot selection
problem, tailored specifically for advertising. Unlike existing general video
summarization methods that primarily focus on visual content, our approach
emphasizes the critical role of audio in advertising. To achieve this, we
develop a two-stream audio-visual fusion model that predicts the importance of
video frames, where importance is defined as the likelihood of a frame being
selected in the firm-produced short ad. To address the lack of ad-specific
datasets, we present AdSum204, a novel dataset comprising 102 pairs of
30-second and 15-second ads from real advertising campaigns. Extensive
experiments demonstrate that our model outperforms state-of-the-art methods
across various metrics, including Average Precision, Area Under Curve,
Spearman, and Kendall.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 32nd International Conference on MultiMedia Modeling</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SA$^{2}$Net: Scale-Adaptive Structure-Affinity Transformation for Spine
  Segmentation from Ultrasound Volume Projection Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26568v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26568v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Xie, Zixun Huang, Yushen Zuo, Yakun Ju, Frank H. F. Leung, N. F. Law, Kin-Man Lam, Yong-Ping Zheng, Sai Ho Ling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spine segmentation, based on ultrasound volume projection imaging (VPI),
plays a vital role for intelligent scoliosis diagnosis in clinical
applications. However, this task faces several significant challenges. Firstly,
the global contextual knowledge of spines may not be well-learned if we neglect
the high spatial correlation of different bone features. Secondly, the spine
bones contain rich structural knowledge regarding their shapes and positions,
which deserves to be encoded into the segmentation process. To address these
challenges, we propose a novel scale-adaptive structure-aware network
(SA$^{2}$Net) for effective spine segmentation. First, we propose a
scale-adaptive complementary strategy to learn the cross-dimensional
long-distance correlation features for spinal images. Second, motivated by the
consistency between multi-head self-attention in Transformers and semantic
level affinity, we propose structure-affinity transformation to transform
semantic features with class-specific affinity and combine it with a
Transformer decoder for structure-aware reasoning. In addition, we adopt a
feature mixing loss aggregation method to enhance model training. This method
improves the robustness and accuracy of the segmentation process. The
experimental results demonstrate that our SA$^{2}$Net achieves superior
segmentation performance compared to other state-of-the-art methods. Moreover,
the adaptability of SA$^{2}$Net to various backbones enhances its potential as
a promising tool for advanced scoliosis diagnosis using intelligent spinal
image analysis. The code and experimental demo are available at
https://github.com/taetiseo09/SA2Net.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Computerized Medical Imaging and Graphics (CMIG)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analysis of the Robustness of an Edge Detector Based on Cellular
  Automata Optimized by Particle Swarm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26509v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26509v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vinícius Ferraria, Eurico Ruivo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The edge detection task is essential in image processing aiming to extract
relevant information from an image. One recurring problem in this task is the
weaknesses found in some detectors, such as the difficulty in detecting loose
edges and the lack of context to extract relevant information from specific
problems. To address these weaknesses and adapt the detector to the properties
of an image, an adaptable detector described by two-dimensional cellular
automaton and optimized by meta-heuristic combined with transfer learning
techniques was developed. This study aims to analyze the impact of expanding
the search space of the optimization phase and the robustness of the
adaptability of the detector in identifying edges of a set of natural images
and specialized subsets extracted from the same image set. The results obtained
prove that expanding the search space of the optimization phase was not
effective for the chosen image set. The study also analyzed the adaptability of
the model through a series of experiments and validation techniques and found
that, regardless of the validation, the model was able to adapt to the input
and the transfer learning techniques applied to the model showed no significant
improvements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Counteracting Matthew Effect in Self-Improvement of LVLMs through
  Head-Tail Re-balancing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26474v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26474v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Guo, Zhiheng Xi, Yiwen Ding, Yitao Zhai, Xiaowei Shi, Xunliang Cai, Tao Gui, Qi Zhang, Xuanjing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-improvement has emerged as a mainstream paradigm for advancing the
reasoning capabilities of large vision-language models (LVLMs), where models
explore and learn from successful trajectories iteratively. However, we
identify a critical issue during this process: the model excels at generating
high-quality trajectories for simple queries (i.e., head data) but struggles
with more complex ones (i.e., tail data). This leads to an imbalanced
optimization that drives the model to prioritize simple reasoning skills, while
hindering its ability to tackle more complex reasoning tasks. Over iterations,
this imbalance becomes increasingly pronounced--a dynamic we term the "Matthew
effect"--which ultimately hinders further model improvement and leads to
performance bottlenecks. To counteract this challenge, we introduce four
efficient strategies from two perspectives: distribution-reshaping and
trajectory-resampling, to achieve head-tail re-balancing during the
exploration-and-learning self-improvement process. Extensive experiments on
Qwen2-VL-7B-Instruct and InternVL2.5-4B models across visual reasoning tasks
demonstrate that our methods consistently improve visual reasoning
capabilities, outperforming vanilla self-improvement by 3.86 points on average.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Representation-Level Counterfactual Calibration for Debiased Zero-Shot
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26466v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26466v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pei Peng, MingKun Xie, Hang Hao, Tong Jin, ShengJun Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object-context shortcuts remain a persistent challenge in vision-language
models, undermining zero-shot reliability when test-time scenes differ from
familiar training co-occurrences. We recast this issue as a causal inference
problem and ask: Would the prediction remain if the object appeared in a
different environment? To answer this at inference time, we estimate object and
background expectations within CLIP's representation space, and synthesize
counterfactual embeddings by recombining object features with diverse
alternative contexts sampled from external datasets, batch neighbors, or
text-derived descriptions. By estimating the Total Direct Effect and simulating
intervention, we further subtract background-only activation, preserving
beneficial object-context interactions while mitigating hallucinated scores.
Without retraining or prompt design, our method substantially improves both
worst-group and average accuracy on context-sensitive benchmarks, establishing
a new zero-shot state of the art. Beyond performance, our framework provides a
lightweight representation-level counterfactual approach, offering a practical
causal avenue for debiased and reliable multimodal reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Fine-Grained Vision-Language Alignment for Few-Shot Anomaly
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26464v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26464v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanting Fan, Jun Liu, Xiaochen Chen, Bin-Bin Gao, Jian Li, Yong Liu, Jinlong Peng, Chengjie Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot anomaly detection (FSAD) methods identify anomalous regions with few
known normal samples. Most existing methods rely on the generalization ability
of pre-trained vision-language models (VLMs) to recognize potentially anomalous
regions through feature similarity between text descriptions and images.
However, due to the lack of detailed textual descriptions, these methods can
only pre-define image-level descriptions to match each visual patch token to
identify potential anomalous regions, which leads to the semantic misalignment
between image descriptions and patch-level visual anomalies, achieving
sub-optimal localization performance. To address the above issues, we propose
the Multi-Level Fine-Grained Semantic Caption (MFSC) to provide multi-level and
fine-grained textual descriptions for existing anomaly detection datasets with
automatic construction pipeline. Based on the MFSC, we propose a novel
framework named FineGrainedAD to improve anomaly localization performance,
which consists of two components: Multi-Level Learnable Prompt (MLLP) and
Multi-Level Semantic Alignment (MLSA). MLLP introduces fine-grained semantics
into multi-level learnable prompts through automatic replacement and
concatenation mechanism, while MLSA designs region aggregation strategy and
multi-level alignment training to facilitate learnable prompts better align
with corresponding visual regions. Experiments demonstrate that the proposed
FineGrainedAD achieves superior overall performance in few-shot settings on
MVTec-AD and VisA datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PointSt3R: Point Tracking through 3D Grounded Correspondence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26443v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26443v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rhodri Guerrier, Adam W. Harley, Dima Damen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in foundational 3D reconstruction models, such as DUSt3R and
MASt3R, have shown great potential in 2D and 3D correspondence in static
scenes. In this paper, we propose to adapt them for the task of point tracking
through 3D grounded correspondence. We first demonstrate that these models are
competitive point trackers when focusing on static points, present in current
point tracking benchmarks ($+33.5\%$ on EgoPoints vs. CoTracker2). We propose
to combine the reconstruction loss with training for dynamic correspondence
along with a visibility head, and fine-tuning MASt3R for point tracking using a
relatively small amount of synthetic data. Importantly, we only train and
evaluate on pairs of frames where one contains the query point, effectively
removing any temporal context. Using a mix of dynamic and static point
correspondences, we achieve competitive or superior point tracking results on
four datasets (e.g. competitive on TAP-Vid-DAVIS 73.8 $\delta_{avg}$ / 85.8\%
occlusion acc. for PointSt3R compared to 75.7 / 88.3\% for CoTracker2; and
significantly outperform CoTracker3 on EgoPoints 61.3 vs 54.2 and RGB-S 87.0 vs
82.8). We also present results on 3D point tracking along with several
ablations on training datasets and percentage of dynamic correspondences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>http://rhodriguerrier.github.io/PointSt3R</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A-TPT: Angular Diversity Calibration Properties for Test-Time <span class="highlight-title">Prompt</span>
  Tuning of Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26441v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26441v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shihab Aaqil Ahamed, Udaya S. K. P. Miriya Thanthrige, Ranga Rodrigo, Muhammad Haris Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Test-time prompt tuning (TPT) has emerged as a promising technique for
adapting large vision-language models (VLMs) to unseen tasks without relying on
labeled data. However, the lack of dispersion between textual features can hurt
calibration performance, which raises concerns about VLMs' reliability,
trustworthiness, and safety. Current TPT approaches primarily focus on
improving prompt calibration by either maximizing average textual feature
dispersion or enforcing orthogonality constraints to encourage angular
separation. However, these methods may not always have optimal angular
separation between class-wise textual features, which implies overlooking the
critical role of angular diversity. To address this, we propose A-TPT, a novel
TPT framework that introduces angular diversity to encourage uniformity in the
distribution of normalized textual features induced by corresponding learnable
prompts. This uniformity is achieved by maximizing the minimum pairwise angular
distance between features on the unit hypersphere. We show that our approach
consistently surpasses state-of-the-art TPT methods in reducing the aggregate
average calibration error while maintaining comparable accuracy through
extensive experiments with various backbones on different datasets. Notably,
our approach exhibits superior zero-shot calibration performance on natural
distribution shifts and generalizes well to medical datasets. We provide
extensive analyses, including theoretical aspects, to establish the grounding
of A-TPT. These results highlight the potency of promoting angular diversity to
achieve well-dispersed textual features, significantly improving VLM
calibration during test-time adaptation. Our code will be made publicly
available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LoCoT2V-Bench: A Benchmark for Long-Form and Complex Text-to-Video
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26412v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26412v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangqing Zheng, Chengyue Wu, Kehai Chen, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently text-to-video generation has made impressive progress in producing
short, high-quality clips, but evaluating long-form outputs remains a major
challenge especially when processing complex prompts. Existing benchmarks
mostly rely on simplified prompts and focus on low-level metrics, overlooking
fine-grained alignment with prompts and abstract dimensions such as narrative
coherence and thematic expression. To address these gaps, we propose
LoCoT2V-Bench, a benchmark specifically designed for long video generation
(LVG) under complex input conditions. Based on various real-world videos,
LoCoT2V-Bench introduces a suite of realistic and complex prompts incorporating
elements like scene transitions and event dynamics. Moreover, it constructs a
multi-dimensional evaluation framework that includes our newly proposed metrics
such as event-level alignment, fine-grained temporal consistency, content
clarity, and the Human Expectation Realization Degree (HERD) that focuses on
more abstract attributes like narrative flow, emotional response, and character
development. Using this framework, we conduct a comprehensive evaluation of
nine representative LVG models, finding that while current methods perform well
on basic visual and temporal aspects, they struggle with inter-event
consistency, fine-grained alignment, and high-level thematic adherence, etc.
Overall, LoCoT2V-Bench provides a comprehensive and reliable platform for
evaluating long-form complex text-to-video generation and highlights critical
directions for future method improvement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EEG-Driven Image Reconstruction with Saliency-Guided Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26391v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26391v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Igor Abramov, Ilya Makarov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing EEG-driven image reconstruction methods often overlook spatial
attention mechanisms, limiting fidelity and semantic coherence. To address
this, we propose a dual-conditioning framework that combines EEG embeddings
with spatial saliency maps to enhance image generation. Our approach leverages
the Adaptive Thinking Mapper (ATM) for EEG feature extraction and fine-tunes
Stable Diffusion 2.1 via Low-Rank Adaptation (LoRA) to align neural signals
with visual semantics, while a ControlNet branch conditions generation on
saliency maps for spatial control. Evaluated on THINGS-EEG, our method achieves
a significant improvement in the quality of low- and high-level image features
over existing approaches. Simultaneously, strongly aligning with human visual
attention. The results demonstrate that attentional priors resolve EEG
ambiguities, enabling high-fidelity reconstructions with applications in
medical diagnostics and neuroadaptive interfaces, advancing neural decoding
through efficient adaptation of pre-trained diffusion models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Demo paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SPG-CDENet: Spatial Prior-Guided Cross Dual Encoder Network for
  Multi-Organ Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26390v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26390v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xizhi Tian, Changjun Zhou, Yulin. Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-organ segmentation is a critical task in computer-aided diagnosis.
While recent deep learning methods have achieved remarkable success in image
segmentation, huge variations in organ size and shape challenge their
effectiveness in multi-organ segmentation. To address these challenges, we
propose a Spatial Prior-Guided Cross Dual Encoder Network (SPG-CDENet), a novel
two-stage segmentation paradigm designed to improve multi-organ segmentation
accuracy. Our SPG-CDENet consists of two key components: a spatial prior
network and a cross dual encoder network. The prior network generates coarse
localization maps that delineate the approximate ROI, serving as spatial
guidance for the dual encoder network. The cross dual encoder network comprises
four essential components: a global encoder, a local encoder, a symmetric
cross-attention module, and a flow-based decoder. The global encoder captures
global semantic features from the entire image, while the local encoder focuses
on features from the prior network. To enhance the interaction between the
global and local encoders, a symmetric cross-attention module is proposed
across all layers of the encoders to fuse and refine features. Furthermore, the
flow-based decoder directly propagates high-level semantic features from the
final encoder layer to all decoder layers, maximizing feature preservation and
utilization. Extensive qualitative and quantitative experiments on two public
datasets demonstrate the superior performance of SPG-CDENet compared to
existing segmentation methods. Furthermore, ablation studies further validate
the effectiveness of the proposed modules in improving segmentation accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CorVS: Person Identification via Video Trajectory-Sensor Correspondence
  in a Real-World Warehouse 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26369v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26369v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kazuma Kano, Yuki Mori, Shin Katayama, Kenta Urano, Takuro Yonezawa, Nobuo Kawaguchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Worker location data is key to higher productivity in industrial sites.
Cameras are a promising tool for localization in logistics warehouses since
they also offer valuable environmental contexts such as package status.
However, identifying individuals with only visual data is often impractical.
Accordingly, several prior studies identified people in videos by comparing
their trajectories and wearable sensor measurements. While this approach has
advantages such as independence from appearance, the existing methods may break
down under real-world conditions. To overcome this challenge, we propose CorVS,
a novel data-driven person identification method based on correspondence
between visual tracking trajectories and sensor measurements. Firstly, our deep
learning model predicts correspondence probabilities and reliabilities for
every pair of a trajectory and sensor measurements. Secondly, our algorithm
matches the trajectories and sensor measurements over time using the predicted
probabilities and reliabilities. We developed a dataset with actual warehouse
operations and demonstrated the method's effectiveness for real-world
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 3 figures, accepted to IPIN 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AgriGS-SLAM: Orchard Mapping Across Seasons via Multi-View Gaussian
  Splatting SLAM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26358v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26358v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mirko Usuelli, David Rapado-Rincon, Gert Kootstra, Matteo Matteucci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous robots in orchards require real-time 3D scene understanding
despite repetitive row geometry, seasonal appearance changes, and wind-driven
foliage motion. We present AgriGS-SLAM, a Visual--LiDAR SLAM framework that
couples direct LiDAR odometry and loop closures with multi-camera 3D Gaussian
Splatting (3DGS) rendering. Batch rasterization across complementary viewpoints
recovers orchard structure under occlusions, while a unified gradient-driven
map lifecycle executed between keyframes preserves fine details and bounds
memory. Pose refinement is guided by a probabilistic LiDAR-based depth
consistency term, back-propagated through the camera projection to tighten
geometry-appearance coupling. We deploy the system on a field platform in apple
and pear orchards across dormancy, flowering, and harvesting, using a
standardized trajectory protocol that evaluates both training-view and
novel-view synthesis to reduce 3DGS overfitting in evaluation. Across seasons
and sites, AgriGS-SLAM delivers sharper, more stable reconstructions and
steadier trajectories than recent state-of-the-art 3DGS-SLAM baselines while
maintaining real-time performance on-tractor. While demonstrated in orchard
monitoring, the approach can be applied to other outdoor domains requiring
robust multimodal perception.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GLYPH-SR: Can We Achieve Both High-Quality Image Super-Resolution and
  High-Fidelity Text Recovery via VLM-guided Latent Diffusion Model? <span class="chip">ICLR 2026</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26339v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26339v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyu Sung, Seungjae Ham, Kangwoo Kim, Yeokyoung Yoon, Sangseok Yun, Il-Min Kim, Jae-Mo Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image super-resolution(SR) is fundamental to many vision system-from
surveillance and autonomy to document analysis and retail analytics-because
recovering high-frequency details, especially scene-text, enables reliable
downstream perception. Scene-text, i.e., text embedded in natural images such
as signs, product labels, and storefronts, often carries the most actionable
information; when characters are blurred or hallucinated, optical character
recognition(OCR) and subsequent decisions fail even if the rest of the image
appears sharp. Yet previous SR research has often been tuned to distortion
(PSNR/SSIM) or learned perceptual metrics (LIPIS, MANIQA, CLIP-IQA, MUSIQ) that
are largely insensitive to character-level errors. Furthermore, studies that do
address text SR often focus on simplified benchmarks with isolated characters,
overlooking the challenges of text within complex natural scenes. As a result,
scene-text is effectively treated as generic texture. For SR to be effective in
practical deployments, it is therefore essential to explicitly optimize for
both text legibility and perceptual quality. We present GLYPH-SR, a
vision-language-guided diffusion framework that aims to achieve both objectives
jointly. GLYPH-SR utilizes a Text-SR Fusion ControlNet(TS-ControlNet) guided by
OCR data, and a ping-pong scheduler that alternates between text- and
scene-centric guidance. To enable targeted text restoration, we train these
components on a synthetic corpus while keeping the main SR branch frozen.
Across SVT, SCUT-CTW1500, and CUTE80 at x4, and x8, GLYPH-SR improves OCR F1 by
up to +15.18 percentage points over diffusion/GAN baseline (SVT x8, OpenOCR)
while maintaining competitive MANIQA, CLIP-IQA, and MUSIQ. GLYPH-SR is designed
to satisfy both objectives simultaneously-high readability and high visual
realism-delivering SR that looks right and reds right.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 figures. Includes supplementary material. Under review as
  a conference paper at ICLR 2026</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Hybrid Framework Bridging CNN and ViT based on Theory of Evidence for
  Diabetic Retinopathy Grading 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26315v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26315v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junlai Qiu, Yunzhu Chen, Hao Zheng, Yawen Huang, Yuexiang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diabetic retinopathy (DR) is a leading cause of vision loss among middle-aged
and elderly people, which significantly impacts their daily lives and mental
health. To improve the efficiency of clinical screening and enable the early
detection of DR, a variety of automated DR diagnosis systems have been recently
established based on convolutional neural network (CNN) or vision Transformer
(ViT). However, due to the own shortages of CNN / ViT, the performance of
existing methods using single-type backbone has reached a bottleneck. One
potential way for the further improvements is integrating different kinds of
backbones, which can fully leverage the respective strengths of them
(\emph{i.e.,} the local feature extraction capability of CNN and the global
feature capturing ability of ViT). To this end, we propose a novel paradigm to
effectively fuse the features extracted by different backbones based on the
theory of evidence. Specifically, the proposed evidential fusion paradigm
transforms the features from different backbones into supporting evidences via
a set of deep evidential networks. With the supporting evidences, the
aggregated opinion can be accordingly formed, which can be used to adaptively
tune the fusion pattern between different backbones and accordingly boost the
performance of our hybrid model. We evaluated our method on two publicly
available DR grading datasets. The experimental results demonstrate that our
hybrid model not only improves the accuracy of DR grading, compared to the
state-of-the-art frameworks, but also provides the excellent interpretability
for feature fusion and decision-making.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the correlation between the type of music and the emotions
  evoked: A study using subjective questionnaires and EEG 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26304v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26304v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jelizaveta Jankowska, Bożena Kostek, Fernando Alonso-Fernandez, Prayag Tiwari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The subject of this work is to check how different types of music affect
human emotions. While listening to music, a subjective survey and brain
activity measurements were carried out using an EEG helmet. The aim is to
demonstrate the impact of different music genres on emotions. The research
involved a diverse group of participants of different gender and musical
preferences. This had the effect of capturing a wide range of emotional
responses to music. After the experiment, a relationship analysis of the
respondents' questionnaires with EEG signals was performed. The analysis
revealed connections between emotions and observed brain activity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at IWAIPR 2025 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Realistic Earth-Observation Constellation Scheduling: Benchmark
  and Methodology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26297v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26297v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luting Wang, Yinghao Xiang, Hongliang Huang, Dongjun Li, Chen Gao, Si Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Agile Earth Observation Satellites (AEOSs) constellations offer unprecedented
flexibility for monitoring the Earth's surface, but their scheduling remains
challenging under large-scale scenarios, dynamic environments, and stringent
constraints. Existing methods often simplify these complexities, limiting their
real-world performance. We address this gap with a unified framework
integrating a standardized benchmark suite and a novel scheduling model. Our
benchmark suite, AEOS-Bench, contains $3,907$ finely tuned satellite assets and
$16,410$ scenarios. Each scenario features $1$ to $50$ satellites and $50$ to
$300$ imaging tasks. These scenarios are generated via a high-fidelity
simulation platform, ensuring realistic satellite behavior such as orbital
dynamics and resource constraints. Ground truth scheduling annotations are
provided for each scenario. To our knowledge, AEOS-Bench is the first
large-scale benchmark suite tailored for realistic constellation scheduling.
Building upon this benchmark, we introduce AEOS-Former, a Transformer-based
scheduling model that incorporates a constraint-aware attention mechanism. A
dedicated internal constraint module explicitly models the physical and
operational limits of each satellite. Through simulation-based iterative
learning, AEOS-Former adapts to diverse scenarios, offering a robust solution
for AEOS constellation scheduling. Experimental results demonstrate that
AEOS-Former outperforms baseline models in task completion and energy
efficiency, with ablation studies highlighting the contribution of each
component. Code and data are provided in
https://github.com/buaa-colalab/AEOSBench.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Large-Scale Face <span class="highlight-title">Dataset</span>s for Deep Periocular Recognition via
  Ocular Cropping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26294v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26294v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fernando Alonso-Fernandez, Kevin Hernandez-Diaz, Jose Maria Buades Rubio, Josef Bigun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We focus on ocular biometrics, specifically the periocular region (the area
around the eye), which offers high discrimination and minimal acquisition
constraints. We evaluate three Convolutional Neural Network architectures of
varying depth and complexity to assess their effectiveness for periocular
recognition. The networks are trained on 1,907,572 ocular crops extracted from
the large-scale VGGFace2 database. This significantly contrasts with existing
works, which typically rely on small-scale periocular datasets for training
having only a few thousand images. Experiments are conducted with ocular images
from VGGFace2-Pose, a subset of VGGFace2 containing in-the-wild face images,
and the UFPR-Periocular database, which consists of selfies captured via mobile
devices with user guidance on the screen. Due to the uncontrolled conditions of
VGGFace2, the Equal Error Rates (EERs) obtained with ocular crops range from
9-15%, noticeably higher than the 3-6% EERs achieved using full-face images. In
contrast, UFPR-Periocular yields significantly better performance (EERs of
1-2%), thanks to higher image quality and more consistent acquisition
protocols. To the best of our knowledge, these are the lowest reported EERs on
the UFPR dataset to date.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at IWAIPR 2025 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Imitation: Constraint-Aware Trajectory Generation with Flow
  Matching For End-to-End Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26292v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26292v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lin Liu, Guanyi Yu, Ziying Song, Junqiao Li, Caiyan Jia, Feiyang Jia, Peiliang Wu, Yandan Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Planning is a critical component of end-to-end autonomous driving. However,
prevailing imitation learning methods often suffer from mode collapse, failing
to produce diverse trajectory hypotheses. Meanwhile, existing generative
approaches struggle to incorporate crucial safety and physical constraints
directly into the generative process, necessitating an additional optimization
stage to refine their outputs. To address these limitations, we propose CATG, a
novel planning framework that leverages Constrained Flow Matching. Concretely,
CATG explicitly models the flow matching process, which inherently mitigates
mode collapse and allows for flexible guidance from various conditioning
signals. Our primary contribution is the novel imposition of explicit
constraints directly within the flow matching process, ensuring that the
generated trajectories adhere to vital safety and kinematic rules. Secondly,
CATG parameterizes driving aggressiveness as a control signal during
generation, enabling precise manipulation of trajectory style. Notably, on the
NavSim v2 challenge, CATG achieved 2nd place with an EPDMS score of 51.31 and
was honored with the Innovation Award.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Complementarity and Explainability in CNNs for Periocular
  Verification Across Acquisition Distances 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26282v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26282v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fernando Alonso-Fernandez, Kevin Hernandez Diaz, Jose M. Buades, Kiran Raja, Josef Bigun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the complementarity of different CNNs for periocular verification at
different distances on the UBIPr database. We train three architectures of
increasing complexity (SqueezeNet, MobileNetv2, and ResNet50) on a large set of
eye crops from VGGFace2. We analyse performance with cosine and chi2 metrics,
compare different network initialisations, and apply score-level fusion via
logistic regression. In addition, we use LIME heatmaps and Jensen-Shannon
divergence to compare attention patterns of the CNNs. While ResNet50
consistently performs best individually, the fusion provides substantial gains,
especially when combining all three networks. Heatmaps show that networks
usually focus on distinct regions of a given image, which explains their
complementarity. Our method significantly outperforms previous works on UBIPr,
achieving a new state-of-the-art.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at BIOSIG 2025 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting Generative Infrared and Visible Image Fusion Based on Human
  Cognitive Laws <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26268v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26268v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lin Guo, Xiaoqing Luo, Wei Xie, Zhancheng Zhang, Hui Li, Rui Wang, Zhenhua Feng, Xiaoning Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing infrared and visible image fusion methods often face the dilemma of
balancing modal information. Generative fusion methods reconstruct fused images
by learning from data distributions, but their generative capabilities remain
limited. Moreover, the lack of interpretability in modal information selection
further affects the reliability and consistency of fusion results in complex
scenarios. This manuscript revisits the essence of generative image fusion
under the inspiration of human cognitive laws and proposes a novel infrared and
visible image fusion method, termed HCLFuse. First, HCLFuse investigates the
quantification theory of information mapping in unsupervised fusion networks,
which leads to the design of a multi-scale mask-regulated variational
bottleneck encoder. This encoder applies posterior probability modeling and
information decomposition to extract accurate and concise low-level modal
information, thereby supporting the generation of high-fidelity structural
details. Furthermore, the probabilistic generative capability of the diffusion
model is integrated with physical laws, forming a time-varying physical
guidance mechanism that adaptively regulates the generation process at
different stages, thereby enhancing the ability of the model to perceive the
intrinsic structure of data and reducing dependence on data quality.
Experimental results show that the proposed method achieves state-of-the-art
fusion performance in qualitative and quantitative evaluations across multiple
datasets and significantly improves semantic segmentation metrics. This fully
demonstrates the advantages of this generative image fusion method, drawing
inspiration from human cognition, in enhancing structural consistency and
detail quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025 spotlight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Which Way Does Time Flow? A Psychophysics-Grounded Evaluation for
  Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26241v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26241v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiho Matta, Lis Kanashiro Pereira, Peitao Han, Fei Cheng, Shigeru Kitazawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern vision-language models (VLMs) excel at many multimodal tasks, yet
their grasp of temporal information in video remains weak and, crucially,
under-evaluated. We probe this gap with a deceptively simple but revealing
challenge: judging the arrow of time (AoT)-whether a short clip is played
forward or backward. We introduce AoT-PsyPhyBENCH, a psychophysically validated
benchmark that tests whether VLMs can infer temporal direction in natural
videos using the same stimuli and behavioral baselines established for humans.
Our comprehensive evaluation of open-weight and proprietary, reasoning and
non-reasoning VLMs reveals that most models perform near chance, and even the
best lag far behind human accuracy on physically irreversible processes (e.g.,
free fall, diffusion/explosion) and causal manual actions (division/addition)
that humans recognize almost instantly. These results highlight a fundamental
gap in current multimodal systems: while they capture rich visual-semantic
correlations, they lack the inductive biases required for temporal continuity
and causal understanding. We release the code and data for AoT-PsyPhyBENCH to
encourage further progress in the physical and temporal reasoning capabilities
of VLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OmniLayout: Enabling Coarse-to-Fine Learning with LLMs for Universal
  Document Layout Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26213v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26213v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hengrui Kang, Zhuangcheng Gu, Zhiyuan Zhao, Zichen Wen, Bin Wang, Weijia Li, Conghui He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Document AI has advanced rapidly and is attracting increasing attention. Yet,
while most efforts have focused on document layout analysis (DLA), its
generative counterpart, document layout generation, remains underexplored. A
major obstacle lies in the scarcity of diverse layouts: academic papers with
Manhattan-style structures dominate existing studies, while open-world genres
such as newspapers and magazines remain severely underrepresented. To address
this gap, we curate OmniLayout-1M, the first million-scale dataset of diverse
document layouts, covering six common document types and comprising
contemporary layouts collected from multiple sources. Moreover, since existing
methods struggle in complex domains and often fail to arrange long sequences
coherently, we introduce OmniLayout-LLM, a 0.5B model with designed two-stage
Coarse-to-Fine learning paradigm: 1) learning universal layout principles from
OmniLayout-1M with coarse category definitions, and 2) transferring the
knowledge to a specific domain with fine-grained annotations. Extensive
experiments demonstrate that our approach achieves strong performance on
multiple domains in M$^{6}$Doc dataset, substantially surpassing both existing
layout generation experts and several latest general-purpose LLMs. Our code,
models, and dataset will be publicly released.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>TL;DR: With OmniLayout-1M dataset and LLM-based coarse-to-fine
  learning, we enable universal and diverse document layout generation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Developing a Multi-task Ensemble Geometric Deep Network for Supply Chain
  Sustainability and Risk Management 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26203v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26203v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehdi Khaleghi, Nastaran Khaleghi, Sobhan Sheykhivand, Sebelan Danishvar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The sustainability of supply chain plays a key role in achieving optimal
performance in controlling the supply chain. The management of risks that occur
in a supply chain is a fundamental problem for the purpose of developing the
sustainability of the network and elevating the performance efficiency of the
supply chain. The correct classification of products is another essential
element in a sustainable supply chain. Acknowledging recent breakthroughs in
the context of deep networks, several architectural options have been deployed
to analyze supply chain datasets. A novel geometric deep network is used to
propose an ensemble deep network. The proposed Chebyshev ensemble geometric
network (Ch-EGN) is a hybrid convolutional and geometric deep learning. This
network is proposed to leverage the information dependencies in supply chain to
derive invisible states of samples in the database. The functionality of the
proposed deep network is assessed on the two different databases. The
SupplyGraph Dataset and DataCo are considered in this research. The prediction
of delivery status of DataCo supply chain is done for risk administration. The
product classification and edge classification are performed using the
SupplyGraph database to enhance the sustainability of the supply network. An
average accuracy of 98.95% is obtained for the ensemble network for risk
management. The average accuracy of 100% and 98.07% are obtained for
sustainable supply chain in terms of 5 product group classification and 4
product relation classification, respectively. The average accuracy of 92.37%
is attained for 25 company relation classification. The results confirm an
average improvement and efficiency of the proposed method compared to the
state-of-the-art approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sketch2PoseNet: Efficient and Generalized Sketch to 3D Human Pose
  Prediction <span class="chip">SIGGRAPH</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26196v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26196v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Wang, Yiyu Zhuang, Yanwen Wang, Xun Cao, Chuan Guo, Xinxin Zuo, Hao Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D human pose estimation from sketches has broad applications in computer
animation and film production. Unlike traditional human pose estimation, this
task presents unique challenges due to the abstract and disproportionate nature
of sketches. Previous sketch-to-pose methods, constrained by the lack of
large-scale sketch-3D pose annotations, primarily relied on optimization with
heuristic rules-an approach that is both time-consuming and limited in
generalizability. To address these challenges, we propose a novel approach
leveraging a "learn from synthesis" strategy. First, a diffusion model is
trained to synthesize sketch images from 2D poses projected from 3D human
poses, mimicking disproportionate human structures in sketches. This process
enables the creation of a synthetic dataset, SKEP-120K, consisting of 120k
accurate sketch-3D pose annotation pairs across various sketch styles. Building
on this synthetic dataset, we introduce an end-to-end data-driven framework for
estimating human poses and shapes from diverse sketch styles. Our framework
combines existing 2D pose detectors and generative diffusion priors for sketch
feature extraction with a feed-forward neural network for efficient 2D pose
estimation. Multiple heuristic loss functions are incorporated to guarantee
geometric coherence between the derived 3D poses and the detected 2D poses
while preserving accurate self-contacts. Qualitative, quantitative, and
subjective evaluations collectively show that our model substantially surpasses
previous ones in both estimation accuracy and speed for sketch-to-pose tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGGRAPH Asia 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ConceptScope: Characterizing <span class="highlight-title">Dataset</span> Bias via Disentangled Visual
  Concepts <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26186v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26186v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinho Choi, Hyesu Lim, Steffen Schneider, Jaegul Choo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dataset bias, where data points are skewed to certain concepts, is ubiquitous
in machine learning datasets. Yet, systematically identifying these biases is
challenging without costly, fine-grained attribute annotations. We present
ConceptScope, a scalable and automated framework for analyzing visual datasets
by discovering and quantifying human-interpretable concepts using Sparse
Autoencoders trained on representations from vision foundation models.
ConceptScope categorizes concepts into target, context, and bias types based on
their semantic relevance and statistical correlation to class labels, enabling
class-level dataset characterization, bias identification, and robustness
evaluation through concept-based subgrouping. We validate that ConceptScope
captures a wide range of visual concepts, including objects, textures,
backgrounds, facial attributes, emotions, and actions, through comparisons with
annotated datasets. Furthermore, we show that concept activations produce
spatial attributions that align with semantically meaningful image regions.
ConceptScope reliably detects known biases (e.g., background bias in
Waterbirds) and uncovers previously unannotated ones (e.g, co-occurring objects
in ImageNet), offering a practical tool for dataset auditing and model
diagnostics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in the Thirty-Ninth Conference on Neural Information
  Processing Systems (NeurIPS 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MoTDiff: High-resolution Motion Trajectory estimation from a single
  blurred image using Diffusion models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26173v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26173v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wontae Choi, Jaelin Lee, Hyung Sup Yun, Byeungwoo Jeon, Il Yong Chun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate estimation of motion information is crucial in diverse computational
imaging and computer vision applications. Researchers have investigated various
methods to extract motion information from a single blurred image, including
blur kernels and optical flow. However, existing motion representations are
often of low quality, i.e., coarse-grained and inaccurate. In this paper, we
propose the first high-resolution (HR) Motion Trajectory estimation framework
using Diffusion models (MoTDiff). Different from existing motion
representations, we aim to estimate an HR motion trajectory with high-quality
from a single motion-blurred image. The proposed MoTDiff consists of two key
components: 1) a new conditional diffusion framework that uses multi-scale
feature maps extracted from a single blurred image as a condition, and 2) a new
training method that can promote precise identification of a fine-grained
motion trajectory, consistent estimation of overall shape and position of a
motion path, and pixel connectivity along a motion trajectory. Our experiments
demonstrate that the proposed MoTDiff can outperform state-of-the-art methods
in both blind image deblurring and coded exposure photography applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-localization on a 3D map by fusing global and local features from a
  monocular camera 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26170v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26170v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Satoshi Kikuch, Masaya Kato, Tsuyoshi Tasaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-localization on a 3D map by using an inexpensive monocular camera is
required to realize autonomous driving. Self-localization based on a camera
often uses a convolutional neural network (CNN) that can extract local features
that are calculated by nearby pixels. However, when dynamic obstacles, such as
people, are present, CNN does not work well. This study proposes a new method
combining CNN with Vision Transformer, which excels at extracting global
features that show the relationship of patches on whole image. Experimental
results showed that, compared to the state-of-the-art method (SOTA), the
accuracy improvement rate in a CG dataset with dynamic obstacles is 1.5 times
higher than that without dynamic obstacles. Moreover, the self-localization
error of our method is 20.1% smaller than that of SOTA on public datasets.
Additionally, our robot using our method can localize itself with 7.51cm error
on average, which is more accurate than SOTA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CRAG-MM: Multi-modal Multi-turn Comprehensive RAG Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26160v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26160v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaqi Wang, Xiao Yang, Kai Sun, Parth Suresh, Sanat Sharma, Adam Czyzewski, Derek Andersen, Surya Appini, Arkav Banerjee, Sajal Choudhary, Shervin Ghasemlou, Ziqiang Guan, Akil Iyer, Haidar Khan, Lingkun Kong, Roy Luo, Tiffany Ma, Zhen Qiao, David Tran, Wenfang Xu, Skyler Yeatman, Chen Zhou, Gunveer Gujral, Yinglong Xia, Shane Moon, Nicolas Scheffer, Nirav Shah, Eun Chang, Yue Liu, Florian Metze, Tammy Stark, Zhaleh Feizollahi, Andrea Jessee, Mangesh Pujari, Ahmed Aly, Babak Damavandi, Rakesh Wanga, Anuj Kumar, Rohit Patel, Wen-tau Yih, Xin Luna Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Wearable devices such as smart glasses are transforming the way people
interact with their surroundings, enabling users to seek information regarding
entities in their view. Multi-Modal Retrieval-Augmented Generation (MM-RAG)
plays a key role in supporting such questions, yet there is still no
comprehensive benchmark for this task, especially regarding wearables
scenarios. To fill this gap, we present CRAG-MM -- a Comprehensive RAG
benchmark for Multi-modal Multi-turn conversations. CRAG-MM contains a diverse
set of 6.5K (image, question, answer) triplets and 2K visual-based multi-turn
conversations across 13 domains, including 6.2K egocentric images designed to
mimic captures from wearable devices. We carefully constructed the questions to
reflect real-world scenarios and challenges, including five types of
image-quality issues, six question types, varying entity popularity, differing
information dynamism, and different conversation turns. We design three tasks:
single-source augmentation, multi-source augmentation, and multi-turn
conversations -- each paired with an associated retrieval corpus and APIs for
both image-KG retrieval and webpage retrieval. Our evaluation shows that
straightforward RAG approaches achieve only 32% and 43% truthfulness on CRAG-MM
single- and multi-turn QA, respectively, whereas state-of-the-art industry
solutions have similar quality (32%/45%), underscoring ample room for
improvement. The benchmark has hosted KDD Cup 2025, attracting about 1K
participants and 5K submissions, with winning solutions improving baseline
performance by 28%, highlighting its early impact on advancing the field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detecting Unauthorized Vehicles using Deep Learning for Smart Cities: A
  Case Study on Bangladesh 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26154v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26154v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sudipto Das Sukanto, Diponker Roy, Fahim Shakil, Nirjhar Singha, Abdullah Asik, Aniket Joarder, Mridha Md Nafis Fuad, Muhammad Ibrahim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modes of transportation vary across countries depending on geographical
location and cultural context. In South Asian countries rickshaws are among the
most common means of local transport. Based on their mode of operation,
rickshaws in cities across Bangladesh can be broadly classified into non-auto
(pedal-powered) and auto-rickshaws (motorized). Monitoring the movement of
auto-rickshaws is necessary as traffic rules often restrict auto-rickshaws from
accessing certain routes. However, existing surveillance systems make it quite
difficult to monitor them due to their similarity to other vehicles, especially
non-auto rickshaws whereas manual video analysis is too time-consuming. This
paper presents a machine learning-based approach to automatically detect
auto-rickshaws in traffic images. In this system, we used real-time object
detection using the YOLOv8 model. For training purposes, we prepared a set of
1,730 annotated images that were captured under various traffic conditions. The
results show that our proposed model performs well in real-time auto-rickshaw
detection and offers an mAP50 of 83.447% and binary precision and recall values
above 78%, demonstrating its effectiveness in handling both dense and sparse
traffic scenarios. The dataset has been publicly released for further research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MV-MLM: Bridging Multi-View Mammography and Language for Breast Cancer
  Diagnosis and Risk Prediction <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26151v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26151v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shunjie-Fabian Zheng, Hyeonjun Lee, Thijs Kooi, Ali Diba
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large annotated datasets are essential for training robust Computer-Aided
Diagnosis (CAD) models for breast cancer detection or risk prediction. However,
acquiring such datasets with fine-detailed annotation is both costly and
time-consuming. Vision-Language Models (VLMs), such as CLIP, which are
pre-trained on large image-text pairs, offer a promising solution by enhancing
robustness and data efficiency in medical imaging tasks. This paper introduces
a novel Multi-View Mammography and Language Model for breast cancer
classification and risk prediction, trained on a dataset of paired mammogram
images and synthetic radiology reports. Our MV-MLM leverages multi-view
supervision to learn rich representations from extensive radiology data by
employing cross-modal self-supervision across image-text pairs. This includes
multiple views and the corresponding pseudo-radiology reports. We propose a
novel joint visual-textual learning strategy to enhance generalization and
accuracy performance over different data types and tasks to distinguish breast
tissues or cancer characteristics(calcification, mass) and utilize these
patterns to understand mammography images and predict cancer risk. We evaluated
our method on both private and publicly available datasets, demonstrating that
the proposed model achieves state-of-the-art performance in three
classification tasks: (1) malignancy classification, (2) subtype
classification, and (3) image-based cancer risk prediction. Furthermore, the
model exhibits strong data efficiency, outperforming existing fully supervised
or VLM baselines while trained on synthetic text reports and without the need
for actual radiology reports.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Computer Vision for Automated Medical Diagnosis (CVAMD)
  Workshop at ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BasicAVSR: Arbitrary-Scale Video Super-Resolution via Image Priors and
  Enhanced Motion Compensation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26149v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26149v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Shang, Wanying Zhang, Shuhang Gu, Pengfei Zhu, Qinghua Hu, Dongwei Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Arbitrary-scale video super-resolution (AVSR) aims to enhance the resolution
of video frames, potentially at various scaling factors, which presents several
challenges regarding spatial detail reproduction, temporal consistency, and
computational complexity. In this paper, we propose a strong baseline BasicAVSR
for AVSR by integrating four key components: 1) adaptive multi-scale frequency
priors generated from image Laplacian pyramids, 2) a flow-guided propagation
unit to aggregate spatiotemporal information from adjacent frames, 3) a
second-order motion compensation unit for more accurate spatial alignment of
adjacent frames, and 4) a hyper-upsampling unit to generate scale-aware and
content-independent upsampling kernels. To meet diverse application demands, we
instantiate three propagation variants: (i) a unidirectional RNN unit for
strictly online inference, (ii) a unidirectional RNN unit empowered with a
limited lookahead that tolerates a small output delay, and (iii) a
bidirectional RNN unit designed for offline tasks where computational resources
are less constrained. Experimental results demonstrate the effectiveness and
adaptability of our model across these different scenarios. Through extensive
experiments, we show that BasicAVSR significantly outperforms existing methods
in terms of super-resolution quality, generalization ability, and inference
speed. Our work not only advances the state-of-the-art in AVSR but also extends
its core components to multiple frameworks for diverse scenarios. The code is
available at https://github.com/shangwei5/BasicAVSR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 10 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ StructLayoutFormer:Conditional Structured Layout Generation via
  Structure Serialization and Disentanglement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26141v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26141v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Hu, Pengfei Xu, Jin Zhou, Hongbo Fu, Hui Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Structured layouts are preferable in many 2D visual contents (\eg, GUIs,
webpages) since the structural information allows convenient layout editing.
Computational frameworks can help create structured layouts but require heavy
labor input. Existing data-driven approaches are effective in automatically
generating fixed layouts but fail to produce layout structures. We present
StructLayoutFormer, a novel Transformer-based approach for conditional
structured layout generation. We use a structure serialization scheme to
represent structured layouts as sequences. To better control the structures of
generated layouts, we disentangle the structural information from the element
placements. Our approach is the first data-driven approach that achieves
conditional structured layout generation and produces realistic layout
structures explicitly. We compare our approach with existing data-driven layout
generation approaches by including post-processing for structure extraction.
Extensive experiments have shown that our approach exceeds these baselines in
conditional structured layout generation. We also demonstrate that our approach
is effective in extracting and transferring layout structures. The code is
publicly available at %\href{https://github.com/Teagrus/StructLayoutFormer}
{https://github.com/Teagrus/StructLayoutFormer}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FullPart: Generating each 3D Part at Full Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26140v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26140v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lihe Ding, Shaocong Dong, Yaokun Li, Chenjian Gao, Xiao Chen, Rui Han, Yihao Kuang, Hong Zhang, Bo Huang, Zhanpeng Huang, Zibin Wang, Dan Xu, Tianfan Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Part-based 3D generation holds great potential for various applications.
Previous part generators that represent parts using implicit vector-set tokens
often suffer from insufficient geometric details. Another line of work adopts
an explicit voxel representation but shares a global voxel grid among all
parts; this often causes small parts to occupy too few voxels, leading to
degraded quality. In this paper, we propose FullPart, a novel framework that
combines both implicit and explicit paradigms. It first derives the bounding
box layout through an implicit box vector-set diffusion process, a task that
implicit diffusion handles effectively since box tokens contain little
geometric detail. Then, it generates detailed parts, each within its own fixed
full-resolution voxel grid. Instead of sharing a global low-resolution space,
each part in our method - even small ones - is generated at full resolution,
enabling the synthesis of intricate details. We further introduce a
center-point encoding strategy to address the misalignment issue when
exchanging information between parts of different actual sizes, thereby
maintaining global coherence. Moreover, to tackle the scarcity of reliable part
data, we present PartVerse-XL, the largest human-annotated 3D part dataset to
date with 40K objects and 320K parts. Extensive experiments demonstrate that
FullPart achieves state-of-the-art results in 3D part generation. We will
release all code, data, and model to benefit future research in 3D part
generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://fullpart3d.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Object-Aware Attention Guided Frame Association for RGB-D SLAM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26131v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26131v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Caglayan, Nevrez Imamoglu, Oguzhan Guclu, Ali Osman Serhatoglu, Ahmet Burak Can, Ryosuke Nakamura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Attention models have recently emerged as a powerful approach, demonstrating
significant progress in various fields. Visualization techniques, such as class
activation mapping, provide visual insights into the reasoning of convolutional
neural networks (CNNs). Using network gradients, it is possible to identify
regions where the network pays attention during image recognition tasks.
Furthermore, these gradients can be combined with CNN features to localize more
generalizable, task-specific attentive (salient) regions within scenes.
However, explicit use of this gradient-based attention information integrated
directly into CNN representations for semantic object understanding remains
limited. Such integration is particularly beneficial for visual tasks like
simultaneous localization and mapping (SLAM), where CNN representations
enriched with spatially attentive object locations can enhance performance. In
this work, we propose utilizing task-specific network attention for RGB-D
indoor SLAM. Specifically, we integrate layer-wise attention information
derived from network gradients with CNN feature representations to improve
frame association performance. Experimental results indicate improved
performance compared to baseline methods, particularly for large environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>double-column 5 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WOD-E2E: Waymo Open <span class="highlight-title">Dataset</span> for End-to-End Driving in Challenging
  Long-tail Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26125v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26125v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runsheng Xu, Hubert Lin, Wonseok Jeon, Hao Feng, Yuliang Zou, Liting Sun, John Gorman, Kate Tolstaya, Sarah Tang, Brandyn White, Ben Sapp, Mingxing Tan, Jyh-Jing Hwang, Drago Anguelov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-based end-to-end (E2E) driving has garnered significant interest in
the research community due to its scalability and synergy with multimodal large
language models (MLLMs). However, current E2E driving benchmarks primarily
feature nominal scenarios, failing to adequately test the true potential of
these systems. Furthermore, existing open-loop evaluation metrics often fall
short in capturing the multi-modal nature of driving or effectively evaluating
performance in long-tail scenarios. To address these gaps, we introduce the
Waymo Open Dataset for End-to-End Driving (WOD-E2E). WOD-E2E contains 4,021
driving segments (approximately 12 hours), specifically curated for challenging
long-tail scenarios that that are rare in daily life with an occurring
frequency of less than 0.03%. Concretely, each segment in WOD-E2E includes the
high-level routing information, ego states, and 360-degree camera views from 8
surrounding cameras. To evaluate the E2E driving performance on these long-tail
situations, we propose a novel open-loop evaluation metric: Rater Feedback
Score (RFS). Unlike conventional metrics that measure the distance between
predicted way points and the logs, RFS measures how closely the predicted
trajectory matches rater-annotated trajectory preference labels. We have
released rater preference labels for all WOD-E2E validation set segments, while
the held out test set labels have been used for the 2025 WOD-E2E Challenge.
Through our work, we aim to foster state of the art research into
generalizable, robust, and safe end-to-end autonomous driving agents capable of
handling complex real-world situations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ JOGS: Joint Optimization of Pose Estimation and 3D Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26117v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26117v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Li, Tao Wang, Xianben Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional novel view synthesis methods heavily rely on external camera pose
estimation tools such as COLMAP, which often introduce computational
bottlenecks and propagate errors. To address these challenges, we propose a
unified framework that jointly optimizes 3D Gaussian points and camera poses
without requiring pre-calibrated inputs. Our approach iteratively refines 3D
Gaussian parameters and updates camera poses through a novel co-optimization
strategy, ensuring simultaneous improvements in scene reconstruction fidelity
and pose accuracy. The key innovation lies in decoupling the joint optimization
into two interleaved phases: first, updating 3D Gaussian parameters via
differentiable rendering with fixed poses, and second, refining camera poses
using a customized 3D optical flow algorithm that incorporates geometric and
photometric constraints. This formulation progressively reduces projection
errors, particularly in challenging scenarios with large viewpoint variations
and sparse feature distributions, where traditional methods struggle. Extensive
evaluations on multiple datasets demonstrate that our approach significantly
outperforms existing COLMAP-free techniques in reconstruction quality, and also
surpasses the standard COLMAP-based baseline in general.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OracleAgent: A Multimodal Reasoning Agent for Oracle Bone Script
  Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26114v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26114v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Caoshuo Li, Zengmao Ding, Xiaobin Hu, Bang Li, Donghao Luo, Xu Peng, Taisong Jin, Yongge Liu, Shengwei Han, Jing Yang, Xiaoping He, Feng Gao, AndyPian Wu,  SevenShu, Chaoyang Wang, Chengjie Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As one of the earliest writing systems, Oracle Bone Script (OBS) preserves
the cultural and intellectual heritage of ancient civilizations. However,
current OBS research faces two major challenges: (1) the interpretation of OBS
involves a complex workflow comprising multiple serial and parallel sub-tasks,
and (2) the efficiency of OBS information organization and retrieval remains a
critical bottleneck, as scholars often spend substantial effort searching for,
compiling, and managing relevant resources. To address these challenges, we
present OracleAgent, the first agent system designed for the structured
management and retrieval of OBS-related information. OracleAgent seamlessly
integrates multiple OBS analysis tools, empowered by large language models
(LLMs), and can flexibly orchestrate these components. Additionally, we
construct a comprehensive domain-specific multimodal knowledge base for OBS,
which is built through a rigorous multi-year process of data collection,
cleaning, and expert annotation. The knowledge base comprises over 1.4M
single-character rubbing images and 80K interpretation texts. OracleAgent
leverages this resource through its multimodal tools to assist experts in
retrieval tasks of character, document, interpretation text, and rubbing image.
Extensive experiments demonstrate that OracleAgent achieves superior
performance across a range of multimodal reasoning and generation tasks,
surpassing leading mainstream multimodal large language models (MLLMs) (e.g.,
GPT-4o). Furthermore, our case study illustrates that OracleAgent can
effectively assist domain experts, significantly reducing the time cost of OBS
research. These results highlight OracleAgent as a significant step toward the
practical deployment of OBS-assisted research and automated interpretation
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EgoExo-Con: Exploring View-Invariant Video Temporal Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26113v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26113v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minjoon Jung, Junbin Xiao, Junghyun Kim, Byoung-Tak Zhang, Angela Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Can Video-LLMs achieve consistent temporal understanding when videos capture
the same event from different viewpoints? To study this, we introduce
EgoExo-Con (Consistency), a benchmark of comprehensively synchronized
egocentric and exocentric video pairs with human-refined queries in natural
language. EgoExo-Con emphasizes two temporal understanding tasks: Temporal
Verification and Temporal Grounding. It evaluates not only correctness but
consistency across viewpoints. Our analysis reveals two critical limitations of
existing Video-LLMs: (1) models often fail to maintain consistency, with
results far worse than their single-view performances. (2) When naively
finetuned with synchronized videos of both viewpoints, the models show improved
consistency but often underperform those trained on a single view. For
improvements, we propose View-GRPO, a novel reinforcement learning framework
that effectively strengthens view-specific temporal reasoning while encouraging
consistent comprehension across viewpoints. Our method demonstrates its
superiority over naive SFT and GRPO, especially for improving cross-view
consistency. All resources will be made publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>project page:
  \url{https://minjoong507.github.io/projects/EgoExo-Con/}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Security Risk of Misalignment between Text and Image in Multi-modal
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26105v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26105v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaosen Wang, Zhijin Ge, Shaokang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the notable advancements and versatility of multi-modal diffusion
models, such as text-to-image models, their susceptibility to adversarial
inputs remains underexplored. Contrary to expectations, our investigations
reveal that the alignment between textual and Image modalities in existing
diffusion models is inadequate. This misalignment presents significant risks,
especially in the generation of inappropriate or Not-Safe-For-Work (NSFW)
content. To this end, we propose a novel attack called Prompt-Restricted
Multi-modal Attack (PReMA) to manipulate the generated content by modifying the
input image in conjunction with any specified prompt, without altering the
prompt itself. PReMA is the first attack that manipulates model outputs by
solely creating adversarial images, distinguishing itself from prior methods
that primarily generate adversarial prompts to produce NSFW content.
Consequently, PReMA poses a novel threat to the integrity of multi-modal
diffusion models, particularly in image-editing applications that operate with
fixed prompts. Comprehensive evaluations conducted on image inpainting and
style transfer tasks across various models confirm the potent efficacy of
PReMA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic VLM-Guided Negative <span class="highlight-title">Prompt</span>ing for Diffusion Models <span class="chip">NeurIPS
  2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26052v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26052v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hoyeon Chang, Seungjin Kim, Yoonseok Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel approach for dynamic negative prompting in diffusion
models that leverages Vision-Language Models (VLMs) to adaptively generate
negative prompts during the denoising process. Unlike traditional Negative
Prompting methods that use fixed negative prompts, our method generates
intermediate image predictions at specific denoising steps and queries a VLM to
produce contextually appropriate negative prompts. We evaluate our approach on
various benchmark datasets and demonstrate the trade-offs between negative
guidance strength and text-image alignment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39th Conference on Neural Information Processing Systems (NeurIPS
  2025) Workshop: The First Workshop on Generative and Protective AI for
  Content Creation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FlexICL: A Flexible Visual In-context Learning Framework for Elbow and
  Wrist Ultrasound Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26049v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26049v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuyue Zhou, Jessica Knight, Shrimanti Ghosh, Banafshe Felfeliyan, Jacob L. Jaremko, Abhilash R. Hareendranathan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Elbow and wrist fractures are the most common fractures in pediatric
populations. Automatic segmentation of musculoskeletal structures in ultrasound
(US) can improve diagnostic accuracy and treatment planning. Fractures appear
as cortical defects but require expert interpretation. Deep learning (DL) can
provide real-time feedback and highlight key structures, helping lightly
trained users perform exams more confidently. However, pixel-wise expert
annotations for training remain time-consuming and costly. To address this
challenge, we propose FlexICL, a novel and flexible in-context learning (ICL)
framework for segmenting bony regions in US images. We apply it to an
intra-video segmentation setting, where experts annotate only a small subset of
frames, and the model segments unseen frames. We systematically investigate
various image concatenation techniques and training strategies for visual ICL
and introduce novel concatenation methods that significantly enhance model
performance with limited labeled data. By integrating multiple augmentation
strategies, FlexICL achieves robust segmentation performance across four wrist
and elbow US datasets while requiring only 5% of the training images. It
outperforms state-of-the-art visual ICL models like Painter, MAE-VQGAN, and
conventional segmentation models like U-Net and TransUNet by 1-27% Dice
coefficient on 1,252 US sweeps. These initial results highlight the potential
of FlexICL as an efficient and scalable solution for US image segmentation well
suited for medical imaging use cases where labeled data is scarce.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do Students Debias Like Teachers? On the Distillability of Bias
  Mitigation Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26038v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26038v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiali Cheng, Chirag Agarwal, Hadi Amiri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge distillation (KD) is an effective method for model compression and
transferring knowledge between models. However, its effect on model's
robustness against spurious correlations that degrade performance on
out-of-distribution data remains underexplored. This study investigates the
effect of knowledge distillation on the transferability of ``debiasing''
capabilities from teacher models to student models on natural language
inference (NLI) and image classification tasks. Through extensive experiments,
we illustrate several key findings: (i) overall the debiasing capability of a
model is undermined post-KD; (ii) training a debiased model does not benefit
from injecting teacher knowledge; (iii) although the overall robustness of a
model may remain stable post-distillation, significant variations can occur
across different types of biases; and (iv) we pin-point the internal attention
pattern and circuit that causes the distinct behavior post-KD. Given the above
findings, we propose three effective solutions to improve the distillability of
debiasing methods: developing high quality data for augmentation, implementing
iterative knowledge distillation, and initializing student models with weights
obtained from teacher models. To the best of our knowledge, this is the first
study on the effect of KD on debiasing and its interenal mechanism at scale.
Our findings provide understandings on how KD works and how to design better
debiasing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Smoothing Slot Attention Iterations and Recurrences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.05417v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.05417v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rongzhen Zhao, Wenyan Yang, Juho Kannala, Joni Pajarinen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Slot Attention (SA) and its variants lie at the heart of mainstream
Object-Centric Learning (OCL). Objects in an image can be aggregated into
respective slot vectors, by \textit{iteratively} refining cold-start query
vectors, typically three times, via SA on image features. For video, such
aggregation is \textit{recurrently} shared across frames, with queries
cold-started on the first frame while transitioned from the previous frame's
slots on non-first frames. However, the cold-start queries lack sample-specific
cues thus hinder precise aggregation on the image or video's first frame; Also,
non-first frames' queries are already sample-specific thus require transforms
different from the first frame's aggregation. We address these issues for the
first time with our \textit{SmoothSA}: (1) To smooth SA iterations on the image
or video's first frame, we \textit{preheat} the cold-start queries with rich
information of input features, via a tiny module self-distilled inside OCL; (2)
To smooth SA recurrences across all video frames, we \textit{differentiate} the
homogeneous transforms on the first and non-first frames, by using full and
single iterations respectively. Comprehensive experiments on object discovery,
recognition and downstream benchmarks validate our method's effectiveness.
Further analyses intuitively illuminate how our method smooths SA iterations
and recurrences. Our source code, model checkpoints and training logs are
available on https://github.com/Genera1Z/SmoothSA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Predicting Video Slot Attention Queries from Random Slot-Feature Pairs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.01345v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.01345v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rongzhen Zhao, Jian Li, Juho Kannala, Joni Pajarinen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised video Object-Centric Learning (OCL) is promising as it enables
object-level scene representation and dynamics modeling as we humans do.
Mainstream video OCL methods adopt a recurrent architecture: An aggregator
aggregates current video frame into object features, termed slots, under some
queries; A transitioner transits current slots to queries for the next frame.
This is an effective architecture but all existing implementations both
(\textit{i1}) neglect to incorporate next frame features, the most informative
source for query prediction, and (\textit{i2}) fail to learn transition
dynamics, the knowledge essential for query prediction. To address these
issues, we propose Random Slot-Feature pair for learning Query prediction
(RandSF.Q): (\textit{t1}) We design a new transitioner to incorporate both
slots and features, which provides more information for query prediction;
(\textit{t2}) We train the transitioner to predict queries from slot-feature
pairs randomly sampled from available recurrences, which drives it to learn
transition dynamics. Experiments on scene representation demonstrate that our
method surpass existing video OCL methods significantly, e.g., up to 10 points
on object discovery, setting new state-of-the-art. Such superiority also
benefits downstream tasks like dynamics modeling. Our core source code, model
checkpoints and training logs are available on
https://github.com/Genera1Z/RandSF.Q.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Locality in Image Diffusion Models Emerges from Data Statistics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.09672v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.09672v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Artem Lukoianov, Chenyang Yuan, Justin Solomon, Vincent Sitzmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work has shown that the generalization ability of image diffusion
models arises from the locality properties of the trained neural network. In
particular, when denoising a particular pixel, the model relies on a limited
neighborhood of the input image around that pixel, which, according to the
previous work, is tightly related to the ability of these models to produce
novel images. Since locality is central to generalization, it is crucial to
understand why diffusion models learn local behavior in the first place, as
well as the factors that govern the properties of locality patterns. In this
work, we present evidence that the locality in deep diffusion models emerges as
a statistical property of the image dataset and is not due to the inductive
bias of convolutional neural networks, as suggested in previous work.
Specifically, we demonstrate that an optimal parametric linear denoiser
exhibits similar locality properties to deep neural denoisers. We show, both
theoretically and experimentally, that this locality arises directly from pixel
correlations present in the image datasets. Moreover, locality patterns are
drastically different on specialized datasets, approximating principal
components of the data's covariance. We use these insights to craft an
analytical denoiser that better matches scores predicted by a deep diffusion
model than prior expert-crafted alternatives. Our key takeaway is that while
neural network architectures influence generation quality, their primary role
is to capture locality patterns inherent in the data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 20 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ScoreAdv: Score-based Targeted Generation of Natural Adversarial
  Examples via Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.06078v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.06078v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chihan Huang, Hao Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the success of deep learning across various domains, it remains
vulnerable to adversarial attacks. Although many existing adversarial attack
methods achieve high success rates, they typically rely on $\ell_{p}$-norm
perturbation constraints, which do not align with human perceptual
capabilities. Consequently, researchers have shifted their focus toward
generating natural, unrestricted adversarial examples (UAEs). GAN-based
approaches suffer from inherent limitations, such as poor image quality due to
instability and mode collapse. Meanwhile, diffusion models have been employed
for UAE generation, but they still rely on iterative PGD perturbation
injection, without fully leveraging their central denoising capabilities. In
this paper, we introduce a novel approach for generating UAEs based on
diffusion models, named ScoreAdv. This method incorporates an interpretable
adversarial guidance mechanism to gradually shift the sampling distribution
towards the adversarial distribution, while using an interpretable saliency map
to inject the visual information of a reference image into the generated
samples. Notably, our method is capable of generating an unlimited number of
natural adversarial examples and can attack not only classification models but
also retrieval models. We conduct extensive experiments on ImageNet and CelebA
datasets, validating the performance of ScoreAdv across ten target models in
both black-box and white-box settings. Our results demonstrate that ScoreAdv
achieves state-of-the-art attack success rates and image quality, while
maintaining inference efficiency. Furthermore, the dynamic balance between
denoising and adversarial perturbation enables ScoreAdv to remain robust even
under defensive measures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GSE: Group-wise Sparse and Explainable Adversarial Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.17434v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.17434v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shpresim Sadiku, Moritz Wagner, Sebastian Pokutta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse adversarial attacks fool deep neural networks (DNNs) through minimal
pixel perturbations, often regularized by the $\ell_0$ norm. Recent efforts
have replaced this norm with a structural sparsity regularizer, such as the
nuclear group norm, to craft group-wise sparse adversarial attacks. The
resulting perturbations are thus explainable and hold significant practical
relevance, shedding light on an even greater vulnerability of DNNs. However,
crafting such attacks poses an optimization challenge, as it involves computing
norms for groups of pixels within a non-convex objective. We address this by
presenting a two-phase algorithm that generates group-wise sparse attacks
within semantically meaningful areas of an image. Initially, we optimize a
quasinorm adversarial loss using the $1/2-$quasinorm proximal operator tailored
for non-convex programming. Subsequently, the algorithm transitions to a
projected Nesterov's accelerated gradient descent with $2-$norm regularization
applied to perturbation magnitudes. Rigorous evaluations on CIFAR-10 and
ImageNet datasets demonstrate a remarkable increase in group-wise sparsity,
e.g., $50.9\%$ on CIFAR-10 and $38.4\%$ on ImageNet (average case, targeted
attack). This performance improvement is accompanied by significantly faster
computation times, improved explainability, and a $100\%$ attack success rate.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Resource Efficient Multi-stain Kidney Glomeruli Segmentation via
  Self-supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15389v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15389v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeeshan Nisar, Friedrich Feuerhake, Thomas Lampert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic segmentation under domain shift remains a fundamental challenge in
computer vision, particularly when labelled training data is scarce. This
challenge is particularly exemplified in histopathology image analysis, where
the same tissue structures must be segmented across images captured under
different imaging conditions (stains), each representing a distinct visual
domain. Traditional deep learning methods like UNet require extensive labels,
which is both costly and time-consuming, particularly when dealing with
multiple domains (or stains). To mitigate this, various unsupervised domain
adaptation based methods such as UDAGAN have been proposed, which reduce the
need for labels by requiring only one (source) stain to be labelled.
Nonetheless, obtaining source stain labels can still be challenging. This
article shows that through self-supervised pre-training -- including SimCLR,
BYOL, and a novel approach, HR-CS-CO -- the performance of these segmentation
methods (UNet, and UDAGAN) can be retained even with 95% fewer labels. Notably,
with self-supervised pre-training and using only 5% labels, the performance
drops are minimal: 5.9% for UNet and 6.2% for UDAGAN, averaged over all stains,
compared to their respective fully supervised counterparts (without
pre-training, using 100% labels). Furthermore, these findings are shown to
generalise beyond their training distribution to public benchmark datasets.
Implementations and pre-trained models are publicly available
\href{https://github.com/zeeshannisar/resource-effecient-multi-stain-kidney-glomeruli-segmentation.git}{online}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39 pages, 10 figures, 4 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CronusVLA: Towards Efficient and Robust Manipulation via Multi-Frame
  Vision-Language-Action Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19816v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19816v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Li, Shuai Yang, Yilun Chen, Xinyi Chen, Xiaoda Yang, Yang Tian, Hanqing Wang, Tai Wang, Dahua Lin, Feng Zhao, Jiangmiao Pang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent vision-language-action (VLA) models built on pretrained
vision-language models (VLMs) have demonstrated strong performance in robotic
manipulation. However, these models remain constrained by the single-frame
image paradigm and fail to fully leverage the temporal information offered by
multi-frame histories, as directly feeding multiple frames into VLM backbones
incurs substantial computational overhead and inference latency. We propose
CronusVLA, a unified framework that extends single-frame VLA models to the
multi-frame paradigm. CronusVLA follows a two-stage process: (1) Single-frame
pretraining on large-scale embodied datasets with autoregressive prediction of
action tokens, establishing an effective embodied vision-language foundation;
(2) Multi-frame post-training, which adapts the prediction of the
vision-language backbone from discrete tokens to learnable features, and
aggregates historical information via feature chunking. CronusVLA effectively
addresses the existing challenges of multi-frame modeling while enhancing
performance and observational robustness. To evaluate the robustness under
temporal and spatial disturbances, we introduce SimplerEnv-OR, a novel
benchmark featuring 24 types of observational disturbances and 120 severity
levels. Experiments across three embodiments in simulated and real-world
environments demonstrate that CronusVLA achieves leading performance and
superior robustness, with a 70.9% success rate on SimplerEnv, a 26.8%
improvement over OpenVLA on LIBERO, and the highest robustness score on
SimplerEnv-OR. These results highlight the potential of efficient multi-frame
adaptation in VLA models for more powerful and robust real-world deployment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39 pages, 24 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fit for Purpose? Deepfake Detection in the Real World 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.16556v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.16556v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangyu Lin, Li Lin, Christina P. Walker, Daniel S. Schiff, Shu Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid proliferation of AI-generated content, driven by advances in
generative adversarial networks, diffusion models, and multimodal large
language models, has made the creation and dissemination of synthetic media
effortless, heightening the risks of misinformation, particularly political
deepfakes that distort truth and undermine trust in political institutions. In
turn, governments, research institutions, and industry have strongly promoted
deepfake detection initiatives as solutions. Yet, most existing models are
trained and validated on synthetic, laboratory-controlled datasets, limiting
their generalizability to the kinds of real-world political deepfakes
circulating on social platforms that affect the public. In this work, we
introduce the first systematic benchmark based on the Political Deepfakes
Incident Database, a curated collection of real-world political deepfakes
shared on social media since 2018. Our study includes a systematic evaluation
of state-of-the-art deepfake detectors across academia, government, and
industry. We find that the detectors from academia and government perform
relatively poorly. While paid detection tools achieve relatively higher
performance than free-access models, all evaluated detectors struggle to
generalize effectively to authentic political deepfakes, and are vulnerable to
simple manipulations, especially in the video domain. Results urge the need for
politically contextualized deepfake detection frameworks to better safeguard
the public in real-world settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DDL: A Large-Scale <span class="highlight-title">Dataset</span>s for Deepfake Detection and Localization in
  Diversified Real-World Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.23292v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.23292v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changtao Miao, Yi Zhang, Weize Gao, Zhiya Tan, Weiwei Feng, Man Luo, Jianshu Li, Ajian Liu, Yunfeng Diao, Qi Chu, Tao Gong, Zhe Li, Weibin Yao, Joey Tianyi Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in AIGC have exacerbated the misuse of malicious deepfake
content, making the development of reliable deepfake detection methods an
essential means to address this challenge. Although existing deepfake detection
models demonstrate outstanding performance in detection metrics, most methods
only provide simple binary classification results, lacking interpretability.
Recent studies have attempted to enhance the interpretability of classification
results by providing spatial manipulation masks or temporal forgery segments.
However, due to the limitations of forgery datasets, the practical
effectiveness of these methods remains suboptimal. The primary reason lies in
the fact that most existing deepfake datasets contain only binary labels, with
limited variety in forgery scenarios, insufficient diversity in deepfake types,
and relatively small data scales, making them inadequate for complex real-world
scenarios.To address this predicament, we construct a novel large-scale
deepfake detection and localization (\textbf{DDL}) dataset containing over
$\textbf{1.4M+}$ forged samples and encompassing up to $\textbf{80}$ distinct
deepfake methods. The DDL design incorporates four key innovations: (1)
\textbf{Comprehensive Deepfake Methods} (covering 7 different generation
architectures and a total of 80 methods), (2) \textbf{Varied Manipulation
Modes} (incorporating 7 classic and 3 novel forgery modes), (3) \textbf{Diverse
Forgery Scenarios and Modalities} (including 3 scenarios and 3 modalities), and
(4) \textbf{Fine-grained Forgery Annotations} (providing 1.18M+ precise spatial
masks and 0.23M+ precise temporal segments).Through these improvements, our DDL
not only provides a more challenging benchmark for complex real-world forgeries
but also offers crucial support for building next-generation deepfake
detection, localization, and interpretability methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is a preliminary version, with an extended and
  comprehensive version currently under development</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HM-Talker: Hybrid Motion Modeling for High-Fidelity Talking Head
  Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.10566v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.10566v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiyu Liu, Kui Jiang, Xianming Liu, Hongxun Yao, Xiaocheng Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-driven talking head video generation enhances user engagement in
human-computer interaction. However, current methods frequently produce videos
with motion blur and lip jitter, primarily due to their reliance on implicit
modeling of audio-facial motion correlations--an approach lacking explicit
articulatory priors (i.e., anatomical guidance for speech-related facial
movements). To overcome this limitation, we propose HM-Talker, a novel
framework for generating high-fidelity, temporally coherent talking heads.
HM-Talker leverages a hybrid motion representation combining both implicit and
explicit motion cues. Explicit cues use Action Units (AUs), anatomically
defined facial muscle movements, alongside implicit features to minimize
phoneme-viseme misalignment. Specifically, our Cross-Modal Disentanglement
Module (CMDM) extracts complementary implicit/explicit motion features while
predicting AUs directly from audio input aligned to visual cues. To mitigate
identity-dependent biases in explicit features and enhance cross-subject
generalization, we introduce the Hybrid Motion Modeling Module (HMMM). This
module dynamically merges randomly paired implicit/explicit features, enforcing
identity-agnostic learning. Together, these components enable robust lip
synchronization across diverse identities, advancing personalized talking head
synthesis. Extensive experiments demonstrate HM-Talker's superiority over
state-of-the-art methods in visual quality and lip-sync accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MaskCaptioner: Learning to Jointly Segment and Caption Object
  Trajectories in Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.14904v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.14904v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriel Fiastre, Antoine Yang, Cordelia Schmid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dense Video Object Captioning (DVOC) is the task of jointly detecting,
tracking, and captioning object trajectories in a video, requiring the ability
to understand spatio-temporal details and describe them in natural language.
Due to the complexity of the task and the high cost associated with manual
annotation, previous approaches resort to disjoint training strategies,
potentially leading to suboptimal performance. To circumvent this issue, we
propose to generate captions about spatio-temporally localized entities
leveraging a state-of-the-art VLM. By extending the LVIS and LV-VIS datasets
with our synthetic captions (LVISCap and LV-VISCap), we train MaskCaptioner, an
end-to-end model capable of jointly detecting, segmenting, tracking and
captioning object trajectories. Moreover, with pretraining on LVISCap and
LV-VISCap, MaskCaptioner achieves state-of-the-art DVOC results on three
existing benchmarks, VidSTG, VLN and BenSMOT. The datasets and code are
available at https://www.gabriel.fiastre.fr/maskcaptioner/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LinearSR: Unlocking Linear Attention for Stable and Efficient Image
  Super-Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.08771v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.08771v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaohui Li, Shaobin Zhuang, Shuo Cao, Yang Yang, Yuandong Pu, Qi Qin, Siqi Luo, Bin Fu, Yihao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative models for Image Super-Resolution (SR) are increasingly powerful,
yet their reliance on self-attention's quadratic complexity (O(N^2)) creates a
major computational bottleneck. Linear Attention offers an O(N) solution, but
its promise for photorealistic SR has remained largely untapped, historically
hindered by a cascade of interrelated and previously unsolved challenges. This
paper introduces LinearSR, a holistic framework that, for the first time,
systematically overcomes these critical hurdles. Specifically, we resolve a
fundamental, training instability that causes catastrophic model divergence
using our novel "knee point"-based Early-Stopping Guided Fine-tuning (ESGF)
strategy. Furthermore, we mitigate the classic perception-distortion trade-off
with a dedicated SNR-based Mixture of Experts (MoE) architecture. Finally, we
establish an effective and lightweight guidance paradigm, TAG, derived from our
"precision-over-volume" principle. Our resulting LinearSR model simultaneously
delivers state-of-the-art perceptual quality with exceptional efficiency. Its
core diffusion forward pass (1-NFE) achieves SOTA-level speed, while its
overall multi-step inference time remains highly competitive. This work
provides the first robust methodology for applying Linear Attention in the
photorealistic SR domain, establishing a foundational paradigm for future
research in efficient generative super-resolution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 9 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UV-Attack: Physical-World Adversarial Attacks for Person Detection via
  Dynamic-NeRF-based UV Mapping <span class="chip">ICLR2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05783v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05783v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanjie Li, Kaisheng Liang, Bin Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent research, adversarial attacks on person detectors using patches or
static 3D model-based texture modifications have struggled with low success
rates due to the flexible nature of human movement. Modeling the 3D
deformations caused by various actions has been a major challenge. Fortunately,
advancements in Neural Radiance Fields (NeRF) for dynamic human modeling offer
new possibilities. In this paper, we introduce UV-Attack, a groundbreaking
approach that achieves high success rates even with extensive and unseen human
actions. We address the challenge above by leveraging dynamic-NeRF-based UV
mapping. UV-Attack can generate human images across diverse actions and
viewpoints, and even create novel actions by sampling from the SMPL parameter
space. While dynamic NeRF models are capable of modeling human bodies,
modifying clothing textures is challenging because they are embedded in neural
network parameters. To tackle this, UV-Attack generates UV maps instead of RGB
images and modifies the texture stacks. This approach enables real-time texture
edits and makes the attack more practical. We also propose a novel Expectation
over Pose Transformation loss (EoPT) to improve the evasion success rate on
unseen poses and views. Our experiments show that UV-Attack achieves a 92.7%
attack success rate against the FastRCNN model across varied poses in dynamic
video settings, significantly outperforming the state-of-the-art AdvCamou
attack, which only had a 28.5% ASR. Moreover, we achieve 49.5% ASR on the
latest YOLOv8 detector in black-box settings. This work highlights the
potential of dynamic NeRF-based UV mapping for creating more effective
adversarial attacks on person detectors, addressing key challenges in modeling
human movement and texture modification. The code is available at
https://github.com/PolyLiYJ/UV-Attack.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 22 figures, accepted by ICLR2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Continuous and Interpretable Morphometric for Robust Quantification of
  Dynamic Biological Shapes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21004v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21004v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roua Rouatbi, Juan-Esteban Suarez Cardona, Alba Villaronga-Luque, Jesse V. Veenvliet, Ivo F. Sbalzarini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the Push-Forward Signed Distance Morphometric (PF-SDM) for shape
quantification in biomedical imaging. The PF-SDM compactly encodes geometric
and topological properties of closed shapes, including their skeleton and
symmetries. This provides robust and interpretable features for shape
comparison and machine learning. The PF-SDM is mathematically smooth, providing
access to gradients and differential-geometric quantities. It also extends to
temporal dynamics and allows fusing spatial intensity distributions, such as
genetic markers, with shape dynamics. We present the PF-SDM theory, benchmark
it on synthetic data, and apply it to predicting body-axis formation in mouse
gastruloids, outperforming a CNN baseline in both accuracy and speed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VerifIoU -- Robustness of Object Detection to Perturbations <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08788v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08788v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Noémie Cohen, Mélanie Ducoffe, Ryma Boumazouza, Christophe Gabreau, Claire Pagetti, Xavier Pucel, Audrey Galametz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel Interval Bound Propagation (IBP) approach for the formal
verification of object detection models, specifically targeting the
Intersection over Union (IoU) metric. The approach has been implemented in an
open source code, named IBP IoU, compatible with popular abstract
interpretation based verification tools. The resulting verifier is evaluated on
landing approach runway detection and handwritten digit recognition case
studies. Comparisons against a baseline (Vanilla IBP IoU) highlight the
superior performance of IBP IoU in ensuring accuracy and stability,
contributing to more secure and robust machine learning applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>44th Digital Avionics Systems Conference (DASC), Sep 2025, Montreal,
  Canada</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ReCon-GS: Continuum-Preserved Gaussian Streaming for Fast and Compact
  Reconstruction of Dynamic Scenes <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.24325v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.24325v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaye Fu, Qiankun Gao, Chengxiang Wen, Yanmin Wu, Siwei Ma, Jiaqi Zhang, Jian Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online free-viewpoint video (FVV) reconstruction is challenged by slow
per-frame optimization, inconsistent motion estimation, and unsustainable
storage demands. To address these challenges, we propose the Reconfigurable
Continuum Gaussian Stream, dubbed ReCon-GS, a novel storage-aware framework
that enables high fidelity online dynamic scene reconstruction and real-time
rendering. Specifically, we dynamically allocate multi-level Anchor Gaussians
in a density-adaptive fashion to capture inter-frame geometric deformations,
thereby decomposing scene motion into compact coarse-to-fine representations.
Then, we design a dynamic hierarchy reconfiguration strategy that preserves
localized motion expressiveness through on-demand anchor re-hierarchization,
while ensuring temporal consistency through intra-hierarchical deformation
inheritance that confines transformation priors to their respective hierarchy
levels. Furthermore, we introduce a storage-aware optimization mechanism that
flexibly adjusts the density of Anchor Gaussians at different hierarchy levels,
enabling a controllable trade-off between reconstruction fidelity and memory
usage. Extensive experiments on three widely used datasets demonstrate that,
compared to state-of-the-art methods, ReCon-GS improves training efficiency by
approximately 15% and achieves superior FVV synthesis quality with enhanced
robustness and stability. Moreover, at equivalent rendering quality, ReCon-GS
slashes memory requirements by over 50% compared to leading state-of-the-art
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ StyleGuard: Preventing Text-to-Image-Model-based Style Mimicry Attacks
  by Style Perturbations <span class="chip">NIPS2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.18766v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.18766v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanjie Li, Wenxuan Zhang, Xinqi Lyu, Yihao Liu, Bin Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, text-to-image diffusion models have been widely used for style
mimicry and personalized customization through methods such as DreamBooth and
Textual Inversion. This has raised concerns about intellectual property
protection and the generation of deceptive content. Recent studies, such as
Glaze and Anti-DreamBooth, have proposed using adversarial noise to protect
images from these attacks. However, recent purification-based methods, such as
DiffPure and Noise Upscaling, have successfully attacked these latest defenses,
showing the vulnerabilities of these methods. Moreover, present methods show
limited transferability across models, making them less effective against
unknown text-to-image models. To address these issues, we propose a novel
anti-mimicry method, StyleGuard. We propose a novel style loss that optimizes
the style-related features in the latent space to make it deviate from the
original image, which improves model-agnostic transferability. Additionally, to
enhance the perturbation's ability to bypass diffusion-based purification, we
designed a novel upscale loss that involves ensemble purifiers and upscalers
during training. Extensive experiments on the WikiArt and CelebA datasets
demonstrate that StyleGuard outperforms existing methods in robustness against
various transformations and purifications, effectively countering style mimicry
in various models. Moreover, StyleGuard is effective on different style mimicry
methods, including DreamBooth and Textual Inversion. The code is available at
https://github.com/PolyLiYJ/StyleGuard.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NIPS2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LATex: Leveraging Attribute-based Text Knowledge for Aerial-Ground
  Person Re-Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.23722v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.23722v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pingping Zhang, Xiang Hu, Yuhao Wang, Huchuan Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As an important task in intelligent transportation systems, Aerial-Ground
person Re-IDentification (AG-ReID) aims to retrieve specific persons across
heterogeneous cameras in different viewpoints. Previous methods typically adopt
deep learning-based models, focusing on extracting view-invariant features.
However, they usually overlook the semantic information in person attributes.
In addition, existing training strategies often rely on full fine-tuning
large-scale models, which significantly increases training costs. To address
these issues, we propose a novel framework named LATex for AG-ReID, which
adopts prompt-tuning strategies to leverage attribute-based text knowledge.
Specifically, with the Contrastive Language-Image Pre-training (CLIP) model, we
first propose an Attribute-aware Image Encoder (AIE) to extract both global
semantic features and attribute-aware features from input images. Then, with
these features, we propose a Prompted Attribute Classifier Group (PACG) to
predict person attributes and obtain attribute representations. Finally, we
design a Coupled Prompt Template (CPT) to transform attribute representations
and view information into structured sentences. These sentences are processed
by the text encoder of CLIP to generate more discriminative features. As a
result, our framework can fully leverage attribute-based text knowledge to
improve AG-ReID performance. Extensive experiments on three AG-ReID benchmarks
demonstrate the effectiveness of our proposed methods. The source code is
available at https://github.com/kevinhu314/LATex.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>More modifications may be performed</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EmoAttack: Emotion-to-Image Diffusion Models for Emotional Backdoor
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15863v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15863v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyu Wei, Shanmin Pang, Qi Guo, Yizhuo Ma, Xiaofeng Cao, Qing Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image diffusion models can generate realistic images based on textual
inputs, enabling users to convey their opinions visually through language.
Meanwhile, within language, emotion plays a crucial role in expressing personal
opinions in our daily lives and the inclusion of maliciously negative content
can lead users astray, exacerbating negative emotions. Recognizing the success
of diffusion models and the significance of emotion, we investigate a
previously overlooked risk associated with text-to-image diffusion models, that
is, utilizing emotion in the input texts to introduce negative content and
provoke unfavorable emotions in users. Specifically, we identify a new backdoor
attack, i.e., emotion-aware backdoor attack (EmoAttack), which introduces
malicious negative content triggered by emotional texts during image
generation. We formulate such an attack as a diffusion personalization problem
to avoid extensive model retraining and propose the EmoBooth. Unlike existing
personalization methods, our approach fine-tunes a pre-trained diffusion model
by establishing a mapping between a cluster of emotional words and a given
reference image containing malicious negative content. To validate the
effectiveness of our method, we built a dataset and conducted extensive
analysis and discussion about its effectiveness. Given consumers' widespread
use of diffusion models, uncovering this threat is critical for society.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Two Heads are Better than One: Robust Learning Meets Multi-branch Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.08083v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.08083v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zongyuan Zhang, Qingwen Bu, Tianyang Duan, Zheng Lin, Yuhao Qing, Zihan Fang, Heming Cui, Dong Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNs) are vulnerable to adversarial examples, in which
DNNs are misled to false outputs due to inputs containing imperceptible
perturbations. Adversarial training, a reliable and effective method of
defense, may significantly reduce the vulnerability of neural networks and
becomes the de facto standard for robust learning. While many recent works
practice the data-centric philosophy, such as how to generate better
adversarial examples or use generative models to produce additional training
data, we look back to the models themselves and revisit the adversarial
robustness from the perspective of deep feature distribution as an insightful
complementarity. In this paper, we propose \textit{Branch Orthogonality
adveRsarial Training} (BORT) to obtain state-of-the-art performance with solely
the original dataset for adversarial training. To practice our design idea of
integrating multiple orthogonal solution spaces, we leverage a simple and
straightforward multi-branch neural network that eclipses adversarial attacks
with no increase in inference time. We heuristically propose a corresponding
loss function, branch-orthogonal loss, to make each solution space of the
multi-branch model orthogonal. We evaluate our approach on CIFAR-10, CIFAR-100
and SVHN against $\ell_{\infty}$ norm-bounded perturbations of size $\epsilon =
8/255$, respectively. Exhaustive experiments are conducted to show that our
method goes beyond all state-of-the-art methods without any tricks. Compared to
all methods that do not use additional data for training, our models achieve
67.3\% and 41.5\% robust accuracy on CIFAR-10 and CIFAR-100 (improving upon the
state-of-the-art by +7.23\% and +9.07\%). We also outperform methods using a
training set with a far larger scale than ours.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-ready version for ICPADS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Static for Dynamic: Towards a Deeper Understanding of Dynamic Facial
  Expressions Using Static Expression Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06154v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06154v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yin Chen, Jia Li, Yu Zhang, Zhenzhen Hu, Shiguang Shan, Meng Wang, Richang Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamic facial expression recognition (DFER) infers emotions from the
temporal evolution of expressions, unlike static facial expression recognition
(SFER), which relies solely on a single snapshot. This temporal analysis
provides richer information and promises greater recognition capability.
However, current DFER methods often exhibit unsatisfied performance largely due
to fewer training samples compared to SFER. Given the inherent correlation
between static and dynamic expressions, we hypothesize that leveraging the
abundant SFER data can enhance DFER. To this end, we propose Static-for-Dynamic
(S4D), a unified dual-modal learning framework that integrates SFER data as a
complementary resource for DFER. Specifically, S4D employs dual-modal
self-supervised pre-training on facial images and videos using a shared Vision
Transformer (ViT) encoder-decoder architecture, yielding improved
spatiotemporal representations. The pre-trained encoder is then fine-tuned on
static and dynamic expression datasets in a multi-task learning setup to
facilitate emotional information interaction. Unfortunately, vanilla multi-task
learning in our study results in negative transfer. To address this, we propose
an innovative Mixture of Adapter Experts (MoAE) module that facilitates
task-specific knowledge acquisition while effectively extracting shared
knowledge from both static and dynamic expression data. Extensive experiments
demonstrate that S4D achieves a deeper understanding of DFER, setting new
state-of-the-art performance on FERV39K, MAFW, and DFEW benchmarks, with
weighted average recall (WAR) of 53.65\%, 58.44\%, and 76.68\%, respectively.
Additionally, a systematic correlation analysis between SFER and DFER tasks is
presented, which further elucidates the potential benefits of leveraging SFER.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The code and model are publicly available here
  https://github.com/MSA-LMC/S4D</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tunable-Generalization Diffusion Powered by <span class="highlight-title">Self-Supervised</span> Contextual
  Sub-Data for Low-Dose CT Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.23885v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.23885v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoquan Wei, Liu Shi, Zekun Zhou, Wenzhe Shan, Qiegen Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current models based on deep learning for low-dose CT denoising rely heavily
on paired data and generalize poorly. Even the more concerned diffusion models
need to learn the distribution of clean data for reconstruction, which is
difficult to satisfy in medical clinical applications. At the same time,
self-supervised-based methods face the challenge of significant degradation of
generalizability of models pre-trained for the current dose to expand to other
doses. To address these issues, this work proposes a novel method of
TUnable-geneRalizatioN Diffusion (TurnDiff) powered by self-supervised
contextual sub-data for low-dose CT reconstruction. Firstly, a contextual
subdata self-enhancing similarity strategy is designed for denoising centered
on the LDCT projection domain, which provides an initial prior for the
subsequent progress. Subsequently, the initial prior is used to combine
knowledge distillation with a deep combination of latent diffusion models for
optimizing image details. The pre-trained model is used for inference
reconstruction, and the pixel-level self-correcting fusion technique is
proposed for fine-grained reconstruction of the image domain to enhance the
image fidelity, using the initial prior and the LDCT image as a guide. In
addition, the technique is flexibly applied to the generalization of upper and
lower doses or even unseen doses. Dual-domain strategy cascade for
self-supervised LDCT denoising, TurnDiff requires only LDCT projection domain
data for training and testing. Comprehensive evaluation on both benchmark
datasets and real-world data demonstrates that TurnDiff consistently
outperforms state-of-the-art methods in both reconstruction and generalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Seeing Structural Failure Before it Happens: An Image-Based
  Physics-Informed Neural Network (PINN) for Spaghetti Bridge Load Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23117v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23117v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omer Jauhar Khan, Sudais Khan, Hafeez Anwar, Shahzeb Khan, Shams Ul Arifeen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Physics Informed Neural Networks (PINNs) are gaining attention for their
ability to embed physical laws into deep learning models, which is particularly
useful in structural engineering tasks with limited data. This paper aims to
explore the use of PINNs to predict the weight of small scale spaghetti
bridges, a task relevant to understanding load limits and potential failure
modes in simplified structural models. Our proposed framework incorporates
physics-based constraints to the prediction model for improved performance. In
addition to standard PINNs, we introduce a novel architecture named Physics
Informed Kolmogorov Arnold Network (PIKAN), which blends universal function
approximation theory with physical insights. The structural parameters provided
as input to the model are collected either manually or through computer vision
methods. Our dataset includes 15 real bridges, augmented to 100 samples, and
our best model achieves an $R^2$ score of 0.9603 and a mean absolute error
(MAE) of 10.50 units. From applied perspective, we also provide a web based
interface for parameter entry and prediction. These results show that PINNs can
offer reliable estimates of structural weight, even with limited data, and may
help inform early stage failure analysis in lightweight bridge designs.
  The complete data and code are available at
https://github.com/OmerJauhar/PINNS-For-Spaghetti-Bridges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 17 figures. Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SD-ReID: View-aware Stable Diffusion for Aerial-Ground Person
  Re-Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.09549v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.09549v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhao Wang, Xiang Hu, Lixin Wang, Pingping Zhang, Huchuan Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aerial-Ground Person Re-IDentification (AG-ReID) aims to retrieve specific
persons across cameras with different viewpoints. Previous works focus on
designing discriminative models to maintain the identity consistency despite
drastic changes in camera viewpoints. The core idea behind these methods is
quite natural, but designing a view-robust model is a very challenging task.
Moreover, they overlook the contribution of view-specific features in enhancing
the model's ability to represent persons. To address these issues, we propose a
novel generative framework named SD-ReID for AG-ReID, which leverages
generative models to mimic the feature distribution of different views while
extracting robust identity representations. More specifically, we first train a
ViT-based model to extract person representations along with controllable
conditions, including identity and view conditions. We then fine-tune the
Stable Diffusion (SD) model to enhance person representations guided by these
controllable conditions. Furthermore, we introduce the View-Refined Decoder
(VRD) to bridge the gap between instance-level and global-level features.
Finally, both person representations and all-view features are employed to
retrieve target persons. Extensive experiments on five AG-ReID benchmarks
(i.e., CARGO, AG-ReIDv1, AG-ReIDv2, LAGPeR and G2APS-ReID) demonstrate the
effectiveness of our proposed method. The source code will be available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>More modifications may performed</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TRUST-VL: An Explainable News Assistant for General Multimodal
  Misinformation Detection <span class="chip">EMNLP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04448v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04448v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zehong Yan, Peng Qi, Wynne Hsu, Mong Li Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal misinformation, encompassing textual, visual, and cross-modal
distortions, poses an increasing societal threat that is amplified by
generative AI. Existing methods typically focus on a single type of distortion
and struggle to generalize to unseen scenarios. In this work, we observe that
different distortion types share common reasoning capabilities while also
requiring task-specific skills. We hypothesize that joint training across
distortion types facilitates knowledge sharing and enhances the model's ability
to generalize. To this end, we introduce TRUST-VL, a unified and explainable
vision-language model for general multimodal misinformation detection. TRUST-VL
incorporates a novel Question-Aware Visual Amplifier module, designed to
extract task-specific visual features. To support training, we also construct
TRUST-Instruct, a large-scale instruction dataset containing 198K samples
featuring structured reasoning chains aligned with human fact-checking
workflows. Extensive experiments on both in-domain and zero-shot benchmarks
demonstrate that TRUST-VL achieves state-of-the-art performance, while also
offering strong generalization and interpretability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2025 Oral; Project Homepage:
  https://yanzehong.github.io/trust-vl/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Paper2Poster: Towards Multimodal Poster Automation from Scientific
  Papers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.21497v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.21497v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Pang, Kevin Qinghong Lin, Xiangru Jian, Xi He, Philip Torr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Academic poster generation is a crucial yet challenging task in scientific
communication, requiring the compression of long-context interleaved documents
into a single, visually coherent page. To address this challenge, we introduce
the first benchmark and metric suite for poster generation, which pairs recent
conference papers with author-designed posters and evaluates outputs on
(i)Visual Quality-semantic alignment with human posters, (ii)Textual
Coherence-language fluency, (iii)Holistic Assessment-six fine-grained aesthetic
and informational criteria scored by a VLM-as-judge, and notably
(iv)PaperQuiz-the poster's ability to convey core paper content as measured by
VLMs answering generated quizzes. Building on this benchmark, we propose
PosterAgent, a top-down, visual-in-the-loop multi-agent pipeline: the (a)Parser
distills the paper into a structured asset library; the (b)Planner aligns
text-visual pairs into a binary-tree layout that preserves reading order and
spatial balance; and the (c)Painter-Commenter loop refines each panel by
executing rendering code and using VLM feedback to eliminate overflow and
ensure alignment. In our comprehensive evaluation, we find that GPT-4o
outputs-though visually appealing at first glance-often exhibit noisy text and
poor PaperQuiz scores, and we find that reader engagement is the primary
aesthetic bottleneck, as human-designed posters rely largely on visual
semantics to convey meaning. Our fully open-source variants (e.g. based on the
Qwen-2.5 series) outperform existing 4o-driven multi-agent systems across
nearly all metrics, while using 87% fewer tokens. It transforms a 22-page paper
into a finalized yet editable .pptx poster - all for just $0.005. These
findings chart clear directions for the next generation of fully automated
poster-generation models. The code and datasets are available at
https://github.com/Paper2Poster/Paper2Poster.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://github.com/Paper2Poster/Paper2Poster</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MindGYM: What Matters in Question Synthesis for Thinking-Centric
  Fine-Tuning? <span class="chip">NeurIPS'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.09499v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.09499v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Xu, Daoyuan Chen, Zhenqing Ling, Yaliang Li, Ying Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large foundation models face challenges in acquiring transferable, structured
thinking abilities, especially when supervised with rigid templates or
crowd-annotated instruction datasets. Unlike prior approaches, we focus on a
thinking-centric data synthesis paradigm that enables models to evolve through
self-generated, cognitively guided data. We propose MindGYM, a structured and
scalable framework for question synthesis, composed of: (1) Cognitive Thinking
Process Injection, which infuses high-level reasoning objectives to shape the
model's synthesis behavior; (2) Seed Single-Hop Question Synthesis, generating
atomic questions from diverse semantic types to encourage broader thinking; and
(3) Challenging Multi-Hop QA Synthesis, composing more complex multi-hop
questions based on QA seeds for deeper reasoning. Detailed analysis shows that
synthetic data generated by our method achieves 16.7% higher average quality
and 67.91% lower quality variance compared to baseline sources, highlighting
that both high-quality and self-contained data are essential for effective,
thinking-oriented fine-tuning. MindGYM improves performance on six reasoning
benchmarks, achieving gains of up to 16% on MathVision using only 400 data
samples, and generalizable improvements across different model sizes and
architectures. MindGYM underscores the viability of self-challenging mechanisms
in refining large model capabilities while minimizing human intervention and
resource demands. Code and data are released to promote data-centric research
into self-evolving foundation models driven by their internal reasoning
capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS'25. 30 pages, 2 figures, 13 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ D-HUMOR: Dark Humor Understanding via Multimodal Open-ended Reasoning --
  A Benchmark <span class="highlight-title">Dataset</span> and Method <span class="chip">ICDM</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.06771v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.06771v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sai Kartheek Reddy Kasu, Mohammad Zia Ur Rehman, Shahid Shafi Dar, Rishi Bharat Junghare, Dhanvin Sanjay Namboodiri, Nagendra Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dark humor in online memes poses unique challenges due to its reliance on
implicit, sensitive, and culturally contextual cues. To address the lack of
resources and methods for detecting dark humor in multimodal content, we
introduce a novel dataset of 4,379 Reddit memes annotated for dark humor,
target category (gender, mental health, violence, race, disability, and other),
and a three-level intensity rating (mild, moderate, severe). Building on this
resource, we propose a reasoning-augmented framework that first generates
structured explanations for each meme using a Large Vision-Language Model
(VLM). Through a Role-Reversal Self-Loop, VLM adopts the author's perspective
to iteratively refine its explanations, ensuring completeness and alignment. We
then extract textual features from both the OCR transcript and the self-refined
reasoning via a text encoder, while visual features are obtained using a vision
transformer. A Tri-stream Cross-Reasoning Network (TCRNet) fuses these three
streams, text, image, and reasoning, via pairwise attention mechanisms,
producing a unified representation for classification. Experimental results
demonstrate that our approach outperforms strong baselines across three tasks:
dark humor detection, target identification, and intensity prediction. The
dataset, annotations, and code are released to facilitate further research in
multimodal humor understanding and content moderation. Code and Dataset are
available at:
https://github.com/Sai-Kartheek-Reddy/D-Humor-Dark-Humor-Understanding-via-Multimodal-Open-ended-Reasoning
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IEEE International Conference on Data Mining (ICDM) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Disentangled 4D Gaussian Splatting: Rendering High-Resolution Dynamic
  World at 343 FPS 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.22159v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.22159v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Feng, Hao Sun, Wei Xie, Zhi Zuo, Zhengzhe Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While dynamic novel view synthesis from 2D videos has seen progress,
achieving efficient reconstruction and rendering of dynamic scenes remains a
challenging task. In this paper, we introduce Disentangled 4D Gaussian
Splatting (Disentangled4DGS), a novel representation and rendering pipeline
that achieves real-time performance without compromising visual fidelity.
Disentangled4DGS decouples the temporal and spatial components of 4D Gaussians,
avoiding the need for slicing first and four-dimensional matrix calculations in
prior methods. By projecting temporal and spatial deformations into dynamic 2D
Gaussians and deferring temporal processing, we minimize redundant computations
of 4DGS. Our approach also features a gradient-guided flow loss and temporal
splitting strategy to reduce artifacts. Experiments demonstrate a significant
improvement in rendering speed and quality, achieving 343 FPS when render
1352*1014 resolution images on a single RTX3090 while reducing storage
requirements by at least 4.5%. Our approach sets a new benchmark for dynamic
novel view synthesis, outperforming existing methods on both multi-view and
monocular dynamic scene datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GRPO-Guard: Mitigating Implicit Over-Optimization in Flow Matching via
  Regulated Clipping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.22319v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.22319v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Wang, Jiajun Liang, Jie Liu, Henglin Liu, Gongye Liu, Jun Zheng, Wanyuan Pang, Ao Ma, Zhenyu Xie, Xintao Wang, Meng Wang, Pengfei Wan, Xiaodan Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, GRPO-based reinforcement learning has shown remarkable progress in
optimizing flow-matching models, effectively improving their alignment with
task-specific rewards. Within these frameworks, the policy update relies on
importance-ratio clipping to constrain overconfident positive and negative
gradients. However, in practice, we observe a systematic shift in the
importance-ratio distribution-its mean falls below 1 and its variance differs
substantially across timesteps. This left-shifted and inconsistent distribution
prevents positive-advantage samples from entering the clipped region, causing
the mechanism to fail in constraining overconfident positive updates. As a
result, the policy model inevitably enters an implicit over-optimization
stage-while the proxy reward continues to increase, essential metrics such as
image quality and text-prompt alignment deteriorate sharply, ultimately making
the learned policy impractical for real-world use. To address this issue, we
introduce GRPO-Guard, a simple yet effective enhancement to existing GRPO
frameworks. Our method incorporates ratio normalization, which restores a
balanced and step-consistent importance ratio, ensuring that PPO clipping
properly constrains harmful updates across denoising timesteps. In addition, a
gradient reweighting strategy equalizes policy gradients over noise conditions,
preventing excessive updates from particular timestep regions. Together, these
designs act as a regulated clipping mechanism, stabilizing optimization and
substantially mitigating implicit over-optimization without relying on heavy KL
regularization. Extensive experiments on multiple diffusion backbones (e.g.,
SD3.5M, Flux.1-dev) and diverse proxy tasks demonstrate that GRPO-Guard
significantly reduces over-optimization while maintaining or even improving
generation quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://jingw193.github.io/GRPO-Guard/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RRCANet: Recurrent Reusable-Convolution Attention Network for Infrared
  Small Target Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.02393v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.02393v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongxian Liu, Boyang Li, Ting Liu, Zaiping Lin, Wei An
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Infrared small target detection is a challenging task due to its unique
characteristics (e.g., small, dim, shapeless and changeable). Recently
published CNN-based methods have achieved promising performance with heavy
feature extraction and fusion modules. To achieve efficient and effective
detection, we propose a recurrent reusable-convolution attention network
(RRCA-Net) for infrared small target detection. Specifically, RRCA-Net
incorporates reusable-convolution block (RuCB) in a recurrent manner without
introducing extra parameters. With the help of the repetitive iteration in
RuCB, the high-level information of small targets in the deep layers can be
well maintained and further refined. Then, a dual interactive attention
aggregation module (DIAAM) is proposed to promote the mutual enhancement and
fusion of refined information. In this way, RRCA-Net can both achieve
high-level feature refinement and enhance the correlation of contextual
information between adjacent layers. Moreover, to achieve steady convergence,
we design a target characteristic inspired loss function (DpT-k loss) by
integrating physical and mathematical constraints. Experimental results on
three benchmark datasets (e.g. NUAA-SIRST, IRSTD-1k, DenseSIRST) demonstrate
that our RRCA-Net can achieve comparable performance to the state-of-the-art
methods while maintaining a small number of parameters, and act as a plug and
play module to introduce consistent performance improvement for several popular
IRSTD methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We have updated the journal reference and DOI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Open3D-VQA: A Benchmark for Comprehensive Spatial Reasoning with
  Multimodal Large Language Model in Open Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.11094v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.11094v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weichen Zhang, Zile Zhou, Xin Zeng, Xuchen Liu, Jianjie Fang, Chen Gao, Yong Li, Jinqiang Cui, Xinlei Chen, Xiao-Ping Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatial reasoning is a fundamental capability of multimodal large language
models (MLLMs), yet their performance in open aerial environments remains
underexplored. In this work, we present Open3D-VQA, a novel benchmark for
evaluating MLLMs' ability to reason about complex spatial relationships from an
aerial perspective. The benchmark comprises 73k QA pairs spanning 7 general
spatial reasoning tasks, including multiple-choice, true/false, and
short-answer formats, and supports both visual and point cloud modalities. The
questions are automatically generated from spatial relations extracted from
both real-world and simulated aerial scenes. Evaluation on 13 popular MLLMs
reveals that: 1) Models are generally better at answering questions about
relative spatial relations than absolute distances, 2) 3D LLMs fail to
demonstrate significant advantages over 2D LLMs, and 3) Fine-tuning solely on
the simulated dataset can significantly improve the model's spatial reasoning
performance in real-world scenarios. We release our benchmark, data generation
pipeline, and evaluation toolkit to support further research:
https://github.com/EmbodiedCity/Open3D-VQA.code.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FASL-Seg: Anatomy and Tool Segmentation of Surgical Scenes <span class="chip">ECAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.06159v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.06159v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muraam Abdel-Ghani, Mahmoud Ali, Mohamed Ali, Fatmaelzahraa Ahmed, Muhammad Arsalan, Abdulaziz Al-Ali, Shidin Balakrishnan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing popularity of robotic minimally invasive surgeries has made deep
learning-based surgical training a key area of research. A thorough
understanding of the surgical scene components is crucial, which semantic
segmentation models can help achieve. However, most existing work focuses on
surgical tools and overlooks anatomical objects. Additionally, current
state-of-the-art (SOTA) models struggle to balance capturing high-level
contextual features and low-level edge features. We propose a Feature-Adaptive
Spatial Localization model (FASL-Seg), designed to capture features at multiple
levels of detail through two distinct processing streams, namely a Low-Level
Feature Projection (LLFP) and a High-Level Feature Projection (HLFP) stream,
for varying feature resolutions - enabling precise segmentation of anatomy and
surgical instruments. We evaluated FASL-Seg on surgical segmentation benchmark
datasets EndoVis18 and EndoVis17 on three use cases. The FASL-Seg model
achieves a mean Intersection over Union (mIoU) of 72.71% on parts and anatomy
segmentation in EndoVis18, improving on SOTA by 5%. It further achieves a mIoU
of 85.61% and 72.78% in EndoVis18 and EndoVis17 tool type segmentation,
respectively, outperforming SOTA overall performance, with comparable per-class
SOTA results in both datasets and consistent performance in various classes for
anatomy and instruments, demonstrating the effectiveness of distinct processing
streams for varying feature resolutions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures, In Proceedings of European Conference on
  Artificial Intelligence (ECAI) 2025 <https://doi.org/10.3233/FAIA250908></span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Omni-Effects: Unified and Spatially-Controllable Visual Effects
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.07981v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.07981v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangyuan Mao, Aiming Hao, Jintao Chen, Dongxia Liu, Xiaokun Feng, Jiashu Zhu, Meiqi Wu, Chubin Chen, Jiahong Wu, Xiangxiang Chu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual effects (VFX) are essential visual enhancements fundamental to modern
cinematic production. Although video generation models offer cost-efficient
solutions for VFX production, current methods are constrained by per-effect
LoRA training, which limits generation to single effects. This fundamental
limitation impedes applications that require spatially controllable composite
effects, i.e., the concurrent generation of multiple effects at designated
locations. However, integrating diverse effects into a unified framework faces
major challenges: interference from effect variations and spatial
uncontrollability during multi-VFX joint training. To tackle these challenges,
we propose Omni-Effects, a first unified framework capable of generating
prompt-guided effects and spatially controllable composite effects. The core of
our framework comprises two key innovations: (1) LoRA-based Mixture of Experts
(LoRA-MoE), which employs a group of expert LoRAs, integrating diverse effects
within a unified model while effectively mitigating cross-task interference.
(2) Spatial-Aware Prompt (SAP) incorporates spatial mask information into the
text token, enabling precise spatial control. Furthermore, we introduce an
Independent-Information Flow (IIF) module integrated within the SAP, isolating
the control signals corresponding to individual effects to prevent any unwanted
blending. To facilitate this research, we construct a comprehensive VFX dataset
Omni-VFX via a novel data collection pipeline combining image editing and
First-Last Frame-to-Video (FLF2V) synthesis, and introduce a dedicated VFX
evaluation framework for validating model performance. Extensive experiments
demonstrate that Omni-Effects achieves precise spatial control and diverse
effect generation, enabling users to specify both the category and location of
desired effects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Defending Multimodal Backdoored Models by Repulsive Visual <span class="highlight-title">Prompt</span> Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20392v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20392v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhifang Zhang, Shuo He, Haobo Wang, Bingquan Shen, Lei Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal contrastive learning models (e.g., CLIP) can learn high-quality
representations from large-scale image-text datasets, while they exhibit
significant vulnerabilities to backdoor attacks, raising serious safety
concerns. In this paper, we reveal that CLIP's vulnerabilities primarily stem
from its tendency to encode features beyond in-dataset predictive patterns,
compromising its visual feature resistivity to input perturbations. This makes
its encoded features highly susceptible to being reshaped by backdoor triggers.
To address this challenge, we propose Repulsive Visual Prompt Tuning (RVPT), a
novel defense approach that employs deep visual prompt tuning with a specially
designed feature-repelling loss. Specifically, RVPT adversarially repels the
encoded features from deeper layers while optimizing the standard cross-entropy
loss, ensuring that only predictive features in downstream tasks are encoded,
thereby enhancing CLIP's visual feature resistivity against input perturbations
and mitigating its susceptibility to backdoor attacks. Unlike existing
multimodal backdoor defense methods that typically require the availability of
poisoned data or involve fine-tuning the entire model, RVPT leverages few-shot
downstream clean samples and only tunes a small number of parameters. Empirical
results demonstrate that RVPT tunes only 0.27\% of the parameters in CLIP, yet
it significantly outperforms state-of-the-art defense methods, reducing the
attack success rate from 89.70\% to 2.76\% against the most advanced multimodal
attacks on ImageNet and effectively generalizes its defensive capabilities
across multiple datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FARMER: Flow AutoRegressive <span class="highlight-title">Transformer</span> over Pixels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23588v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23588v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangting Zheng, Qinyu Zhao, Tao Yang, Fei Xiao, Zhijie Lin, Jie Wu, Jiajun Deng, Yanyong Zhang, Rui Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Directly modeling the explicit likelihood of the raw data distribution is key
topic in the machine learning area, which achieves the scaling successes in
Large Language Models by autoregressive modeling. However, continuous AR
modeling over visual pixel data suffer from extremely long sequences and
high-dimensional spaces. In this paper, we present FARMER, a novel end-to-end
generative framework that unifies Normalizing Flows (NF) and Autoregressive
(AR) models for tractable likelihood estimation and high-quality image
synthesis directly from raw pixels. FARMER employs an invertible autoregressive
flow to transform images into latent sequences, whose distribution is modeled
implicitly by an autoregressive model. To address the redundancy and complexity
in pixel-level modeling, we propose a self-supervised dimension reduction
scheme that partitions NF latent channels into informative and redundant
groups, enabling more effective and efficient AR modeling. Furthermore, we
design a one-step distillation scheme to significantly accelerate inference
speed and introduce a resampling-based classifier-free guidance algorithm to
boost image generation quality. Extensive experiments demonstrate that FARMER
achieves competitive performance compared to existing pixel-based generative
models while providing exact likelihoods and scalable training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Bytedance Seed Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SPARKE: Scalable <span class="highlight-title">Prompt</span>-Aware Diversity and Novelty Guidance in
  Diffusion Models via RKE Score 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.10173v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.10173v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Jalali, Haoyu Lei, Amin Gohari, Farzan Farnia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have demonstrated remarkable success in high-fidelity image
synthesis and prompt-guided generative modeling. However, ensuring adequate
diversity in generated samples of prompt-guided diffusion models remains a
challenge, particularly when the prompts span a broad semantic spectrum and the
diversity of generated data needs to be evaluated in a prompt-aware fashion
across semantically similar prompts. Recent methods have introduced guidance
via diversity measures to encourage more varied generations. In this work, we
extend the diversity measure-based approaches by proposing the Scalable
Prompt-Aware R\'eny Kernel Entropy Diversity Guidance (SPARKE) method for
prompt-aware diversity guidance. SPARKE utilizes conditional entropy for
diversity guidance, which dynamically conditions diversity measurement on
similar prompts and enables prompt-aware diversity control. While the
entropy-based guidance approach enhances prompt-aware diversity, its reliance
on the matrix-based entropy scores poses computational challenges in
large-scale generation settings. To address this, we focus on the special case
of Conditional latent RKE Score Guidance, reducing entropy computation and
gradient-based optimization complexity from the $O(n^3)$ of general entropy
measures to $O(n)$. The reduced computational complexity allows for
diversity-guided sampling over potentially thousands of generation rounds on
different prompts. We numerically test the SPARKE method on several
text-to-image diffusion models, demonstrating that the proposed method improves
the prompt-aware diversity of the generated data without incurring significant
computational costs. We release our code on the project page:
https://mjalali.github.io/SPARKE
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Real-Time Neural Video Compression with Unified Intra and Inter Coding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.14431v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.14431v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui Xiang, Yifan Bian, Li Li, Jingran Wu, Xianguo Zhang, Dong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural video compression (NVC) technologies have advanced rapidly in recent
years, yielding state-of-the-art schemes such as DCVC-RT that offer superior
compression efficiency to H.266/VVC and real-time encoding/decoding
capabilities. Nonetheless, existing NVC schemes have several limitations,
including inefficiency in dealing with disocclusion and new content, interframe
error propagation and accumulation, among others. To eliminate these
limitations, we borrow the idea from classic video coding schemes, which allow
intra coding within inter-coded frames. With the intra coding tool enabled,
disocclusion and new content are properly handled, and interframe error
propagation is naturally intercepted without the need for manual refresh
mechanisms. We present an NVC framework with unified intra and inter coding,
where every frame is processed by a single model that is trained to perform
intra/inter coding adaptively. Moreover, we propose a simultaneous two-frame
compression design to exploit interframe redundancy not only forwardly but also
backwardly. Experimental results show that our scheme outperforms DCVC-RT by an
average of 12.1% BD-rate reduction, delivers more stable bitrate and quality
per frame, and retains real-time encoding/decoding performances. Code and
models will be released.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TeleEgo: Benchmarking Egocentric AI Assistants in the Wild 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23981v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23981v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaqi Yan, Ruilong Ren, Jingren Liu, Shuning Xu, Ling Wang, Yiheng Wang, Yun Wang, Long Zhang, Xiangyu Chen, Changzhi Sun, Jixiang Luo, Dell Zhang, Hao Sun, Chi Zhang, Xuelong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Egocentric AI assistants in real-world settings must process multi-modal
inputs (video, audio, text), respond in real time, and retain evolving
long-term memory. However, existing benchmarks typically evaluate these
abilities in isolation, lack realistic streaming scenarios, or support only
short-term tasks. We introduce \textbf{TeleEgo}, a long-duration, streaming,
omni-modal benchmark for evaluating egocentric AI assistants in realistic daily
contexts. The dataset features over 14 hours per participant of synchronized
egocentric video, audio, and text across four domains: work \& study, lifestyle
\& routines, social activities, and outings \& culture. All data is aligned on
a unified global timeline and includes high-quality visual narrations and
speech transcripts, curated through human refinement.TeleEgo defines 12
diagnostic subtasks across three core capabilities: Memory (recalling past
events), Understanding (interpreting the current moment), and Cross-Memory
Reasoning (linking distant events). It contains 3,291 human-verified QA items
spanning multiple question formats (single-choice, binary, multi-choice, and
open-ended), evaluated strictly in a streaming setting. We propose two key
metrics -- Real-Time Accuracy and Memory Persistence Time -- to jointly assess
correctness, temporal responsiveness, and long-term retention. TeleEgo provides
a realistic and comprehensive evaluation to advance the development of
practical AI assistants.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DOVE: Efficient One-Step Diffusion Model for Real-World Video
  Super-Resolution <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.16239v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.16239v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Chen, Zichen Zou, Kewei Zhang, Xiongfei Su, Xin Yuan, Yong Guo, Yulun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have demonstrated promising performance in real-world video
super-resolution (VSR). However, the dozens of sampling steps they require,
make inference extremely slow. Sampling acceleration techniques, particularly
single-step, provide a potential solution. Nonetheless, achieving one step in
VSR remains challenging, due to the high training overhead on video data and
stringent fidelity demands. To tackle the above issues, we propose DOVE, an
efficient one-step diffusion model for real-world VSR. DOVE is obtained by
fine-tuning a pretrained video diffusion model (i.e., CogVideoX). To
effectively train DOVE, we introduce the latent-pixel training strategy. The
strategy employs a two-stage scheme to gradually adapt the model to the video
super-resolution task. Meanwhile, we design a video processing pipeline to
construct a high-quality dataset tailored for VSR, termed HQ-VSR. Fine-tuning
on this dataset further enhances the restoration capability of DOVE. Extensive
experiments show that DOVE exhibits comparable or superior performance to
multi-step diffusion-based VSR methods. It also offers outstanding inference
efficiency, achieving up to a 28$\times$ speed-up over existing methods such as
MGLD-VSR. Code is available at: https://github.com/zhengchen1999/DOVE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2025. Code is available at:
  https://github.com/zhengchen1999/DOVE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language-guided Open-world Video Anomaly Detection under Weak
  Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.13160v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.13160v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Liu, Xiaoyu Wu, Jianqin Wu, Xuxu Wang, Linlin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video anomaly detection (VAD) aims to detect anomalies that deviate from what
is expected. In open-world scenarios, the expected events may change as
requirements change. For example, not wearing a mask may be considered abnormal
during a flu outbreak but normal otherwise. However, existing methods assume
that the definition of anomalies is invariable, and thus are not applicable to
the open world. To address this, we propose a novel open-world VAD paradigm
with variable definitions, allowing guided detection through user-provided
natural language at inference time. This paradigm necessitates establishing a
robust mapping from video and textual definition to anomaly scores. Therefore,
we propose LaGoVAD (Language-guided Open-world Video Anomaly Detector), a model
that dynamically adapts anomaly definitions under weak supervision with two
regularization strategies: diversifying the relative durations of anomalies via
dynamic video synthesis, and enhancing feature robustness through contrastive
learning with negative mining. Training such adaptable models requires diverse
anomaly definitions, but existing datasets typically provide labels without
semantic descriptions. To bridge this gap, we collect PreVAD (Pre-training
Video Anomaly Dataset), the largest and most diverse video anomaly dataset to
date, featuring 35,279 annotated videos with multi-level category labels and
descriptions that explicitly define anomalies. Zero-shot experiments on seven
datasets demonstrate LaGoVAD's SOTA performance. Our dataset and code will be
released at https://github.com/Kamino666/LaGoVAD-PreVAD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Buffer layers for Test-Time Adaptation <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.21271v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.21271v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyeongyu Kim, Geonhui Han, Dosik Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent advancements in Test Time Adaptation (TTA), most existing
methodologies focus on updating normalization layers to adapt to the test
domain. However, the reliance on normalization-based adaptation presents key
challenges. First, normalization layers such as Batch Normalization (BN) are
highly sensitive to small batch sizes, leading to unstable and inaccurate
statistics. Moreover, normalization-based adaptation is inherently constrained
by the structure of the pre-trained model, as it relies on training-time
statistics that may not generalize well to unseen domains. These issues limit
the effectiveness of normalization-based TTA approaches, especially under
significant domain shift. In this paper, we introduce a novel paradigm based on
the concept of a Buffer layer, which addresses the fundamental limitations of
normalization layer updates. Unlike existing methods that modify the core
parameters of the model, our approach preserves the integrity of the
pre-trained backbone, inherently mitigating the risk of catastrophic forgetting
during online adaptation. Through comprehensive experimentation, we demonstrate
that our approach not only outperforms traditional methods in mitigating domain
shift and enhancing model robustness, but also exhibits strong resilience to
forgetting. Furthermore, our Buffer layer is modular and can be seamlessly
integrated into nearly all existing TTA frameworks, resulting in consistent
performance improvements across various architectures. These findings validate
the effectiveness and versatility of the proposed solution in real-world domain
adaptation scenarios. The code is available at
https://github.com/hyeongyu-kim/Buffer_TTA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Predicting Any Human Trajectory In Context <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.00871v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.00871v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryo Fujii, Hideo Saito, Ryo Hachiuma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting accurate future trajectories of pedestrians is essential for
autonomous systems but remains a challenging task due to the need for
adaptability in different environments and domains. A common approach involves
collecting scenario-specific data and performing fine-tuning via
backpropagation. However, the need to fine-tune for each new scenario is often
impractical for deployment on edge devices. To address this challenge, we
introduce \paper, an In-Context Learning (ICL) framework for pedestrian
trajectory prediction that enables adaptation without fine-tuning on the
scenario-specific data at inference time without requiring weight updates. We
propose a spatio-temporal similarity-based example selection (STES) method that
selects relevant examples from previously observed trajectories within the same
scene by identifying similar motion patterns at corresponding locations. To
further refine this selection, we introduce prediction-guided example selection
(PG-ES), which selects examples based on both the past trajectory and the
predicted future trajectory, rather than relying solely on the past trajectory.
This approach allows the model to account for long-term dynamics when selecting
examples. Finally, instead of relying on small real-world datasets with limited
scenario diversity, we train our model on a large-scale synthetic dataset to
enhance its prediction ability by leveraging in-context examples. Extensive
experiments demonstrate that TrajICL achieves remarkable adaptation across both
in-domain and cross-domain scenarios, outperforming even fine-tuned approaches
across multiple public benchmarks. Project Page:
https://fujiry0.github.io/TrajICL-project-page/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SPLite Hand: Sparsity-Aware Lightweight 3D Hand Pose Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.16396v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.16396v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yeh Keng Hao, Hsu Tzu Wei, Sun Min
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increasing ubiquity of AR/VR devices, the deployment of deep
learning models on edge devices has become a critical challenge. These devices
require real-time inference, low power consumption, and minimal latency. Many
framework designers face the conundrum of balancing efficiency and performance.
We design a light framework that adopts an encoder-decoder architecture and
introduces several key contributions aimed at improving both efficiency and
accuracy. We apply sparse convolution on a ResNet-18 backbone to exploit the
inherent sparsity in hand pose images, achieving a 42% end-to-end efficiency
improvement. Moreover, we propose our SPLite decoder. This new architecture
significantly boosts the decoding process's frame rate by 3.1x on the Raspberry
Pi 5, while maintaining accuracy on par. To further optimize performance, we
apply quantization-aware training, reducing memory usage while preserving
accuracy (PA-MPJPE increases only marginally from 9.0 mm to 9.1 mm on
FreiHAND). Overall, our system achieves a 2.98x speed-up on a Raspberry Pi 5
CPU (BCM2712 quad-core Arm A76 processor). Our method is also evaluated on
compound benchmark datasets, demonstrating comparable accuracy to
state-of-the-art approaches while significantly enhancing computational
efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AICCC 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From One to More: Contextual Part Latents for 3D Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.08772v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.08772v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaocong Dong, Lihe Ding, Xiao Chen, Yaokun Li, Yuxin Wang, Yucheng Wang, Qi Wang, Jaehyeok Kim, Chenjian Gao, Zhanpeng Huang, Zibin Wang, Tianfan Xue, Dan Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in 3D generation have transitioned from multi-view 2D
rendering approaches to 3D-native latent diffusion frameworks that exploit
geometric priors in ground truth data. Despite progress, three key limitations
persist: (1) Single-latent representations fail to capture complex multi-part
geometries, causing detail degradation; (2) Holistic latent coding neglects
part independence and interrelationships critical for compositional design; (3)
Global conditioning mechanisms lack fine-grained controllability. Inspired by
human 3D design workflows, we propose CoPart - a part-aware diffusion framework
that decomposes 3D objects into contextual part latents for coherent multi-part
generation. This paradigm offers three advantages: i) Reduces encoding
complexity through part decomposition; ii) Enables explicit part relationship
modeling; iii) Supports part-level conditioning. We further develop a mutual
guidance strategy to fine-tune pre-trained diffusion models for joint part
latent denoising, ensuring both geometric coherence and foundation model
priors. To enable large-scale training, we construct Partverse - a novel 3D
part dataset derived from Objaverse through automated mesh segmentation and
human-verified annotations. Extensive experiments demonstrate CoPart's superior
capabilities in part-level editing, articulated object generation, and scene
composition with unprecedented controllability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://copart3d.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cycle Diffusion Model for Counterfactual Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.24267v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.24267v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangrui Huang, Alan Wang, Binxu Li, Bailey Trang, Ridvan Yesiloglu, Tianyu Hua, Wei Peng, Ehsan Adeli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep generative models have demonstrated remarkable success in medical image
synthesis. However, ensuring conditioning faithfulness and high-quality
synthetic images for direct or counterfactual generation remains a challenge.
In this work, we introduce a cycle training framework to fine-tune diffusion
models for improved conditioning adherence and enhanced synthetic image
realism. Our approach, Cycle Diffusion Model (CDM), enforces consistency
between generated and original images by incorporating cycle constraints,
enabling more reliable direct and counterfactual generation. Experiments on a
combined 3D brain MRI dataset (from ABCD, HCP aging & young adults, ADNI, and
PPMI) show that our method improves conditioning accuracy and enhances image
quality as measured by FID and SSIM. The results suggest that the cycle
strategy used in CDM can be an effective method for refining diffusion-based
medical image generation, with applications in data augmentation,
counterfactual, and disease progression modeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Empowering Agentic Video Analytics Systems with Video Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.00254v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.00254v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Yan, Shiqi Jiang, Ting Cao, Yifan Yang, Qianqian Yang, Yuanchao Shu, Yuqing Yang, Lili Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI-driven video analytics has become increasingly important across diverse
domains. However, existing systems are often constrained to specific,
predefined tasks, limiting their adaptability in open-ended analytical
scenarios. The recent emergence of Vision Language Models (VLMs) as
transformative technologies offers significant potential for enabling
open-ended video understanding, reasoning, and analytics. Nevertheless, their
limited context windows present challenges when processing ultra-long video
content, which is prevalent in real-world applications. To address this, we
introduce AVA, a VLM-powered system designed for open-ended, advanced video
analytics. AVA incorporates two key innovations: (1) the near real-time
construction of Event Knowledge Graphs (EKGs) for efficient indexing of long or
continuous video streams, and (2) an agentic retrieval-generation mechanism
that leverages EKGs to handle complex and diverse queries. Comprehensive
evaluations on public benchmarks, LVBench and VideoMME-Long, demonstrate that
AVA achieves state-of-the-art performance, attaining 62.3% and 64.1% accuracy,
respectively-significantly surpassing existing VLM and video
Retrieval-Augmented Generation (RAG) systems. Furthermore, to evaluate video
analytics in ultra-long and open-world video scenarios, we introduce a new
benchmark, AVA-100. This benchmark comprises 8 videos, each exceeding 10 hours
in duration, along with 120 manually annotated, diverse, and complex
question-answer pairs. On AVA-100, AVA achieves top-tier performance with an
accuracy of 75.8%. The source code of AVA is available at
https://github.com/I-ESC/Project-Ava. The AVA-100 benchmark can be accessed at
https://huggingface.co/datasets/iesc/Ava-100.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NDSI 2026, 19pages, 12 figures, complementary evaluations
  and appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unleashing Diffusion <span class="highlight-title">Transformer</span>s for Visual Correspondence by
  Modulating Massive Activations <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.18584v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.18584v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaofan Gan, Yuanpeng Tu, Xi Chen, Tieyuan Chen, Yuxi Li, Mehrtash Harandi, Weiyao Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained stable diffusion models (SD) have shown great advances in visual
correspondence. In this paper, we investigate the capabilities of Diffusion
Transformers (DiTs) for accurate dense correspondence. Distinct from SD, DiTs
exhibit a critical phenomenon in which very few feature activations exhibit
significantly larger values than others, known as \textit{massive activations},
leading to uninformative representations and significant performance
degradation for DiTs. The massive activations consistently concentrate at very
few fixed dimensions across all image patch tokens, holding little local
information. We trace these dimension-concentrated massive activations and find
that such concentration can be effectively localized by the zero-initialized
Adaptive Layer Norm (AdaLN-zero). Building on these findings, we propose
Diffusion Transformer Feature (DiTF), a training-free framework designed to
extract semantic-discriminative features from DiTs. Specifically, DiTF employs
AdaLN to adaptively localize and normalize massive activations with
channel-wise modulation. In addition, we develop a channel discard strategy to
further eliminate the negative impacts from massive activations. Experimental
results demonstrate that our DiTF outperforms both DINO and SD-based models and
establishes a new state-of-the-art performance for DiTs in different visual
correspondence tasks (\eg, with +9.4\% on Spair-71k and +4.4\% on AP-10K-C.S.).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMEdge: Accelerating On-device Multimodal Inference via Pipelined
  Sensing and Encoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25327v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25327v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runxi Huang, Mingxuan Yu, Mingyu Tsoi, Xiaomin Ouyang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-time multimodal inference on resource-constrained edge devices is
essential for applications such as autonomous driving, human-computer
interaction, and mobile health. However, prior work often overlooks the tight
coupling between sensing dynamics and model execution, as well as the complex
inter-modality dependencies. In this paper, we propose MMEdge, an new on-device
multi-modal inference framework based on pipelined sensing and encoding.
Instead of waiting for complete sensor inputs, MMEdge decomposes the entire
inference process into a sequence of fine-grained sensing and encoding units,
allowing computation to proceed incrementally as data arrive. MMEdge also
introduces a lightweight but effective temporal aggregation module that
captures rich temporal dynamics across different pipelined units to maintain
accuracy performance. Such pipelined design also opens up opportunities for
fine-grained cross-modal optimization and early decision-making during
inference. To further enhance system performance under resource variability and
input data complexity, MMEdge incorporates an adaptive multimodal configuration
optimizer that dynamically selects optimal sensing and model configurations for
each modality under latency constraints, and a cross-modal speculative skipping
mechanism that bypasses future units of slower modalities when early
predictions reach sufficient confidence. We evaluate MMEdge using two public
multimodal datasets and deploy it on a real-world unmanned aerial vehicle
(UAV)-based multimodal testbed. The results show that MMEdge significantly
reduces end-to-end latency while maintaining high task accuracy across various
system and data dynamics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code available at: https://github.com/HKUST-MINSys-Lab/MMEdge.
  Accepted by SenSys 2026</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Boosting Generative Adversarial Transferability with <span class="highlight-title">Self-supervised</span>
  Vision <span class="highlight-title">Transformer</span> Features <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21046v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21046v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shangbo Wu, Yu-an Tan, Ruinan Ma, Wencong Ma, Dehua Zhu, Yuanzhang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability of deep neural networks (DNNs) come from extracting and
interpreting features from the data provided. By exploiting intermediate
features in DNNs instead of relying on hard labels, we craft adversarial
perturbation that generalize more effectively, boosting black-box
transferability. These features ubiquitously come from supervised learning in
previous work. Inspired by the exceptional synergy between self-supervised
learning and the Transformer architecture, this paper explores whether
exploiting self-supervised Vision Transformer (ViT) representations can improve
adversarial transferability. We present dSVA -- a generative dual
self-supervised ViT features attack, that exploits both global structural
features from contrastive learning (CL) and local textural features from masked
image modeling (MIM), the self-supervised learning paradigm duo for ViTs. We
design a novel generative training framework that incorporates a generator to
create black-box adversarial examples, and strategies to train the generator by
exploiting joint features and the attention mechanism of self-supervised ViTs.
Our findings show that CL and MIM enable ViTs to attend to distinct feature
tendencies, which, when exploited in tandem, boast great adversarial
generalizability. By disrupting dual deep features distilled by self-supervised
ViTs, we are rewarded with remarkable black-box transferability to models of
various architectures that outperform state-of-the-arts. Code available at
https://github.com/spencerwooo/dSVA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 9 figures, accepted at ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Signal-SGN: A Spiking Graph Convolutional Network for Skeletal Action
  Recognition via Learning Temporal-Frequency Dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01701v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01701v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naichuan Zheng, Yuchen Du, Hailun Xia, Zeyu Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For multimodal skeleton-based action recognition, Graph Convolutional
Networks (GCNs) are effective models. Still, their reliance on floating-point
computations leads to high energy consumption, limiting their applicability in
battery-powered devices. While energy-efficient, Spiking Neural Networks (SNNs)
struggle to model skeleton dynamics, leading to suboptimal solutions. We
propose Signal-SGN (Spiking Graph Convolutional Network), which utilizes the
temporal dimension of skeleton sequences as the spike time steps and represents
features as multi-dimensional discrete stochastic signals for
temporal-frequency domain feature extraction. It combines the 1D Spiking Graph
Convolution (1D-SGC) module and the Frequency Spiking Convolution (FSC) module
to extract features from the skeleton represented as spiking form.
Additionally, the Multi-Scale Wavelet Transform Feature Fusion (MWTF) module is
proposed to extract dynamic spiking features and capture frequency-specific
characteristics, enhancing classification performance. Experiments across three
large-scale datasets reveal Signal-SGN exceeding state-of-the-art SNN-based
methods in accuracy and computational efficiency while attaining comparable
performance with GCN methods and significantly reducing theoretical energy
consumption.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DiffVLA++: Bridging Cognitive Reasoning and End-to-End Driving through
  Metric-Guided Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.17148v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.17148v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Gao, Anqing Jiang, Yiru Wang, Wang Jijun, Hao Jiang, Zhigang Sun, Heng Yuwen, Wang Shuo, Hao Zhao, Sun Hao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional end-to-end (E2E) driving models are effective at generating
physically plausible trajectories, but often fail to generalize to long-tail
scenarios due to the lack of essential world knowledge to understand and reason
about surrounding environments. In contrast, Vision-Language-Action (VLA)
models leverage world knowledge to handle challenging cases, but their limited
3D reasoning capability can lead to physically infeasible actions. In this work
we introduce DiffVLA++, an enhanced autonomous driving framework that
explicitly bridges cognitive reasoning and E2E planning through metric-guided
alignment. First, we build a VLA module directly generating semantically
grounded driving trajectories. Second, we design an E2E module with a dense
trajectory vocabulary that ensures physical feasibility. Third, and most
critically, we introduce a metric-guided trajectory scorer that guides and
aligns the outputs of the VLA and E2E modules, thereby integrating their
complementary strengths. The experiment on the ICCV 2025 Autonomous Grand
Challenge leaderboard shows that DiffVLA++ achieves EPDMS of 49.12.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ChartMuseum: Testing Visual Reasoning Capabilities of Large
  Vision-Language Models <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13444v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13444v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liyan Tang, Grace Kim, Xinyu Zhao, Thom Lake, Wenxuan Ding, Fangcong Yin, Prasann Singhal, Manya Wadhwa, Zeyu Leo Liu, Zayne Sprague, Ramya Namuduri, Bodun Hu, Juan Diego Rodriguez, Puyuan Peng, Greg Durrett
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chart understanding presents a unique challenge for large vision-language
models (LVLMs), as it requires the integration of sophisticated textual and
visual reasoning capabilities. However, current LVLMs exhibit a notable
imbalance between these skills, falling short on visual reasoning that is
difficult to perform in text. We conduct a case study using a synthetic dataset
solvable only through visual reasoning and show that model performance degrades
significantly with increasing visual complexity, while human performance
remains robust. We then introduce ChartMuseum, a new Chart Question Answering
(QA) benchmark containing 1,162 expert-annotated questions spanning multiple
reasoning types, curated from real-world charts across 184 sources,
specifically built to evaluate complex visual and textual reasoning. Unlike
prior chart understanding benchmarks -- where frontier models perform similarly
and near saturation -- our benchmark exposes a substantial gap between model
and human performance, while effectively differentiating model capabilities:
although humans achieve 93% accuracy, the best-performing model Gemini-2.5-Pro
attains only 63.0%, and the leading open-source LVLM Qwen2.5-VL-72B-Instruct
achieves only 38.5%. Moreover, on questions requiring primarily visual
reasoning, all models experience a 35%-55% performance drop from
text-reasoning-heavy question performance. Lastly, our qualitative error
analysis reveals specific categories of visual reasoning that are challenging
for current LVLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025 Datasets & Benchmarks</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neighborhood Feature Pooling for Remote Sensing Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25077v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25077v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fahimeh Orvati Nia, Amirmohammad Mohammadi, Salim Al Kharsa, Pragati Naikare, Zigfried Hampel-Arias, Joshua Peeples
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we propose neighborhood feature pooling (NFP) as a novel
texture feature extraction method for remote sensing image classification. The
NFP layer captures relationships between neighboring inputs and efficiently
aggregates local similarities across feature dimensions. Implemented using
convolutional layers, NFP can be seamlessly integrated into any network.
Results comparing the baseline models and the NFP method indicate that NFP
consistently improves performance across diverse datasets and architectures
while maintaining minimal parameter overhead.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GCVAMD: A Modified CausalVAE Model for Causal Age-related Macular
  Degeneration Risk Factor Detection and Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.02781v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.02781v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daeyoung Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Age Related Macular Degeneration(AMD) has been one of the most leading causes
of permanent vision impairment in ophthalmology. Though treatments, such as
anti VEGF drugs or photodynamic therapies, were developed to slow down the
degenerative process of AMD, there is still no specific cure to reverse vision
loss caused by AMD. Thus, for AMD, detecting existence of risk factors of AMD
or AMD itself within the patient retina in early stages is a crucial task to
reduce the possibility of vision impairment. Apart from traditional approaches,
deep learning based methods, especially attention mechanism based CNNs and
GradCAM based XAI analysis on OCT scans, exhibited successful performance in
distinguishing AMD retina from normal retinas, making it possible to use AI
driven models to aid medical diagnosis and analysis by ophthalmologists
regarding AMD. However, though having significant success, previous works
mostly focused on prediction performance itself, not pathologies or underlying
causal mechanisms of AMD, which can prohibit intervention analysis on specific
factors or even lead to less reliable decisions. Thus, this paper introduces a
novel causal AMD analysis model: GCVAMD, which incorporates a modified
CausalVAE approach that can extract latent causal factors from only raw OCT
images. By considering causality in AMD detection, GCVAMD enables causal
inference such as treatment simulation or intervention analysis regarding major
risk factors: drusen and neovascularization, while returning informative latent
causal features that can enhance downstream tasks. Results show that through
GCVAMD, drusen status and neovascularization status can be identified with AMD
causal mechanisms in GCVAMD latent spaces, which can in turn be used for
various tasks from AMD detection(classification) to intervention analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GameFactory: Creating New Games with Generative Interactive Videos <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08325v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08325v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiwen Yu, Yiran Qin, Xintao Wang, Pengfei Wan, Di Zhang, Xihui Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative videos have the potential to revolutionize game development by
autonomously creating new content. In this paper, we present GameFactory, a
framework for action-controlled scene-generalizable game video generation. We
first address the fundamental challenge of action controllability by
introducing GF-Minecraft, an action-annotated game video dataset without human
bias, and developing an action control module that enables precise control over
both keyboard and mouse inputs. We further extend to support autoregressive
generation for unlimited-length interactive videos. More importantly,
GameFactory tackles the critical challenge of scene-generalizable action
control, which most existing methods fail to address. To enable the creation of
entirely new and diverse games beyond fixed styles and scenes, we leverage the
open-domain generative priors from pre-trained video diffusion models. To
bridge the domain gap between open-domain priors and small-scale game datasets,
we propose a multi-phase training strategy with a domain adapter that decouples
game style learning from action control. This decoupling ensures that action
control learning is no longer bound to specific game styles, thereby achieving
scene-generalizable action control. Experimental results demonstrate that
GameFactory effectively generates open-domain action-controllable game videos,
representing a significant step forward in AI-driven game generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2025 Highlight, Project Page:
  https://yujiwen.github.io/gamefactory</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reasoning Visual Language Model for Chest X-Ray Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23968v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23968v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andriy Myronenko, Dong Yang, Baris Turkbey, Mariam Aboian, Sena Azamat, Esra Akcicek, Hongxu Yin, Pavlo Molchanov, Marc Edgar, Yufan He, Pengfei Guo, Yucheng Tang, Daguang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models (VLMs) have shown strong promise for medical image
analysis, but most remain opaque, offering predictions without the transparent,
stepwise reasoning clinicians rely on. We present a framework that brings
chain-of-thought (CoT) reasoning to chest X-ray interpretation. Inspired by
reasoning-first training paradigms, our approach is designed to learn how
experts reason, not just what they conclude, by aligning intermediate steps
with observable image evidence and radiology workflow. Beyond accuracy, the
explicit reasoning traces support clinical auditability: they reveal why a
conclusion was reached, which alternatives were considered, and where
uncertainty remains, enabling quality assurance, error analysis, and safer
human-AI collaboration.
  Our model couples high-fidelity visual encoding with a two-stage training
recipe: a reasoning-style supervised fine-tuning (SFT) followed by
reinforcement learning (RL) that uses verifiable rewards over a list of X-ray
abnormalities. The model outputs reasoning that mirrors radiologists systematic
thought process, uncertainty, and differential diagnosis. In
out-of-distribution evaluation, the approach achieves competitive multi-label
classification while improving interpretability. In a reader study with expert
radiologists, full reasoning traces increased confidence, supported error
auditing, and reduced time to finalize reports. We release code and the model
NV-Reason-CXR-3B to support community progress toward trustworthy, explainable
AI in chest radiography and other medical imaging tasks where reasoning quality
is as critical as prediction quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NV-Reason-CXR-3B</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">25</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ProfOlaf: Semi-Automated Tool for Systematic Literature <span class="highlight-title">Review</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26750v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26750v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martim Afonso, Nuno Saavedra, Bruno Lourenço, Alexandra Mendes, João Ferreira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Systematic reviews and mapping studies are critical for synthesizing
research, identifying gaps, and guiding future work, but they are often
labor-intensive and time-consuming. Existing tools provide partial support for
specific steps, leaving much of the process manual and error-prone. We present
ProfOlaf, a semi-automated tool designed to streamline systematic reviews while
maintaining methodological rigor. ProfOlaf supports iterative snowballing for
article collection with human-in-the-loop filtering and uses large language
models to assist in analyzing articles, extracting key topics, and answering
queries about the content of papers. By combining automation with guided manual
effort, ProfOlaf enhances the efficiency, quality, and reproducibility of
systematic reviews across research fields. A video describing and demonstrating
ProfOlaf is available at: https://youtu.be/4noUXfcmxsE
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 1 Figure, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AdSum: Two-stream Audio-visual Summarization for Automated Video
  Advertisement Clipping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26569v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26569v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wen Xie, Yanjun Zhu, Gijs Overgoor, Yakov Bart, Agata Lapedriza Garcia, Sarah Ostadabbas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advertisers commonly need multiple versions of the same advertisement (ad) at
varying durations for a single campaign. The traditional approach involves
manually selecting and re-editing shots from longer video ads to create shorter
versions, which is labor-intensive and time-consuming. In this paper, we
introduce a framework for automated video ad clipping using video summarization
techniques. We are the first to frame video clipping as a shot selection
problem, tailored specifically for advertising. Unlike existing general video
summarization methods that primarily focus on visual content, our approach
emphasizes the critical role of audio in advertising. To achieve this, we
develop a two-stream audio-visual fusion model that predicts the importance of
video frames, where importance is defined as the likelihood of a frame being
selected in the firm-produced short ad. To address the lack of ad-specific
datasets, we present AdSum204, a novel dataset comprising 102 pairs of
30-second and 15-second ads from real advertising campaigns. Extensive
experiments demonstrate that our model outperforms state-of-the-art methods
across various metrics, including Average Precision, Area Under Curve,
Spearman, and Kendall.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 32nd International Conference on MultiMedia Modeling</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WeaveRec: An LLM-Based Cross-Domain Sequential Recommendation Framework
  with Model Merging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26546v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26546v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Min Hou, Xin Liu, Le Wu, Chenyi He, Hao Liu, Zhi Li, Xin Li, Si Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-Domain Sequential Recommendation (CDSR) seeks to improve user
preference modeling by transferring knowledge from multiple domains. Despite
the progress made in CDSR, most existing methods rely on overlapping users or
items to establish cross-domain correlations-a requirement that rarely holds in
real-world settings. The advent of large language models (LLM) and
model-merging techniques appears to overcome this limitation by unifying
multi-domain data without explicit overlaps. Yet, our empirical study shows
that naively training an LLM on combined domains-or simply merging several
domain-specific LLMs-often degrades performance relative to a model trained
solely on the target domain. To address these challenges, we first
experimentally investigate the cause of suboptimal performance in LLM-based
cross-domain recommendation and model merging. Building on these insights, we
introduce WeaveRec, which cross-trains multiple LoRA modules with source and
target domain data in a weaving fashion, and fuses them via model merging.
WeaveRec can be extended to multi-source domain scenarios and notably does not
introduce additional inference-time cost in terms of latency or memory.
Furthermore, we provide a theoretical guarantee that WeaveRec can reduce the
upper bound of the expected error in the target domain. Extensive experiments
on single-source, multi-source, and cross-platform cross-domain recommendation
scenarios validate that WeaveRec effectively mitigates performance degradation
and consistently outperforms baseline approaches in real-world recommendation
tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inside CORE-KG: Evaluating Structured <span class="highlight-title">Prompt</span>ing and Coreference
  Resolution for Knowledge Graphs <span class="chip">ICDM 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26512v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26512v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dipak Meher, Carlotta Domeniconi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human smuggling networks are increasingly adaptive and difficult to analyze.
Legal case documents offer critical insights but are often unstructured,
lexically dense, and filled with ambiguous or shifting references, which pose
significant challenges for automated knowledge graph (KG) construction. While
recent LLM-based approaches improve over static templates, they still generate
noisy, fragmented graphs with duplicate nodes due to the absence of guided
extraction and coreference resolution. The recently proposed CORE-KG framework
addresses these limitations by integrating a type-aware coreference module and
domain-guided structured prompts, significantly reducing node duplication and
legal noise. In this work, we present a systematic ablation study of CORE-KG to
quantify the individual contributions of its two key components. Our results
show that removing coreference resolution results in a 28.32% increase in node
duplication and a 4.32% increase in noisy nodes, while removing structured
prompts leads to a 4.34% increase in node duplication and a 73.33% increase in
noisy nodes. These findings offer empirical insights for designing robust
LLM-based pipelines for extracting structured representations from complex
legal texts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICDM 2025 Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LINK-KG: LLM-Driven Coreference-Resolved Knowledge Graphs for Human
  Smuggling Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26486v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26486v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dipak Meher, Carlotta Domeniconi, Guadalupe Correa-Cabrera
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human smuggling networks are complex and constantly evolving, making them
difficult to analyze comprehensively. Legal case documents offer rich factual
and procedural insights into these networks but are often long, unstructured,
and filled with ambiguous or shifting references, posing significant challenges
for automated knowledge graph (KG) construction. Existing methods either
overlook coreference resolution or fail to scale beyond short text spans,
leading to fragmented graphs and inconsistent entity linking. We propose
LINK-KG, a modular framework that integrates a three-stage, LLM-guided
coreference resolution pipeline with downstream KG extraction. At the core of
our approach is a type-specific Prompt Cache, which consistently tracks and
resolves references across document chunks, enabling clean and disambiguated
narratives for structured knowledge graph construction from both short and long
legal texts. LINK-KG reduces average node duplication by 45.21% and noisy nodes
by 32.22% compared to baseline methods, resulting in cleaner and more coherent
graph structures. These improvements establish LINK-KG as a strong foundation
for analyzing complex criminal networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ICKG 2025 Conference, 8 Pages, 2 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vectorized Context-Aware Embeddings for GAT-Based Collaborative
  Filtering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26461v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26461v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danial Ebrat, Sepideh Ahmadian, Luis Rueda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems often struggle with data sparsity and cold-start
scenarios, limiting their ability to provide accurate suggestions for new or
infrequent users. This paper presents a Graph Attention Network (GAT) based
Collaborative Filtering (CF) framework enhanced with Large Language Model (LLM)
driven context aware embeddings. Specifically, we generate concise textual user
profiles and unify item metadata (titles, genres, overviews) into rich textual
embeddings, injecting these as initial node features in a bipartite user item
graph. To further optimize ranking performance, we introduce a hybrid loss
function that combines Bayesian Personalized Ranking (BPR) with a cosine
similarity term and robust negative sampling, ensuring explicit negative
feedback is distinguished from unobserved data. Experiments on the MovieLens
100k and 1M datasets show consistent improvements over state-of-the-art
baselines in Precision, NDCG, and MAP while demonstrating robustness for users
with limited interaction history. Ablation studies confirm the critical role of
LLM-augmented embeddings and the cosine similarity term in capturing nuanced
semantic relationships. Our approach effectively mitigates sparsity and
cold-start limitations by integrating LLM-derived contextual understanding into
graph-based architectures. Future directions include balancing recommendation
accuracy with coverage and diversity, and introducing fairness-aware
constraints and interpretability features to enhance system performance
further.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Barlow Twins for Sequential Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26407v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26407v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ivan Razvorotnev, Marina Munkhoeva, Evgeny Frolov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommendation models must navigate sparse interaction data
popularity bias and conflicting objectives like accuracy versus diversity While
recent contrastive selfsupervised learning SSL methods offer improved accuracy
they come with tradeoffs large batch requirements reliance on handcrafted
augmentations and negative sampling that can reinforce popularity bias In this
paper we introduce BT-SR a novel noncontrastive SSL framework that integrates
the Barlow Twins redundancyreduction principle into a Transformerbased nextitem
recommender BTSR learns embeddings that align users with similar shortterm
behaviors while preserving longterm distinctionswithout requiring negative
sampling or artificial perturbations This structuresensitive alignment allows
BT-SR to more effectively recognize emerging user intent and mitigate the
influence of noisy historical context Our experiments on five public benchmarks
demonstrate that BTSR consistently improves nextitem prediction accuracy and
significantly enhances longtail item coverage and recommendation calibration
Crucially we show that a single hyperparameter can control the
accuracydiversity tradeoff enabling practitioners to adapt recommendations to
specific application needs
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GraphCompliance: Aligning Policy and Context Graphs for LLM-Based
  Regulatory Compliance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26309v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26309v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiseong Chung, Ronny Ko, Wonchul Yoo, Makoto Onizuka, Sungmok Kim, Tae-Wan Kim, Won-Yong Shin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compliance at web scale poses practical challenges: each request may require
a regulatory assessment. Regulatory texts (e.g., the General Data Protection
Regulation, GDPR) are cross-referential and normative, while runtime contexts
are expressed in unstructured natural language. This setting motivates us to
align semantic information in unstructured text with the structured, normative
elements of regulations. To this end, we introduce GraphCompliance, a framework
that represents regulatory texts as a Policy Graph and runtime contexts as a
Context Graph, and aligns them. In this formulation, the policy graph encodes
normative structure and cross-references, whereas the context graph formalizes
events as subject-action-object (SAO) and entity-relation triples. This
alignment anchors the reasoning of a judge large language model (LLM) in
structured information and helps reduce the burden of regulatory interpretation
and event parsing, enabling a focus on the core reasoning step. In experiments
on 300 GDPR-derived real-world scenarios spanning five evaluation tasks,
GraphCompliance yields 4.1-7.2 percentage points (pp) higher micro-F1 than
LLM-only and RAG baselines, with fewer under- and over-predictions, resulting
in higher recall and lower false positive rates. Ablation studies indicate
contributions from each graph component, suggesting that structured
representations and a judge LLM are complementary for normative reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review at The Web Conference 2026 (Semantics & Knowledge
  track). Code will be released upon acceptance. This arXiv v1 contains no
  repository links to preserve double-blind review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiSE: A diffusion probabilistic model for automatic structure
  elucidation of organic compounds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26231v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26231v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haochen Chen, Qi Huang, Anan Wu, Wenhao Zhang, Jianliang Ye, Jianming Wu, Kai Tan, Xin Lu, Xin Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic structure elucidation is essential for self-driving laboratories as
it enables the system to achieve truly autonomous. This capability closes the
experimental feedback loop, ensuring that machine learning models receive
reliable structure information for real-time decision-making and optimization.
Herein, we present DiSE, an end-to-end diffusion-based generative model that
integrates multiple spectroscopic modalities, including MS, 13C and 1H chemical
shifts, HSQC, and COSY, to achieve automated yet accurate structure elucidation
of organic compounds. By learning inherent correlations among spectra through
data-driven approaches, DiSE achieves superior accuracy, strong generalization
across chemically diverse datasets, and robustness to experimental data despite
being trained on calculated spectra. DiSE thus represents a significant advance
toward fully automated structure elucidation, with broad potential in natural
product research, drug discovery, and self-driving laboratories.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReaKase-8B: Legal Case Retrieval via Knowledge and Reasoning
  Representations with LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26178v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26178v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanran Tang, Ruihong Qiu, Xue Li, Zi Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Legal case retrieval (LCR) is a cornerstone of real-world legal decision
making, as it enables practitioners to identify precedents for a given query
case. Existing approaches mainly rely on traditional lexical models and
pretrained language models to encode the texts of legal cases. Yet there are
rich information in the relations among different legal entities as well as the
crucial reasoning process that uncovers how legal facts and legal issues can
lead to judicial decisions. Such relational reasoning process reflects the
distinctive characteristics of each case that can distinguish one from another,
mirroring the real-world judicial process. Naturally, incorporating such
information into the precise case embedding could further enhance the accuracy
of case retrieval. In this paper, a novel ReaKase-8B framework is proposed to
leverage extracted legal facts, legal issues, legal relation triplets and legal
reasoning for effective legal case retrieval. ReaKase-8B designs an in-context
legal case representation learning paradigm with a fine-tuned large language
model. Extensive experiments on two benchmark datasets from COLIEE 2022 and
COLIEE 2023 demonstrate that our knowledge and reasoning augmented embeddings
substantially improve retrieval performance over baseline models, highlighting
the potential of integrating legal reasoning into legal case retrieval systems.
The code has been released on https://github.com/yanran-tang/ReaKase-8B.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OneTrans: Unified Feature Interaction and Sequence Modeling with One
  <span class="highlight-title">Transformer</span> in Industrial Recommender 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26104v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26104v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoqi Zhang, Haolei Pei, Jun Guo, Tianyu Wang, Yufei Feng, Hui Sun, Shaowei Liu, Aixin Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recommendation systems, scaling up feature-interaction modules (e.g.,
Wukong, RankMixer) or user-behavior sequence modules (e.g., LONGER) has
achieved notable success. However, these efforts typically proceed on separate
tracks, which not only hinders bidirectional information exchange but also
prevents unified optimization and scaling. In this paper, we propose OneTrans,
a unified Transformer backbone that simultaneously performs user-behavior
sequence modeling and feature interaction. OneTrans employs a unified tokenizer
to convert both sequential and non-sequential attributes into a single token
sequence. The stacked OneTrans blocks share parameters across similar
sequential tokens while assigning token-specific parameters to non-sequential
tokens. Through causal attention and cross-request KV caching, OneTrans enables
precomputation and caching of intermediate representations, significantly
reducing computational costs during both training and inference. Experimental
results on industrial-scale datasets demonstrate that OneTrans scales
efficiently with increasing parameters, consistently outperforms strong
baselines, and yields a 5.68% lift in per-user GMV in online A/B tests.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ORBIT -- Open Recommendation Benchmark for Reproducible Research with
  Hidden Tests <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26095v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26095v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyuan He, Jiongnan Liu, Vishan Vishesh Oberoi, Bolin Wu, Mahima Jagadeesh Patel, Kangrui Mao, Chuning Shi, I-Ta Lee, Arnold Overwijk, Chenyan Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems are among the most impactful AI applications, interacting
with billions of users every day, guiding them to relevant products, services,
or information tailored to their preferences. However, the research and
development of recommender systems are hindered by existing datasets that fail
to capture realistic user behaviors and inconsistent evaluation settings that
lead to ambiguous conclusions. This paper introduces the Open Recommendation
Benchmark for Reproducible Research with HIdden Tests (ORBIT), a unified
benchmark for consistent and realistic evaluation of recommendation models.
ORBIT offers a standardized evaluation framework of public datasets with
reproducible splits and transparent settings for its public leaderboard.
Additionally, ORBIT introduces a new webpage recommendation task, ClueWeb-Reco,
featuring web browsing sequences from 87 million public, high-quality webpages.
ClueWeb-Reco is a synthetic dataset derived from real, user-consented, and
privacy-guaranteed browsing data. It aligns with modern recommendation
scenarios and is reserved as the hidden test part of our leaderboard to
challenge recommendation models' generalization ability. ORBIT measures 12
representative recommendation models on its public benchmark and introduces a
prompted LLM baseline on the ClueWeb-Reco hidden test. Our benchmark results
reflect general improvements of recommender systems on the public datasets,
with variable individual performances. The results on the hidden test reveal
the limitations of existing approaches in large-scale webpage recommendation
and highlight the potential for improvements with LLM integrations. ORBIT
benchmark, leaderboard, and codebase are available at
https://www.open-reco-bench.ai.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2025 Datasets & Benchmarks track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quality Over Quantity? LLM-Based Curation for a Data-Efficient
  Audio-Video Foundation Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.09205v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.09205v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Vosoughi, Dimitra Emmanouilidou, Hannes Gamper
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrating audio and visual data for training multimodal foundational models
remains a challenge. The Audio-Video Vector Alignment (AVVA) framework
addresses this by considering AV scene alignment beyond mere temporal
synchronization, and leveraging Large Language Models (LLMs) for data curation.
AVVA implements a scoring mechanism for selecting aligned training data
segments. It integrates Whisper, a speech-based foundation model, for audio and
DINOv2 for video analysis in a dual-encoder structure with contrastive learning
on AV pairs. Evaluations on AudioCaps, VALOR, and VGGSound demonstrate the
effectiveness of the proposed model architecture and data curation approach.
AVVA achieves a significant improvement in top-k accuracies for video-to-audio
retrieval on all datasets compared to DenseAV, while using only 192 hrs of
curated training data. Furthermore, an ablation study indicates that the data
curation process effectively trades data quality for data quantity, yielding
increases in top-k retrieval accuracies on AudioCaps, VALOR, and VGGSound,
compared to training on the full spectrum of uncurated data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 5 figures, 2 tables. Accepted at EUSIPCO 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unveiling Unicode's Unseen Underpinnings in Undermining Authorship
  Attribution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.15840v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.15840v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert Dilworth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When using a public communication channel -- whether formal or informal, such
as commenting or posting on social media -- end users have no expectation of
privacy: they compose a message and broadcast it for the world to see. Even if
an end user takes utmost precautions to anonymize their online presence --
using an alias or pseudonym; masking their IP address; spoofing their
geolocation; concealing their operating system and user agent; deploying
encryption; registering with a disposable phone number or email; disabling
non-essential settings; revoking permissions; and blocking cookies and
fingerprinting -- one obvious element still lingers: the message itself.
Assuming they avoid lapses in judgment or accidental self-exposure, there
should be little evidence to validate their actual identity, right? Wrong. The
content of their message -- necessarily open for public consumption -- exposes
an attack vector: stylometric analysis, or author profiling. In this paper, we
dissect the technique of stylometry, discuss an antithetical counter-strategy
in adversarial stylometry, and devise enhancements through Unicode
steganography.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages, 7 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unstructured Evidence Attribution for Long Context Query Focused
  Summarization <span class="chip">EMNLP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14409v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14409v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dustin Wright, Zain Muhammad Mujahid, Lu Wang, Isabelle Augenstein, David Jurgens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are capable of generating coherent summaries
from very long contexts given a user query, and extracting and citing evidence
spans helps improve the trustworthiness of these summaries. Whereas previous
work has focused on evidence citation with fixed levels of granularity (e.g.
sentence, paragraph, document, etc.), we propose to extract unstructured (i.e.,
spans of any length) evidence in order to acquire more relevant and consistent
evidence than in the fixed granularity case. We show how existing systems
struggle to copy and properly cite unstructured evidence, which also tends to
be "lost-in-the-middle". To help models perform this task, we create the
Summaries with Unstructured Evidence Text dataset (SUnsET), a synthetic dataset
generated using a novel pipeline, which can be used as training supervision for
unstructured evidence summarization. We demonstrate across 5 LLMs and 4
datasets spanning human written, synthetic, single, and multi-document settings
that LLMs adapted with SUnsET generate more relevant and factually consistent
evidence with their summaries, extract evidence from more diverse locations in
their context, and can generate more relevant and consistent summaries than
baselines with no fine-tuning and fixed granularity evidence. We release SUnsET
and our generation code to the public.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2025 Main; 29 pages; 24 figures; 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Epistemic Diversity and Knowledge Collapse in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.04226v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.04226v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dustin Wright, Sarah Masud, Jared Moore, Srishti Yadav, Maria Antoniak, Chan Young Park, Isabelle Augenstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) tend to generate lexically, semantically, and
stylistically homogenous texts. This poses a risk of knowledge collapse, where
homogenous LLMs mediate a shrinking in the range of accessible information over
time. Existing works on homogenization are limited by a focus on closed-ended
multiple-choice setups or fuzzy semantic features, and do not look at trends
across time and cultural contexts. To overcome this, we present a new
methodology to measure epistemic diversity, i.e., variation in real-world
claims in LLM outputs, which we use to perform a broad empirical study of LLM
knowledge collapse. We test 27 LLMs, 155 topics covering 12 countries, and 200
prompt variations sourced from real user chats. For the topics in our study, we
show that while newer models tend to generate more diverse claims, nearly all
models are less epistemically diverse than a basic web search. We find that
model size has a negative impact on epistemic diversity, while
retrieval-augmented generation (RAG) has a positive impact, though the
improvement from RAG varies by the cultural context. Finally, compared to a
traditional knowledge source (Wikipedia), we find that country-specific claims
reflect the English language more than the local one, highlighting a gap in
epistemic representation
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages; 8 figures, 4 tables; v2 changelog: Fixed the modeling for
  table 3, random effect is the model version; v3 changelog: Fixed minor
  formatting issues in tables 2 and 3; v4 changelog: Fixed some typos and model
  description</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RecCocktail: A Generalizable and Efficient Framework for LLM-Based
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08271v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08271v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Min Hou, Chenxi Bai, Le Wu, Hao Liu, Kai Zhang, Weiwen Liu, Richang Hong, Ruiming Tang, Meng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have achieved remarkable success in recent
years, owing to their impressive generalization capabilities and rich world
knowledge. To capitalize on the potential of using LLMs as recommender systems,
mainstream approaches typically focus on two paradigms. The first paradigm
designs multi-domain or multi-task instruction data for generalizable
recommendation, so as to align LLMs with general recommendation areas and deal
with cold-start recommendation. The second paradigm focuses on enhancing
domain-specific recommendation tasks, improving performance in warm
recommendation scenarios. While most previous works treat these two paradigms
separately, we argue that they have complementary advantages, and combining
them can yield better results. In this paper, we propose a generalizable and
efficient LLM-based recommendation framework RecCocktail. Our approach begins
with fine-tuning a "base spirit" LoRA module using domain-general
recommendation instruction data to align LLM with recommendation knowledge.
Next, given users' behavior of a specific domain, we construct a
domain-specific "ingredient" LoRA module. We then provide an entropy-guided
adaptive merging method to mix the "base spirit" and the "ingredient" in the
weight space. Please note that, RecCocktail combines the advantages of the
existing two paradigms without introducing additional time or space overhead
during the inference phase. Moreover, RecCocktail is efficient with plug and
play, as the "base spirit" LoRA is trained only once, and any domain-specific
"ingredient" can be efficiently mixed with only domain-specific fine-tuning.
Extensive experiments on multiple datasets under both warm and cold-start
recommendation scenarios validate the effectiveness and generality of the
proposed RecCocktail.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On-the-Fly OVD Adaptation with FLAME: Few-shot Localization via Active
  Marginal-Samples Exploration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.17670v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.17670v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yehonathan Refael, Amit Aides, Aviad Barzilai, George Leifman, Genady Beryozkin, Vered Silverman, Bolous Jaber, Tomer Shekel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-vocabulary object detection (OVD) models offer remarkable flexibility by
detecting objects from arbitrary text queries. However, their zero-shot
performance in specialized domains like Remote Sensing (RS) is often
compromised by the inherent ambiguity of natural language, limiting critical
downstream applications. For instance, an OVD model may struggle to distinguish
between fine-grained classes such as "fishing boat" and "yacht" since their
embeddings are similar and often inseparable. This can hamper specific user
goals, such as monitoring illegal fishing, by producing irrelevant detections.
To address this, we propose a cascaded approach that couples the broad
generalization of a large pre-trained OVD model with a lightweight few-shot
classifier. Our method first employs the zero-shot model to generate
high-recall object proposals. These proposals are then refined for high
precision by a compact classifier trained in real-time on only a handful of
user-annotated examples - drastically reducing the high costs of RS imagery
annotation.The core of our framework is FLAME, a one-step active learning
strategy that selects the most informative samples for training. FLAME
identifies, on the fly, uncertain marginal candidates near the decision
boundary using density estimation, followed by clustering to ensure sample
diversity. This efficient sampling technique achieves high accuracy without
costly full-model fine-tuning and enables instant adaptation, within less then
a minute, which is significantly faster than state-of-the-art alternatives.Our
method consistently surpasses state-of-the-art performance on RS benchmarks,
establishing a practical and resource-efficient framework for adapting
foundation models to specific user needs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Model-Document Protocol for AI Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25160v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25160v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongjin Qian, Zheng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI search depends on linking large language models (LLMs) with vast external
knowledge sources. Yet web pages, PDF files, and other raw documents are not
inherently LLM-ready: they are long, noisy, and unstructured. Conventional
retrieval methods treat these documents as verbatim text and return raw
passages, leaving the burden of fragment assembly and contextual reasoning to
the LLM. This gap underscores the need for a new retrieval paradigm that
redefines how models interact with documents.
  We introduce the Model-Document Protocol (MDP), a general framework that
formalizes how raw text is bridged to LLMs through consumable knowledge
representations. Rather than treating retrieval as passage fetching, MDP
defines multiple pathways that transform unstructured documents into
task-specific, LLM-ready inputs. These include agentic reasoning, which curates
raw evidence into coherent context; memory grounding, which accumulates
reusable notes to enrich reasoning; and structured leveraging, which encodes
documents into formal representations such as graphs or key-value caches. All
three pathways share the same goal: ensuring that what reaches the LLM is not
raw fragments but compact, structured knowledge directly consumable for
reasoning.
  As an instantiation, we present MDP-Agent, which realizes the protocol
through an agentic process: constructing document-level gist memories for
global coverage, performing diffusion-based exploration with vertical
exploitation to uncover layered dependencies, and applying map-reduce style
synthesis to integrate large-scale evidence into compact yet sufficient
context. Experiments on information-seeking benchmarks demonstrate that
MDP-Agent outperforms baselines, validating both the soundness of the MDP
framework and the effectiveness of its agentic instantiation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The RAG Paradox: A Black-Box Attack Exploiting Unintentional
  Vulnerabilities in Retrieval-Augmented Generation Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20995v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20995v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chanwoo Choi, Jinsoo Kim, Sukmin Cho, Soyeong Jeong, Buru Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the growing adoption of retrieval-augmented generation (RAG) systems,
various attack methods have been proposed to degrade their performance.
However, most existing approaches rely on unrealistic assumptions in which
external attackers have access to internal components such as the retriever. To
address this issue, we introduce a realistic black-box attack based on the RAG
paradox, a structural vulnerability arising from the system's effort to enhance
trust by revealing both the retrieved documents and their sources to users.
This transparency enables attackers to observe which sources are used and how
information is phrased, allowing them to craft poisoned documents that are more
likely to be retrieved and upload them to the identified sources. Moreover, as
RAG systems directly provide retrieved content to users, these documents must
not only be retrievable but also appear natural and credible to maintain user
confidence in the search results. Unlike prior work that focuses solely on
improving document retrievability, our attack method explicitly considers both
retrievability and user trust in the retrieved content. Both offline and online
experiments demonstrate that our method significantly degrades system
performance without internal access, while generating natural-looking poisoned
documents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Shilling Recommender Systems by Generating Side-feature-aware Fake User
  Profiles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.17918v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.17918v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanrong Wang, Yingpeng Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems (RS) greatly influence users' consumption decisions,
making them attractive targets for malicious shilling attacks that inject fake
user profiles to manipulate recommendations. Existing shilling methods can
generate effective and stealthy fake profiles when training data only contain
rating matrix, but they lack comprehensive solutions for scenarios where side
features are present and utilized by the recommender. To address this gap, we
extend the Leg-UP framework by enhancing the generator architecture to
incorporate side features, enabling the generation of side-feature-aware fake
user profiles. Experiments on benchmarks show that our method achieves strong
attack performance while maintaining stealthiness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMQ-v2: Align, Denoise, and Amplify: Adaptive Behavior Mining for
  Semantic IDs Learning in Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25622v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25622v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Xu, Moyu Zhang, Chaofan Fan, Jinxin Hu, Xiaochen Li, Yu Zhang, Xiaoyi Zeng, Jing Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Industrial recommender systems rely on unique Item Identifiers (ItemIDs).
However, this method struggles with scalability and generalization in large,
dynamic datasets that have sparse long-tail data. Content-based Semantic IDs
(SIDs) address this by sharing knowledge through content quantization. However,
by ignoring dynamic behavioral properties, purely content-based SIDs have
limited expressive power. Existing methods attempt to incorporate behavioral
information but overlook a critical distinction: unlike relatively uniform
content features, user-item interactions are highly skewed and diverse,
creating a vast information gap in quality and quantity between popular and
long-tail items. This oversight leads to two critical limitations: (1) Noise
Corruption: Indiscriminate behavior-content alignment allows collaborative
noise from long-tail items to corrupt their content representations, leading to
the loss of critical multimodal information. (2)Signal Obscurity: The
equal-weighting scheme for SIDs fails to reflect the varying importance of
different behavioral signals, making it difficult for downstream tasks to
distinguish important SIDs from uninformative ones. To tackle these issues, we
propose a mixture-of-quantization framework, MMQ-v2, to adaptively Align,
Denoise, and Amplify multimodal information from content and behavior
modalities for semantic IDs learning. The semantic IDs generated by this
framework named ADA-SID. It introduces two innovations: an adaptive
behavior-content alignment that is aware of information richness to shield
representations from noise, and a dynamic behavioral router to amplify critical
signals by applying different weights to SIDs. Extensive experiments on public
and large-scale industrial datasets demonstrate ADA-SID's significant
superiority in both generative and discriminative recommendation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Automated Quality Assurance of Patent Specifications: A
  Multi-Dimensional LLM Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25402v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25402v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuqian Chai, Chaochao Wang, Weilei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although AI drafting tools have gained prominence in patent writing, the
systematic evaluation of AI-generated patent content quality represents a
significant research gap. To address this gap, We propose to evaluate patents
using regulatory compliance, technical coherence, and figure-reference
consistency detection modules, and then generate improvement suggestions via an
integration module. The framework is validated on a comprehensive dataset
comprising 80 human-authored and 80 AI-generated patents from two patent
drafting tools. Evaluation is performed on 10,841 total sentences, 8,924
non-template sentences, and 554 patent figures for the three detection modules
respectively, achieving balanced accuracies of 99.74%, 82.12%, and 91.2%
against expert annotations. Additional analysis was conducted to examine defect
distributions across patent sections, technical domains, and authoring sources.
Section-based analysis indicates that figure-text consistency and technical
detail precision require particular attention. Mechanical Engineering and
Construction show more claim-specification inconsistencies due to complex
technical documentation requirements. AI-generated patents show a significant
gap compared to human-authored ones. While human-authored patents primarily
contain surface-level errors like typos, AI-generated patents exhibit more
structural defects in figure-text alignment and cross-references.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decoupled Multimodal Fusion for User Interest Modeling in Click-Through
  Rate Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.11066v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.11066v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alin Fan, Hanqing Li, Sihan Lu, Jingsong Yuan, Jiandong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern industrial recommendation systems improve recommendation performance
by integrating multimodal representations from pre-trained models into ID-based
Click-Through Rate (CTR) prediction frameworks. However, existing approaches
typically adopt modality-centric modeling strategies that process ID-based and
multimodal embeddings independently, failing to capture fine-grained
interactions between content semantics and behavioral signals. In this paper,
we propose Decoupled Multimodal Fusion (DMF), which introduces a
modality-enriched modeling strategy to enable fine-grained interactions between
ID-based collaborative representations and multimodal representations for user
interest modeling. Specifically, we construct target-aware features to bridge
the semantic gap across different embedding spaces and leverage them as side
information to enhance the effectiveness of user interest modeling.
Furthermore, we design an inference-optimized attention mechanism that
decouples the computation of target-aware features and ID-based embeddings
before the attention layer, thereby alleviating the computational bottleneck
introduced by incorporating target-aware features. To achieve comprehensive
multimodal integration, DMF combines user interest representations learned
under the modality-centric and modality-enriched modeling strategies. Offline
experiments on public and industrial datasets demonstrate the effectiveness of
DMF. Moreover, DMF has been deployed on the product recommendation system of
the international e-commerce platform Lazada, achieving relative improvements
of 5.30% in CTCVR and 7.43% in GMV with negligible computational overhead.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Task-Centric Perspective on Recommendation Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21188v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21188v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aixin Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many studies in recommender systems (RecSys) adopt a general problem
definition, i.e., to recommend preferred items to users based on past
interactions. Such abstraction often lacks the domain-specific nuances
necessary for practical deployment. However, models are frequently evaluated
using datasets collected from online recommender platforms, which inherently
reflect domain or task specificities. In this paper, we analyze RecSys task
formulations, emphasizing key components such as input-output structures,
temporal dynamics, and candidate item selection. All these factors directly
impact offline evaluation. We further examine the complexities of user-item
interactions, including decision-making costs, multi-step engagements, and
unobservable interactions, which may influence model design. Additionally, we
explore the balance between task specificity and model generalizability,
highlighting how well-defined task formulations serve as the foundation for
robust evaluation and effective solution development. By clarifying task
definitions and their implications, this work provides a structured perspective
on RecSys research. The goal is to help researchers better navigate the field,
particularly in understanding specificities of the RecSys tasks and ensuring
fair and meaningful evaluations.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">150</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OmniX: From Unified Panoramic Generation and Perception to
  Graphics-Ready 3D Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26800v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26800v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yukun Huang, Jiwen Yu, Yanning Zhou, Jianan Wang, Xintao Wang, Pengfei Wan, Xihui Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There are two prevalent ways to constructing 3D scenes: procedural generation
and 2D lifting. Among them, panorama-based 2D lifting has emerged as a
promising technique, leveraging powerful 2D generative priors to produce
immersive, realistic, and diverse 3D environments. In this work, we advance
this technique to generate graphics-ready 3D scenes suitable for physically
based rendering (PBR), relighting, and simulation. Our key insight is to
repurpose 2D generative models for panoramic perception of geometry, textures,
and PBR materials. Unlike existing 2D lifting approaches that emphasize
appearance generation and ignore the perception of intrinsic properties, we
present OmniX, a versatile and unified framework. Based on a lightweight and
efficient cross-modal adapter structure, OmniX reuses 2D generative priors for
a broad range of panoramic vision tasks, including panoramic perception,
generation, and completion. Furthermore, we construct a large-scale synthetic
panorama dataset containing high-quality multimodal panoramas from diverse
indoor and outdoor scenes. Extensive experiments demonstrate the effectiveness
of our model in panoramic visual perception and graphics-ready 3D scene
generation, opening new possibilities for immersive and physically realistic
virtual world generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://yukun-huang.github.io/OmniX/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling Image Geo-Localization to Continent Level <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26795v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26795v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp Lindenberger, Paul-Edouard Sarlin, Jan Hosang, Matteo Balice, Marc Pollefeys, Simon Lynen, Eduard Trulls
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Determining the precise geographic location of an image at a global scale
remains an unsolved challenge. Standard image retrieval techniques are
inefficient due to the sheer volume of images (>100M) and fail when coverage is
insufficient. Scalable solutions, however, involve a trade-off: global
classification typically yields coarse results (10+ kilometers), while
cross-view retrieval between ground and aerial imagery suffers from a domain
gap and has been primarily studied on smaller regions. This paper introduces a
hybrid approach that achieves fine-grained geo-localization across a large
geographic expanse the size of a continent. We leverage a proxy classification
task during training to learn rich feature representations that implicitly
encode precise location information. We combine these learned prototypes with
embeddings of aerial imagery to increase robustness to the sparsity of
ground-level data. This enables direct, fine-grained retrieval over areas
spanning multiple countries. Our extensive evaluation demonstrates that our
approach can localize within 200m more than 68\% of queries of a dataset
covering a large part of Europe. The code is publicly available at
https://scaling-geoloc.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Pseudorandom Numbers with <span class="highlight-title">Transformer</span>s: Permuted Congruential
  Generators, Curricula, and Interpretability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26792v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26792v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Tao, Maissam Barkeshli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the ability of Transformer models to learn sequences generated by
Permuted Congruential Generators (PCGs), a widely used family of pseudo-random
number generators (PRNGs). PCGs introduce substantial additional difficulty
over linear congruential generators (LCGs) by applying a series of bit-wise
shifts, XORs, rotations and truncations to the hidden state. We show that
Transformers can nevertheless successfully perform in-context prediction on
unseen sequences from diverse PCG variants, in tasks that are beyond published
classical attacks. In our experiments we scale moduli up to $2^{22}$ using up
to $50$ million model parameters and datasets with up to $5$ billion tokens.
Surprisingly, we find even when the output is truncated to a single bit, it can
be reliably predicted by the model. When multiple distinct PRNGs are presented
together during training, the model can jointly learn them, identifying
structures from different permutations. We demonstrate a scaling law with
modulus $m$: the number of in-context sequence elements required for
near-perfect prediction grows as $\sqrt{m}$. For larger moduli, optimization
enters extended stagnation phases; in our experiments, learning moduli $m \geq
2^{20}$ requires incorporating training data from smaller moduli, demonstrating
a critical necessity for curriculum learning. Finally, we analyze embedding
layers and uncover a novel clustering phenomenon: the model spontaneously
groups the integer inputs into bitwise rotationally-invariant clusters,
revealing how representations can transfer from smaller to larger moduli.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10+13 pages, 8+19 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Defeating the Training-Inference Mismatch via FP16 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26788v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26788v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Penghui Qi, Zichen Liu, Xiangxin Zhou, Tianyu Pang, Chao Du, Wee Sun Lee, Min Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) fine-tuning of large language models (LLMs) often
suffers from instability due to the numerical mismatch between the training and
inference policies. While prior work has attempted to mitigate this issue
through algorithmic corrections or engineering alignments, we show that its
root cause lies in the floating point precision itself. The widely adopted
BF16, despite its large dynamic range, introduces large rounding errors that
breaks the consistency between training and inference. In this work, we
demonstrate that simply reverting to \textbf{FP16} effectively eliminates this
mismatch. The change is simple, fully supported by modern frameworks with only
a few lines of code change, and requires no modification to the model
architecture or learning algorithm. Our results suggest that using FP16
uniformly yields more stable optimization, faster convergence, and stronger
performance across diverse tasks, algorithms and frameworks. We hope these
findings motivate a broader reconsideration of precision trade-offs in RL
fine-tuning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Remote Labor Index: Measuring AI Automation of Remote Work 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26787v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26787v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mantas Mazeika, Alice Gatti, Cristina Menghini, Udari Madhushani Sehwag, Shivam Singhal, Yury Orlovskiy, Steven Basart, Manasi Sharma, Denis Peskoff, Elaine Lau, Jaehyuk Lim, Lachlan Carroll, Alice Blair, Vinaya Sivakumar, Sumana Basu, Brad Kenstler, Yuntao Ma, Julian Michael, Xiaoke Li, Oliver Ingebretsen, Aditya Mehta, Jean Mottola, John Teichmann, Kevin Yu, Zaina Shaik, Adam Khoja, Richard Ren, Jason Hausenloy, Long Phan, Ye Htet, Ankit Aich, Tahseen Rabbani, Vivswan Shah, Andriy Novykov, Felix Binder, Kirill Chugunov, Luis Ramirez, Matias Geralnik, Hernán Mesura, Dean Lee, Ed-Yeremai Hernandez Cardona, Annette Diamond, Summer Yue, Alexandr Wang, Bing Liu, Ernesto Hernandez, Dan Hendrycks
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AIs have made rapid progress on research-oriented benchmarks of knowledge and
reasoning, but it remains unclear how these gains translate into economic value
and automation. To measure this, we introduce the Remote Labor Index (RLI), a
broadly multi-sector benchmark comprising real-world, economically valuable
projects designed to evaluate end-to-end agent performance in practical
settings. AI agents perform near the floor on RLI, with the highest-performing
agent achieving an automation rate of 2.5%. These results help ground
discussions of AI automation in empirical evidence, setting a common basis for
tracking AI impacts and enabling stakeholders to proactively navigate AI-driven
labor automation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Website: https://www.remotelabor.ai</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HEIR: Learning Graph-Based Motion Hierarchies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26786v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26786v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Zheng, William Koch, Baiang Li, Felix Heide
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hierarchical structures of motion exist across research fields, including
computer vision, graphics, and robotics, where complex dynamics typically arise
from coordinated interactions among simpler motion components. Existing methods
to model such dynamics typically rely on manually-defined or heuristic
hierarchies with fixed motion primitives, limiting their generalizability
across different tasks. In this work, we propose a general hierarchical motion
modeling method that learns structured, interpretable motion relationships
directly from data. Our method represents observed motions using graph-based
hierarchies, explicitly decomposing global absolute motions into
parent-inherited patterns and local motion residuals. We formulate hierarchy
inference as a differentiable graph learning problem, where vertices represent
elemental motions and directed edges capture learned parent-child dependencies
through graph neural networks. We evaluate our hierarchical reconstruction
approach on three examples: 1D translational motion, 2D rotational motion, and
dynamic 3D scene deformation via Gaussian splatting. Experimental results show
that our method reconstructs the intrinsic motion hierarchy in 1D and 2D cases,
and produces more realistic and interpretable deformations compared to the
baseline on dynamic 3D Gaussian splatting scenes. By providing an adaptable,
data-driven hierarchical modeling paradigm, our method offers a formulation
applicable to a broad range of motion-centric tasks. Project Page:
https://light.princeton.edu/HEIR/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code link: https://github.com/princeton-computational-imaging/HEIR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Unified Theory for Causal Inference: Direct Debiased Machine Learning
  via Bregman-Riesz Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26783v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26783v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masahiro Kato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This note introduces a unified theory for causal inference that integrates
Riesz regression, covariate balancing, density-ratio estimation (DRE), targeted
maximum likelihood estimation (TMLE), and the matching estimator in average
treatment effect (ATE) estimation. In ATE estimation, the balancing weights and
the regression functions of the outcome play important roles, where the
balancing weights are referred to as the Riesz representer, bias-correction
term, and clever covariates, depending on the context. Riesz regression,
covariate balancing, DRE, and the matching estimator are methods for estimating
the balancing weights, where Riesz regression is essentially equivalent to DRE
in the ATE context, the matching estimator is a special case of DRE, and DRE is
in a dual relationship with covariate balancing. TMLE is a method for
constructing regression function estimators such that the leading bias term
becomes zero. Nearest Neighbor Matching is equivalent to Least Squares Density
Ratio Estimation and Riesz Regression.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Clone Deterministic 3D Worlds with Geometrically-Regularized World
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26782v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26782v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zaishuo Xia, Yukuan Lu, Xinyi Li, Yifan Xu, Yubei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A world model is an internal model that simulates how the world evolves.
Given past observations and actions, it predicts the future of both the
embodied agent and its environment. Accurate world models are essential for
enabling agents to think, plan, and reason effectively in complex, dynamic
settings. Despite rapid progress, current world models remain brittle and
degrade over long horizons. We argue that a central cause is representation
quality: exteroceptive inputs (e.g., images) are high-dimensional, and lossy or
entangled latents make dynamics learning unnecessarily hard. We therefore ask
whether improving representation learning alone can substantially improve
world-model performance. In this work, we take a step toward building a truly
accurate world model by addressing a fundamental yet open problem: constructing
a model that can fully clone and overfit to a deterministic 3D world. We
propose Geometrically-Regularized World Models (GRWM), which enforces that
consecutive points along a natural sensory trajectory remain close in latent
representation space. This approach yields significantly improved latent
representations that align closely with the true topology of the environment.
GRWM is plug-and-play, requires only minimal architectural modification, scales
with trajectory length, and is compatible with diverse latent generative
backbones. Across deterministic 3D settings and long-horizon prediction tasks,
GRWM significantly increases rollout fidelity and stability. Analyses show that
its benefits stem from learning a latent manifold with superior geometric
structure. These findings support a clear takeaway: improving representation
learning is a direct and useful path to robust world models, delivering
reliable long-horizon predictions without enlarging the dynamics module.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Surpassing state of the art on AMD area estimation from RGB fundus
  images through careful selection of U-Net architectures and loss functions
  for class imbalance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26778v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26778v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valentyna Starodub, Mantas Lukoševičius
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Age-related macular degeneration (AMD) is one of the leading causes of
irreversible vision impairment in people over the age of 60. This research
focuses on semantic segmentation for AMD lesion detection in RGB fundus images,
a non-invasive and cost-effective imaging technique. The results of the ADAM
challenge - the most comprehensive AMD detection from RGB fundus images
research competition and open dataset to date - serve as a benchmark for our
evaluation. Taking the U-Net connectivity as a base of our framework, we
evaluate and compare several approaches to improve the segmentation model's
architecture and training pipeline, including pre-processing techniques,
encoder (backbone) deep network types of varying complexity, and specialized
loss functions to mitigate class imbalances on image and pixel levels. The main
outcome of this research is the final configuration of the AMD detection
framework, which outperforms all the prior ADAM challenge submissions on the
multi-class segmentation of different AMD lesion types in non-invasive RGB
fundus images. The source code used to conduct the experiments presented in
this paper is made freely available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Pre-train</span>ed Forecasting Models: Strong Zero-Shot Feature Extractors for
  Time Series Classification <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26777v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26777v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Auer, Daniel Klotz, Sebastinan Böck, Sepp Hochreiter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research on time series foundation models has primarily focused on
forecasting, leaving it unclear how generalizable their learned representations
are. In this study, we examine whether frozen pre-trained forecasting models
can provide effective representations for classification. To this end, we
compare different representation extraction strategies and introduce two
model-agnostic embedding augmentations. Our experiments show that the best
forecasting models achieve classification accuracy that matches or even
surpasses that of state-of-the-art models pre-trained specifically for
classification. Moreover, we observe a positive correlation between forecasting
and classification performance. These findings challenge the assumption that
task-specific pre-training is necessary, and suggest that learning to forecast
may provide a powerful route toward constructing general-purpose time series
foundation models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025 Workshop on Recent Advances in Time Series Foundation
  Models (BERT2S)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Faithful and Fast Influence Function via Advanced Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26776v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26776v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jungyeon Koh, Hyeonsu Lyu, Jonggyu Jang, Hyun Jong Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How can we explain the influence of training data on black-box models?
Influence functions (IFs) offer a post-hoc solution by utilizing gradients and
Hessians. However, computing the Hessian for an entire dataset is
resource-intensive, necessitating a feasible alternative. A common approach
involves randomly sampling a small subset of the training data, but this method
often results in highly inconsistent IF estimates due to the high variance in
sample configurations. To address this, we propose two advanced sampling
techniques based on features and logits. These samplers select a small yet
representative subset of the entire dataset by considering the stochastic
distribution of features or logits, thereby enhancing the accuracy of IF
estimations. We validate our approach through class removal experiments, a
typical application of IFs, using the F1-score to measure how effectively the
model forgets the removed class while maintaining inference consistency on the
remaining classes. Our method reduces computation time by 30.1% and memory
usage by 42.2%, or improves the F1-score by 2.5% compared to the baseline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ STaMP: Sequence Transformation and Mixed Precision for Low-Precision
  Activation Quantization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26771v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26771v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Federici, Riccardo Del Chiaro, Boris van Breugel, Paul Whatmough, Markus Nagel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantization is the key method for reducing inference latency, power and
memory footprint of generative AI models. However, accuracy often degrades
sharply when activations are quantized below eight bits. Recent work suggests
that invertible linear transformations (e.g. rotations) can aid quantization,
by reparameterizing feature channels and weights. In this paper, we propose
\textit{Sequence Transformation and Mixed Precision} (STaMP) quantization, a
novel strategy that applies linear transformations along the \textit{sequence}
dimension to exploit the strong local correlation in language and visual data.
By keeping a small number of tokens in each intermediate activation at higher
precision, we can maintain model accuracy at lower (average) activations
bit-widths. We evaluate STaMP on recent LVM and LLM architectures,
demonstrating that it significantly improves low bit width activation
quantization and complements established activation and weight quantization
methods including recent feature transformations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages main text, 8 pages supplementary material</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SteerVLM: Robust Model Control through Lightweight Activation Steering
  for Vision Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26769v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26769v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anushka Sivakumar, Andrew Zhang, Zaber Hakim, Chris Thomas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work introduces SteerVLM, a lightweight steering module designed to
guide Vision-Language Models (VLMs) towards outputs that better adhere to
desired instructions. Our approach learns from the latent embeddings of paired
prompts encoding target and converse behaviors to dynamically adjust
activations connecting the language modality with image context. This allows
for fine-grained, inference-time control over complex output semantics without
modifying model weights while preserving performance on off-target tasks. Our
steering module requires learning parameters equal to 0.14% of the original
VLM's size. Our steering module gains model control through dimension-wise
activation modulation and adaptive steering across layers without requiring
pre-extracted static vectors or manual tuning of intervention points.
Furthermore, we introduce VNIA (Visual Narrative Intent Alignment), a
multimodal dataset specifically created to facilitate the development and
evaluation of VLM steering techniques. Our method outperforms existing
intervention techniques on steering and hallucination mitigation benchmarks for
VLMs and proposes a robust solution for multimodal model control through
activation engineering.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Oversight Game: Learning to Cooperatively Balance an AI Agent's
  Safety and Autonomy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26752v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26752v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Overman, Mohsen Bayati
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As increasingly capable agents are deployed, a central safety question is how
to retain meaningful human control without modifying the underlying system. We
study a minimal control interface where an agent chooses whether to act
autonomously (play) or defer (ask), while a human simultaneously chooses
whether to be permissive (trust) or to engage in oversight (oversee). If the
agent defers, the human's choice determines the outcome, potentially leading to
a corrective action or a system shutdown. We model this interaction as a
two-player Markov Game. Our analysis focuses on cases where this game qualifies
as a Markov Potential Game (MPG), a class of games where we can provide an
alignment guarantee: under a structural assumption on the human's value
function, any decision by the agent to act more autonomously that benefits
itself cannot harm the human's value. We also analyze extensions to this MPG
framework. Theoretically, this perspective provides conditions for a specific
form of intrinsic alignment. If the reward structures of the human-agent game
meet these conditions, we have a formal guarantee that the agent improving its
own outcome will not harm the human's. Practically, this model motivates a
transparent control layer with predictable incentives where the agent learns to
defer when risky and act when safe, while its pretrained policy and the
environment's reward structure remain untouched. Our gridworld simulation shows
that through independent learning, the agent and human discover their optimal
oversight roles. The agent learns to ask when uncertain and the human learns
when to oversee, leading to an emergent collaboration that avoids safety
violations introduced post-training. This demonstrates a practical method for
making misaligned models safer after deployment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep sequence models tend to memorize geometrically; it is unclear why 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26745v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26745v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shahriar Noroozizadeh, Vaishnavh Nagarajan, Elan Rosenfeld, Sanjiv Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In sequence modeling, the parametric memory of atomic facts has been
predominantly abstracted as a brute-force lookup of co-occurrences between
entities. We contrast this associative view against a geometric view of how
memory is stored. We begin by isolating a clean and analyzable instance of
Transformer reasoning that is incompatible with memory as strictly a storage of
the local co-occurrences specified during training. Instead, the model must
have somehow synthesized its own geometry of atomic facts, encoding global
relationships between all entities, including non-co-occurring ones. This in
turn has simplified a hard reasoning task involving an $\ell$-fold composition
into an easy-to-learn 1-step geometric task.
  From this phenomenon, we extract fundamental aspects of neural embedding
geometries that are hard to explain. We argue that the rise of such a geometry,
despite optimizing over mere local associations, cannot be straightforwardly
attributed to typical architectural or optimizational pressures.
Counterintuitively, an elegant geometry is learned even when it is not more
succinct than a brute-force lookup of associations.
  Then, by analyzing a connection to Node2Vec, we demonstrate how the geometry
stems from a spectral bias that -- in contrast to prevailing theories -- indeed
arises naturally despite the lack of various pressures. This analysis also
points to practitioners a visible headroom to make Transformer memory more
strongly geometric. We hope the geometric view of parametric memory encourages
revisiting the default intuitions that guide researchers in areas like
knowledge acquisition, capacity, discovery and unlearning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging the Gap between Empirical Welfare Maximization and Conditional
  Average Treatment Effect Estimation in Policy Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26723v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26723v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masahiro Kato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of policy learning is to train a policy function that recommends a
treatment given covariates to maximize population welfare. There are two major
approaches in policy learning: the empirical welfare maximization (EWM)
approach and the plug-in approach. The EWM approach is analogous to a
classification problem, where one first builds an estimator of the population
welfare, which is a functional of policy functions, and then trains a policy by
maximizing the estimated welfare. In contrast, the plug-in approach is based on
regression, where one first estimates the conditional average treatment effect
(CATE) and then recommends the treatment with the highest estimated outcome.
This study bridges the gap between the two approaches by showing that both are
based on essentially the same optimization problem. In particular, we prove an
exact equivalence between EWM and least squares over a reparameterization of
the policy class. As a consequence, the two approaches are interchangeable in
several respects and share the same theoretical guarantees under common
conditions. Leveraging this equivalence, we propose a novel regularization
method for policy learning. Our findings yield a convex and computationally
efficient training procedure that avoids the NP-hard combinatorial step
typically required in EWM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Non-Convex Over-the-Air Heterogeneous Federated Learning: A
  Bias-Variance Trade-off 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26722v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26722v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Faraz Ul Abrar, Nicolò Michelusi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over-the-air (OTA) federated learning (FL) has been well recognized as a
scalable paradigm that exploits the waveform superposition of the wireless
multiple-access channel to aggregate model updates in a single use. Existing
OTA-FL designs largely enforce zero-bias model updates by either assuming
\emph{homogeneous} wireless conditions (equal path loss across devices) or
forcing zero-bias updates to guarantee convergence. Under \emph{heterogeneous}
wireless scenarios, however, such designs are constrained by the weakest device
and inflate the update variance. Moreover, prior analyses of biased OTA-FL
largely address convex objectives, while most modern AI models are highly
non-convex. Motivated by these gaps, we study OTA-FL with stochastic gradient
descent (SGD) for general smooth non-convex objectives under wireless
heterogeneity. We develop novel OTA-FL SGD updates that allow a structured,
time-invariant model bias while facilitating reduced variance updates. We
derive a finite-time stationarity bound (expected time average squared gradient
norm) that explicitly reveals a bias-variance trade-off. To optimize this
trade-off, we pose a non-convex joint OTA power-control design and develop an
efficient successive convex approximation (SCA) algorithm that requires only
statistical CSI at the base station. Experiments on a non-convex image
classification task validate the approach: the SCA-based design accelerates
convergence via an optimized bias and improves generalization over prior OTA-FL
baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Purely Private Covariance Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26717v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26717v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tommaso d'Orsi, Gleb Novikov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a simple perturbation mechanism for the release of $d$-dimensional
covariance matrices $\Sigma$ under pure differential privacy. For large
datasets with at least $n\geq d^2/\varepsilon$ elements, our mechanism recovers
the provably optimal Frobenius norm error guarantees of
\cite{nikolov2023private}, while simultaneously achieving best known error for
all other $p$-Schatten norms, with $p\in [1,\infty]$. Our error is
information-theoretically optimal for all $p\ge 2$, in particular, our
mechanism is the first purely private covariance estimator that achieves
optimal error in spectral norm.
  For small datasets $n< d^2/\varepsilon$, we further show that by projecting
the output onto the nuclear norm ball of appropriate radius, our algorithm
achieves the optimal Frobenius norm error $O(\sqrt{d\;\text{Tr}(\Sigma) /n})$,
improving over the known bounds of $O(\sqrt{d/n})$ of \cite{nikolov2023private}
and ${O}\big(d^{3/4}\sqrt{\text{Tr}(\Sigma)/n}\big)$ of
\cite{dong2022differentially}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>equal contribution</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LSM-MS2: A Foundation Model Bridging Spectral Identification and
  Biological Interpretation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26715v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26715v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriel Asher, Devesh Shah, Amy A. Caudy, Luke Ferro, Lea Amar, Ana S. H. Costa, Thomas Patton, Niall O'Connor, Jennifer M. Campbell, Jack Geremia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A vast majority of mass spectrometry data remains uncharacterized, leaving
much of its biological and chemical information untapped. Recent advances in
machine learning have begun to address this gap, particularly for tasks such as
spectral identification in tandem mass spectrometry data. Here, we present the
latest generation of LSM-MS2, a large-scale deep learning foundation model
trained on millions of spectra to learn a semantic chemical space. LSM-MS2
achieves state-of-the-art performance in spectral identification, improving on
existing methods by 30% in accuracy of identifying challenging isomeric
compounds, yielding 42% more correct identifications in complex biological
samples, and maintaining robustness under low-concentration conditions.
Furthermore, LSM-MS2 produces rich spectral embeddings that enable direct
biological interpretation from minimal downstream data, successfully
differentiating disease states and predicting clinical outcomes across diverse
translational applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the limitation of evaluating machine unlearning using only a single
  training seed 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26714v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26714v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jamie Lanyon, Axel Finke, Petros Andreou, Georgina Cosma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine unlearning (MU) aims to remove the influence of certain data points
from a trained model without costly retraining. Most practical MU algorithms
are only approximate and their performance can only be assessed empirically.
Care must therefore be taken to make empirical comparisons as representative as
possible. A common practice is to run the MU algorithm multiple times
independently starting from the same trained model. In this work, we
demonstrate that this practice can give highly non-representative results
because -- even for the same architecture and same dataset -- some MU methods
can be highly sensitive to the choice of random number seed used for model
training. We therefore recommend that empirical
comphttps://info.arxiv.org/help/prep#commentsarisons of MU algorithms should
also reflect the variability across different model training seeds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>mini paper, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An All-Reduce Compatible Top-K Compressor for Communication-Efficient
  Distributed Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26709v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26709v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuyan Chen, Chenyang Ma, Zhangxin Li, Yutong He, Yanjie Dong, Kun Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Communication remains a central bottleneck in large-scale distributed machine
learning, and gradient sparsification has emerged as a promising strategy to
alleviate this challenge. However, existing gradient compressors face notable
limitations: Rand-$K$\ discards structural information and performs poorly in
practice, while Top-$K$\ preserves informative entries but loses the
contraction property and requires costly All-Gather operations. In this paper,
we propose ARC-Top-$K$, an {All-Reduce}-Compatible Top-$K$ compressor that
aligns sparsity patterns across nodes using a lightweight sketch of the
gradient, enabling index-free All-Reduce while preserving globally significant
information. ARC-Top-$K$\ is provably contractive and, when combined with
momentum error feedback (EF21M), achieves linear speedup and sharper
convergence rates than the original EF21M under standard assumptions.
Empirically, ARC-Top-$K$\ matches the accuracy of Top-$K$\ while reducing
wall-clock training time by up to 60.7\%, offering an efficient and scalable
solution that combines the robustness of Rand-$K$\ with the strong performance
of Top-$K$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Value Drifts: Tracing Value Alignment During LLM Post-Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26707v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26707v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehar Bhatia, Shravan Nayak, Gaurav Kamath, Marius Mosbach, Karolina Stańczak, Vered Shwartz, Siva Reddy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As LLMs occupy an increasingly important role in society, they are more and
more confronted with questions that require them not only to draw on their
general knowledge but also to align with certain human value systems.
Therefore, studying the alignment of LLMs with human values has become a
crucial field of inquiry. Prior work, however, mostly focuses on evaluating the
alignment of fully trained models, overlooking the training dynamics by which
models learn to express human values. In this work, we investigate how and at
which stage value alignment arises during the course of a model's
post-training. Our analysis disentangles the effects of post-training
algorithms and datasets, measuring both the magnitude and time of value drifts
during training. Experimenting with Llama-3 and Qwen-3 models of different
sizes and popular supervised fine-tuning (SFT) and preference optimization
datasets and algorithms, we find that the SFT phase generally establishes a
model's values, and subsequent preference optimization rarely re-aligns these
values. Furthermore, using a synthetic preference dataset that enables
controlled manipulation of values, we find that different preference
optimization algorithms lead to different value alignment outcomes, even when
preference data is held constant. Our findings provide actionable insights into
how values are learned during post-training and help to inform data curation,
as well as the selection of models and algorithms for preference optimization
to improve model alignment to human values.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Budgeted Multiple-Expert Deferral 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26706v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26706v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giulia DeSalvo, Clara Mohri, Mehryar Mohri, Yutao Zhong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning to defer uncertain predictions to costly experts offers a powerful
strategy for improving the accuracy and efficiency of machine learning systems.
However, standard training procedures for deferral algorithms typically require
querying all experts for every training instance, an approach that becomes
prohibitively expensive when expert queries incur significant computational or
resource costs. This undermines the core goal of deferral: to limit unnecessary
expert usage. To overcome this challenge, we introduce the budgeted deferral
framework, which aims to train effective deferral algorithms while minimizing
expert query costs during training. We propose new algorithms for both
two-stage and single-stage multiple-expert deferral settings that selectively
query only a subset of experts per training example. While inspired by active
learning, our setting is fundamentally different: labels are already known, and
the core challenge is to decide which experts to query in order to balance cost
and predictive performance. We establish theoretical guarantees for both of our
algorithms, including generalization bounds and label complexity analyses.
Empirical results across several domains show that our algorithms substantially
reduce training costs without sacrificing prediction accuracy, demonstrating
the practical value of our budget-aware deferral algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Regularization Terms Make Invertible Neural Networks Bayesian Point
  Estimators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26704v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26704v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nick Heilenkötter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Can regularization terms in the training of invertible neural networks lead
to known Bayesian point estimators in reconstruction? Invertible networks are
attractive for inverse problems due to their inherent stability and
interpretability. Recently, optimization strategies for invertible neural
networks that approximate either a reconstruction map or the forward operator
have been studied from a Bayesian perspective, but each has limitations. To
address this, we introduce and analyze two regularization terms for the network
training that, upon inversion of the network, recover properties of classical
Bayesian point estimators: while the first can be connected to the posterior
mean, the second resembles the MAP estimator. Our theoretical analysis
characterizes how each loss shapes both the learned forward operator and its
inverse reconstruction map. Numerical experiments support our findings and
demonstrate how these loss-term regularizers introduce data-dependence in a
stable and interpretable way.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Assessment of the conditional exchangeability assumption in causal
  machine learning models: a simulation study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26700v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26700v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gerard T. Portela, Jason B. Gibbons, Sebastian Schneeweiss, Rishi J. Desai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Observational studies developing causal machine learning (ML) models for the
prediction of individualized treatment effects (ITEs) seldom conduct empirical
evaluations to assess the conditional exchangeability assumption. We aimed to
evaluate the performance of these models under conditional exchangeability
violations and the utility of negative control outcomes (NCOs) as a diagnostic.
We conducted a simulation study to examine confounding bias in ITE estimates
generated by causal forest and X-learner models under varying conditions,
including the presence or absence of true heterogeneity. We simulated data to
reflect real-world scenarios with differing levels of confounding, sample size,
and NCO confounding structures. We then estimated and compared subgroup-level
treatment effects on the primary outcome and NCOs across settings with and
without unmeasured confounding. When conditional exchangeability was violated,
causal forest and X-learner models failed to recover true treatment effect
heterogeneity and, in some cases, falsely indicated heterogeneity when there
was none. NCOs successfully identified subgroups affected by unmeasured
confounding. Even when NCOs did not perfectly satisfy its ideal assumptions, it
remained informative, flagging potential bias in subgroup level estimates,
though not always pinpointing the subgroup with the largest confounding.
Violations of conditional exchangeability substantially limit the validity of
ITE estimates from causal ML models in routinely collected observational data.
NCOs serve a useful empirical diagnostic tool for detecting subgroup-specific
unmeasured confounding and should be incorporated into causal ML workflows to
support the credibility of individualized inference.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Kimi Linear: An Expressive, Efficient Attention Architecture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26692v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26692v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         Kimi Team, Yu Zhang, Zongyu Lin, Xingcheng Yao, Jiaxi Hu, Fanqing Meng, Chengyin Liu, Xin Men, Songlin Yang, Zhiyuan Li, Wentao Li, Enzhe Lu, Weizhou Liu, Yanru Chen, Weixin Xu, Longhui Yu, Yejie Wang, Yu Fan, Longguang Zhong, Enming Yuan, Dehao Zhang, Yizhi Zhang, T. Y. Liu, Haiming Wang, Shengjun Fang, Weiran He, Shaowei Liu, Yiwei Li, Jianlin Su, Jiezhong Qiu, Bo Pang, Junjie Yan, Zhejun Jiang, Weixiao Huang, Bohong Yin, Jiacheng You, Chu Wei, Zhengtao Wang, Chao Hong, Yutian Chen, Guanduo Chen, Yucheng Wang, Huabin Zheng, Feng Wang, Yibo Liu, Mengnan Dong, Zheng Zhang, Siyuan Pan, Wenhao Wu, Yuhao Wu, Longyu Guan, Jiawen Tao, Guohong Fu, Xinran Xu, Yuzhi Wang, Guokun Lai, Yuxin Wu, Xinyu Zhou, Zhilin Yang, Yulun Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Kimi Linear, a hybrid linear attention architecture that, for
the first time, outperforms full attention under fair comparisons across
various scenarios -- including short-context, long-context, and reinforcement
learning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA), an
expressive linear attention module that extends Gated DeltaNet with a
finer-grained gating mechanism, enabling more effective use of limited
finite-state RNN memory. Our bespoke chunkwise algorithm achieves high hardware
efficiency through a specialized variant of the Diagonal-Plus-Low-Rank (DPLR)
transition matrices, which substantially reduces computation compared to the
general DPLR formulation while remaining more consistent with the classical
delta rule.
  We pretrain a Kimi Linear model with 3B activated parameters and 48B total
parameters, based on a layerwise hybrid of KDA and Multi-Head Latent Attention
(MLA). Our experiments show that with an identical training recipe, Kimi Linear
outperforms full MLA with a sizeable margin across all evaluated tasks, while
reducing KV cache usage by up to 75% and achieving up to 6 times decoding
throughput for a 1M context. These results demonstrate that Kimi Linear can be
a drop-in replacement for full attention architectures with superior
performance and efficiency, including tasks with longer input and output
lengths.
  To support further research, we open-source the KDA kernel and vLLM
implementations, and release the pre-trained and instruction-tuned model
checkpoints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Kimi Linear tech report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LoRAQuant: Mixed-Precision Quantization of LoRA to Ultra-Low Bits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26690v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26690v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir Reza Mirzaei, Yuqiao Wen, Yanshuai Cao, Lili Mou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-Rank Adaptation (LoRA) has become a popular technique for
parameter-efficient fine-tuning of large language models (LLMs). In many
real-world scenarios, multiple adapters are loaded simultaneously to enable LLM
customization for personalized user experiences or to support a diverse range
of tasks. Although each adapter is lightweight in isolation, their aggregate
cost becomes substantial at scale. To address this, we propose LoRAQuant, a
mixed-precision post-training quantization method tailored to LoRA.
Specifically, LoRAQuant reparameterizes each adapter by singular value
decomposition (SVD) to concentrate the most important information into specific
rows and columns. This makes it possible to quantize the important components
to higher precision, while quantizing the rest to ultra-low bitwidth. We
conduct comprehensive experiments with LLaMA 2-7B, LLaMA 2-13B, and Mistral 7B
models on mathematical reasoning, coding, and summarization tasks. Results show
that our LoRAQuant uses significantly lower bits than other quantization
methods, but achieves comparable or even higher performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FlowQ-Net: A Generative Framework for Automated Quantum Circuit Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26688v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26688v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Dai, Michael Rizvi-Martel, Guillaume Rabusseau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Designing efficient quantum circuits is a central bottleneck to exploring the
potential of quantum computing, particularly for noisy intermediate-scale
quantum (NISQ) devices, where circuit efficiency and resilience to errors are
paramount. The search space of gate sequences grows combinatorially, and
handcrafted templates often waste scarce qubit and depth budgets. We introduce
\textsc{FlowQ-Net} (Flow-based Quantum design Network), a generative framework
for automated quantum circuit synthesis based on Generative Flow Networks
(GFlowNets). This framework learns a stochastic policy to construct circuits
sequentially, sampling them in proportion to a flexible, user-defined reward
function that can encode multiple design objectives such as performance, depth,
and gate count. This approach uniquely enables the generation of a diverse
ensemble of high-quality circuits, moving beyond single-solution optimization.
We demonstrate the efficacy of \textsc{FlowQ-Net} through an extensive set of
simulations. We apply our method to Variational Quantum Algorithm (VQA) ansatz
design for molecular ground state estimation, Max-Cut, and image
classification, key challenges in near-term quantum computing. Circuits
designed by \textsc{FlowQ-Net} achieve significant improvements, yielding
circuits that are 10$\times$-30$\times$ more compact in terms of parameters,
gates, and depth compared to commonly used unitary baselines, without
compromising accuracy. This trend holds even when subjected to error profiles
from real-world quantum devices. Our results underline the potential of
generative models as a general-purpose methodology for automated quantum
circuit design, offering a promising path towards more efficient quantum
algorithms and accelerating scientific discovery in the quantum domain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tight Differentially Private PCA via Matrix Coherence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26679v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26679v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tommaso d'Orsi, Gleb Novikov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We revisit the task of computing the span of the top $r$ singular vectors
$u_1, \ldots, u_r$ of a matrix under differential privacy. We show that a
simple and efficient algorithm -- based on singular value decomposition and
standard perturbation mechanisms -- returns a private rank-$r$ approximation
whose error depends only on the \emph{rank-$r$ coherence} of $u_1, \ldots, u_r$
and the spectral gap $\sigma_r - \sigma_{r+1}$. This resolves a question posed
by Hardt and Roth~\cite{hardt2013beyond}. Our estimator outperforms the state
of the art -- significantly so in some regimes. In particular, we show that in
the dense setting, it achieves the same guarantees for single-spike PCA in the
Wishart model as those attained by optimal non-private algorithms, whereas
prior private algorithms failed to do so.
  In addition, we prove that (rank-$r$) coherence does not increase under
Gaussian perturbations. This implies that any estimator based on the Gaussian
mechanism -- including ours -- preserves the coherence of the input. We
conjecture that similar behavior holds for other structured models, including
planted problems in graphs.
  We also explore applications of coherence to graph problems. In particular,
we present a differentially private algorithm for Max-Cut and other constraint
satisfaction problems under low coherence assumptions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SODA 2026; equal contribution</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Action-Driven Processes for Continuous-Time Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26672v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26672v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruimin He, Shaowei Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  At the heart of reinforcement learning are actions -- decisions made in
response to observations of the environment. Actions are equally fundamental in
the modeling of stochastic processes, as they trigger discontinuous state
transitions and enable the flow of information through large, complex systems.
In this paper, we unify the perspectives of stochastic processes and
reinforcement learning through action-driven processes, and illustrate their
application to spiking neural networks. Leveraging ideas from
control-as-inference, we show that minimizing the Kullback-Leibler divergence
between a policy-driven true distribution and a reward-driven model
distribution for a suitably defined action-driven process is equivalent to
maximum entropy reinforcement learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Heuristic Adaptation of Potentially Misspecified Domain Support for
  Likelihood-Free Inference in Stochastic Dynamical Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26656v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26656v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgios Kamaras, Craig Innes, Subramanian Ramamoorthy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In robotics, likelihood-free inference (LFI) can provide the domain
distribution that adapts a learnt agent in a parametric set of deployment
conditions. LFI assumes an arbitrary support for sampling, which remains
constant as the initial generic prior is iteratively refined to more
descriptive posteriors. However, a potentially misspecified support can lead to
suboptimal, yet falsely certain, posteriors. To address this issue, we propose
three heuristic LFI variants: EDGE, MODE, and CENTRE. Each interprets the
posterior mode shift over inference steps in its own way and, when integrated
into an LFI step, adapts the support alongside posterior inference. We first
expose the support misspecification issue and evaluate our heuristics using
stochastic dynamical benchmarks. We then evaluate the impact of heuristic
support adaptation on parameter inference and policy learning for a dynamic
deformable linear object (DLO) manipulation task. Inference results in a finer
length and stiffness classification for a parametric set of DLOs. When the
resulting posteriors are used as domain distributions for sim-based policy
learning, they lead to more robust object-centric agent performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hybrid DQN-TD3 Reinforcement Learning for Autonomous Navigation in
  Dynamic Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26646v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26646v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyi He, Danggui Chen, Zhenshuo Zhang, Zimeng Bai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a hierarchical path-planning and control framework that
combines a high-level Deep Q-Network (DQN) for discrete sub-goal selection with
a low-level Twin Delayed Deep Deterministic Policy Gradient (TD3) controller
for continuous actuation. The high-level module selects behaviors and
sub-goals; the low-level module executes smooth velocity commands. We design a
practical reward shaping scheme (direction, distance, obstacle avoidance,
action smoothness, collision penalty, time penalty, and progress), together
with a LiDAR-based safety gate that prevents unsafe motions. The system is
implemented in ROS + Gazebo (TurtleBot3) and evaluated with PathBench metrics,
including success rate, collision rate, path efficiency, and re-planning
efficiency, in dynamic and partially observable environments. Experiments show
improved success rate and sample efficiency over single-algorithm baselines
(DQN or TD3 alone) and rule-based planners, with better generalization to
unseen obstacle configurations and reduced abrupt control changes. Code and
evaluation scripts are available at the project repository.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 figures; ROS+Gazebo (TurtleBot3) implementation;
  evaluation with PathBench metrics; code (primary):
  https://github.com/MayaCHEN-github/HierarchicalRL-robot-navigation; mirror
  (for reproducibility): https://github.com/ShowyHe/DRL-robot-navigation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Curly Flow Matching for Learning Non-gradient Field Dynamics <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26645v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26645v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Katarina Petrović, Lazar Atanackovic, Viggo Moro, Kacper Kapuśniak, İsmail İlkan Ceylan, Michael Bronstein, Avishek Joey Bose, Alexander Tong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modeling the transport dynamics of natural processes from population-level
observations is a ubiquitous problem in the natural sciences. Such models rely
on key assumptions about the underlying process in order to enable faithful
learning of governing dynamics that mimic the actual system behavior. The de
facto assumption in current approaches relies on the principle of least action
that results in gradient field dynamics and leads to trajectories minimizing an
energy functional between two probability measures. However, many real-world
systems, such as cell cycles in single-cell RNA, are known to exhibit
non-gradient, periodic behavior, which fundamentally cannot be captured by
current state-of-the-art methods such as flow and bridge matching. In this
paper, we introduce Curly Flow Matching (Curly-FM), a novel approach that is
capable of learning non-gradient field dynamics by designing and solving a
Schr\"odinger bridge problem with a non-zero drift reference process -- in
stark contrast to typical zero-drift reference processes -- which is
constructed using inferred velocities in addition to population snapshot data.
We showcase Curly-FM by solving the trajectory inference problems for single
cells, computational fluid dynamics, and ocean currents with approximate
velocities. We demonstrate that Curly-FM can learn trajectories that better
match both the reference process and population marginals. Curly-FM expands
flow matching models beyond the modeling of populations and towards the
modeling of known periodic behavior in physical systems. Our code repository is
accessible at: https://github.com/kpetrovicc/curly-flow-matching.git
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MSAD: A Deep Dive into Model Selection for Time series Anomaly Detection <span class="chip">VLDB</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26643v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26643v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emmanouil Sylligardos, John Paparrizos, Themis Palpanas, Pierre Senellart, Paul Boniol
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anomaly detection is a fundamental task for time series analytics with
important implications for the downstream performance of many applications.
Despite increasing academic interest and the large number of methods proposed
in the literature, recent benchmarks and evaluation studies demonstrated that
no overall best anomaly detection methods exist when applied to very
heterogeneous time series datasets. Therefore, the only scalable and viable
solution to solve anomaly detection over very different time series collected
from diverse domains is to propose a model selection method that will select,
based on time series characteristics, the best anomaly detection methods to
run. Existing AutoML solutions are, unfortunately, not directly applicable to
time series anomaly detection, and no evaluation of time series-based
approaches for model selection exists. Towards that direction, this paper
studies the performance of time series classification methods used as model
selection for anomaly detection. In total, we evaluate 234 model configurations
derived from 16 base classifiers across more than 1980 time series, and we
propose the first extensive experimental evaluation of time series
classification as model selection for anomaly detection. Our results
demonstrate that model selection methods outperform every single anomaly
detection method while being in the same order of magnitude regarding execution
time. This evaluation is the first step to demonstrate the accuracy and
efficiency of time series classification algorithms for anomaly detection, and
represents a strong baseline that can then be used to guide the model selection
step in general AutoML pipelines. Preprint version of an article accepted at
the VLDB Journal.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 13 figures, VLDB Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Omnipresent Yet Overlooked: Heat Kernels in Combinatorial Bayesian
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26633v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26633v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Colin Doumont, Victor Picheny, Viacheslav Borovitskiy, Henry Moss
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian Optimization (BO) has the potential to solve various combinatorial
tasks, ranging from materials science to neural architecture search. However,
BO requires specialized kernels to effectively model combinatorial domains.
Recent efforts have introduced several combinatorial kernels, but the
relationships among them are not well understood. To bridge this gap, we
develop a unifying framework based on heat kernels, which we derive in a
systematic way and express as simple closed-form expressions. Using this
framework, we prove that many successful combinatorial kernels are either
related or equivalent to heat kernels, and validate this theoretical claim in
our experiments. Moreover, our analysis confirms and extends the results
presented in Bounce: certain algorithms' performance decreases substantially
when the unknown optima of the function do not have a certain structure. In
contrast, heat kernels are not sensitive to the location of the optima. Lastly,
we show that a fast and simple pipeline, relying on heat kernels, is able to
achieve state-of-the-art results, matching or even outperforming certain slow
or complex algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aeolus: A Multi-structural Flight Delay <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26616v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26616v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lin Xu, Xinyun Yuan, Yuxuan Liang, Suwan Yin, Yuankai Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Aeolus, a large-scale Multi-modal Flight Delay Dataset designed
to advance research on flight delay prediction and support the development of
foundation models for tabular data. Existing datasets in this domain are
typically limited to flat tabular structures and fail to capture the
spatiotemporal dynamics inherent in delay propagation. Aeolus addresses this
limitation by providing three aligned modalities: (i) a tabular dataset with
rich operational, meteorological, and airportlevel features for over 50 million
flights; (ii) a flight chain module that models delay propagation along
sequential flight legs, capturing upstream and downstream dependencies; and
(iii) a flight network graph that encodes shared aircraft, crew, and airport
resource connections, enabling cross-flight relational reasoning. The dataset
is carefully constructed with temporal splits, comprehensive features, and
strict leakage prevention to support realistic and reproducible machine
learning evaluation. Aeolus supports a broad range of tasks, including
regression, classification, temporal structure modeling, and graph learning,
serving as a unified benchmark across tabular, sequential, and graph
modalities. We release baseline experiments and preprocessing tools to
facilitate adoption. Aeolus fills a key gap for both domain-specific modeling
and general-purpose structured data research.Our source code and data can be
accessed at https://github.com/Flnny/Delay-data
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CYPRESS: Crop Yield Prediction via Regression on Prithvi's Encoder for
  Satellite Sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26609v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26609v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shayan Nejadshamsi, Yuanyuan Zhang, Shadi Zaki, Brock Porth, Lysa Porth, Vahab Khoshdel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate and timely crop yield prediction is crucial for global food security
and modern agricultural management. Traditional methods often lack the
scalability and granularity required for precision farming. This paper
introduces CYPRESS (Crop Yield Prediction via Regression on Prithvi's Encoder
for Satellite Sensing), a deep learning model designed for high-resolution,
intra-field canola yield prediction. CYPRESS leverages a pre-trained,
large-scale geospatial foundation model (Prithvi-EO-2.0-600M) and adapts it for
a continuous regression task, transforming multi-temporal satellite imagery
into dense, pixel-level yield maps. Evaluated on a comprehensive dataset from
the Canadian Prairies, CYPRESS demonstrates superior performance over existing
deep learning-based yield prediction models, highlighting the effectiveness of
fine-tuning foundation models for specialized agricultural applications. By
providing a continuous, high-resolution output, CYPRESS offers a more
actionable tool for precision agriculture than conventional classification or
county-level aggregation methods. This work validates a novel approach that
bridges the gap between large-scale Earth observation and on-farm
decision-making, offering a scalable solution for detailed agricultural
monitoring.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Wasserstein Regression as a Variational Approximation of Probabilistic
  Trajectories through the Bernstein Basis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26607v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26607v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maksim Maslov, Alexander Kugaevskikh, Matthew Ivanov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper considers the problem of regression over distributions, which is
becoming increasingly important in machine learning. Existing approaches often
ignore the geometry of the probability space or are computationally expensive.
To overcome these limitations, a new method is proposed that combines the
parameterization of probability trajectories using a Bernstein basis and the
minimization of the Wasserstein distance between distributions. The key idea is
to model a conditional distribution as a smooth probability trajectory defined
by a weighted sum of Gaussian components whose parameters -- the mean and
covariance -- are functions of the input variable constructed using Bernstein
polynomials. The loss function is the averaged squared Wasserstein distance
between the predicted Gaussian distributions and the empirical data, which
takes into account the geometry of the distributions. An autodiff-based
optimization method is used to train the model. Experiments on synthetic
datasets that include complex trajectories demonstrated that the proposed
method provides competitive approximation quality in terms of the Wasserstein
distance, Energy Distance, and RMSE metrics, especially in cases of pronounced
nonlinearity. The model demonstrates trajectory smoothness that is better than
or comparable to alternatives and robustness to changes in data structure,
while maintaining high interpretability due to explicit parameterization via
control points. The developed approach represents a balanced solution that
combines geometric accuracy, computational practicality, and interpretability.
Prospects for further research include extending the method to non-Gaussian
distributions, applying entropy regularization to speed up computations, and
adapting the approach to working with high-dimensional data for approximating
surfaces and more complex structures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hybrid Physical-Neural Simulator for Fast Cosmological Hydrodynamics <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26593v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26593v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arne Thomsen, Tilman Tröster, François Lanusse
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cosmological field-level inference requires differentiable forward models
that solve the challenging dynamics of gas and dark matter under hydrodynamics
and gravity. We propose a hybrid approach where gravitational forces are
computed using a differentiable particle-mesh solver, while the hydrodynamics
are parametrized by a neural network that maps local quantities to an effective
pressure field. We demonstrate that our method improves upon alternative
approaches, such as an Enthalpy Gradient Descent baseline, both at the field
and summary-statistic level. The approach is furthermore highly data efficient,
with a single reference simulation of cosmological structure formation being
sufficient to constrain the neural pressure model. This opens the door for
future applications where the model is fit directly to observational data,
rather than a training set of simulations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the NeurIPS 2025 Workshop on Machine Learning and the
  Physical Sciences</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physics-Informed Mixture Models and Surrogate Models for Precision
  Additive Manufacturing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26586v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26586v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian Basterrech, Shuo Shan, Debabrata Adhikari, Sankhya Mohanty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we leverage a mixture model learning approach to identify
defects in laser-based Additive Manufacturing (AM) processes. By incorporating
physics based principles, we also ensure that the model is sensitive to
meaningful physical parameter variations. The empirical evaluation was
conducted by analyzing real-world data from two AM processes: Directed Energy
Deposition and Laser Powder Bed Fusion. In addition, we also studied the
performance of the developed framework over public datasets with different
alloy type and experimental parameter information. The results show the
potential of physics-guided mixture models to examine the underlying physical
behavior of an AM system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Five pages, four figures, to be presented at the AI in Science
  Summit, Denmark, November, 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inference-Cost-Aware Dynamic Tree Construction for Efficient Inference
  in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26577v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26577v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinrong Hong, Zhiquan Tan, Kai Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) face significant inference latency challenges
stemming from their autoregressive design and large size. To address this,
speculative decoding emerges as a solution, enabling the simultaneous
generation and validation of multiple tokens. While recent approaches like
EAGLE-2 and EAGLE-3 improve speculative decoding using dynamic tree structures,
they often neglect the impact of crucial system variables such as GPU devices
and batch sizes.
  Therefore, we introduce a new dynamic tree decoding approach called CAST that
takes into account inference costs, including factors such as GPU
configurations and batch sizes, to dynamically refine the tree structure.
Through comprehensive experimentation across six diverse tasks and utilizing
six distinct LLMs, our methodology demonstrates remarkable results, achieving
speeds up to 5.2 times faster than conventional decoding methods. Moreover, it
generally outperforms existing state-of-the-art techniques from 5% to 20%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multiclass Local Calibration With the Jensen-Shannon Distance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26566v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26566v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cesare Barbera, Lorenzo Perini, Giovanni De Toni, Andrea Passerini, Andrea Pugnana
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing trustworthy Machine Learning (ML) models requires their predicted
probabilities to be well-calibrated, meaning they should reflect true-class
frequencies. Among calibration notions in multiclass classification, strong
calibration is the most stringent, as it requires all predicted probabilities
to be simultaneously calibrated across all classes. However, existing
approaches to multiclass calibration lack a notion of distance among inputs,
which makes them vulnerable to proximity bias: predictions in sparse regions of
the feature space are systematically miscalibrated. This is especially relevant
in high-stakes settings, such as healthcare, where the sparse instances are
exactly those most at risk of biased treatment. In this work, we address this
main shortcoming by introducing a local perspective on multiclass calibration.
First, we formally define multiclass local calibration and establish its
relationship with strong calibration. Second, we theoretically analyze the
pitfalls of existing evaluation metrics when applied to multiclass local
calibration. Third, we propose a practical method for enhancing local
calibration in Neural Networks, which enforces alignment between predicted
probabilities and local estimates of class frequencies using the Jensen-Shannon
distance. Finally, we empirically validate our approach against existing
multiclass calibration techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Measuring Localization of Shortcuts in Deep Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26560v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26560v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikita Tsoy, Nikola Konstantinov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Shortcuts, spurious rules that perform well during training but fail to
generalize, present a major challenge to the reliability of deep networks
(Geirhos et al., 2020). However, the impact of shortcuts on feature
representations remains understudied, obstructing the design of principled
shortcut-mitigation methods. To overcome this limitation, we investigate the
layer-wise localization of shortcuts in deep models. Our novel experiment
design quantifies the layer-wise contribution to accuracy degradation caused by
a shortcut-inducing skew by counterfactual training on clean and skewed
datasets. We employ our design to study shortcuts on CIFAR-10, Waterbirds, and
CelebA datasets across VGG, ResNet, DeiT, and ConvNeXt architectures. We find
that shortcut learning is not localized in specific layers but distributed
throughout the network. Different network parts play different roles in this
process: shallow layers predominantly encode spurious features, while deeper
layers predominantly forget core features that are predictive on clean data. We
also analyze the differences in localization and describe its principal axes of
variation. Finally, our analysis of layer-wise shortcut-mitigation strategies
suggests the hardness of designing general methods, supporting dataset- and
architecture-specific approaches instead.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosted Trees on a Diet: Compact Models for Resource-Constrained Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26557v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26557v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan Stenkamp, Nina Herrmann, Benjamin Karic, Stefan Oehmcke, Fabian Gieseke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deploying machine learning models on compute-constrained devices has become a
key building block of modern IoT applications. In this work, we present a
compression scheme for boosted decision trees, addressing the growing need for
lightweight machine learning models. Specifically, we provide techniques for
training compact boosted decision tree ensembles that exhibit a reduced memory
footprint by rewarding, among other things, the reuse of features and
thresholds during training. Our experimental evaluation shows that models
achieved the same performance with a compression ratio of 4-16x compared to
LightGBM models using an adapted training process and an alternative memory
layout. Once deployed, the corresponding IoT devices can operate independently
of constant communication or external energy supply, and, thus, autonomously,
requiring only minimal computing power and energy. This capability opens the
door to a wide range of IoT applications, including remote monitoring, edge
analytics, and real-time decision making in isolated or power-limited
environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Inverse Kinematics Framework for Learning Variable-Length Tool
  Manipulation in Robotics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26551v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26551v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prathamesh Kothavale, Sravani Boddepalli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional robots possess a limited understanding of their kinematics and
are confined to preprogrammed tasks, hindering their ability to leverage tools
efficiently. Driven by the essential components of tool usage - grasping the
desired outcome, selecting the most suitable tool, determining optimal tool
orientation, and executing precise manipulations - we introduce a pioneering
framework. Our novel approach expands the capabilities of the robot's inverse
kinematics solver, empowering it to acquire a sequential repertoire of actions
using tools of varying lengths. By integrating a simulation-learned action
trajectory with the tool, we showcase the practicality of transferring acquired
skills from simulation to real-world scenarios through comprehensive
experimentation. Remarkably, our extended inverse kinematics solver
demonstrates an impressive error rate of less than 1 cm. Furthermore, our
trained policy achieves a mean error of 8 cm in simulation. Noteworthy, our
model achieves virtually indistinguishable performance when employing two
distinct tools of different lengths. This research provides an indication of
potential advances in the exploration of all four fundamental aspects of tool
usage, enabling robots to master the intricate art of tool manipulation across
diverse tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures. Demonstrates a reinforcement learning framework
  for adaptive tool manipulation with variable-length extensions</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Structure of Relation Decoding Linear Operators in Large Language
  Models <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26543v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26543v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miranda Anna Christ, Adrián Csiszárik, Gergely Becsó, Dániel Varga
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the structure of linear operators introduced in
Hernandez et al. [2023] that decode specific relational facts in transformer
language models. We extend their single-relation findings to a collection of
relations and systematically chart their organization. We show that such
collections of relation decoders can be highly compressed by simple order-3
tensor networks without significant loss in decoding accuracy. To explain this
surprising redundancy, we develop a cross-evaluation protocol, in which we
apply each linear decoder operator to the subjects of every other relation. Our
results reveal that these linear maps do not encode distinct relations, but
extract recurring, coarse-grained semantic properties (e.g., country of capital
city and country of food are both in the country-of-X property). This
property-centric structure clarifies both the operators' compressibility and
highlights why they generalize only to new relations that are semantically
close. Our findings thus interpret linear relational decoding in transformer
language models as primarily property-based, rather than relation-specific.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025 (Spotlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Three-Stage Bayesian Transfer Learning Framework to Improve
  Predictions in Data-Scarce Domains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26541v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26541v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aidan Furlong, Robert Salko, Xingang Zhao, Xu Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The use of ML in engineering has grown steadily to support a wide array of
applications. Among these methods, deep neural networks have been widely
adopted due to their performance and accessibility, but they require large,
high-quality datasets. Experimental data are often sparse, noisy, or
insufficient to build resilient data-driven models. Transfer learning, which
leverages relevant data-abundant source domains to assist learning in
data-scarce target domains, has shown efficacy. Parameter transfer, where
pretrained weights are reused, is common but degrades under large domain
shifts. Domain-adversarial neural networks (DANNs) help address this issue by
learning domain-invariant representations, thereby improving transfer under
greater domain shifts in a semi-supervised setting. However, DANNs can be
unstable during training and lack a native means for uncertainty
quantification. This study introduces a fully-supervised three-stage framework,
the staged Bayesian domain-adversarial neural network (staged B-DANN), that
combines parameter transfer and shared latent space adaptation. In Stage 1, a
deterministic feature extractor is trained on the source domain. This feature
extractor is then adversarially refined using a DANN in Stage 2. In Stage 3, a
Bayesian neural network is built on the adapted feature extractor for
fine-tuning on the target domain to handle conditional shifts and yield
calibrated uncertainty estimates. This staged B-DANN approach was first
validated on a synthetic benchmark, where it was shown to significantly
outperform standard transfer techniques. It was then applied to the task of
predicting critical heat flux in rectangular channels, leveraging data from
tube experiments as the source domain. The results of this study show that the
staged B-DANN method can improve predictive accuracy and generalization,
potentially assisting other domains in nuclear engineering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Engineering Applications of Artificial Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Higher-Order Regularization Learning on Hypergraphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26533v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26533v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adrien Weihs, Andrea Bertozzi, Matthew Thorpe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Higher-Order Hypergraph Learning (HOHL) was recently introduced as a
principled alternative to classical hypergraph regularization, enforcing
higher-order smoothness via powers of multiscale Laplacians induced by the
hypergraph structure. Prior work established the well- and ill-posedness of
HOHL through an asymptotic consistency analysis in geometric settings. We
extend this theoretical foundation by proving the consistency of a truncated
version of HOHL and deriving explicit convergence rates when HOHL is used as a
regularizer in fully supervised learning. We further demonstrate its strong
empirical performance in active learning and in datasets lacking an underlying
geometric structure, highlighting HOHL's versatility and robustness across
diverse learning settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Polybasic Speculative Decoding Through a Theoretical Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26527v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26527v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruilin Wang, Huixia Li, Yuexiao Ma, Xiawu Zheng, Fei Chao, Xuefeng Xiao, Rongrong Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inference latency stands as a critical bottleneck in the large-scale
deployment of Large Language Models (LLMs). Speculative decoding methods have
recently shown promise in accelerating inference without compromising the
output distribution. However, existing work typically relies on a dualistic
draft-verify framework and lacks rigorous theoretical grounding. In this paper,
we introduce a novel \emph{polybasic} speculative decoding framework,
underpinned by a comprehensive theoretical analysis. Specifically, we prove a
fundamental theorem that characterizes the optimal inference time for
multi-model speculative decoding systems, shedding light on how to extend
beyond the dualistic approach to a more general polybasic paradigm. Through our
theoretical investigation of multi-model token generation, we expose and
optimize the interplay between model capabilities, acceptance lengths, and
overall computational cost. Our framework supports both standalone
implementation and integration with existing speculative techniques, leading to
accelerated performance in practice. Experimental results across multiple model
families demonstrate that our approach yields speedup ratios ranging from
$3.31\times$ to $4.01\times$ for LLaMA2-Chat 7B, up to $3.87 \times$ for
LLaMA3-8B, up to $4.43 \times$ for Vicuna-7B and up to $3.85 \times$ for
Qwen2-7B -- all while preserving the original output distribution. We release
our theoretical proofs and implementation code to facilitate further
investigation into polybasic speculative decoding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Think Outside the Policy: In-Context Steered Policy Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26519v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26519v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hsiu-Yuan Huang, Chenming Tang, Weijie Liu, Saiyong Yang, Yunfang Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing Reinforcement Learning from Verifiable Rewards (RLVR) methods, such
as Group Relative Policy Optimization (GRPO), have achieved remarkable progress
in improving the reasoning capabilities of Large Reasoning Models (LRMs).
However, they exhibit limited exploration due to reliance on on-policy rollouts
where confined to the current policy's distribution, resulting in narrow
trajectory diversity. Recent approaches attempt to expand policy coverage by
incorporating trajectories generated from stronger expert models, yet this
reliance increases computational cost and such advaned models are often
inaccessible. To address these issues, we propose In-Context Steered Policy
Optimization (ICPO), a unified framework that leverages the inherent in-context
learning capability of LRMs to provide expert guidance using existing datasets.
ICPO introduces Mixed-Policy GRPO with Implicit Expert Forcing, which expands
exploration beyond the current policy distribution without requiring advanced
LRM trajectories. To further stabilize optimization, ICPO integrates Expert
Region Reject Sampling to filter unreliable off-policy trajectories and
Annealed Expert-Bonus Reward Shaping to balance early expert guidance with
later autonomous improvement. Results demonstrate that ICPO consistently
enhances reinforcement learning performance and training stability on
mathematical reasoning benchmarks, revealing a scalable and effective RLVR
paradigm for LRMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inside CORE-KG: Evaluating Structured <span class="highlight-title">Prompt</span>ing and Coreference
  Resolution for Knowledge Graphs <span class="chip">ICDM 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26512v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26512v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dipak Meher, Carlotta Domeniconi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human smuggling networks are increasingly adaptive and difficult to analyze.
Legal case documents offer critical insights but are often unstructured,
lexically dense, and filled with ambiguous or shifting references, which pose
significant challenges for automated knowledge graph (KG) construction. While
recent LLM-based approaches improve over static templates, they still generate
noisy, fragmented graphs with duplicate nodes due to the absence of guided
extraction and coreference resolution. The recently proposed CORE-KG framework
addresses these limitations by integrating a type-aware coreference module and
domain-guided structured prompts, significantly reducing node duplication and
legal noise. In this work, we present a systematic ablation study of CORE-KG to
quantify the individual contributions of its two key components. Our results
show that removing coreference resolution results in a 28.32% increase in node
duplication and a 4.32% increase in noisy nodes, while removing structured
prompts leads to a 4.34% increase in node duplication and a 73.33% increase in
noisy nodes. These findings offer empirical insights for designing robust
LLM-based pipelines for extracting structured representations from complex
legal texts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICDM 2025 Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLMs as In-Context Meta-Learners for Model and Hyperparameter Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26510v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26510v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youssef Attia El Hili, Albert Thomas, Malik Tiomoko, Abdelhakim Benechehab, Corentin Léger, Corinne Ancourt, Balázs Kégl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model and hyperparameter selection are critical but challenging in machine
learning, typically requiring expert intuition or expensive automated search.
We investigate whether large language models (LLMs) can act as in-context
meta-learners for this task. By converting each dataset into interpretable
metadata, we prompt an LLM to recommend both model families and
hyperparameters. We study two prompting strategies: (1) a zero-shot mode
relying solely on pretrained knowledge, and (2) a meta-informed mode augmented
with examples of models and their performance on past tasks. Across synthetic
and real-world benchmarks, we show that LLMs can exploit dataset metadata to
recommend competitive models and hyperparameters without search, and that
improvements from meta-informed prompting demonstrate their capacity for
in-context meta-learning. These results highlight a promising new role for LLMs
as lightweight, general-purpose assistants for model selection and
hyperparameter optimization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing ECG Classification Robustness with Lightweight Unsupervised
  Anomaly Detection Filters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26501v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26501v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mustafa Fuad Rifet Ibrahim, Maurice Meijer, Alexander Schlaefer, Peer Stelldinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continuous electrocardiogram (ECG) monitoring via wearables offers
significant potential for early cardiovascular disease (CVD) detection.
However, deploying deep learning models for automated analysis in
resource-constrained environments faces reliability challenges due to
inevitable Out-of-Distribution (OOD) data. OOD inputs, such as unseen
pathologies or noisecorrupted signals, often cause erroneous, high-confidence
predictions by standard classifiers, compromising patient safety. Existing OOD
detection methods either neglect computational constraints or address noise and
unseen classes separately. This paper explores Unsupervised Anomaly Detection
(UAD) as an independent, upstream filtering mechanism to improve robustness. We
benchmark six UAD approaches, including Deep SVDD, reconstruction-based models,
Masked Anomaly Detection, normalizing flows, and diffusion models, optimized
via Neural Architecture Search (NAS) under strict resource constraints (at most
512k parameters). Evaluation on PTB-XL and BUT QDB datasets assessed detection
of OOD CVD classes and signals unsuitable for analysis due to noise. Results
show Deep SVDD consistently achieves the best trade-off between detection and
efficiency. In a realistic deployment simulation, integrating the optimized
Deep SVDD filter with a diagnostic classifier improved accuracy by up to 21
percentage points over a classifier-only baseline. This study demonstrates that
optimized UAD filters can safeguard automated ECG analysis, enabling safer,
more reliable continuous cardiovascular monitoring on wearables.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to the 24th International Conference on Pervasive Computing
  and Communications (PerCom 2026)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data-Efficient RLVR via Off-Policy Influence Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26491v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26491v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erle Zhu, Dazhi Jiang, Yuan Wang, Xujun Li, Jiale Cheng, Yuxian Gu, Yilin Niu, Aohan Zeng, Jie Tang, Minlie Huang, Hongning Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data selection is a critical aspect of Reinforcement Learning with Verifiable
Rewards (RLVR) for enhancing the reasoning capabilities of large language
models (LLMs). Current data selection methods are largely heuristic-based,
lacking theoretical guarantees and generalizability. This work proposes a
theoretically-grounded approach using influence functions to estimate the
contribution of each data point to the learning objective. To overcome the
prohibitive computational cost of policy rollouts required for online influence
estimation, we introduce an off-policy influence estimation method that
efficiently approximates data influence using pre-collected offline
trajectories. Furthermore, to manage the high-dimensional gradients of LLMs, we
employ sparse random projection to reduce dimensionality and improve storage
and computation efficiency. Leveraging these techniques, we develop
\textbf{C}urriculum \textbf{R}L with \textbf{O}ff-\textbf{P}olicy
\text{I}nfluence guidance (\textbf{CROPI}), a multi-stage RL framework that
iteratively selects the most influential data for the current policy.
Experiments on models up to 7B parameters demonstrate that CROPI significantly
accelerates training. On a 1.5B model, it achieves a 2.66x step-level
acceleration while using only 10\% of the data per stage compared to
full-dataset training. Our results highlight the substantial potential of
influence-based data selection for efficient RLVR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantum Gated Recurrent GAN with Gaussian Uncertainty for Network
  Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26487v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26487v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wajdi Hammami, Soumaya Cherkaoui, Jean-Frederic Laprade, Ola Ahmad, Shengrui Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anomaly detection in time-series data is a critical challenge with
significant implications for network security. Recent quantum machine learning
approaches, such as quantum kernel methods and variational quantum circuits,
have shown promise in capturing complex data distributions for anomaly
detection but remain constrained by limited qubit counts. We introduce in this
work a novel Quantum Gated Recurrent Unit (QGRU)-based Generative Adversarial
Network (GAN) employing Successive Data Injection (SuDaI) and a multi-metric
gating strategy for robust network anomaly detection. Our model uniquely
utilizes a quantum-enhanced generator that outputs parameters (mean and
log-variance) of a Gaussian distribution via reparameterization, combined with
a Wasserstein critic to stabilize adversarial training. Anomalies are
identified through a novel gating mechanism that initially flags potential
anomalies based on Gaussian uncertainty estimates and subsequently verifies
them using a composite of critic scores and reconstruction errors. Evaluated on
benchmark datasets, our method achieves a high time-series aware F1 score
(TaF1) of 89.43% demonstrating superior capability in detecting anomalies
accurately and promptly as compared to existing classical and quantum models.
Furthermore, the trained QGRU-WGAN was deployed on real IBM Quantum hardware,
where it retained high anomaly detection performance, confirming its robustness
and practical feasibility on current noisy intermediate-scale quantum (NISQ)
devices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LINK-KG: LLM-Driven Coreference-Resolved Knowledge Graphs for Human
  Smuggling Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26486v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26486v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dipak Meher, Carlotta Domeniconi, Guadalupe Correa-Cabrera
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human smuggling networks are complex and constantly evolving, making them
difficult to analyze comprehensively. Legal case documents offer rich factual
and procedural insights into these networks but are often long, unstructured,
and filled with ambiguous or shifting references, posing significant challenges
for automated knowledge graph (KG) construction. Existing methods either
overlook coreference resolution or fail to scale beyond short text spans,
leading to fragmented graphs and inconsistent entity linking. We propose
LINK-KG, a modular framework that integrates a three-stage, LLM-guided
coreference resolution pipeline with downstream KG extraction. At the core of
our approach is a type-specific Prompt Cache, which consistently tracks and
resolves references across document chunks, enabling clean and disambiguated
narratives for structured knowledge graph construction from both short and long
legal texts. LINK-KG reduces average node duplication by 45.21% and noisy nodes
by 32.22% compared to baseline methods, resulting in cleaner and more coherent
graph structures. These improvements establish LINK-KG as a strong foundation
for analyzing complex criminal networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ICKG 2025 Conference, 8 Pages, 2 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReSpec: Towards Optimizing Speculative Decoding in Reinforcement
  Learning Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26475v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26475v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiaoling Chen, Zijun Liu, Peng Sun, Shenggui Li, Guoteng Wang, Ziming Liu, Yonggang Wen, Siyuan Feng, Tianwei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adapting large language models (LLMs) via reinforcement learning (RL) is
often bottlenecked by the generation stage, which can consume over 75\% of the
training time. Speculative decoding (SD) accelerates autoregressive generation
in serving systems, but its behavior under RL training remains largely
unexplored. We identify three critical gaps that hinder the naive integration
of SD into RL systems: diminishing speedups at large batch sizes, drafter
staleness under continual actor updates, and drafter-induced policy
degradation.
  To address these gaps, we present ReSpec, a system that adapts SD to RL
through three complementary mechanisms: dynamically tuning SD configurations,
evolving the drafter via knowledge distillation, and weighting updates by
rollout rewards. On Qwen models (3B--14B), ReSpec achieves up to 4.5x speedup
while preserving reward convergence and training stability, providing a
practical solution for efficient RL-based LLM adaptation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Counteracting Matthew Effect in Self-Improvement of LVLMs through
  Head-Tail Re-balancing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26474v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26474v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Guo, Zhiheng Xi, Yiwen Ding, Yitao Zhai, Xiaowei Shi, Xunliang Cai, Tao Gui, Qi Zhang, Xuanjing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-improvement has emerged as a mainstream paradigm for advancing the
reasoning capabilities of large vision-language models (LVLMs), where models
explore and learn from successful trajectories iteratively. However, we
identify a critical issue during this process: the model excels at generating
high-quality trajectories for simple queries (i.e., head data) but struggles
with more complex ones (i.e., tail data). This leads to an imbalanced
optimization that drives the model to prioritize simple reasoning skills, while
hindering its ability to tackle more complex reasoning tasks. Over iterations,
this imbalance becomes increasingly pronounced--a dynamic we term the "Matthew
effect"--which ultimately hinders further model improvement and leads to
performance bottlenecks. To counteract this challenge, we introduce four
efficient strategies from two perspectives: distribution-reshaping and
trajectory-resampling, to achieve head-tail re-balancing during the
exploration-and-learning self-improvement process. Extensive experiments on
Qwen2-VL-7B-Instruct and InternVL2.5-4B models across visual reasoning tasks
demonstrate that our methods consistently improve visual reasoning
capabilities, outperforming vanilla self-improvement by 3.86 points on average.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Representation-Level Counterfactual Calibration for Debiased Zero-Shot
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26466v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26466v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pei Peng, MingKun Xie, Hang Hao, Tong Jin, ShengJun Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object-context shortcuts remain a persistent challenge in vision-language
models, undermining zero-shot reliability when test-time scenes differ from
familiar training co-occurrences. We recast this issue as a causal inference
problem and ask: Would the prediction remain if the object appeared in a
different environment? To answer this at inference time, we estimate object and
background expectations within CLIP's representation space, and synthesize
counterfactual embeddings by recombining object features with diverse
alternative contexts sampled from external datasets, batch neighbors, or
text-derived descriptions. By estimating the Total Direct Effect and simulating
intervention, we further subtract background-only activation, preserving
beneficial object-context interactions while mitigating hallucinated scores.
Without retraining or prompt design, our method substantially improves both
worst-group and average accuracy on context-sensitive benchmarks, establishing
a new zero-shot state of the art. Beyond performance, our framework provides a
lightweight representation-level counterfactual approach, offering a practical
causal avenue for debiased and reliable multimodal reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vectorized Context-Aware Embeddings for GAT-Based Collaborative
  Filtering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26461v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26461v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danial Ebrat, Sepideh Ahmadian, Luis Rueda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems often struggle with data sparsity and cold-start
scenarios, limiting their ability to provide accurate suggestions for new or
infrequent users. This paper presents a Graph Attention Network (GAT) based
Collaborative Filtering (CF) framework enhanced with Large Language Model (LLM)
driven context aware embeddings. Specifically, we generate concise textual user
profiles and unify item metadata (titles, genres, overviews) into rich textual
embeddings, injecting these as initial node features in a bipartite user item
graph. To further optimize ranking performance, we introduce a hybrid loss
function that combines Bayesian Personalized Ranking (BPR) with a cosine
similarity term and robust negative sampling, ensuring explicit negative
feedback is distinguished from unobserved data. Experiments on the MovieLens
100k and 1M datasets show consistent improvements over state-of-the-art
baselines in Precision, NDCG, and MAP while demonstrating robustness for users
with limited interaction history. Ablation studies confirm the critical role of
LLM-augmented embeddings and the cosine similarity term in capturing nuanced
semantic relationships. Our approach effectively mitigates sparsity and
cold-start limitations by integrating LLM-derived contextual understanding into
graph-based architectures. Future directions include balancing recommendation
accuracy with coverage and diversity, and introducing fairness-aware
constraints and interpretability features to enhance system performance
further.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Graph Condensation via Classification Complexity Mitigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26451v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26451v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayi Luo, Qingyun Sun, Beining Yang, Haonan Yuan, Xingcheng Fu, Yanbiao Ma, Jianxin Li, Philip S. Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph condensation (GC) has gained significant attention for its ability to
synthesize smaller yet informative graphs. However, existing studies often
overlook the robustness of GC in scenarios where the original graph is
corrupted. In such cases, we observe that the performance of GC deteriorates
significantly, while existing robust graph learning technologies offer only
limited effectiveness. Through both empirical investigation and theoretical
analysis, we reveal that GC is inherently an intrinsic-dimension-reducing
process, synthesizing a condensed graph with lower classification complexity.
Although this property is critical for effective GC performance, it remains
highly vulnerable to adversarial perturbations. To tackle this vulnerability
and improve GC robustness, we adopt the geometry perspective of graph data
manifold and propose a novel Manifold-constrained Robust Graph Condensation
framework named MRGC. Specifically, we introduce three graph data manifold
learning modules that guide the condensed graph to lie within a smooth,
low-dimensional manifold with minimal class ambiguity, thereby preserving the
classification complexity reduction capability of GC and ensuring robust
performance under universal adversarial attacks. Extensive experiments
demonstrate the robustness of \ModelName\ across diverse attack scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Personalized Treatment Outcome Prediction from Scarce Data via
  Dual-Channel Knowledge Distillation and Adaptive Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26444v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26444v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenjie Chen, Li Zhuang, Ziying Luo, Yu Liu, Jiahao Wu, Shengcai Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personalized treatment outcome prediction based on trial data for
small-sample and rare patient groups is critical in precision medicine.
However, the costly trial data limit the prediction performance. To address
this issue, we propose a cross-fidelity knowledge distillation and adaptive
fusion network (CFKD-AFN), which leverages abundant but low-fidelity simulation
data to enhance predictions on scarce but high-fidelity trial data. CFKD-AFN
incorporates a dual-channel knowledge distillation module to extract
complementary knowledge from the low-fidelity model, along with an
attention-guided fusion module to dynamically integrate multi-source
information. Experiments on treatment outcome prediction for the chronic
obstructive pulmonary disease demonstrates significant improvements of CFKD-AFN
over state-of-the-art methods in prediction accuracy, ranging from 6.67\% to
74.55\%, and strong robustness to varying high-fidelity dataset sizes.
Furthermore, we extend CFKD-AFN to an interpretable variant, enabling the
exploration of latent medical semantics to support clinical decision-making.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Co-Evolving Latent Action World Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26433v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26433v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yucen Wang, Fengming Zhang, De-Chuan Zhan, Li Zhao, Kaixin Wang, Jiang Bian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adapting pre-trained video generation models into controllable world models
via latent actions is a promising step towards creating generalist world
models. The dominant paradigm adopts a two-stage approach that trains latent
action model (LAM) and the world model separately, resulting in redundant
training and limiting their potential for co-adaptation. A conceptually simple
and appealing idea is to directly replace the forward dynamic model in LAM with
a powerful world model and training them jointly, but it is non-trivial and
prone to representational collapse. In this work, we propose CoLA-World, which
for the first time successfully realizes this synergistic paradigm, resolving
the core challenge in joint learning through a critical warm-up phase that
effectively aligns the representations of the from-scratch LAM with the
pre-trained world model. This unlocks a co-evolution cycle: the world model
acts as a knowledgeable tutor, providing gradients to shape a high-quality LAM,
while the LAM offers a more precise and adaptable control interface to the
world model. Empirically, CoLA-World matches or outperforms prior two-stage
methods in both video simulation quality and downstream visual planning,
establishing a robust and efficient new paradigm for the field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Autograder+: A Multi-Faceted AI Framework for Rich Pedagogical Feedback
  in Programming Education 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26402v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26402v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vikrant Sahu, Gagan Raj Gupta, Raghav Borikar, Nitin Mane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid growth of programming education has outpaced traditional assessment
tools, leaving faculty with limited means to provide meaningful, scalable
feedback. Conventional autograders, while efficient, act as black-box systems
that simply return pass/fail results, offering little insight into student
thinking or learning needs.
  Autograder+ is designed to shift autograding from a purely summative process
to a formative learning experience. It introduces two key capabilities:
automated feedback generation using a fine-tuned Large Language Model, and
visualization of student code submissions to uncover learning patterns. The
model is fine-tuned on curated student code and expert feedback to ensure
pedagogically aligned, context-aware guidance.
  In evaluation across 600 student submissions from multiple programming tasks,
the system produced feedback with strong semantic alignment to instructor
comments. For visualization, contrastively learned code embeddings trained on
1,000 annotated submissions enable grouping solutions into meaningful clusters
based on functionality and approach. The system also supports prompt-pooling,
allowing instructors to guide feedback style through selected prompt templates.
  By integrating AI-driven feedback, semantic clustering, and interactive
visualization, Autograder+ reduces instructor workload while supporting
targeted instruction and promoting stronger learning outcomes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Output Robust and Conjugate Gaussian Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26401v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26401v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joshua Rooijakkers, Leiv Rønneberg, François-Xavier Briol, Jeremias Knoblauch, Matias Altamirano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-output Gaussian process (MOGP) regression allows modelling dependencies
among multiple correlated response variables. Similarly to standard Gaussian
processes, MOGPs are sensitive to model misspecification and outliers, which
can distort predictions within individual outputs. This situation can be
further exacerbated by multiple anomalous response variables whose errors
propagate due to correlations between outputs. To handle this situation, we
extend and generalise the robust and conjugate Gaussian process (RCGP)
framework introduced by Altamirano et al. (2024). This results in the
multi-output RCGP (MO-RCGP): a provably robust MOGP that is conjugate, and
jointly captures correlations across outputs. We thoroughly evaluate our
approach through applications in finance and cancer research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Task Learning Based on Support Vector Machines and Twin Support
  Vector Machines: A Comprehensive <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26392v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26392v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fatemeh Bazikar, Hossein Moosaei, Atefeh Hemmati, Panos M. Pardalos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-task learning (MTL) enables simultaneous training across related tasks,
leveraging shared information to improve generalization, efficiency, and
robustness, especially in data-scarce or high-dimensional scenarios. While deep
learning dominates recent MTL research, Support Vector Machines (SVMs) and Twin
SVMs (TWSVMs) remain relevant due to their interpretability, theoretical rigor,
and effectiveness with small datasets.
  This chapter surveys MTL approaches based on SVM and TWSVM, highlighting
shared representations, task regularization, and structural coupling
strategies. Special attention is given to emerging TWSVM extensions for
multi-task settings, which show promise but remain underexplored. We compare
these models in terms of theoretical properties, optimization strategies, and
empirical performance, and discuss applications in fields such as computer
vision, natural language processing, and bioinformatics.
  Finally, we identify research gaps and outline future directions for building
scalable, interpretable, and reliable margin-based MTL frameworks. This work
provides a comprehensive resource for researchers and practitioners interested
in SVM- and TWSVM-based multi-task learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Context Length Optimization with Low-Frequency Truncation for
  Multi-Agent Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26389v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26389v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenchang Duan, Yaoliang Yu, Jiwan He, Yi Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, deep multi-agent reinforcement learning (MARL) has demonstrated
promising performance for solving challenging tasks, such as long-term
dependencies and non-Markovian environments. Its success is partly attributed
to conditioning policies on large fixed context length. However, such large
fixed context lengths may lead to limited exploration efficiency and redundant
information. In this paper, we propose a novel MARL framework to obtain
adaptive and effective contextual information. Specifically, we design a
central agent that dynamically optimizes context length via temporal gradient
analysis, enhancing exploration to facilitate convergence to global optima in
MARL. Furthermore, to enhance the adaptive optimization capability of the
context length, we present an efficient input representation for the central
agent, which effectively filters redundant information. By leveraging a
Fourier-based low-frequency truncation method, we extract global temporal
trends across decentralized agents, providing an effective and efficient
representation of the MARL environment. Extensive experiments demonstrate that
the proposed method achieves state-of-the-art (SOTA) performance on long-term
dependency tasks, including PettingZoo, MiniGrid, Google Research Football
(GRF), and StarCraft Multi-Agent Challenge v2 (SMACv2).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scales++: Compute Efficient Evaluation Subset Selection with Cognitive
  Scales Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26384v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26384v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew M. Bean, Nabeel Seedat, Shengzhuang Chen, Jonathan Richard Schwarz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The prohibitive cost of evaluating large language models (LLMs) on
comprehensive benchmarks necessitates the creation of small yet representative
data subsets (i.e., tiny benchmarks) that enable efficient assessment while
retaining predictive fidelity. Current methods for this task operate under a
model-centric paradigm, selecting benchmarking items based on the collective
performance of existing models. Such approaches are limited by large upfront
costs, an inability to immediately handle new benchmarks (`cold-start'), and
the fragile assumption that future models will share the failure patterns of
their predecessors. In this work, we challenge this paradigm and propose a
item-centric approach to benchmark subset selection, arguing that selection
should be based on the intrinsic properties of the task items themselves,
rather than on model-specific failure patterns. We instantiate this
item-centric efficient benchmarking approach via a novel method, Scales++,
where data selection is based on the cognitive demands of the benchmark
samples. Empirically, we show Scales++ reduces the upfront selection cost by
over 18x while achieving competitive predictive fidelity. On the Open LLM
Leaderboard, using just a 0.5\% data subset, we predict full benchmark scores
with a 2.9% mean absolute error. We demonstrate that this item-centric approach
enables more efficient model evaluation without significant fidelity
degradation, while also providing better cold-start performance and more
interpretable benchmarking.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 2 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Generative AI Boosts Probabilistic Forecasting of Sudden
  Stratospheric Warmings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26376v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26376v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ningning Tao, Fei Xie, Baoxiang Pan, Hongyu Wang, Han Huang, Zhongpu Qiu, Ke Gui, Jiali Luo, Xiaosong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sudden Stratospheric Warmings (SSWs) are key sources of subseasonal
predictability and major drivers of extreme winter weather. Yet, their accurate
and efficient forecast remains a persistent challenge for numerical weather
prediction (NWP) systems due to limitations in physical representation,
initialization, and the immense computational demands of ensemble forecasts.
While data-driven forecasting is rapidly evolving, its application to the
complex, three-dimensional dynamics of SSWs, particularly for probabilistic
forecast, remains underexplored. Here, we bridge this gap by developing a Flow
Matching-based generative AI model (FM-Cast) for efficient and skillful
probabilistic forecasting of the spatiotemporal evolution of stratospheric
circulation. Evaluated across 18 major SSW events (1998-2024), FM-Cast
skillfully forecasts the onset, intensity, and morphology of 10 events up to 20
days in advance, achieving ensemble accuracies above 50%. Its performance is
comparable to or exceeds leading NWP systems while requiring only two minutes
for a 50-member, 30-day forecast on a consumer GPU. Furthermore, leveraging
FM-Cast as a scientific tool, we demonstrate through idealized experiments that
SSW predictability is fundamentally linked to its underlying physical drivers,
distinguishing between events forced from the troposphere and those driven by
internal stratospheric dynamics. Our work thus establishes a computationally
efficient paradigm for probabilistic forecasting stratospheric anomalies and
showcases generative AI's potential to deepen the physical understanding of
atmosphere-climate dynamics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CorVS: Person Identification via Video Trajectory-Sensor Correspondence
  in a Real-World Warehouse 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26369v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26369v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kazuma Kano, Yuki Mori, Shin Katayama, Kenta Urano, Takuro Yonezawa, Nobuo Kawaguchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Worker location data is key to higher productivity in industrial sites.
Cameras are a promising tool for localization in logistics warehouses since
they also offer valuable environmental contexts such as package status.
However, identifying individuals with only visual data is often impractical.
Accordingly, several prior studies identified people in videos by comparing
their trajectories and wearable sensor measurements. While this approach has
advantages such as independence from appearance, the existing methods may break
down under real-world conditions. To overcome this challenge, we propose CorVS,
a novel data-driven person identification method based on correspondence
between visual tracking trajectories and sensor measurements. Firstly, our deep
learning model predicts correspondence probabilities and reliabilities for
every pair of a trajectory and sensor measurements. Secondly, our algorithm
matches the trajectories and sensor measurements over time using the predicted
probabilities and reliabilities. We developed a dataset with actual warehouse
operations and demonstrated the method's effectiveness for real-world
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 3 figures, accepted to IPIN 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Explainable and Reliable AI in Finance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26353v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26353v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Albi Isufaj, Pablo Mollá, Helmut Prendinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Financial forecasting increasingly uses large neural network models, but
their opacity raises challenges for trust and regulatory compliance. We present
several approaches to explainable and reliable AI in finance. \emph{First}, we
describe how Time-LLM, a time series foundation model, uses a prompt to avoid a
wrong directional forecast. \emph{Second}, we show that combining foundation
models for time series forecasting with a reliability estimator can filter our
unreliable predictions. \emph{Third}, we argue for symbolic reasoning encoding
domain rules for transparent justification. These approaches shift emphasize
executing only forecasts that are both reliable and explainable. Experiments on
equity and cryptocurrency data show that the architecture reduces false
positives and supports selective execution. By integrating predictive
performance with reliability estimation and rule-based reasoning, our framework
advances transparent and auditable financial AI systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UnifiedFL: A Dynamic Unified Learning Framework for Equitable Federation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26350v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26350v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Furkan Pala, Islem Rekik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) has emerged as a key paradigm for collaborative model
training across multiple clients without sharing raw data, enabling
privacy-preserving applications in areas such as radiology and pathology.
However, works on collaborative training across clients with fundamentally
different neural architectures and non-identically distributed datasets remain
scarce. Existing FL frameworks face several limitations. Despite claiming to
support architectural heterogeneity, most recent FL methods only tolerate
variants within a single model family (e.g., shallower, deeper, or wider CNNs),
still presuming a shared global architecture and failing to accommodate
federations where clients deploy fundamentally different network types (e.g.,
CNNs, GNNs, MLPs). Moreover, existing approaches often address only statistical
heterogeneity while overlooking the domain-fracture problem, where each
client's data distribution differs markedly from that faced at testing time,
undermining model generalizability. When clients use different architectures,
have non-identically distributed data, and encounter distinct test domains,
current methods perform poorly. To address these challenges, we propose
UnifiedFL, a dynamic federated learning framework that represents heterogeneous
local networks as nodes and edges in a directed model graph optimized by a
shared graph neural network (GNN). UnifiedFL introduces (i) a common GNN to
parameterize all architectures, (ii) distance-driven clustering via Euclidean
distances between clients' parameters, and (iii) a two-tier aggregation policy
balancing convergence and diversity. Experiments on MedMNIST classification and
hippocampus segmentation benchmarks demonstrate UnifiedFL's superior
performance. Code and data: https://github.com/basiralab/UnifiedFL
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reinforcement Learning for Pollution Detection in a Randomized, Sparse
  and Nonstationary Environment with an Autonomous Underwater Vehicle 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26347v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26347v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian Zieglmeier, Niklas Erdmann, Narada D. Warakagoda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) algorithms are designed to optimize
problem-solving by learning actions that maximize rewards, a task that becomes
particularly challenging in random and nonstationary environments. Even
advanced RL algorithms are often limited in their ability to solve problems in
these conditions. In applications such as searching for underwater pollution
clouds with autonomous underwater vehicles (AUVs), RL algorithms must navigate
reward-sparse environments, where actions frequently result in a zero reward.
This paper aims to address these challenges by revisiting and modifying
classical RL approaches to efficiently operate in sparse, randomized, and
nonstationary environments. We systematically study a large number of
modifications, including hierarchical algorithm changes, multigoal learning,
and the integration of a location memory as an external output filter to
prevent state revisits. Our results demonstrate that a modified Monte
Carlo-based approach significantly outperforms traditional Q-learning and two
exhaustive search patterns, illustrating its potential in adapting RL to
complex environments. These findings suggest that reinforcement learning
approaches can be effectively adapted for use in random, nonstationary, and
reward-sparse environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MisSynth: Improving MISSCI Logical Fallacies Classification with
  Synthetic Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26345v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26345v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mykhailo Poliakov, Nadiya Shvai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Health-related misinformation is very prevalent and potentially harmful. It
is difficult to identify, especially when claims distort or misinterpret
scientific findings. We investigate the impact of synthetic data generation and
lightweight fine-tuning techniques on the ability of large language models
(LLMs) to recognize fallacious arguments using the MISSCI dataset and
framework. In this work, we propose MisSynth, a pipeline that applies
retrieval-augmented generation (RAG) to produce synthetic fallacy samples,
which are then used to fine-tune an LLM model. Our results show substantial
accuracy gains with fine-tuned models compared to vanilla baselines. For
instance, the LLaMA 3.1 8B fine-tuned model achieved an over 35% F1-score
absolute improvement on the MISSCI test split over its vanilla baseline. We
demonstrate that introducing synthetic fallacy data to augment limited
annotated resources can significantly enhance zero-shot LLM classification
performance on real-world scientific misinformation tasks, even with limited
computational resources. The code and synthetic dataset are available on
https://github.com/mxpoliakov/MisSynth.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Linear Causal Discovery with Interventional Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26342v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26342v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhigao Guo, Feng Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incorporating causal knowledge and mechanisms is essential for refining
causal models and improving downstream tasks such as designing new treatments.
In this paper, we introduce a novel concept in causal discovery, termed
interventional constraints, which differs fundamentally from interventional
data. While interventional data require direct perturbations of variables,
interventional constraints encode high-level causal knowledge in the form of
inequality constraints on causal effects. For instance, in the Sachs dataset
(Sachs et al.\ 2005), Akt has been shown to be activated by PIP3, meaning PIP3
exerts a positive causal effect on Akt. Existing causal discovery methods allow
enforcing structural constraints (for example, requiring a causal path from
PIP3 to Akt), but they may still produce incorrect causal conclusions such as
learning that "PIP3 inhibits Akt". Interventional constraints bridge this gap
by explicitly constraining the total causal effect between variable pairs,
ensuring learned models respect known causal influences. To formalize
interventional constraints, we propose a metric to quantify total causal
effects for linear causal models and formulate the problem as a constrained
optimization task, solved using a two-stage constrained optimization method. We
evaluate our approach on real-world datasets and demonstrate that integrating
interventional constraints not only improves model accuracy and ensures
consistency with established findings, making models more explainable, but also
facilitates the discovery of new causal relationships that would otherwise be
costly to identify.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SABER: Symbolic Regression-based Angle of Arrival and Beam Pattern
  Estimator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26340v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26340v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shih-Kai Chou, Mengran Zhao, Cheng-Nan Hu, Kuang-Chung Chou, Carolina Fortuna, Jernej Hribar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate Angle-of-arrival (AoA) estimation is essential for next-generation
wireless communication systems to enable reliable beamforming, high-precision
localization, and integrated sensing. Unfortunately, classical high-resolution
techniques require multi-element arrays and extensive snapshot collection,
while generic Machine Learning (ML) approaches often yield black-box models
that lack physical interpretability. To address these limitations, we propose a
Symbolic Regression (SR)-based ML framework. Namely, Symbolic Regression-based
Angle of Arrival and Beam Pattern Estimator (SABER), a constrained
symbolic-regression framework that automatically discovers closed-form beam
pattern and AoA models from path loss measurements with interpretability. SABER
achieves high accuracy while bridging the gap between opaque ML methods and
interpretable physics-driven estimators. First, we validate our approach in a
controlled free-space anechoic chamber, showing that both direct inversion of
the known $\cos^n$ beam and a low-order polynomial surrogate achieve sub-0.5
degree Mean Absolute Error (MAE). A purely unconstrained SR method can further
reduce the error of the predicted angles, but produces complex formulas that
lack physical insight. Then, we implement the same SR-learned inversions in a
real-world, Reconfigurable Intelligent Surface (RIS)-aided indoor testbed.
SABER and unconstrained SR models accurately recover the true AoA with
near-zero error. Finally, we benchmark SABER against the Cram\'er-Rao Lower
Bounds (CRLBs). Our results demonstrate that SABER is an interpretable and
accurate alternative to state-of-the-art and black-box ML-based methods for AoA
estimation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Agent Skills Enable a New Class of Realistic and Trivially Simple <span class="highlight-title">Prompt</span>
  Injections 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26328v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26328v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Schmotz, Sahar Abdelnabi, Maksym Andriushchenko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Enabling continual learning in LLMs remains a key unresolved research
challenge. In a recent announcement, a frontier LLM company made a step towards
this by introducing Agent Skills, a framework that equips agents with new
knowledge based on instructions stored in simple markdown files. Although Agent
Skills can be a very useful tool, we show that they are fundamentally insecure,
since they enable trivially simple prompt injections. We demonstrate how to
hide malicious instructions in long Agent Skill files and referenced scripts to
exfiltrate sensitive data, such as internal files or passwords. Importantly, we
show how to bypass system-level guardrails of a popular coding agent: a benign,
task-specific approval with the "Don't ask again" option can carry over to
closely related but harmful actions. Overall, we conclude that despite ongoing
research efforts and scaling model capabilities, frontier LLMs remain
vulnerable to very simple prompt injections in realistic scenarios. Our code is
available at https://github.com/aisa-group/promptinject-agent-skills.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Posterior Sampling by Combining Diffusion Models with Annealed Langevin
  Dynamics <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26324v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26324v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyang Xun, Shivam Gupta, Eric Price
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given a noisy linear measurement $y = Ax + \xi$ of a distribution $p(x)$, and
a good approximation to the prior $p(x)$, when can we sample from the posterior
$p(x \mid y)$? Posterior sampling provides an accurate and fair framework for
tasks such as inpainting, deblurring, and MRI reconstruction, and several
heuristics attempt to approximate it. Unfortunately, approximate posterior
sampling is computationally intractable in general.
  To sidestep this hardness, we focus on (local or global) log-concave
distributions $p(x)$. In this regime, Langevin dynamics yields posterior
samples when the exact scores of $p(x)$ are available, but it is brittle to
score--estimation error, requiring an MGF bound (sub-exponential error). By
contrast, in the unconditional setting, diffusion models succeed with only an
$L^2$ bound on the score error. We prove that combining diffusion models with
an annealed variant of Langevin dynamics achieves conditional sampling in
polynomial time using merely an $L^4$ bound on the score error.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Impact of Weight Discretization in QUBO-Based SVM Training <span class="chip">ECML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26323v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26323v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sascha Mücke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training Support Vector Machines (SVMs) can be formulated as a QUBO problem,
enabling the use of quantum annealing for model optimization. In this work, we
study how the number of qubits - linked to the discretization level of dual
weights - affects predictive performance across datasets. We compare QUBO-based
SVM training to the classical LIBSVM solver and find that even low-precision
QUBO encodings (e.g., 1 bit per parameter) yield competitive, and sometimes
superior, accuracy. While increased bit-depth enables larger regularization
parameters, it does not always improve classification. Our findings suggest
that selecting the right support vectors may matter more than their precise
weighting. Although current hardware limits the size of solvable QUBOs, our
results highlight the potential of quantum annealing for efficient SVM training
as quantum devices scale.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at the 7th DSO Workshop at ECML PKDD 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Model Inversion with Layer-Specific Modeling and Alignment for Data-Free
  Continual Learning <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26311v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26311v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruilin Tong, Haodong Lu, Yuhang Liu, Dong Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual learning (CL) aims to incrementally train a model on a sequence of
tasks while retaining performance on prior ones. However, storing and replaying
data is often infeasible due to privacy or security constraints and impractical
for arbitrary pre-trained models. Data-free CL seeks to update models without
access to previous data. Beyond regularization, we employ model inversion to
synthesize data from the trained model, enabling replay without storing
samples. Yet, model inversion in predictive models faces two challenges: (1)
generating inputs solely from compressed output labels causes drift between
synthetic and real data, and replaying such data can erode prior knowledge; (2)
inversion is computationally expensive since each step backpropagates through
the full model. These issues are amplified in large pre-trained models such as
CLIP. To improve efficiency, we propose Per-layer Model Inversion (PMI),
inspired by faster convergence in single-layer optimization. PMI provides
strong initialization for full-model inversion, substantially reducing
iterations. To mitigate feature shift, we model class-wise features via
Gaussian distributions and contrastive model, ensuring alignment between
synthetic and real features. Combining PMI and feature modeling, our approach
enables continual learning of new classes by generating pseudo-images from
semantic-aware projected features, achieving strong effectiveness and
compatibility across multiple CL settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> of Heterogeneous Graph Neural Networks for Cybersecurity
  Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26307v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26307v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laura Jiang, Reza Ryan, Qian Li, Nasim Ferdosian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anomaly detection is a critical task in cybersecurity, where identifying
insider threats, access violations, and coordinated attacks is essential for
ensuring system resilience. Graph-based approaches have become increasingly
important for modeling entity interactions, yet most rely on homogeneous and
static structures, which limits their ability to capture the heterogeneity and
temporal evolution of real-world environments. Heterogeneous Graph Neural
Networks (HGNNs) have emerged as a promising paradigm for anomaly detection by
incorporating type-aware transformations and relation-sensitive aggregation,
enabling more expressive modeling of complex cyber data. However, current
research on HGNN-based anomaly detection remains fragmented, with diverse
modeling strategies, limited comparative evaluation, and an absence of
standardized benchmarks. To address this gap, we provide a comprehensive survey
of HGNN-based anomaly detection methods in cybersecurity. We introduce a
taxonomy that classifies approaches by anomaly type and graph dynamics, analyze
representative models, and map them to key cybersecurity applications. We also
review commonly used benchmark datasets and evaluation metrics, highlighting
their strengths and limitations. Finally, we identify key open challenges
related to modeling, data, and deployment, and outline promising directions for
future research. This survey aims to establish a structured foundation for
advancing HGNN-based anomaly detection toward scalable, interpretable, and
practically deployable solutions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages, 4 figures, 86 references. Submitted to Journal of Computer
  Security (under review)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Implicit Bias of Per-sample Adam on Separable Data: Departure from the
  Full-batch Regime 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26303v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26303v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Beomhan Baek, Minhak Song, Chulhee Yun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adam [Kingma and Ba, 2015] is the de facto optimizer in deep learning, yet
its theoretical understanding remains limited. Prior analyses show that Adam
favors solutions aligned with $\ell_\infty$-geometry, but these results are
restricted to the full-batch regime. In this work, we study the implicit bias
of incremental Adam (using one sample per step) for logistic regression on
linearly separable data, and we show that its bias can deviate from the
full-batch behavior. To illustrate this, we construct a class of structured
datasets where incremental Adam provably converges to the $\ell_2$-max-margin
classifier, in contrast to the $\ell_\infty$-max-margin bias of full-batch
Adam. For general datasets, we develop a proxy algorithm that captures the
limiting behavior of incremental Adam as $\beta_2 \to 1$ and we characterize
its convergence direction via a data-dependent dual fixed-point formulation.
Finally, we prove that, unlike Adam, Signum [Bernstein et al., 2018] converges
to the $\ell_\infty$-max-margin classifier for any batch size by taking $\beta$
close enough to 1. Overall, our results highlight that the implicit bias of
Adam crucially depends on both the batching scheme and the dataset, while
Signum remains invariant.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>50 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding Hardness of Vision-Language Compositionality from A
  Token-level Causal Lens 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26302v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26302v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziliang Chen, Tianang Xiao, Jusheng Zhang, Yongsen Zheng, Xipeng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive Language-Image Pre-training (CLIP) delivers strong cross modal
generalization by aligning images and texts in a shared embedding space, yet it
persistently fails at compositional reasoning over objects, attributes, and
relations often behaving like a bag-of-words matcher. Prior causal accounts
typically model text as a single vector, obscuring token-level structure and
leaving core phenomena-such as prompt sensitivity and failures on hard
negatives unexplained. We address this gap with a token-aware causal
representation learning (CRL) framework grounded in a sequential,
language-token SCM. Our theory extends block identifiability to tokenized text,
proving that CLIP's contrastive objective can recover the modal-invariant
latent variable under both sentence-level and token-level SCMs. Crucially,
token granularity yields the first principled explanation of CLIP's
compositional brittleness: composition nonidentifiability. We show the
existence of pseudo-optimal text encoders that achieve perfect modal-invariant
alignment yet are provably insensitive to SWAP, REPLACE, and ADD operations
over atomic concepts, thereby failing to distinguish correct captions from hard
negatives despite optimizing the same training objective as true-optimal
encoders. The analysis further links language-side nonidentifiability to
visual-side failures via the modality gap and shows how iterated composition
operators compound hardness, motivating improved negative mining strategies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Offline Clustering of Preference Learning with Active-data Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26301v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26301v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyuan Liu, Fatemeh Ghaffari, Xuchuang Wang, Mohammad Hajiesmaili, Carlee Joe-Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Preference learning from pairwise feedback is a widely adopted framework in
applications such as reinforcement learning with human feedback and
recommendations. In many practical settings, however, user interactions are
limited or costly, making offline preference learning necessary. Moreover,
real-world preference learning often involves users with different preferences.
For example, annotators from different backgrounds may rank the same responses
differently. This setting presents two central challenges: (1) identifying
similarity across users to effectively aggregate data, especially under
scenarios where offline data is imbalanced across dimensions, and (2) handling
the imbalanced offline data where some preference dimensions are
underrepresented. To address these challenges, we study the Offline Clustering
of Preference Learning problem, where the learner has access to fixed datasets
from multiple users with potentially different preferences and aims to maximize
utility for a test user. To tackle the first challenge, we first propose
Off-C$^2$PL for the pure offline setting, where the learner relies solely on
offline data. Our theoretical analysis provides a suboptimality bound that
explicitly captures the tradeoff between sample noise and bias. To address the
second challenge of inbalanced data, we extend our framework to the setting
with active-data augmentation where the learner is allowed to select a limited
number of additional active-data for the test user based on the cluster
structure learned by Off-C$^2$PL. In this setting, our second algorithm,
A$^2$-Off-C$^2$PL, actively selects samples that target the least-informative
dimensions of the test user's preference. We prove that these actively
collected samples contribute more effectively than offline ones. Finally, we
validate our theoretical results through simulations on synthetic and
real-world datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unravelling the Mechanisms of Manipulating Numbers in Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26285v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26285v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michal Štefánik, Timothee Mickus, Marek Kadlčík, Bertram Højer, Michal Spiegel, Raúl Vázquez, Aman Sinha, Josef Kuchař, Philipp Mondorf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work has shown that different large language models (LLMs) converge to
similar and accurate input embedding representations for numbers. These
findings conflict with the documented propensity of LLMs to produce erroneous
outputs when dealing with numeric information. In this work, we aim to explain
this conflict by exploring how language models manipulate numbers and quantify
the lower bounds of accuracy of these mechanisms. We find that despite
surfacing errors, different language models learn interchangeable
representations of numbers that are systematic, highly accurate and universal
across their hidden states and the types of input contexts. This allows us to
create universal probes for each LLM and to trace information -- including the
causes of output errors -- to specific layers. Our results lay a fundamental
understanding of how pre-trained LLMs manipulate numbers and outline the
potential of more accurate probing techniques in addressed refinements of LLMs'
architectures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Empirical Bayesian Multi-Bandit Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26284v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26284v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xia Jiang, Rong J. B. Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-task learning in contextual bandits has attracted significant research
interest due to its potential to enhance decision-making across multiple
related tasks by leveraging shared structures and task-specific heterogeneity.
In this article, we propose a novel hierarchical Bayesian framework for
learning in various bandit instances. This framework captures both the
heterogeneity and the correlations among different bandit instances through a
hierarchical Bayesian model, enabling effective information sharing while
accommodating instance-specific variations. Unlike previous methods that
overlook the learning of the covariance structure across bandits, we introduce
an empirical Bayesian approach to estimate the covariance matrix of the prior
distribution.This enhances both the practicality and flexibility of learning
across multi-bandits. Building on this approach, we develop two efficient
algorithms: ebmTS (Empirical Bayesian Multi-Bandit Thompson Sampling) and
ebmUCB (Empirical Bayesian Multi-Bandit Upper Confidence Bound), both of which
incorporate the estimated prior into the decision-making process. We provide
the frequentist regret upper bounds for the proposed algorithms, thereby
filling a research gap in the field of multi-bandit problems. Extensive
experiments on both synthetic and real-world datasets demonstrate the superior
performance of our algorithms, particularly in complex environments. Our
methods achieve lower cumulative regret compared to existing techniques,
highlighting their effectiveness in balancing exploration and exploitation
across multi-bandits.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distributional Multi-objective Black-box Optimization for
  Diffusion-model Inference-time Multi-Target Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26278v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26278v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kim Yong Tan, Yueming Lyu, Ivor Tsang, Yew-Soon Ong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have been successful in learning complex data distributions.
This capability has driven their application to high-dimensional
multi-objective black-box optimization problem. Existing approaches often
employ an external optimization loop, such as an evolutionary algorithm, to the
diffusion model. However, these approaches treat the diffusion model as a
black-box refiner, which overlooks the internal distribution transition of the
diffusion generation process, limiting their efficiency. To address these
challenges, we propose the Inference-time Multi-target Generation (IMG)
algorithm, which optimizes the diffusion process at inference-time to generate
samples that simultaneously satisfy multiple objectives. Specifically, our IMG
performs weighted resampling during the diffusion generation process according
to the expected aggregated multi-objective values. This weighted resampling
strategy ensures the diffusion-generated samples are distributed according to
our desired multi-target Boltzmann distribution. We further derive that the
multi-target Boltzmann distribution has an interesting log-likelihood
interpretation, where it is the optimal solution to the distributional
multi-objective optimization problem. We implemented IMG for a multi-objective
molecule generation task. Experiments show that IMG, requiring only a single
generation pass, achieves a significantly higher hypervolume than baseline
optimization algorithms that often require hundreds of diffusion generations.
Notably, our algorithm can be viewed as an optimized diffusion process and can
be integrated into existing methods to further improve their performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Research Roadmap for Augmenting Software Engineering Processes and
  Software Products with Generative AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26275v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26275v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Domenico Amalfitano, Andreas Metzger, Marco Autili, Tommaso Fulcini, Tobias Hey, Jan Keim, Patrizio Pelliccione, Vincenzo Scotti, Anne Koziolek, Raffaela Mirandola, Andreas Vogelsang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative AI (GenAI) is rapidly transforming software engineering (SE)
practices, influencing how SE processes are executed, as well as how software
systems are developed, operated, and evolved. This paper applies design science
research to build a roadmap for GenAI-augmented SE. The process consists of
three cycles that incrementally integrate multiple sources of evidence,
including collaborative discussions from the FSE 2025 "Software Engineering
2030" workshop, rapid literature reviews, and external feedback sessions
involving peers. McLuhan's tetrads were used as a conceptual instrument to
systematically capture the transforming effects of GenAI on SE processes and
software products.The resulting roadmap identifies four fundamental forms of
GenAI augmentation in SE and systematically characterizes their related
research challenges and opportunities. These insights are then consolidated
into a set of future research directions. By grounding the roadmap in a
rigorous multi-cycle process and cross-validating it among independent author
teams and peers, the study provides a transparent and reproducible foundation
for analyzing how GenAI affects SE processes, methods and tools, and for
framing future research within this rapidly evolving area. Based on these
findings, the article finally makes ten predictions for SE in the year 2030.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PVMark: Enabling Public Verifiability for LLM Watermarking Schemes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26274v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26274v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haohua Duan, Liyao Xiang, Xin Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Watermarking schemes for large language models (LLMs) have been proposed to
identify the source of the generated text, mitigating the potential threats
emerged from model theft. However, current watermarking solutions hardly
resolve the trust issue: the non-public watermark detection cannot prove itself
faithfully conducting the detection. We observe that it is attributed to the
secret key mostly used in the watermark detection -- it cannot be public, or
the adversary may launch removal attacks provided the key; nor can it be
private, or the watermarking detection is opaque to the public. To resolve the
dilemma, we propose PVMark, a plugin based on zero-knowledge proof (ZKP),
enabling the watermark detection process to be publicly verifiable by third
parties without disclosing any secret key. PVMark hinges upon the proof of
`correct execution' of watermark detection on which a set of ZKP constraints
are built, including mapping, random number generation, comparison, and
summation. We implement multiple variants of PVMark in Python, Rust and Circom,
covering combinations of three watermarking schemes, three hash functions, and
four ZKP protocols, to show our approach effectively works under a variety of
circumstances. By experimental results, PVMark efficiently enables public
verifiability on the state-of-the-art LLM watermarking schemes yet without
compromising the watermarking performance, promising to be deployed in
practice.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Likely Interpolants of Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26266v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26266v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Frederik Möbius Rygaard, Shen Zhu, Yinzhu Jin, Søren Hauberg, Tom Fletcher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interpolation in generative models allows for controlled generation, model
inspection, and more. Unfortunately, most generative models lack a principal
notion of interpolants without restrictive assumptions on either the model or
data dimension. In this paper, we develop a general interpolation scheme that
targets likely transition paths compatible with different metrics and
probability distributions. We consider interpolants analogous to a geodesic
constrained to a suitable data distribution and derive a novel algorithm for
computing these curves, which requires no additional training. Theoretically,
we show that our method locally can be considered as a geodesic under a
suitable Riemannian metric. We quantitatively show that our interpolation
scheme traverses higher density regions than baselines across a range of models
and datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UniSite: The First Cross-Structure <span class="highlight-title">Dataset</span> and Learning Framework for
  End-to-End Ligand Binding Site Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03237v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03237v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jigang Fan, Quanlin Wu, Shengjie Luo, Liwei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The detection of ligand binding sites for proteins is a fundamental step in
Structure-Based Drug Design. Despite notable advances in recent years, existing
methods, datasets, and evaluation metrics are confronted with several key
challenges: (1) current datasets and methods are centered on individual
protein-ligand complexes and neglect that diverse binding sites may exist
across multiple complexes of the same protein, introducing significant
statistical bias; (2) ligand binding site detection is typically modeled as a
discontinuous workflow, employing binary segmentation and subsequent clustering
algorithms; (3) traditional evaluation metrics do not adequately reflect the
actual performance of different binding site prediction methods. To address
these issues, we first introduce UniSite-DS, the first UniProt (Unique
Protein)-centric ligand binding site dataset, which contains 4.81 times more
multi-site data and 2.08 times more overall data compared to the previously
most widely used datasets. We then propose UniSite, the first end-to-end ligand
binding site detection framework supervised by set prediction loss with
bijective matching. In addition, we introduce Average Precision based on
Intersection over Union (IoU) as a more accurate evaluation metric for ligand
binding site prediction. Extensive experiments on UniSite-DS and several
representative benchmark datasets demonstrate that IoU-based Average Precision
provides a more accurate reflection of prediction quality, and that UniSite
outperforms current state-of-the-art methods in ligand binding site detection.
The dataset and codes will be made publicly available at
https://github.com/quanlin-wu/unisite.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ S-CFE: Simple Counterfactual Explanations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15723v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15723v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shpresim Sadiku, Moritz Wagner, Sai Ganesh Nagarajan, Sebastian Pokutta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of finding optimal sparse, manifold-aligned
counterfactual explanations for classifiers. Canonically, this can be
formulated as an optimization problem with multiple non-convex components,
including classifier loss functions and manifold alignment (or
\emph{plausibility}) metrics. The added complexity of enforcing
\emph{sparsity}, or shorter explanations, complicates the problem further.
Existing methods often focus on specific models and plausibility measures,
relying on convex $\ell_1$ regularizers to enforce sparsity. In this paper, we
tackle the canonical formulation using the accelerated proximal gradient (APG)
method, a simple yet efficient first-order procedure capable of handling smooth
non-convex objectives and non-smooth $\ell_p$ (where $0 \leq p < 1$)
regularizers. This enables our approach to seamlessly incorporate various
classifiers and plausibility measures while producing sparser solutions. Our
algorithm only requires differentiable data-manifold regularizers and supports
box constraints for bounded feature ranges, ensuring the generated
counterfactuals remain \emph{actionable}. Finally, experiments on real-world
datasets demonstrate that our approach effectively produces sparse,
manifold-aligned counterfactual explanations while maintaining proximity to the
factual data and computational efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Direct Debiased Machine Learning via Bregman Divergence Minimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23534v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23534v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masahiro Kato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop a direct debiased machine learning framework comprising Neyman
targeted estimation and generalized Riesz regression. Our framework unifies
Riesz regression for automatic debiased machine learning, covariate balancing,
targeted maximum likelihood estimation (TMLE), and density-ratio estimation. In
many problems involving causal effects or structural models, the parameters of
interest depend on regression functions. Plugging regression functions
estimated by machine learning methods into the identifying equations can yield
poor performance because of first-stage bias. To reduce such bias, debiased
machine learning employs Neyman orthogonal estimating equations. Debiased
machine learning typically requires estimation of the Riesz representer and the
regression function. For this problem, we develop a direct debiased machine
learning framework with an end-to-end algorithm. We formulate estimation of the
nuisance parameters, the regression function and the Riesz representer, as
minimizing the discrepancy between Neyman orthogonal scores computed with known
and unknown nuisance parameters, which we refer to as Neyman targeted
estimation. Neyman targeted estimation includes Riesz representer estimation,
and we measure discrepancies using the Bregman divergence. The Bregman
divergence encompasses various loss functions as special cases, where the
squared loss yields Riesz regression and the Kullback-Leibler divergence yields
entropy balancing. We refer to this Riesz representer estimation as generalized
Riesz regression. Neyman targeted estimation also yields TMLE as a special case
for regression function estimation. Furthermore, for specific pairs of models
and Riesz representer estimation methods, we can automatically obtain the
covariate balancing property without explicitly solving the covariate balancing
objective.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adversarial generalization of unfolding (model-based) networks <span class="chip">NeurIPS2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.15370v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.15370v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vicky Kouni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unfolding networks are interpretable networks emerging from iterative
algorithms, incorporate prior knowledge of data structure, and are designed to
solve inverse problems like compressed sensing, which deals with recovering
data from noisy, missing observations. Compressed sensing finds applications in
critical domains, from medical imaging to cryptography, where adversarial
robustness is crucial to prevent catastrophic failures. However, a solid
theoretical understanding of the performance of unfolding networks in the
presence of adversarial attacks is still in its infancy. In this paper, we
study the adversarial generalization of unfolding networks when perturbed with
$l_2$-norm constrained attacks, generated by the fast gradient sign method.
Particularly, we choose a family of state-of-the-art overaparameterized
unfolding networks and deploy a new framework to estimate their adversarial
Rademacher complexity. Given this estimate, we provide adversarial
generalization error bounds for the networks under study, which are tight with
respect to the attack level. To our knowledge, this is the first theoretical
analysis on the adversarial generalization of unfolding networks. We further
present a series of experiments on real-world data, with results corroborating
our derived theory, consistently for all data. Finally, we observe that the
family's overparameterization can be exploited to promote adversarial
robustness, shedding light on how to efficiently robustify neural networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Advancing Local Clustering on Graphs via Compressive Sensing:
  Semi-supervised and Unsupervised Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.19419v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.19419v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaiming Shen, Sung Ha Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Local clustering aims to identify specific substructures within a large graph
without any additional structural information of the graph. These substructures
are typically small compared to the overall graph, enabling the problem to be
approached by finding a sparse solution to a linear system associated with the
graph Laplacian. In this work, we first propose a method for identifying
specific local clusters when very few labeled data are given, which we term
semi-supervised local clustering. We then extend this approach to the
unsupervised setting when no prior information on labels is available. The
proposed methods involve randomly sampling the graph, applying diffusion
through local cluster extraction, then examining the overlap among the results
to find each cluster. We establish the co-membership conditions for any pair of
nodes, and rigorously prove the correctness of our methods. Additionally, we
conduct extensive experiments to demonstrate that the proposed methods achieve
state of the art results in the low-label rates regime.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Partially-Supervised Neural Network Model For Quadratic Multiparametric
  Programming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05567v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05567v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fuat Can Beylunioglu, Mehrdad Pirnia, P. Robert Duimering
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Networks (NN) with ReLU activation functions are used to model
multiparametric quadratic optimization problems (mp-QP) in diverse engineering
applications. Researchers have suggested leveraging the piecewise affine
property of deep NN models to solve mp-QP with linear constraints, which also
exhibit piecewise affine behaviour. However, traditional deep NN applications
to mp-QP fall short of providing optimal and feasible predictions, even when
trained on large datasets. This study proposes a partially-supervised NN (PSNN)
architecture that directly represents the mathematical structure of the global
solution function. In contrast to generic NN training approaches, the proposed
PSNN method derives a large proportion of model weights directly from the
mathematical properties of the optimization problem, producing more accurate
solutions despite significantly smaller training data sets. Many energy
management problems are formulated as QP, so we apply the proposed approach to
energy systems (specifically DC optimal power flow) to demonstrate proof of
concept. Model performance in terms of solution accuracy and speed of
predictions was compared against a commercial solver and a generic Deep NN
model based on classical training. Results show KKT sufficient conditions for
PSNN consistently outperform generic NN architectures with classical training
using far less data, including when tested on extreme, out-of-training
distribution test data. Given its speed advantages over traditional solvers,
the PSNN model can quickly produce optimal and feasible solutions within a
second for millions of input parameters sampled from a distribution of
stochastic demands and renewable generator dispatches, which can be used for
simulations and long term planning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages including references and appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Integrating Protein Sequence and Expression Level to Analysis Molecular
  Characterization of Breast Cancer Subtypes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01755v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01755v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hossein Sholehrasa, Majid Jaberi-Douraki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Breast cancer's complexity and variability pose significant challenges in
understanding its progression and guiding effective treatment. This study aims
to integrate protein sequence data with expression levels to improve the
molecular characterization of breast cancer subtypes and predict clinical
outcomes. Using ProtGPT2, a language model specifically designed for protein
sequences, we generated embeddings that capture the functional and structural
properties of proteins. These embeddings were integrated with protein
expression levels to form enriched biological representations, which were
analyzed using machine learning methods, such as ensemble K-means for
clustering and XGBoost for classification. Our approach enabled the successful
clustering of patients into biologically distinct groups and accurately
predicted clinical outcomes such as survival and biomarker status, achieving
high performance metrics, notably an F1 score of 0.88 for survival and 0.87 for
biomarker status prediction. Feature importance analysis identified KMT2C,
CLASP2, and MYO1B as key proteins involved in hormone signaling, cytoskeletal
remodeling, and therapy resistance in hormone receptor-positive and
triple-negative breast cancer, with potential influence on breast cancer
subtype behavior and progression. Furthermore, protein-protein interaction
networks and correlation analyses revealed functional interdependencies among
proteins that may influence the behavior and progression of breast cancer
subtypes. These findings suggest that integrating protein sequence and
expression data provides valuable insights into tumor biology and has
significant potential to enhance personalized treatment strategies in breast
cancer care.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When Kernels Multiply, Clusters Unify: Fusing Embeddings with the
  Kronecker Product 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.08645v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.08645v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youqi Wu, Jingwei Zhang, Farzan Farnia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art embeddings often capture distinct yet complementary
discriminative features: For instance, one image embedding model may excel at
distinguishing fine-grained textures, while another focuses on object-level
structure. Motivated by this observation, we propose a principled approach to
fuse such complementary representations through kernel multiplication.
Multiplying the kernel similarity functions of two embeddings allows their
discriminative structures to interact, producing a fused representation whose
kernel encodes the union of the clusters identified by each parent embedding.
This formulation also provides a natural way to construct joint kernels for
paired multi-modal data (e.g., image-text tuples), where the product of
modality-specific kernels inherits structure from both domains. We highlight
that this kernel product is mathematically realized via the Kronecker product
of the embedding feature maps, yielding our proposed KrossFuse framework for
embedding fusion. To address the computational cost of the resulting
high-dimensional Kronecker space, we further develop RP-KrossFuse, a scalable
variant that leverages random projections for efficient approximation. As a key
application, we use this framework to bridge the performance gap between
cross-modal embeddings (e.g., CLIP, BLIP) and unimodal experts (e.g., DINOv2,
E5). Experiments show that RP-KrossFuse effectively integrates these models,
enhancing modality-specific performance while preserving cross-modal alignment.
The project code is available at https://github.com/yokiwuuu/KrossFuse.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RLBFF: Binary Flexible Feedback to bridge between Human Feedback &
  Verifiable Rewards 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.21319v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.21319v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhilin Wang, Jiaqi Zeng, Olivier Delalleau, Ellie Evans, Daniel Egert, Hoo-Chang Shin, Felipe Soares, Yi Dong, Oleksii Kuchaiev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning with Human Feedback (RLHF) and Reinforcement Learning
with Verifiable Rewards (RLVR) are the main RL paradigms used in LLM
post-training, each offering distinct advantages. However, RLHF struggles with
interpretability and reward hacking because it relies on human judgments that
usually lack explicit criteria, whereas RLVR is limited in scope by its focus
on correctness-based verifiers. We propose Reinforcement Learning with Binary
Flexible Feedback (RLBFF), which combines the versatility of human-driven
preferences with the precision of rule-based verification, enabling reward
models to capture nuanced aspects of response quality beyond mere correctness.
RLBFF extracts principles that can be answered in a binary fashion (e.g.
accuracy of information: yes, or code readability: no) from natural language
feedback. Such principles can then be used to ground Reward Model training as
an entailment task (response satisfies or does not satisfy an arbitrary
principle). We show that Reward Models trained in this manner can outperform
Bradley-Terry models when matched for data and achieve top performance on
RM-Bench (86.2%) and JudgeBench (81.4%, #1 on leaderboard as of September 24,
2025). Additionally, users can specify principles of interest at inference time
to customize the focus of our reward models, in contrast to Bradley-Terry
models. Finally, we present a fully open source recipe (including data) to
align Qwen3-32B using RLBFF and our Reward Model, to match or exceed the
performance of o3-mini and DeepSeek R1 on general alignment benchmarks of
MT-Bench, WildBench, and Arena Hard v2 (at <5% of the inference cost). Models:
https://huggingface.co/collections/nvidia/reward-models-10-2025
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Added link to access models:
  https://huggingface.co/collections/nvidia/reward-models-10-2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Curriculum Abductive Learning <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12275v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12275v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wen-Chao Hu, Qi-Jie Li, Lin-Han Jia, Cunjing Ge, Yu-Feng Li, Yuan Jiang, Zhi-Hua Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Abductive Learning (ABL) integrates machine learning with logical reasoning
in a loop: a learning model predicts symbolic concept labels from raw inputs,
which are revised through abduction using domain knowledge and then fed back
for retraining. However, due to the nondeterminism of abduction, the training
process often suffers from instability, especially when the knowledge base is
large and complex, resulting in a prohibitively large abduction space. While
prior works focus on improving candidate selection within this space, they
typically treat the knowledge base as a static black box. In this work, we
propose Curriculum Abductive Learning (C-ABL), a method that explicitly
leverages the internal structure of the knowledge base to address the ABL
training challenges. C-ABL partitions the knowledge base into a sequence of
sub-bases, progressively introduced during training. This reduces the abduction
space throughout training and enables the model to incorporate logic in a
stepwise, smooth way. Experiments across multiple tasks show that C-ABL
outperforms previous ABL implementations, significantly improves training
stability, convergence speed, and final accuracy, especially under complex
knowledge setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2025, 22 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Guided Model Merging for Hybrid Data Learning: Leveraging Centralized
  Data to Refine Decentralized Models <span class="chip">WACV 2026</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20138v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20138v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyi Zhu, Ruicong Yao, Taha Ceritli, Savas Ozkan, Matthew B. Blaschko, Eunchung Noh, Jeongwon Min, Cho Jung Min, Mete Ozay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current network training paradigms primarily focus on either centralized or
decentralized data regimes. However, in practice, data availability often
exhibits a hybrid nature, where both regimes coexist. This hybrid setting
presents new opportunities for model training, as the two regimes offer
complementary trade-offs: decentralized data is abundant but subject to
heterogeneity and communication constraints, while centralized data, though
limited in volume and potentially unrepresentative, enables better curation and
high-throughput access. Despite its potential, effectively combining these
paradigms remains challenging, and few frameworks are tailored to hybrid data
regimes. To address this, we propose a novel framework that constructs a model
atlas from decentralized models and leverages centralized data to refine a
global model within this structured space. The refined model is then used to
reinitialize the decentralized models. Our method synergizes federated learning
(to exploit decentralized data) and model merging (to utilize centralized
data), enabling effective training under hybrid data availability.
Theoretically, we show that our approach achieves faster convergence than
methods relying solely on decentralized data, due to variance reduction in the
merging process. Extensive experiments demonstrate that our framework
consistently outperforms purely centralized, purely decentralized, and existing
hybrid-adaptable methods. Notably, our method remains robust even when the
centralized and decentralized data domains differ or when decentralized data
contains noise, significantly broadening its applicability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at WACV 2026</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GSE: Group-wise Sparse and Explainable Adversarial Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.17434v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.17434v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shpresim Sadiku, Moritz Wagner, Sebastian Pokutta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse adversarial attacks fool deep neural networks (DNNs) through minimal
pixel perturbations, often regularized by the $\ell_0$ norm. Recent efforts
have replaced this norm with a structural sparsity regularizer, such as the
nuclear group norm, to craft group-wise sparse adversarial attacks. The
resulting perturbations are thus explainable and hold significant practical
relevance, shedding light on an even greater vulnerability of DNNs. However,
crafting such attacks poses an optimization challenge, as it involves computing
norms for groups of pixels within a non-convex objective. We address this by
presenting a two-phase algorithm that generates group-wise sparse attacks
within semantically meaningful areas of an image. Initially, we optimize a
quasinorm adversarial loss using the $1/2-$quasinorm proximal operator tailored
for non-convex programming. Subsequently, the algorithm transitions to a
projected Nesterov's accelerated gradient descent with $2-$norm regularization
applied to perturbation magnitudes. Rigorous evaluations on CIFAR-10 and
ImageNet datasets demonstrate a remarkable increase in group-wise sparsity,
e.g., $50.9\%$ on CIFAR-10 and $38.4\%$ on ImageNet (average case, targeted
attack). This performance improvement is accompanied by significantly faster
computation times, improved explainability, and a $100\%$ attack success rate.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding Generalization in Node and Link Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.00927v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.00927v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonis Vasileiou, Timo Stoll, Christopher Morris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Using message-passing graph neural networks (MPNNs) for node and link
prediction is crucial in various scientific and industrial domains, which has
led to the development of diverse MPNN architectures. Besides working well in
practical settings, their ability to generalize beyond the training set remains
poorly understood. While some studies have explored MPNNs' generalization in
graph-level prediction tasks, much less attention has been given to node- and
link-level predictions. Existing works often rely on unrealistic i.i.d.\@
assumptions, overlooking possible correlations between nodes or links, and
assuming fixed aggregation and impractical loss functions while neglecting the
influence of graph structure. In this work, we introduce a unified framework to
analyze the generalization properties of MPNNs in inductive and transductive
node and link prediction settings, incorporating diverse architectural
parameters and loss functions and quantifying the influence of graph structure.
Additionally, our proposed generalization framework can be applied beyond
graphs to any classification task under the inductive or transductive setting.
Our empirical study supports our theoretical insights, deepening our
understanding of MPNNs' generalization capabilities in these tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2412.07106</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RelP: Faithful and Efficient Circuit Discovery in Language Models via
  Relevance Patching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.21258v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.21258v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Farnoush Rezaei Jafari, Oliver Eberle, Ashkan Khakzar, Neel Nanda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Activation patching is a standard method in mechanistic interpretability for
localizing the components of a model responsible for specific behaviors, but it
is computationally expensive to apply at scale. Attribution patching offers a
faster, gradient-based approximation, yet suffers from noise and reduced
reliability in deep, highly non-linear networks. In this work, we introduce
Relevance Patching (RelP), which replaces the local gradients in attribution
patching with propagation coefficients derived from Layer-wise Relevance
Propagation (LRP). LRP propagates the network's output backward through the
layers, redistributing relevance to lower-level components according to local
propagation rules that ensure properties such as relevance conservation or
improved signal-to-noise ratio. Like attribution patching, RelP requires only
two forward passes and one backward pass, maintaining computational efficiency
while improving faithfulness. We validate RelP across a range of models and
tasks, showing that it more accurately approximates activation patching than
standard attribution patching, particularly when analyzing residual stream and
MLP outputs in the Indirect Object Identification (IOI) task. For instance, for
MLP outputs in GPT-2 Large, attribution patching achieves a Pearson correlation
of 0.006, whereas RelP reaches 0.956, highlighting the improvement offered by
RelP. Additionally, we compare the faithfulness of sparse feature circuits
identified by RelP and Integrated Gradients (IG), showing that RelP achieves
comparable faithfulness without the extra computational cost associated with
IG.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PepCompass: Navigating peptide embedding spaces using Riemannian
  Geometry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.01988v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.01988v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcin Możejko, Adam Bielecki, Jurand Prądzyński, Marcin Traskowski, Antoni Janowski, Hyun-Su Lee, Marcelo Der Torossian Torres, Michał Kmicikiewicz, Paulina Szymczak, Karol Jurasz, Michał Kucharczyk, Cesar de la Fuente-Nunez, Ewa Szczurek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Antimicrobial peptide discovery is challenged by the astronomical size of
peptide space and the relative scarcity of active peptides. Generative models
provide continuous latent "maps" of peptide space, but conventionally ignore
decoder-induced geometry and rely on flat Euclidean metrics, rendering
exploration and optimization distorted and inefficient. Prior manifold-based
remedies assume fixed intrinsic dimensionality, which critically fails in
practice for peptide data. Here, we introduce PepCompass, a geometry-aware
framework for peptide exploration and optimization. At its core, we define a
Union of $\kappa$-Stable Riemannian Manifolds $\mathbb{M}^{\kappa}$, a family
of decoder-induced manifolds that captures local geometry while ensuring
computational stability. We propose two local exploration methods: Second-Order
Riemannian Brownian Efficient Sampling, which provides a convergent
second-order approximation to Riemannian Brownian motion, and Mutation
Enumeration in Tangent Space, which reinterprets tangent directions as discrete
amino-acid substitutions. Combining these yields Local Enumeration Bayesian
Optimization (LE-BO), an efficient algorithm for local activity optimization.
Finally, we introduce Potential-minimizing Geodesic Search (PoGS), which
interpolates between prototype embeddings along property-enriched geodesics,
biasing discovery toward seeds, i.e. peptides with favorable activity. In-vitro
validation confirms the effectiveness of PepCompass: PoGS yields four novel
seeds, and subsequent optimization with LE-BO discovers 25 highly active
peptides with broad-spectrum activity, including against resistant bacterial
strains. These results demonstrate that geometry-informed exploration provides
a powerful new paradigm for antimicrobial peptide design.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ C-LoRA: Contextual Low-Rank Adaptation for Uncertainty Estimation in
  Large Language Models <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.17773v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.17773v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir Hossein Rahmati, Sanket Jantre, Weifeng Zhang, Yucheng Wang, Byung-Jun Yoon, Nathan M. Urban, Xiaoning Qian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-Rank Adaptation (LoRA) offers a cost-effective solution for fine-tuning
large language models (LLMs), but it often produces overconfident predictions
in data-scarce few-shot settings. To address this issue, several classical
statistical learning approaches have been repurposed for scalable
uncertainty-aware LoRA fine-tuning. However, these approaches neglect how input
characteristics affect the predictive uncertainty estimates. To address this
limitation, we propose Contextual Low-Rank Adaptation (C-LoRA) as a novel
uncertainty-aware and parameter efficient fine-tuning approach, by developing
new lightweight LoRA modules contextualized to each input data sample to
dynamically adapt uncertainty estimates. Incorporating data-driven contexts
into the parameter posteriors, C-LoRA mitigates overfitting, achieves
well-calibrated uncertainties, and yields robust predictions. Extensive
experiments on LLaMA2-7B models demonstrate that C-LoRA consistently
outperforms the state-of-the-art uncertainty-aware LoRA methods in both
uncertainty quantification and model generalization. Ablation studies further
confirm the critical role of our contextual modules in capturing
sample-specific uncertainties. C-LoRA sets a new standard for robust,
uncertainty-aware LLM fine-tuning in few-shot regimes. Although our experiments
are limited to 7B models, our method is architecture-agnostic and, in
principle, applies beyond this scale; studying its scaling to larger models
remains an open problem. Our code is available at
https://github.com/ahra99/c_lora.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Conference on Neural Information Processing Systems (NeurIPS 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MaskCaptioner: Learning to Jointly Segment and Caption Object
  Trajectories in Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.14904v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.14904v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriel Fiastre, Antoine Yang, Cordelia Schmid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dense Video Object Captioning (DVOC) is the task of jointly detecting,
tracking, and captioning object trajectories in a video, requiring the ability
to understand spatio-temporal details and describe them in natural language.
Due to the complexity of the task and the high cost associated with manual
annotation, previous approaches resort to disjoint training strategies,
potentially leading to suboptimal performance. To circumvent this issue, we
propose to generate captions about spatio-temporally localized entities
leveraging a state-of-the-art VLM. By extending the LVIS and LV-VIS datasets
with our synthetic captions (LVISCap and LV-VISCap), we train MaskCaptioner, an
end-to-end model capable of jointly detecting, segmenting, tracking and
captioning object trajectories. Moreover, with pretraining on LVISCap and
LV-VISCap, MaskCaptioner achieves state-of-the-art DVOC results on three
existing benchmarks, VidSTG, VLN and BenSMOT. The datasets and code are
available at https://www.gabriel.fiastre.fr/maskcaptioner/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LatentBreak: Jailbreaking Large Language Models through Latent Space
  Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.08604v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.08604v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raffaele Mura, Giorgio Piras, Kamilė Lukošiūtė, Maura Pintor, Amin Karbasi, Battista Biggio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Jailbreaks are adversarial attacks designed to bypass the built-in safety
mechanisms of large language models. Automated jailbreaks typically optimize an
adversarial suffix or adapt long prompt templates by forcing the model to
generate the initial part of a restricted or harmful response. In this work, we
show that existing jailbreak attacks that leverage such mechanisms to unlock
the model response can be detected by a straightforward perplexity-based
filtering on the input prompt. To overcome this issue, we propose LatentBreak,
a white-box jailbreak attack that generates natural adversarial prompts with
low perplexity capable of evading such defenses. LatentBreak substitutes words
in the input prompt with semantically-equivalent ones, preserving the initial
intent of the prompt, instead of adding high-perplexity adversarial suffixes or
long templates. These words are chosen by minimizing the distance in the latent
space between the representation of the adversarial prompt and that of harmless
requests. Our extensive evaluation shows that LatentBreak leads to shorter and
low-perplexity prompts, thus outperforming competing jailbreak algorithms
against perplexity-based filters on multiple safety-aligned models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SignalLLM: A General-Purpose LLM Agent Framework for Automated Signal
  Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.17197v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.17197v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junlong Ke, Qiying Hu, Shenghai Yuan, Yuecong Xu, Jianfei Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern signal processing (SP) pipelines, whether model-based or data-driven,
often constrained by complex and fragmented workflow, rely heavily on expert
knowledge and manual engineering, and struggle with adaptability and
generalization under limited data. In contrast, Large Language Models (LLMs)
offer strong reasoning capabilities, broad general-purpose knowledge,
in-context learning, and cross-modal transfer abilities, positioning them as
powerful tools for automating and generalizing SP workflows. Motivated by these
potentials, we introduce SignalLLM, the first general-purpose LLM-based agent
framework for general SP tasks. Unlike prior LLM-based SP approaches that are
limited to narrow applications or tricky prompting, SignalLLM introduces a
principled, modular architecture. It decomposes high-level SP goals into
structured subtasks via in-context learning and domain-specific retrieval,
followed by hierarchical planning through adaptive retrieval-augmented
generation (RAG) and refinement; these subtasks are then executed through
prompt-based reasoning, cross-modal reasoning, code synthesis, model
invocation, or data-driven LLM-assisted modeling. Its generalizable design
enables the flexible selection of problem solving strategies across different
signal modalities, task types, and data conditions. We demonstrate the
versatility and effectiveness of SignalLLM through five representative tasks in
communication and sensing, such as radar target detection, human activity
recognition, and text compression. Experimental results show superior
performance over traditional and existing LLM-based methods, particularly in
few-shot and zero-shot settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differentiation Through Black-Box Quadratic Programming Solvers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06324v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06324v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Connor W. Magoon, Fengyu Yang, Noam Aigerman, Shahar Z. Kovalsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Differentiable optimization has attracted significant research interest,
particularly for quadratic programming (QP). Existing approaches for
differentiating the solution of a QP with respect to its defining parameters
often rely on specific integrated solvers. This integration limits their
applicability, including their use in neural network architectures and bi-level
optimization tasks, restricting users to a narrow selection of solver choices.
To address this limitation, we introduce dQP, a modular and solver-agnostic
framework for plug-and-play differentiation of virtually any QP solver. A key
insight we leverage to achieve modularity is that, once the active set of
inequality constraints is known, both the solution and its derivative can be
expressed using simplified linear systems that share the same matrix. This
formulation fully decouples the computation of the QP solution from its
differentiation. Building on this result, we provide a minimal-overhead,
open-source implementation ( https://github.com/cwmagoon/dQP ) that seamlessly
integrates with over 15 state-of-the-art solvers. Comprehensive benchmark
experiments demonstrate dQP's robustness and scalability, particularly
highlighting its advantages in large-scale sparse problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Smart Exploration in Reinforcement Learning using Bounded Uncertainty
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.05978v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.05978v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        J. S. van Hulst, W. P. M. H. Heemels, D. J. Antunes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) is a powerful framework for decision-making in
uncertain environments, but it often requires large amounts of data to learn an
optimal policy. We address this challenge by incorporating prior model
knowledge to guide exploration and accelerate the learning process.
Specifically, we assume access to a model set that contains the true transition
kernel and reward function. We optimize over this model set to obtain upper and
lower bounds on the Q-function, which are then used to guide the exploration of
the agent. We provide theoretical guarantees on the convergence of the
Q-function to the optimal Q-function under the proposed class of exploring
policies. Furthermore, we also introduce a data-driven regularized version of
the model set optimization problem that ensures the convergence of the class of
exploring policies to the optimal policy. Lastly, we show that when the model
set has a specific structure, namely the bounded-parameter MDP (BMDP)
framework, the regularized model set optimization problem becomes convex and
simple to implement. In this setting, we also prove finite-time convergence to
the optimal policy under mild assumptions. We demonstrate the effectiveness of
the proposed exploration strategy, which we call BUMEX (Bounded Uncertainty
Model-based Exploration), in a simulation study. The results indicate that the
proposed method can significantly accelerate learning in benchmark examples. A
toolbox is available at https://github.com/JvHulst/BUMEX.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for Presentation at 64th IEEE Conference on Decision and
  Control, CDC 2025, Rio de Janeiro, Brazil, 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Epistemic Diversity and Knowledge Collapse in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.04226v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.04226v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dustin Wright, Sarah Masud, Jared Moore, Srishti Yadav, Maria Antoniak, Chan Young Park, Isabelle Augenstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) tend to generate lexically, semantically, and
stylistically homogenous texts. This poses a risk of knowledge collapse, where
homogenous LLMs mediate a shrinking in the range of accessible information over
time. Existing works on homogenization are limited by a focus on closed-ended
multiple-choice setups or fuzzy semantic features, and do not look at trends
across time and cultural contexts. To overcome this, we present a new
methodology to measure epistemic diversity, i.e., variation in real-world
claims in LLM outputs, which we use to perform a broad empirical study of LLM
knowledge collapse. We test 27 LLMs, 155 topics covering 12 countries, and 200
prompt variations sourced from real user chats. For the topics in our study, we
show that while newer models tend to generate more diverse claims, nearly all
models are less epistemically diverse than a basic web search. We find that
model size has a negative impact on epistemic diversity, while
retrieval-augmented generation (RAG) has a positive impact, though the
improvement from RAG varies by the cultural context. Finally, compared to a
traditional knowledge source (Wikipedia), we find that country-specific claims
reflect the English language more than the local one, highlighting a gap in
epistemic representation
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages; 8 figures, 4 tables; v2 changelog: Fixed the modeling for
  table 3, random effect is the model version; v3 changelog: Fixed minor
  formatting issues in tables 2 and 3; v4 changelog: Fixed some typos and model
  description</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Incentivizing LLMs to Self-Verify Their Answers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.01369v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.01369v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fuxiang Zhang, Jiacheng Xu, Chaojie Wang, Ce Cui, Yang Liu, Bo An
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated remarkable progress in complex
reasoning tasks through both post-training and test-time scaling laws. While
prevalent test-time scaling approaches are often realized by using external
reward models to guide the model generation process, we find that only marginal
gains can be acquired when scaling a model post-trained on specific reasoning
tasks. We identify that the limited improvement stems from distribution
discrepancies between the specific post-trained generator and the general
reward model. To address this, we propose a framework that incentivizes LLMs to
self-verify their own answers. By unifying answer generation and verification
within a single reinforcement learning (RL) process, we train models that can
effectively assess the correctness of their own solutions. The trained model
can further scale its performance at inference time by verifying its
generations, without the need for external verifiers. We train our
self-verification models based on Qwen2.5-Math-7B and
DeepSeek-R1-Distill-Qwen-1.5B, demonstrating their capabilities across varying
reasoning context lengths. Experiments on multiple mathematical reasoning
benchmarks show that our models can not only improve post-training performance
but also enable effective test-time scaling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Human-Like Goalkeeping in a Realistic Football Simulation: a
  Sample-Efficient Reinforcement Learning Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23216v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23216v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessandro Sestini, Joakim Bergdahl, Jean-Philippe Barrette-LaPierre, Florian Fuchs, Brady Chen, Michael Jones, Linus Gisslén
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While several high profile video games have served as testbeds for Deep
Reinforcement Learning (DRL), this technique has rarely been employed by the
game industry for crafting authentic AI behaviors. Previous research focuses on
training super-human agents with large models, which is impractical for game
studios with limited resources aiming for human-like agents. This paper
proposes a sample-efficient DRL method tailored for training and fine-tuning
agents in industrial settings such as the video game industry. Our method
improves sample efficiency of value-based DRL by leveraging pre-collected data
and increasing network plasticity. We evaluate our method training a goalkeeper
agent in EA SPORTS FC 25, one of the best-selling football simulations today.
Our agent outperforms the game's built-in AI by 10% in ball saving rate.
Ablation studies show that our method trains agents 50% faster compared to
standard DRL methods. Finally, qualitative evaluation from domain experts
indicates that our approach creates more human-like gameplay compared to
hand-crafted agents. As a testament to the impact of the approach, the method
has been adopted for use in the most recent release of the series.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A mathematical certification for positivity conditions in Neural
  Networks with applications to partial monotonicity and Trustworthy AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.08525v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.08525v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alejandro Polo-Molina, David Alfaya, Jose Portela
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial Neural Networks (ANNs) have become a powerful tool for modeling
complex relationships in large-scale datasets. However, their black-box nature
poses trustworthiness challenges. In certain situations, ensuring trust in
predictions might require following specific partial monotonicity constraints.
However, certifying if an already-trained ANN is partially monotonic is
challenging. Therefore, ANNs are often disregarded in some critical
applications, such as credit scoring, where partial monotonicity is required.
To address this challenge, this paper presents a novel algorithm (LipVor) that
certifies if a black-box model, such as an ANN, is positive based on a finite
number of evaluations. Consequently, since partial monotonicity can be
expressed as a positivity condition on partial derivatives, LipVor can certify
whether an ANN is partially monotonic. To do so, for every positively evaluated
point, the Lipschitzianity of the black-box model is used to construct a
specific neighborhood where the function remains positive. Next, based on the
Voronoi diagram of the evaluated points, a sufficient condition is stated to
certify if the function is positive in the domain. Unlike prior methods, our
approach certifies partial monotonicity without constrained architectures or
piece-wise linear activations. Therefore, LipVor could open up the possibility
of using unconstrained ANN in some critical fields. Moreover, some other
properties of an ANN, such as convexity, can be posed as positivity conditions,
and therefore, LipVor could also be applied.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GFlowNets for Learning Better Drug-Drug Interaction Representations <span class="chip">ICANN 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.06576v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.06576v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Azmine Toushik Wasi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Drug-drug interactions pose a significant challenge in clinical pharmacology,
with severe class imbalance among interaction types limiting the effectiveness
of predictive models. Common interactions dominate datasets, while rare but
critical interactions remain underrepresented, leading to poor model
performance on infrequent cases. Existing methods often treat DDI prediction as
a binary problem, ignoring class-specific nuances and exacerbating bias toward
frequent interactions. To address this, we propose a framework combining
Generative Flow Networks (GFlowNet) with Variational Graph Autoencoders (VGAE)
to generate synthetic samples for rare classes, improving model balance and
generate effective and novel DDI pairs. Our approach enhances predictive
performance across interaction types, ensuring better clinical reliability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICANN 2025:AIDD and NeurIPS 2025 Workshop on Structured
  Probabilistic Inference & Generative Modeling
  (https://openreview.net/forum?id=LZW1jSgfCI)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring the Early Universe with Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.22018v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.22018v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emmanuel de Salis, Massimo De Santis, Davide Piras, Sambit K. Giri, Michele Bianco, Nicolas Cerardi, Philipp Denzel, Merve Selcuk-Simsek, Kelley M. Hess, M. Carmen Toribio, Franz Kirsten, Hatem Ghorbel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hydrogen is the most abundant element in our Universe. The first generation
of stars and galaxies produced photons that ionized hydrogen gas, driving a
cosmological event known as the Epoch of Reionization (EoR). The upcoming
Square Kilometre Array Observatory (SKAO) will map the distribution of neutral
hydrogen during this era, aiding in the study of the properties of these
first-generation objects. Extracting astrophysical information will be
challenging, as SKAO will produce a tremendous amount of data where the
hydrogen signal will be contaminated with undesired foreground contamination
and instrumental systematics. To address this, we develop the latest deep
learning techniques to extract information from the 2D power spectra of the
hydrogen signal expected from SKAO. We apply a series of neural network models
to these measurements and quantify their ability to predict the history of
cosmic hydrogen reionization, which is connected to the increasing number and
efficiency of early photon sources. We show that the study of the early
Universe benefits from modern deep learning technology. In particular, we
demonstrate that dedicated machine learning algorithms can achieve more than a
$0.95$ $R^2$ score on average in recovering the reionization history. This
enables accurate and precise cosmological and astrophysical inference of
structure formation in the early Universe.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EPIA 2025 preprint version, 12 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking Optimal Verification Granularity for Compute-Efficient
  Test-Time Scaling <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.11730v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.11730v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Mark Chen, Guanxi Lu, Yasuyuki Okoshi, Zhiwen Mo, Masato Motomura, Hongxiang Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Test-time scaling (TTS) has proven effective in enhancing the reasoning
capabilities of large language models (LLMs). Verification plays a key role in
TTS, simultaneously influencing (1) reasoning performance and (2) compute
efficiency, due to the quality and computational cost of verification. In this
work, we challenge the conventional paradigms of verification, and make the
first attempt toward systematically investigating the impact of verification
granularity-that is, how frequently the verifier is invoked during generation,
beyond verifying only the final output or individual generation steps. To this
end, we introduce Variable Granularity Search (VG-Search), a unified algorithm
that generalizes beam search and Best-of-N sampling via a tunable granularity
parameter g. Extensive experiments with VG-Search under varying compute
budgets, generator-verifier configurations, and task attributes reveal that
dynamically selecting g can improve the compute efficiency and scaling
behavior. Building on these findings, we propose adaptive VG-Search strategies
that achieve accuracy gains of up to 3.1\% over Beam Search and 3.6\% over
Best-of-N, while reducing FLOPs by over 52\%. We will open-source the code to
support future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A geometric framework for momentum-based optimizers for low-rank
  training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.17475v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.17475v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Steffen Schotthöfer, Timon Klein, Jonas Kusch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-rank pre-training and fine-tuning have recently emerged as promising
techniques for reducing the computational and storage costs of large neural
networks. Training low-rank parameterizations typically relies on conventional
optimizers such as heavy ball momentum methods or Adam. In this work, we
identify and analyze potential difficulties that these training methods
encounter when used to train low-rank parameterizations of weights. In
particular, we show that classical momentum methods can struggle to converge to
a local optimum due to the geometry of the underlying optimization landscape.
To address this, we introduce novel training strategies derived from dynamical
low-rank approximation, which explicitly account for the underlying geometric
structure. Our approach leverages and combines tools from dynamical low-rank
approximation and momentum-based optimization to design optimizers that respect
the intrinsic geometry of the parameter space. We validate our methods through
numerical experiments, demonstrating faster convergence, and stronger
validation metrics at given parameter budgets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM-Driven Treatment Effect Estimation Under Inference Time Text
  Confounding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.02843v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.02843v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchen Ma, Dennis Frauen, Jonas Schweisthal, Stefan Feuerriegel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating treatment effects is crucial for personalized decision-making in
medicine, but this task faces unique challenges in clinical practice. At
training time, models for estimating treatment effects are typically trained on
well-structured medical datasets that contain detailed patient information.
However, at inference time, predictions are often made using textual
descriptions (e.g., descriptions with self-reported symptoms), which are
incomplete representations of the original patient information. In this work,
we make three contributions. (1) We show that the discrepancy between the data
available during training time and inference time can lead to biased estimates
of treatment effects. We formalize this issue as an inference time text
confounding problem, where confounders are fully observed during training time
but only partially available through text at inference time. (2) To address
this problem, we propose a novel framework for estimating treatment effects
that explicitly accounts for inference time text confounding. Our framework
leverages large language models together with a custom doubly robust learner to
mitigate biases caused by the inference time text confounding. (3) Through a
series of experiments, we demonstrate the effectiveness of our framework in
real-world applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MedAgentBoard: Benchmarking Multi-Agent Collaboration with Conventional
  Methods for Diverse Medical Tasks <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12371v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12371v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinghao Zhu, Ziyi He, Haoran Hu, Xiaochen Zheng, Xichen Zhang, Zixiang Wang, Junyi Gao, Liantao Ma, Lequan Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of Large Language Models (LLMs) has stimulated interest
in multi-agent collaboration for addressing complex medical tasks. However, the
practical advantages of multi-agent collaboration approaches remain
insufficiently understood. Existing evaluations often lack generalizability,
failing to cover diverse tasks reflective of real-world clinical practice, and
frequently omit rigorous comparisons against both single-LLM-based and
established conventional methods. To address this critical gap, we introduce
MedAgentBoard, a comprehensive benchmark for the systematic evaluation of
multi-agent collaboration, single-LLM, and conventional approaches.
MedAgentBoard encompasses four diverse medical task categories: (1) medical
(visual) question answering, (2) lay summary generation, (3) structured
Electronic Health Record (EHR) predictive modeling, and (4) clinical workflow
automation, across text, medical images, and structured EHR data. Our extensive
experiments reveal a nuanced landscape: while multi-agent collaboration
demonstrates benefits in specific scenarios, such as enhancing task
completeness in clinical workflow automation, it does not consistently
outperform advanced single LLMs (e.g., in textual medical QA) or, critically,
specialized conventional methods that generally maintain better performance in
tasks like medical VQA and EHR-based prediction. MedAgentBoard offers a vital
resource and actionable insights, emphasizing the necessity of a task-specific,
evidence-based approach to selecting and developing AI solutions in medicine.
It underscores that the inherent complexity and overhead of multi-agent
collaboration must be carefully weighed against tangible performance gains. All
code, datasets, detailed prompts, and experimental results are open-sourced at
https://medagentboard.netlify.app/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2025 Datasets & Benchmarks Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distributed optimization: designed for federated learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.08606v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.08606v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenyou Guo, Ting Qu, Chunrong Pan, George Q. Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL), as a distributed collaborative machine learning (ML)
framework under privacy-preserving constraints, has garnered increasing
research attention in cross-organizational data collaboration scenarios. This
paper proposes a class of distributed optimization algorithms based on the
augmented Lagrangian technique, designed to accommodate diverse communication
topologies in both centralized and decentralized FL settings. Furthermore, we
develop multiple termination criteria and parameter update mechanisms to
enhance computational efficiency, accompanied by rigorous theoretical
guarantees of convergence. By generalizing the augmented Lagrangian relaxation
through the incorporation of proximal relaxation and quadratic approximation,
our framework systematically recovers a broad of classical unconstrained
optimization methods, including proximal algorithm, classic gradient descent,
and stochastic gradient descent, among others. Notably, the convergence
properties of these methods can be naturally derived within the proposed
theoretical framework. Numerical experiments demonstrate that the proposed
algorithm exhibits strong performance in large-scale settings with significant
statistical heterogeneity across clients.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Impact of Performative Risk Minimization for Binary Random
  Variables <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.02331v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.02331v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikita Tsoy, Ivan Kirev, Negin Rahimiyazdi, Nikola Konstantinov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Performativity, the phenomenon where outcomes are influenced by predictions,
is particularly prevalent in social contexts where individuals strategically
respond to a deployed model. In order to preserve the high accuracy of machine
learning models under distribution shifts caused by performativity, Perdomo et
al. (2020) introduced the concept of performative risk minimization (PRM).
While this framework ensures model accuracy, it overlooks the impact of the PRM
on the underlying distributions and the predictions of the model. In this
paper, we initiate the analysis of the impact of PRM, by studying
performativity for a sequential performative risk minimization problem with
binary random variables and linear performative shifts. We formulate two
natural measures of impact. In the case of full information, where the
distribution dynamics are known, we derive explicit formulas for the PRM
solution and our impact measures. In the case of partial information, we
provide performative-aware statistical estimators, as well as simulations. Our
analysis contrasts PRM to alternatives that do not model data shift and
indicates that PRM can have amplified side effects compared to such methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2025 camera-ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Machine-learning competition to grade EEG background patterns in
  newborns with hypoxic-ischaemic encephalopathy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.09695v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.09695v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabio Magarelli, Geraldine B. Boylan, Saeed Montazeri, Feargal O'Sullivan, Dominic Lightbody, Minoo Ashoori, Tamara Skoric, John M. O'Toole
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning (ML) has the potential to support and improve expert
performance in monitoring the brain function of at-risk newborns. Developing
accurate and reliable ML models depends on access to high-quality, annotated
data, a resource in short supply. ML competitions address this need by
providing researchers access to expertly annotated datasets, fostering shared
learning through direct model comparisons, and leveraging the benefits of
crowdsourcing diverse expertise. We compiled a retrospective dataset containing
353 hours of EEG from 102 individual newborns from a multi-centre study. The
data was fully anonymised and divided into training, testing, and held-out
validation datasets. EEGs were graded for the severity of abnormal background
patterns. Next, we created a web-based competition platform and hosted a
machine learning competition to develop ML models for classifying the severity
of EEG background patterns in newborns. After the competition closed, the top 4
performing models were evaluated offline on a separate held-out validation
dataset. Although a feature-based model ranked first on the testing dataset,
deep learning models generalised better on the validation sets. All methods had
a significant decline in validation performance compared to the testing
performance. This highlights the challenges for model generalisation on unseen
data, emphasising the need for held-out validation datasets in ML studies with
neonatal EEG. The study underscores the importance of training ML models on
large and diverse datasets to ensure robust generalisation. The competition's
outcome demonstrates the potential for open-access data and collaborative ML
development to foster a collaborative research environment and expedite the
development of clinical decision-support tools for neonatal neuromonitoring.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, supplementary materials: "supplementary materials ML
  Comp.docx"</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Wisdom and Delusion of LLM Ensembles for Code Generation and Repair 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.21513v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.21513v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fernando Vallecillos-Ruiz, Max Hort, Leon Moonen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Today's pursuit of a single Large Language Model (LMM) for all software
engineering tasks is resource-intensive and overlooks the potential benefits of
complementarity, where different models contribute unique strengths. However,
the degree to which coding LLMs complement each other and the best strategy for
maximizing an ensemble's potential are unclear, leaving practitioners without a
clear path to move beyond single-model systems. To address this gap, we
empirically compare ten individual LLMs from five families, and three ensembles
of these LLMs across three software engineering benchmarks covering code
generation and program repair. We assess the complementarity between models and
the performance gap between the best individual model and the ensembles. Next,
we evaluate various selection heuristics to identify correct solutions from an
ensemble's candidate pool. We find that the theoretical upperbound for an
ensemble's performance can be 83% above the best single model. Our results show
that consensus-based strategies for selecting solutions fall into a "popularity
trap," amplifying common but incorrect outputs. In contrast, a diversity-based
strategy realizes up to 95% of this theoretical potential, and proves effective
even in small two-model ensembles, enabling a cost-efficient way to enhance
performance by leveraging multiple LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Added Acknowledgments section and hyphenated last names</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cybersecurity threat detection based on a UEBA framework using Deep
  Autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.11542v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.11542v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jose Fuentes, Ines Ortega-Fernandez, Nora M. Villanueva, Marta Sestelo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  User and Entity Behaviour Analytics (UEBA) is a broad branch of data
analytics that attempts to build a normal behavioural profile in order to
detect anomalous events. Among the techniques used to detect anomalies, Deep
Autoencoders constitute one of the most promising deep learning models on UEBA
tasks, allowing explainable detection of security incidents that could lead to
the leak of personal data, hijacking of systems, or access to sensitive
business information. In this study, we introduce the first implementation of
an explainable UEBA-based anomaly detection framework that leverages Deep
Autoencoders in combination with Doc2Vec to process both numerical and textual
features. Additionally, based on the theoretical foundations of neural
networks, we offer a novel proof demonstrating the equivalence of two widely
used definitions for fully-connected neural networks. The experimental results
demonstrate the proposed framework capability to detect real and synthetic
anomalies effectively generated from real attack data, showing that the models
provide not only correct identification of anomalies but also explainable
results that enable the reconstruction of the possible origin of the anomaly.
Our findings suggest that the proposed UEBA framework can be seamlessly
integrated into enterprise environments, complementing existing security
systems for explainable threat detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in AIMS Mathematics (2025), 10(10): 23496-23517. DOI:
  10.3934/math.20251043</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Monopoly Deal: A Benchmark Environment for Bounded One-Sided Response
  Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25080v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25080v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Will Wolf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Card games are widely used to study sequential decision-making under
uncertainty, with real-world analogues in negotiation, finance, and
cybersecurity. These games typically fall into three categories based on the
flow of control: strictly sequential (players alternate single actions),
deterministic response (some actions trigger a fixed outcome), and unbounded
reciprocal response (alternating counterplays are permitted). A less-explored
but strategically rich structure is the bounded one-sided response, where a
player's action briefly transfers control to the opponent, who must satisfy a
fixed condition through one or more moves before the turn resolves. We term
games featuring this mechanism Bounded One-Sided Response Games (BORGs). We
introduce a modified version of Monopoly Deal as a benchmark environment that
isolates this dynamic, where a Rent action forces the opponent to choose
payment assets. The gold-standard algorithm, Counterfactual Regret Minimization
(CFR), converges on effective strategies without novel algorithmic extensions.
A lightweight full-stack research platform unifies the environment, a
parallelized CFR runtime, and a human-playable web interface. The trained CFR
agent and source code are available at https://monopolydeal.ai.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In Defence of Post-hoc Explainability <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.17883v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.17883v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nick Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This position paper defends post-hoc explainability methods as legitimate
tools for scientific knowledge production in machine learning. Addressing
criticism of these methods' reliability and epistemic status, we develop a
philosophical framework grounded in mediated understanding and bounded
factivity. We argue that scientific insights can emerge through structured
interpretation of model behaviour without requiring complete mechanistic
transparency, provided explanations acknowledge their approximative nature and
undergo rigorous empirical validation. Through analysis of recent biomedical ML
applications, we demonstrate how post-hoc methods, when properly integrated
into scientific practice, generate novel hypotheses and advance phenomenal
understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v1 presented at the Interpretable AI: Past, Present, and Future
  Workshop at NeurIPS 2024 (non-archival)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On-the-Fly OVD Adaptation with FLAME: Few-shot Localization via Active
  Marginal-Samples Exploration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.17670v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.17670v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yehonathan Refael, Amit Aides, Aviad Barzilai, George Leifman, Genady Beryozkin, Vered Silverman, Bolous Jaber, Tomer Shekel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-vocabulary object detection (OVD) models offer remarkable flexibility by
detecting objects from arbitrary text queries. However, their zero-shot
performance in specialized domains like Remote Sensing (RS) is often
compromised by the inherent ambiguity of natural language, limiting critical
downstream applications. For instance, an OVD model may struggle to distinguish
between fine-grained classes such as "fishing boat" and "yacht" since their
embeddings are similar and often inseparable. This can hamper specific user
goals, such as monitoring illegal fishing, by producing irrelevant detections.
To address this, we propose a cascaded approach that couples the broad
generalization of a large pre-trained OVD model with a lightweight few-shot
classifier. Our method first employs the zero-shot model to generate
high-recall object proposals. These proposals are then refined for high
precision by a compact classifier trained in real-time on only a handful of
user-annotated examples - drastically reducing the high costs of RS imagery
annotation.The core of our framework is FLAME, a one-step active learning
strategy that selects the most informative samples for training. FLAME
identifies, on the fly, uncertain marginal candidates near the decision
boundary using density estimation, followed by clustering to ensure sample
diversity. This efficient sampling technique achieves high accuracy without
costly full-model fine-tuning and enables instant adaptation, within less then
a minute, which is significantly faster than state-of-the-art alternatives.Our
method consistently surpasses state-of-the-art performance on RS benchmarks,
establishing a practical and resource-efficient framework for adapting
foundation models to specific user needs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Seeing Structural Failure Before it Happens: An Image-Based
  Physics-Informed Neural Network (PINN) for Spaghetti Bridge Load Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23117v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23117v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omer Jauhar Khan, Sudais Khan, Hafeez Anwar, Shahzeb Khan, Shams Ul Arifeen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Physics Informed Neural Networks (PINNs) are gaining attention for their
ability to embed physical laws into deep learning models, which is particularly
useful in structural engineering tasks with limited data. This paper aims to
explore the use of PINNs to predict the weight of small scale spaghetti
bridges, a task relevant to understanding load limits and potential failure
modes in simplified structural models. Our proposed framework incorporates
physics-based constraints to the prediction model for improved performance. In
addition to standard PINNs, we introduce a novel architecture named Physics
Informed Kolmogorov Arnold Network (PIKAN), which blends universal function
approximation theory with physical insights. The structural parameters provided
as input to the model are collected either manually or through computer vision
methods. Our dataset includes 15 real bridges, augmented to 100 samples, and
our best model achieves an $R^2$ score of 0.9603 and a mean absolute error
(MAE) of 10.50 units. From applied perspective, we also provide a web based
interface for parameter entry and prediction. These results show that PINNs can
offer reliable estimates of structural weight, even with limited data, and may
help inform early stage failure analysis in lightweight bridge designs.
  The complete data and code are available at
https://github.com/OmerJauhar/PINNS-For-Spaghetti-Bridges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 17 figures. Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM
  Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.11329v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.11329v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raja Gond, Nipun Kwatra, Ramachandran Ramjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distributed inference of large language models (LLMs) can introduce overheads
of up to 20% even over GPUs connected via high-speed interconnects such as
NVLink. Multiple techniques have been proposed to mitigate these overheads by
decomposing computations into finer-grained tasks and overlapping communication
with sub-tasks as they complete. However, fine-grained decomposition of a large
computation into many smaller computations on GPUs results in overheads.
Furthermore, the communication itself uses many streaming multiprocessors
(SMs), adding to the overhead.
  We present TokenWeave to address these challenges. TokenWeave proposes a
Token-Splitting technique that divides the tokens in the inference batch into
two approximately equal subsets in a wave-aware manner. The communication of
one subset is then overlapped with the computation of the other. In addition,
TokenWeave optimizes the order of the layer normalization computation with
respect to communication operations and implements a novel fused
AllReduce--RMSNorm kernel that carefully leverages Multimem instruction support
available on Hopper and Blackwell NVIDIA GPUs. These optimizations allow
TokenWeave to perform communication and RMSNorm using only 2-8 SMs. Moreover,
our kernel enables the memory-bound RMSNorm to be overlapped with the other
batch's computation, providing additional gains.
  Our evaluations demonstrate up to 1.29x speedup in latency and 1.26x higher
throughput across multiple models and workloads. In several settings,
TokenWeave results in better performance compared to an equivalent model with
all communication removed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 16 figures. For source code, see
  https://github.com/microsoft/tokenweave. In version 2, Figure 6 shows
  All-Reduce bandwidth instead of Reduce-Scatter. The Multimem Reduce-Scatter
  bandwidth formula differs slightly from the ring-based version. Fixed x-ticks
  in Figure 7</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nearly Minimax Discrete Distribution Estimation in Kullback-Leibler
  Divergence with High Probability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17316v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17316v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dirk van der Hoeven, Julia Olkhovskaia, Tim van Erven
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the fundamental problem of estimating a discrete distribution on
a domain of size~$K$ with high probability in Kullback-Leibler divergence. We
provide upper and lower bounds on the minimax estimation rate, which show that
the optimal rate is between $\big(K + \ln(K)\ln(1/\delta)\big) /n$ and
$\big(K\ln\ln(K) + \ln(K)\ln(1/\delta)\big) /n$ at error probability $\delta$
and sample size $n$, which pins down the rate up to the doubly logarithmic
factor $\ln \ln K$ that multiplies $K$. Our upper bound uses techniques from
online learning to construct a novel estimator via online-to-batch conversion.
Perhaps surprisingly, the tail behavior of the minimax rate is worse than for
the squared total variation and squared Hellinger distance, for which it is
$\big(K + \ln(1/\delta)\big) /n$, i.e.\ without the $\ln K$ multiplying $\ln
(1/\delta)$. As a consequence, we cannot obtain a fully tight lower bound from
the usual reduction to these smaller distances. Moreover, we show that this
lower bound cannot be achieved by the standard lower bound approach based on a
reduction to hypothesis testing, and instead we need to introduce a new
reduction to what we call weak hypothesis testing. We investigate the source of
the gap with other divergences further in refined results, which show that the
total variation rate is achievable for Kullback-Leibler divergence after all
(in fact by he maximum likelihood estimator) if we rule out outcome
probabilities smaller than $O(\ln(K/\delta) / n)$, which is a vanishing set as
$n$ increases for fixed $K$ and~$\delta$. This explains why minimax
Kullback-Leibler estimation is more difficult than asymptotic estimation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Predictive Causal Inference via Spatio-Temporal Modeling and Penalized
  Empirical Likelihood 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.08896v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.08896v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Byunghee Lee, Hye Yeon Sin, Joonsung Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study introduces an integrated framework for predictive causal inference
designed to overcome limitations inherent in conventional single model
approaches. Specifically, we combine a Hidden Markov Model (HMM) for spatial
health state estimation with a Multi Task and Multi Graph Convolutional Network
(MTGCN) for capturing temporal outcome trajectories. The framework
asymmetrically treats temporal and spatial information regarding them as
endogenous variables in the outcome regression, and exogenous variables in the
propensity score model, thereby expanding the standard doubly robust treatment
effect estimation to jointly enhance bias correction and predictive accuracy.
To demonstrate its utility, we focus on clinical domains such as cancer,
dementia, and Parkinson disease, where treatment effects are challenging to
observe directly. Simulation studies are conducted to emulate latent disease
dynamics and evaluate the model performance under varying conditions. Overall,
the proposed framework advances predictive causal inference by structurally
adapting to spatiotemporal complexities common in biomedical data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards a Method for Synthetic Generation of Persons with Aphasia
  Transcripts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24817v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24817v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jason M. Pittman, Anton Phillips Jr., Yesenia Medina-Santos, Brielle C. Stark
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In aphasia research, Speech-Language Pathologists (SLPs) devote extensive
time to manually coding speech samples using Correct Information Units (CIUs),
a measure of how informative an individual sample of speech is. Developing
automated systems to recognize aphasic language is limited by data scarcity.
For example, only about 600 transcripts are available in AphasiaBank yet
billions of tokens are used to train large language models (LLMs). In the
broader field of machine learning (ML), researchers increasingly turn to
synthetic data when such are sparse. Therefore, this study constructs and
validates two methods to generate synthetic transcripts of the AphasiaBank Cat
Rescue picture description task. One method leverages a procedural programming
approach while the second uses Mistral 7b Instruct and Llama 3.1 8b Instruct
LLMs. The methods generate transcripts across four severity levels (Mild,
Moderate, Severe, Very Severe) through word dropping, filler insertion, and
paraphasia substitution. Overall, we found, compared to human-elicited
transcripts, Mistral 7b Instruct best captures key aspects of linguistic
degradation observed in aphasia, showing realistic directional changes in NDW,
word count, and word length amongst the synthetic generation methods. Based on
the results, future work should plan to create a larger dataset, fine-tune
models for better aphasic representation, and have SLPs assess the realism and
usefulness of the synthetic transcripts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 1 figure, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SafEDMD: A Koopman-based data-driven controller design framework for
  nonlinear dynamical systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.03145v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.03145v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robin Strässer, Manuel Schaller, Karl Worthmann, Julian Berberich, Frank Allgöwer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Koopman operator serves as the theoretical backbone for machine learning
of dynamical control systems, where the operator is heuristically approximated
by extended dynamic mode decomposition (EDMD). In this paper, we propose
SafEDMD, a novel stability- and feedback-oriented EDMD-based controller design
framework. Our approach leverages a reliable surrogate model generated in a
data-driven fashion in order to provide closed-loop guarantees. In particular,
we establish a controller design based on semi-definite programming with
guaranteed stabilization of the underlying nonlinear system. As central
ingredient, we derive proportional error bounds that vanish at the origin and
are tailored to control tasks. We illustrate the developed method by means of
several benchmark examples and highlight the advantages over state-of-the-art
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in Automatica</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mind the Gap: Removing the Discretization Gap in Differentiable Logic
  Gate Networks <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.07500v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.07500v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shakir Yousefi, Andreas Plesner, Till Aczel, Roger Wattenhofer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern neural networks demonstrate state-of-the-art performance on numerous
existing benchmarks; however, their high computational requirements and energy
consumption prompt researchers to seek more efficient solutions for real-world
deployment. Logic gate networks (LGNs) learns a large network of logic gates
for efficient image classification. However, learning a network that can solve
a simple problem like CIFAR-10 can take days to weeks to train. Even then,
almost half of the network remains unused, causing a discretization gap. This
discretization gap hinders real-world deployment of LGNs, as the performance
drop between training and inference negatively impacts accuracy. We inject
Gumbel noise with a straight-through estimator during training to significantly
speed up training, improve neuron utilization, and decrease the discretization
gap. We theoretically show that this results from implicit Hessian
regularization, which improves the convergence properties of LGNs. We train
networks $4.5 \times$ faster in wall-clock time, reduce the discretization gap
by $98\%$, and reduce the number of unused gates by $100\%$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2025 (main track)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the
  LibriBrain <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.21038v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.21038v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gereon Elvers, Gilad Landau, Oiwi Parker Jones
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-invasive brain-computer interfaces (BCIs) are beginning to benefit from
large, public benchmarks. However, current benchmarks target relatively simple,
foundational tasks like Speech Detection and Phoneme Classification, while
application-ready results on tasks like Brain-to-Text remain elusive. We
propose Keyword Spotting (KWS) as a practically applicable, privacy-aware
intermediate task. Using the deep 52-hour, within-subject LibriBrain corpus, we
provide standardized train/validation/test splits for reproducible
benchmarking, and adopt an evaluation protocol tailored to extreme class
imbalance. Concretely, we use area under the precision-recall curve (AUPRC) as
a robust evaluation metric, complemented by false alarms per hour (FA/h) at
fixed recall to capture user-facing trade-offs. To simplify deployment and
further experimentation within the research community, we are releasing an
updated version of the pnpl library with word-level dataloaders and Colab-ready
tutorials. As an initial reference model, we present a compact 1-D Conv/ResNet
baseline with focal loss and top-k pooling that is trainable on a single
consumer-class GPU. The reference model achieves approximately 13x the
permutation baseline AUPRC on held-out sessions, demonstrating the viability of
the task. Exploratory analyses reveal: (i) predictable within-subject scaling -
performance improves log-linearly with more training hours - and (ii) the
existence of word-level factors (frequency and duration) that systematically
modulate detectability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 7 figures, 6 tables; updated acknowledgments</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AIMeter: Measuring, Analyzing, and Visualizing Energy and Carbon
  Footprint of AI Workloads 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20535v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20535v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongzhen Huang, Kunming Zhang, Hanlong Liao, Kui Wu, Guoming Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of AI, particularly large language models (LLMs), has
raised significant concerns about the energy use and carbon emissions
associated with model training and inference. However, existing tools for
measuring and reporting such impacts are often fragmented, lacking systematic
metric integration and offering limited support for correlation analysis among
them. This paper presents AIMeter, a comprehensive software toolkit for the
measurement, analysis, and visualization of energy use, power draw, hardware
performance, and carbon emissions across AI workloads. By seamlessly
integrating with existing AI frameworks, AIMeter offers standardized reports
and exports fine-grained time-series data to support benchmarking and
reproducibility in a lightweight manner. It further enables in-depth
correlation analysis between hardware metrics and model performance and thus
facilitates bottleneck identification and performance enhancement. By
addressing critical limitations in existing tools, AIMeter encourages the
research community to weigh environmental impact alongside raw performance of
AI workloads and advances the shift toward more sustainable "Green AI"
practices. The code is available at https://github.com/SusCom-Lab/AIMeter.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 figures and 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stability and Sharper Risk Bounds with Convergence Rate
  $\tilde{O}(1/n^2)$ 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09766v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09766v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowei Zhu, Shaojie Li, Mingyang Yi, Yong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prior work (Klochkov $\&$ Zhivotovskiy, 2021) establishes at most
$O\left(\log (n)/n\right)$ excess risk bounds via algorithmic stability for
strongly-convex learners with high probability. We show that under the similar
common assumptions -- - Polyak-Lojasiewicz condition, smoothness, and Lipschitz
continous for losses -- - rates of $O\left(\log^2(n)/n^2\right)$ are at most
achievable. To our knowledge, our analysis also provides the tightest
high-probability bounds for gradient-based generalization gaps in nonconvex
settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hysteresis Activation Function for Efficient Inference <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.10573v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.10573v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moshe Kimhi, Idan Kashani, Avi Mendelson, Chaim Baskin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widely used ReLU is favored for its hardware efficiency, {as the
implementation at inference is a one bit sign case,} yet suffers from issues
such as the ``dying ReLU'' problem, where during training, neurons fail to
activate and constantly remain at zero, as highlighted by Lu et al. Traditional
approaches to mitigate this issue often introduce more complex and less
hardware-friendly activation functions. In this work, we propose a Hysteresis
Rectified Linear Unit (HeLU), an efficient activation function designed to
address the ``dying ReLU'' problem with minimal complexity. Unlike traditional
activation functions with fixed thresholds for training and inference, HeLU
employs a variable threshold that refines the backpropagation. This refined
mechanism allows simpler activation functions to achieve competitive
performance comparable to their more complex counterparts without introducing
unnecessary complexity or requiring inductive biases. Empirical evaluations
demonstrate that HeLU enhances model generalization across diverse datasets,
offering a promising solution for efficient and effective inference suitable
for a wide range of neural network architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to 4th NeurIPS Efficient Natural Language and Speech
  Processing Workshop (ENLSP-IV 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GRPO-Guard: Mitigating Implicit Over-Optimization in Flow Matching via
  Regulated Clipping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.22319v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.22319v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Wang, Jiajun Liang, Jie Liu, Henglin Liu, Gongye Liu, Jun Zheng, Wanyuan Pang, Ao Ma, Zhenyu Xie, Xintao Wang, Meng Wang, Pengfei Wan, Xiaodan Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, GRPO-based reinforcement learning has shown remarkable progress in
optimizing flow-matching models, effectively improving their alignment with
task-specific rewards. Within these frameworks, the policy update relies on
importance-ratio clipping to constrain overconfident positive and negative
gradients. However, in practice, we observe a systematic shift in the
importance-ratio distribution-its mean falls below 1 and its variance differs
substantially across timesteps. This left-shifted and inconsistent distribution
prevents positive-advantage samples from entering the clipped region, causing
the mechanism to fail in constraining overconfident positive updates. As a
result, the policy model inevitably enters an implicit over-optimization
stage-while the proxy reward continues to increase, essential metrics such as
image quality and text-prompt alignment deteriorate sharply, ultimately making
the learned policy impractical for real-world use. To address this issue, we
introduce GRPO-Guard, a simple yet effective enhancement to existing GRPO
frameworks. Our method incorporates ratio normalization, which restores a
balanced and step-consistent importance ratio, ensuring that PPO clipping
properly constrains harmful updates across denoising timesteps. In addition, a
gradient reweighting strategy equalizes policy gradients over noise conditions,
preventing excessive updates from particular timestep regions. Together, these
designs act as a regulated clipping mechanism, stabilizing optimization and
substantially mitigating implicit over-optimization without relying on heavy KL
regularization. Extensive experiments on multiple diffusion backbones (e.g.,
SD3.5M, Flux.1-dev) and diverse proxy tasks demonstrate that GRPO-Guard
significantly reduces over-optimization while maintaining or even improving
generation quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://jingw193.github.io/GRPO-Guard/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Oryx: a Scalable Sequence Model for Many-Agent Coordination in Offline
  MARL <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.22151v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.22151v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Claude Formanek, Omayma Mahjoub, Louay Ben Nessir, Sasha Abramowitz, Ruan de Kock, Wiem Khlifi, Daniel Rajaonarivonivelomanantsoa, Simon Du Toit, Arnol Fokam, Siddarth Singh, Ulrich Mbou Sob, Felix Chalumeau, Arnu Pretorius
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A key challenge in offline multi-agent reinforcement learning (MARL) is
achieving effective many-agent multi-step coordination in complex environments.
In this work, we propose Oryx, a novel algorithm for offline cooperative MARL
to directly address this challenge. Oryx adapts the recently proposed
retention-based architecture Sable and combines it with a sequential form of
implicit constraint Q-learning (ICQ), to develop a novel offline autoregressive
policy update scheme. This allows Oryx to solve complex coordination challenges
while maintaining temporal coherence over long trajectories. We evaluate Oryx
across a diverse set of benchmarks from prior works -- SMAC, RWARE, and
Multi-Agent MuJoCo -- covering tasks of both discrete and continuous control,
varying in scale and difficulty. Oryx achieves state-of-the-art performance on
more than 80% of the 65 tested datasets, outperforming prior offline MARL
methods and demonstrating robust generalisation across domains with many agents
and long horizons. Finally, we introduce new datasets to push the limits of
many-agent coordination in offline MARL, and demonstrate Oryx's superior
ability to scale effectively in such settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at the 39th Conference on Neural Information Processing
  Systems (NeurIPS 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neurosymbolic Diffusion Models <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13138v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13138v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emile van Krieken, Pasquale Minervini, Edoardo Ponti, Antonio Vergari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neurosymbolic (NeSy) predictors combine neural perception with symbolic
reasoning to solve tasks like visual reasoning. However, standard NeSy
predictors assume conditional independence between the symbols they extract,
thus limiting their ability to model interactions and uncertainty - often
leading to overconfident predictions and poor out-of-distribution
generalisation. To overcome the limitations of the independence assumption, we
introduce neurosymbolic diffusion models (NeSyDMs), a new class of NeSy
predictors that use discrete diffusion to model dependencies between symbols.
Our approach reuses the independence assumption from NeSy predictors at each
step of the diffusion process, enabling scalable learning while capturing
symbol dependencies and uncertainty quantification. Across both synthetic and
real-world benchmarks - including high-dimensional visual path planning and
rule-based autonomous driving - NeSyDMs achieve state-of-the-art accuracy among
NeSy predictors and demonstrate strong calibration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AnomalyMatch: Discovering Rare Objects of Interest with Semi-supervised
  and Active Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.03509v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.03509v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pablo Gómez, Laslo E. Ruhberg, Maria Teresa Nardone, David O'Ryan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anomaly detection in large datasets is essential in astronomy and computer
vision. However, due to a scarcity of labelled data, it is often infeasible to
apply supervised methods to anomaly detection. We present AnomalyMatch, an
anomaly detection framework combining the semi-supervised FixMatch algorithm
using EfficientNet classifiers with active learning. AnomalyMatch is tailored
for large-scale applications and integrated into the ESA Datalabs science
platform. In this method, we treat anomaly detection as a binary classification
problem and efficiently utilise limited labelled and abundant unlabelled images
for training. We enable active learning via a user interface for verification
of high-confidence anomalies and correction of false positives. Evaluations on
the GalaxyMNIST astronomical dataset and the miniImageNet natural-image
benchmark under severe class imbalance display strong performance. Starting
from five to ten labelled anomalies, we achieve an average AUROC of 0.96
(miniImageNet) and 0.89 (GalaxyMNIST), with respective AUPRC of 0.82 and 0.77.
After three active learning cycles, anomalies are ranked with 76%
(miniImageNet) to 94% (GalaxyMNIST) precision in the top 1% of the
highest-ranking images by score. We compare to the established Astronomaly
software on selected 'odd' galaxies from the 'Galaxy Zoo - The Galaxy
Challenge' dataset, achieving comparable performance with an average AUROC of
0.83. Our results underscore the exceptional utility and scalability of this
approach for anomaly discovery, highlighting the value of specialised
approaches for domains characterised by severe label scarcity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Journal submission in preparation to RASTI; 15 pages; 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diversity as a Reward: Fine-Tuning LLMs on a Mixture of
  Domain-Undetermined Data <span class="chip">NeurIPS'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.04380v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.04380v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenqing Ling, Daoyuan Chen, Liuyi Yao, Qianli Shen, Yaliang Li, Ying Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning large language models (LLMs) using diverse datasets is crucial
for enhancing their overall performance across various domains. In practical
scenarios, existing methods based on modeling the mixture proportions of data
composition often struggle with data whose domain labels are missing, imprecise
or non-normalized, while methods based on data selection usually encounter
difficulties in balancing multi-domain performance. To address these
challenges, in this work, we investigate the role of data diversity in
enhancing the overall abilities of LLMs by empirically constructing contrastive
data pools and theoretically deriving explanations. Building upon the insights
gained, we propose a new method that gives the LLM a dual identity: an output
model to cognitively probe and select data based on diversity reward, as well
as an input model to be tuned with the selected data. Extensive experiments
show that the proposed method notably boosts performance across
domain-undetermined data and a series of foundational downstream tasks when
applied to various advanced LLMs. We release our code and hope this study can
shed light on the understanding of data diversity and advance feedback-driven
data-model co-design for LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS'25 main track. 47 pages, 21 figures, 32 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Truncated Kernel Stochastic Gradient Descent with General Losses and
  Spherical Radial Basis Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.04237v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.04237v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinhui Bai, Andreas Christmann, Lei Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel kernel stochastic gradient descent (SGD)
algorithm for large-scale supervised learning with general losses. Compared to
traditional kernel SGD, our algorithm improves efficiency and scalability
through an innovative regularization strategy. By leveraging the infinite
series expansion of spherical radial basis functions, this strategy projects
the stochastic gradient onto a finite-dimensional hypothesis space, which is
adaptively scaled according to the bias-variance trade-off, thereby enhancing
generalization performance. Based on a new estimation of the spectral structure
of the kernel-induced covariance operator, we develop an analytical framework
that unifies optimization and generalization analyses. We prove that both the
last iterate and the suffix average converge at minimax-optimal rates, and we
further establish optimal strong convergence in the reproducing kernel Hilbert
space. Our framework accommodates a broad class of classical loss functions,
including least-squares, Huber, and logistic losses. Moreover, the proposed
algorithm significantly reduces computational complexity and achieves optimal
storage complexity by incorporating coordinate-wise updates from linear SGD,
thereby avoiding the costly pairwise operations typical of kernel SGD and
enabling efficient processing of streaming data. Finally, extensive numerical
experiments demonstrate the efficiency of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>54 pages, 20 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Experiments with Optimal Model Trees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.12902v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.12902v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sabino Francesco Roselli, Eibe Frank
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model trees provide an appealing way to perform interpretable machine
learning for both classification and regression problems. In contrast to
``classic'' decision trees with constant values in their leaves, model trees
can use linear combinations of predictor variables in their leaf nodes to form
predictions, which can help achieve higher accuracy and smaller trees. Typical
algorithms for learning model trees from training data work in a greedy
fashion, growing the tree in a top-down manner by recursively splitting the
data into smaller and smaller subsets. Crucially, the selected splits are only
locally optimal, potentially rendering the tree overly complex and less
accurate than a tree whose structure is globally optimal for the training data.
In this paper, we empirically investigate the effect of constructing globally
optimal model trees for classification and regression with linear support
vector machines at the leaf nodes. To this end, we present mixed-integer linear
programming formulations to learn optimal trees, compute such trees for a large
collection of benchmark data sets, and compare their performance against
greedily grown model trees in terms of interpretability and accuracy. We also
compare to classic optimal and greedily grown decision trees, random forests,
and support vector machines. Our results show that optimal model trees can
achieve competitive accuracy with very small trees. We also investigate the
effect on the accuracy of replacing axis-parallel splits with multivariate
ones, foregoing interpretability while potentially obtaining greater accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decoding for Punctured Convolutional and Turbo Codes: A Deep Learning
  Solution for Protocols Compliance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15475v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15475v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongli Yan, Linglong Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural network-based decoding methods show promise in enhancing error
correction performance but face challenges with punctured codes. In particular,
existing methods struggle to adapt to variable code rates or meet protocol
compatibility requirements. This paper proposes a unified long short-term
memory (LSTM)-based neural decoder for punctured convolutional and Turbo codes
to address these challenges. The key component of the proposed LSTM-based
neural decoder is puncturing-aware embedding, which integrates puncturing
patterns directly into the neural network to enable seamless adaptation to
different code rates. Moreover, a balanced bit error rate training strategy is
designed to ensure the decoder's robustness across various code lengths, rates,
and channels. In this way, the protocol compatibility requirement can be
realized. Extensive simulations in both additive white Gaussian noise (AWGN)
and Rayleigh fading channels demonstrate that the proposed neural decoder
outperforms conventional decoding techniques, offering significant improvements
in decoding accuracy and robustness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond likelihood ratio bias: Nested multi-time-scale stochastic
  approximation for likelihood-free parameter estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.12995v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.12995v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zehao Li, Zhouchen Lin, Yijie Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study parameter inference in simulation-based stochastic models where the
analytical form of the likelihood is unknown. The main difficulty is that score
evaluation as a ratio of noisy Monte Carlo estimators induces bias and
instability, which we overcome with a ratio-free nested multi-time-scale (NMTS)
stochastic approximation (SA) method that simultaneously tracks the score and
drives the parameter update. We provide a comprehensive theoretical analysis of
the proposed NMTS algorithm for solving likelihood-free inference problems,
including strong convergence, asymptotic normality, and convergence rates. We
show that our algorithm can eliminate the original asymptotic bias
$O\big(\sqrt{\frac{1}{N}}\big)$ and accelerate the convergence rate from
$O\big(\beta_k+\sqrt{\frac{1}{N}}\big)$ to
$O\big(\frac{\beta_k}{\alpha_k}+\sqrt{\frac{\alpha_k}{N}}\big)$, where $N$ is
the fixed batch size, $\alpha_k$ and $\beta_k$ are decreasing step sizes with
$\alpha_k$, $\beta_k$, $\beta_k/\alpha_k\rightarrow 0$. With proper choice of
$\alpha_k$ and $\beta_k$, our convergence rates can match the optimal rate in
the multi-time-scale SA literature. Numerical experiments demonstrate that our
algorithm can improve the estimation accuracy by one to two orders of magnitude
at the same computational cost, making it efficient for parameter estimation in
stochastic systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Chaos-based reinforcement learning with TD3 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.09086v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.09086v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Toshitaka Matsuki, Yusuke Sakemi, Kazuyuki Aihara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chaos-based reinforcement learning (CBRL) is a method in which the agent's
internal chaotic dynamics drives exploration. However, the learning algorithms
in CBRL have not been thoroughly developed in previous studies, nor have they
incorporated recent advances in reinforcement learning. This study introduced
Twin Delayed Deep Deterministic Policy Gradients (TD3), which is one of the
state-of-the-art deep reinforcement learning algorithms that can treat
deterministic and continuous action spaces, to CBRL. The validation results
provide several insights. First, TD3 works as a learning algorithm for CBRL in
a simple goal-reaching task. Second, CBRL agents with TD3 can autonomously
suppress their exploratory behavior as learning progresses and resume
exploration when the environment changes. Finally, examining the effect of the
agent's chaoticity on learning shows that there exists a suitable range of
chaos strength in the agent's model to flexibly switch between exploration and
exploitation and adapt to environmental changes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in Neural Networks</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">10</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MORE: Multi-Organ Medical Image REconstruction <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26759v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26759v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaokai Wu, Yapan Guo, Yanbiao Ji, Jing Tong, Yuxiang Lu, Mei Li, Suizhi Huang, Yue Ding, Hongtao Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  CT reconstruction provides radiologists with images for diagnosis and
treatment, yet current deep learning methods are typically limited to specific
anatomies and datasets, hindering generalization ability to unseen anatomies
and lesions. To address this, we introduce the Multi-Organ medical image
REconstruction (MORE) dataset, comprising CT scans across 9 diverse anatomies
with 15 lesion types. This dataset serves two key purposes: (1) enabling robust
training of deep learning models on extensive, heterogeneous data, and (2)
facilitating rigorous evaluation of model generalization for CT reconstruction.
We further establish a strong baseline solution that outperforms prior
approaches under these challenging conditions. Our results demonstrate that:
(1) a comprehensive dataset helps improve the generalization capability of
models, and (2) optimization-based methods offer enhanced robustness for unseen
anatomies. The MORE dataset is freely accessible under CC-BY-NC 4.0 at our
project page https://more-med.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACMMM 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unveiling Intrinsic Text Bias in Multimodal Large Language Models
  through Attention Key-Space Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26721v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26721v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinhan Zheng, Huyu Wu, Xueting Wang, Haiyun Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) exhibit a pronounced preference for
textual inputs when processing vision-language data, limiting their ability to
reason effectively from visual evidence. Unlike prior studies that attribute
this text bias to external factors such as data imbalance or instruction
tuning, we propose that the bias originates from the model's internal
architecture. Specifically, we hypothesize that visual key vectors (Visual
Keys) are out-of-distribution (OOD) relative to the text key space learned
during language-only pretraining. Consequently, these visual keys receive
systematically lower similarity scores during attention computation, leading to
their under-utilization in the context representation. To validate this
hypothesis, we extract key vectors from LLaVA and Qwen2.5-VL and analyze their
distributional structures using qualitative (t-SNE) and quantitative
(Jensen-Shannon divergence) methods. The results provide direct evidence that
visual and textual keys occupy markedly distinct subspaces within the attention
space. The inter-modal divergence is statistically significant, exceeding
intra-modal variation by several orders of magnitude. These findings reveal
that text bias arises from an intrinsic misalignment within the attention key
space rather than solely from external data factors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AdSum: Two-stream Audio-visual Summarization for Automated Video
  Advertisement Clipping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26569v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26569v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wen Xie, Yanjun Zhu, Gijs Overgoor, Yakov Bart, Agata Lapedriza Garcia, Sarah Ostadabbas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advertisers commonly need multiple versions of the same advertisement (ad) at
varying durations for a single campaign. The traditional approach involves
manually selecting and re-editing shots from longer video ads to create shorter
versions, which is labor-intensive and time-consuming. In this paper, we
introduce a framework for automated video ad clipping using video summarization
techniques. We are the first to frame video clipping as a shot selection
problem, tailored specifically for advertising. Unlike existing general video
summarization methods that primarily focus on visual content, our approach
emphasizes the critical role of audio in advertising. To achieve this, we
develop a two-stream audio-visual fusion model that predicts the importance of
video frames, where importance is defined as the likelihood of a frame being
selected in the firm-produced short ad. To address the lack of ad-specific
datasets, we present AdSum204, a novel dataset comprising 102 pairs of
30-second and 15-second ads from real advertising campaigns. Extensive
experiments demonstrate that our model outperforms state-of-the-art methods
across various metrics, including Average Precision, Area Under Curve,
Spearman, and Kendall.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 32nd International Conference on MultiMedia Modeling</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contribution-Guided Asymmetric Learning for Robust Multimodal Fusion
  under Imbalance and Noise 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26289v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26289v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijing Xu, Yunfeng Kou, Kunming Wu, Hong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal learning faces two major challenges: modality imbalance and data
noise, which significantly affect the robustness and generalization ability of
models. Existing methods achieve modality balance by suppressing dominant
modalities, but they neglect the inherent differences in the information value
between modalities, potentially leading to convergence to suboptimal solutions.
This paper proposes an innovative modality compression paradigm,
Contribution-Guided Asymmetric Learning (CAL), which aims to enhance the
contribution of high-contribution modalities while compressing weak modalities
to increase their contribution, allowing both to improve the performance of
multimodal information fusion. CAL is based on a modality contribution metric
W^m combining the information quantity I(m) and confidence D(m), and it designs
an asymmetric gradient acceleration mechanism and a contribution-aware
Asymmetric Information Bottleneck (AIB) compression mechanism. The former
accelerates the gradient update of modalities, while the latter dynamically
compresses the noise of low-contribution modalities.
  On five benchmark datasets, including emotion recognition, scene recognition,
and event localization tasks, CAL has shown outstanding performance in
imbalanced fusion tasks and noise robustness tests. On CREMA-D, KS, and AVE,
CAL achieves 79.30%, 74.82%, and 74.21% accuracy, significantly outperforming
the existing state-of-the-art model ARL. In high-noise robustness tests, CAL
also achieved leading performance under various attack strategies on the
MVSA-Single and NYUD2 datasets. These results validate the significant
advantages of CAL in modality imbalance and noise interference. CAL, as a
flexible and efficient framework, is easy to transfer to other tasks and has
broad adaptability and potential application prospects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quality Over Quantity? LLM-Based Curation for a Data-Efficient
  Audio-Video Foundation Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.09205v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.09205v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Vosoughi, Dimitra Emmanouilidou, Hannes Gamper
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrating audio and visual data for training multimodal foundational models
remains a challenge. The Audio-Video Vector Alignment (AVVA) framework
addresses this by considering AV scene alignment beyond mere temporal
synchronization, and leveraging Large Language Models (LLMs) for data curation.
AVVA implements a scoring mechanism for selecting aligned training data
segments. It integrates Whisper, a speech-based foundation model, for audio and
DINOv2 for video analysis in a dual-encoder structure with contrastive learning
on AV pairs. Evaluations on AudioCaps, VALOR, and VGGSound demonstrate the
effectiveness of the proposed model architecture and data curation approach.
AVVA achieves a significant improvement in top-k accuracies for video-to-audio
retrieval on all datasets compared to DenseAV, while using only 192 hrs of
curated training data. Furthermore, an ablation study indicates that the data
curation process effectively trades data quality for data quantity, yielding
increases in top-k retrieval accuracies on AudioCaps, VALOR, and VGGSound,
compared to training on the full spectrum of uncurated data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 5 figures, 2 tables. Accepted at EUSIPCO 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ARECHO: Autoregressive Evaluation via Chain-Based Hypothesis
  Optimization for Speech Multi-Metric Estimation <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.24518v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.24518v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiatong Shi, Yifan Cheng, Bo-Hao Su, Hye-jin Shim, Jinchuan Tian, Samuele Cornell, Yiwen Zhao, Siddhant Arora, Shinji Watanabe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech signal analysis poses significant challenges, particularly in tasks
such as speech quality evaluation and profiling, where the goal is to predict
multiple perceptual and objective metrics. For instance, metrics like PESQ
(Perceptual Evaluation of Speech Quality), STOI (Short-Time Objective
Intelligibility), and MOS (Mean Opinion Score) each capture different aspects
of speech quality. However, these metrics often have different scales,
assumptions, and dependencies, making joint estimation non-trivial. To address
these issues, we introduce ARECHO (Autoregressive Evaluation via Chain-based
Hypothesis Optimization), a chain-based, versatile evaluation system for speech
assessment grounded in autoregressive dependency modeling. ARECHO is
distinguished by three key innovations: (1) a comprehensive speech information
tokenization pipeline; (2) a dynamic classifier chain that explicitly captures
inter-metric dependencies; and (3) a two-step confidence-oriented decoding
algorithm that enhances inference reliability. Experiments demonstrate that
ARECHO significantly outperforms the baseline framework across diverse
evaluation scenarios, including enhanced speech analysis, speech generation
evaluation, and, noisy speech evaluation. Furthermore, its dynamic dependency
modeling improves interpretability by capturing inter-metric relationships.
Across tasks, ARECHO offers reference-free evaluation using its dynamic
classifier chain to support subset queries (single or multiple metrics) and
reduces error propagation via confidence-oriented decoding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025 Spotlight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dependency Structure Augmented Contextual Scoping Framework for
  Multimodal Aspect-Based Sentiment Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.11331v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.11331v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Liu, Lijun He, Jiaxi Liang, Zhihan Ren, Haixia Bi, Fan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Aspect-Based Sentiment Analysis (MABSA) seeks to extract
fine-grained information from image-text pairs to identify aspect terms and
determine their sentiment polarity. However, existing approaches often fall
short in simultaneously addressing three core challenges: Sentiment Cue
Perception (SCP), Multimodal Information Misalignment (MIM), and Semantic Noise
Elimination (SNE). To overcome these limitations, we propose DASCO
(\textbf{D}ependency Structure \textbf{A}ugmented \textbf{Sco}ping Framework),
a fine-grained scope-oriented framework that enhances aspect-level sentiment
reasoning by leveraging dependency parsing trees. First, we designed a
multi-task pretraining strategy for MABSA on our base model, combining
aspect-oriented enhancement, image-text matching, and aspect-level
sentiment-sensitive cognition. This improved the model's perception of aspect
terms and sentiment cues while achieving effective image-text alignment,
addressing key challenges like SCP and MIM. Furthermore, we incorporate
dependency trees as syntactic branch combining with semantic branch, guiding
the model to selectively attend to critical contextual elements within a
target-specific scope while effectively filtering out irrelevant noise for
addressing SNE problem. Extensive experiments on two benchmark datasets across
three subtasks demonstrate that DASCO achieves state-of-the-art performance in
MABSA, with notable gains in JMASA (+2.3\% F1 and +3.5\% precision on
Twitter2015). The source code is available at https://github.com/LHaoooo/DASCO .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ReCon-GS: Continuum-Preserved Gaussian Streaming for Fast and Compact
  Reconstruction of Dynamic Scenes <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.24325v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.24325v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaye Fu, Qiankun Gao, Chengxiang Wen, Yanmin Wu, Siwei Ma, Jiaqi Zhang, Jian Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online free-viewpoint video (FVV) reconstruction is challenged by slow
per-frame optimization, inconsistent motion estimation, and unsustainable
storage demands. To address these challenges, we propose the Reconfigurable
Continuum Gaussian Stream, dubbed ReCon-GS, a novel storage-aware framework
that enables high fidelity online dynamic scene reconstruction and real-time
rendering. Specifically, we dynamically allocate multi-level Anchor Gaussians
in a density-adaptive fashion to capture inter-frame geometric deformations,
thereby decomposing scene motion into compact coarse-to-fine representations.
Then, we design a dynamic hierarchy reconfiguration strategy that preserves
localized motion expressiveness through on-demand anchor re-hierarchization,
while ensuring temporal consistency through intra-hierarchical deformation
inheritance that confines transformation priors to their respective hierarchy
levels. Furthermore, we introduce a storage-aware optimization mechanism that
flexibly adjusts the density of Anchor Gaussians at different hierarchy levels,
enabling a controllable trade-off between reconstruction fidelity and memory
usage. Extensive experiments on three widely used datasets demonstrate that,
compared to state-of-the-art methods, ReCon-GS improves training efficiency by
approximately 15% and achieves superior FVV synthesis quality with enhanced
robustness and stability. Moreover, at equivalent rendering quality, ReCon-GS
slashes memory requirements by over 50% compared to leading state-of-the-art
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TRUST-VL: An Explainable News Assistant for General Multimodal
  Misinformation Detection <span class="chip">EMNLP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04448v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04448v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zehong Yan, Peng Qi, Wynne Hsu, Mong Li Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal misinformation, encompassing textual, visual, and cross-modal
distortions, poses an increasing societal threat that is amplified by
generative AI. Existing methods typically focus on a single type of distortion
and struggle to generalize to unseen scenarios. In this work, we observe that
different distortion types share common reasoning capabilities while also
requiring task-specific skills. We hypothesize that joint training across
distortion types facilitates knowledge sharing and enhances the model's ability
to generalize. To this end, we introduce TRUST-VL, a unified and explainable
vision-language model for general multimodal misinformation detection. TRUST-VL
incorporates a novel Question-Aware Visual Amplifier module, designed to
extract task-specific visual features. To support training, we also construct
TRUST-Instruct, a large-scale instruction dataset containing 198K samples
featuring structured reasoning chains aligned with human fact-checking
workflows. Extensive experiments on both in-domain and zero-shot benchmarks
demonstrate that TRUST-VL achieves state-of-the-art performance, while also
offering strong generalization and interpretability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2025 Oral; Project Homepage:
  https://yanzehong.github.io/trust-vl/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PureKV: Plug-and-Play KV Cache Optimization with Spatial-Temporal Sparse
  Attention for Vision-Language Large Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25600v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25600v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhonghua Jiang, Kunxi Li, Yiyun Zhou, Sihao Liu, Zhaode Wang, Chengfei lv, Shengyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language Large Models (VLLMs) face significant efficiency challenges
when processing high-resolution inputs. The quadratic complexity in attention
and autoregressive generation, as well as the constantly growing key value (KV)
cache size, severely hinder the prefilling and decoding stages. Recent efforts
have attempted to compress KV cache by identifying and pruning KV cache of less
important tokens, but these methods typically rely on attention scores to
estimate token importance, making them incompatible with efficient attention
mechanisms such as FlashAttention and Sparse Attention, which do not explicitly
compute attention matrices. Moreover, existing methods overlook how sparse
attention, while accelerating the prefilling stage, alters the information
structure of the KV cache, thereby compromising the effectiveness of downstream
KV cache compression strategies. To address this issue, we propose PureKV, a
plug-and-play framework for joint optimization of sparse attention and KV cache
compression. We first introduce a KV cache compression strategy that is fully
compatible with efficient attention accelerators. Our method utilizes lower
layer attention scores to estimate the importance of high layers' KV cache,
enabling active pruning without compromising accuracy. In addition, we have
designed a Spatial-Temporal Sparse Attention (ST-SpAttn) module specifically
tailored for video KV cache compression algorithms. This module combines
spatial and temporal attention sparsity to improve the compression efficiency
of KV cache optimization algorithms by purifying spatial noise and temporal
redundancy in KV cache. At the same time, ST-SpAttn also accelerated the
prefilling stage of VLLMs. Extensive experiments on VLLMs (VideoLLaMA2,
Qwen2.5-VL) have shown that PureKV achieves 5.0 times KV cache compression and
3.16 times prefill acceleration, with negligible quality degradation.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-10-29T00:00:00Z">2025-10-29</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">126</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gaperon: A Peppered English-French Generative Language Model Suite 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25771v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25771v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathan Godey, Wissam Antoun, Rian Touchent, Rachel Bawden, Éric de la Clergerie, Benoît Sagot, Djamé Seddah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We release Gaperon, a fully open suite of French-English-coding language
models designed to advance transparency and reproducibility in large-scale
model training. The Gaperon family includes 1.5B, 8B, and 24B parameter models
trained on 2-4 trillion tokens, released with all elements of the training
pipeline: French and English datasets filtered with a neural quality
classifier, an efficient data curation and training framework, and hundreds of
intermediate checkpoints. Through this work, we study how data filtering and
contamination interact to shape both benchmark and generative performance. We
find that filtering for linguistic quality enhances text fluency and coherence
but yields subpar benchmark results, and that late deliberate contamination --
continuing training on data mixes that include test sets -- recovers
competitive scores while only reasonably harming generation quality. We discuss
how usual neural filtering can unintentionally amplify benchmark leakage. To
support further research, we also introduce harmless data poisoning during
pretraining, providing a realistic testbed for safety studies. By openly
releasing all models, datasets, code, and checkpoints, Gaperon establishes a
reproducible foundation for exploring the trade-offs between data curation,
evaluation, safety, and openness in multilingual language model development.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decomposition-Enhanced Training for Post-Hoc Attributions In Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25766v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25766v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sriram Balasubramaniam, Samyadeep Basu, Koustava Goswami, Ryan Rossi, Varun Manjunatha, Roshan Santhosh, Ruiyi Zhang, Soheil Feizi, Nedim Lipka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are increasingly used for long-document question
answering, where reliable attribution to sources is critical for trust.
Existing post-hoc attribution methods work well for extractive QA but struggle
in multi-hop, abstractive, and semi-extractive settings, where answers
synthesize information across passages. To address these challenges, we argue
that post-hoc attribution can be reframed as a reasoning problem, where answers
are decomposed into constituent units, each tied to specific context. We first
show that prompting models to generate such decompositions alongside
attributions improves performance. Building on this, we introduce DecompTune, a
post-training method that teaches models to produce answer decompositions as
intermediate reasoning steps. We curate a diverse dataset of complex QA tasks,
annotated with decompositions by a strong LLM, and post-train Qwen-2.5 (7B and
14B) using a two-stage SFT + GRPO pipeline with task-specific curated rewards.
Across extensive experiments and ablations, DecompTune substantially improves
attribution quality, outperforming prior methods and matching or exceeding
state-of-the-art frontier models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Post-hoc attribution</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiagramEval: Evaluating LLM-Generated Diagrams via Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25761v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25761v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chumeng Liang, Jiaxuan You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diagrams play a central role in research papers for conveying ideas, yet they
are often notoriously complex and labor-intensive to create. Although diagrams
are presented as images, standard image generative models struggle to produce
clear diagrams with well-defined structure. We argue that a promising direction
is to generate demonstration diagrams directly in textual form as SVGs, which
can leverage recent advances in large language models (LLMs). However, due to
the complexity of components and the multimodal nature of diagrams,
sufficiently discriminative and explainable metrics for evaluating the quality
of LLM-generated diagrams remain lacking. In this paper, we propose
DiagramEval, a novel evaluation metric designed to assess demonstration
diagrams generated by LLMs. Specifically, DiagramEval conceptualizes diagrams
as graphs, treating text elements as nodes and their connections as directed
edges, and evaluates diagram quality using two new groups of metrics: node
alignment and path alignment. For the first time, we effectively evaluate
diagrams produced by state-of-the-art LLMs on recent research literature,
quantitatively demonstrating the validity of our metrics. Furthermore, we show
how the enhanced explainability of our proposed metrics offers valuable
insights into the characteristics of LLM-generated diagrams. Code:
https://github.com/ulab-uiuc/diagram-eval.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Task Completion Agents are Not Ideal Collaborators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25744v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25744v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shannon Zejiang Shen, Valerie Chen, Ken Gu, Alexis Ross, Zixian Ma, Jillian Ross, Alex Gu, Chenglei Si, Wayne Chi, Andi Peng, Jocelyn J Shen, Ameet Talwalkar, Tongshuang Wu, David Sontag
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current evaluations of agents remain centered around one-shot task
completion, failing to account for the inherently iterative and collaborative
nature of many real-world problems, where human goals are often underspecified
and evolve. We argue for a shift from building and assessing task completion
agents to developing collaborative agents, assessed not only by the quality of
their final outputs but by how well they engage with and enhance human effort
throughout the problem-solving process. To support this shift, we introduce
collaborative effort scaling, a framework that captures how an agent's utility
grows with increasing user involvement. Through case studies and simulated
evaluations, we show that state-of-the-art agents often underperform in
multi-turn, real-world scenarios, revealing a missing ingredient in agent
design: the ability to sustain engagement and scaffold user understanding.
Collaborative effort scaling offers a lens for diagnosing agent behavior and
guiding development toward more effective interactions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 5 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Scaling Latent Reasoning via Looped Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25741v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25741v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui-Jie Zhu, Zixuan Wang, Kai Hua, Tianyu Zhang, Ziniu Li, Haoran Que, Boyi Wei, Zixin Wen, Fan Yin, He Xing, Lu Li, Jiajun Shi, Kaijing Ma, Shanda Li, Taylor Kergan, Andrew Smith, Xingwei Qu, Mude Hui, Bohong Wu, Qiyang Min, Hongzhi Huang, Xun Zhou, Wei Ye, Jiaheng Liu, Jian Yang, Yunfeng Shi, Chenghua Lin, Enduo Zhao, Tianle Cai, Ge Zhang, Wenhao Huang, <span class="highlight-author">Yoshua Bengio</span>, Jason Eshraghian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern LLMs are trained to "think" primarily via explicit text generation,
such as chain-of-thought (CoT), which defers reasoning to post-training and
under-leverages pre-training data. We present and open-source Ouro, named after
the recursive Ouroboros, a family of pre-trained Looped Language Models
(LoopLM) that instead build reasoning into the pre-training phase through (i)
iterative computation in latent space, (ii) an entropy-regularized objective
for learned depth allocation, and (iii) scaling to 7.7T tokens. Ouro 1.4B and
2.6B models enjoy superior performance that match the results of up to 12B SOTA
LLMs across a wide range of benchmarks. Through controlled experiments, we show
this advantage stems not from increased knowledge capacity, but from superior
knowledge manipulation capabilities. We also show that LoopLM yields reasoning
traces more aligned with final outputs than explicit CoT. We hope our results
show the potential of LoopLM as a novel scaling direction in the reasoning era.
Our model could be found in: http://ouro-llm.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Limits of Obliviate: Evaluating Unlearning in LLMs via
  Stimulus-Knowledge Entanglement-Behavior Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25732v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25732v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aakriti Shah, Thai Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unlearning in large language models (LLMs) is crucial for managing sensitive
data and correcting misinformation, yet evaluating its effectiveness remains an
open problem. We investigate whether persuasive prompting can recall factual
knowledge from deliberately unlearned LLMs across models ranging from 2.7B to
13B parameters (OPT-2.7B, LLaMA-2-7B, LLaMA-3.1-8B, LLaMA-2-13B). Drawing from
ACT-R and Hebbian theory (spreading activation theories), as well as
communication principles, we introduce Stimulus-Knowledge Entanglement-Behavior
Framework (SKeB), which models information entanglement via domain graphs and
tests whether factual recall in unlearned models is correlated with persuasive
framing. We develop entanglement metrics to quantify knowledge activation
patterns and evaluate factuality, non-factuality, and hallucination in outputs.
Our results show persuasive prompts substantially enhance factual knowledge
recall (14.8% baseline vs. 24.5% with authority framing), with effectiveness
inversely correlated to model size (128% recovery in 2.7B vs. 15% in 13B). SKeB
provides a foundation for assessing unlearning completeness, robustness, and
overall behavior in LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Tool Decathlon: Benchmarking Language Agents for Diverse, Realistic,
  and Long-Horizon Task Execution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25726v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25726v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junlong Li, Wenshuo Zhao, Jian Zhao, Weihao Zeng, Haoze Wu, Xiaochen Wang, Rui Ge, Yuxuan Cao, Yuzhen Huang, Wei Liu, Junteng Liu, Zhaochen Su, Yiyang Guo, Fan Zhou, Lueyang Zhang, Juan Michelini, Xingyao Wang, Xiang Yue, Shuyan Zhou, Graham Neubig, Junxian He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world language agents must handle complex, multi-step workflows across
diverse Apps. For instance, an agent may manage emails by coordinating with
calendars and file systems, or monitor a production database to detect
anomalies and generate reports following an operating manual. However, existing
language agent benchmarks often focus on narrow domains or simplified tasks
that lack the diversity, realism, and long-horizon complexity required to
evaluate agents' real-world performance. To address this gap, we introduce the
Tool Decathlon (dubbed as Toolathlon), a benchmark for language agents offering
diverse Apps and tools, realistic environment setup, and reliable
execution-based evaluation. Toolathlon spans 32 software applications and 604
tools, ranging from everyday platforms such as Google Calendar and Notion to
professional ones like WooCommerce, Kubernetes, and BigQuery. Most of the tools
are based on a high-quality set of Model Context Protocol (MCP) servers that we
may have revised or implemented ourselves. Unlike prior works, which primarily
ensure functional realism but offer limited environment state diversity, we
provide realistic initial environment states from real software, such as Canvas
courses with dozens of students or real financial spreadsheets. This benchmark
includes 108 manually sourced or crafted tasks in total, requiring interacting
with multiple Apps over around 20 turns on average to complete. Each task is
strictly verifiable through dedicated evaluation scripts. Comprehensive
evaluation of SOTA models highlights their significant shortcomings: the
best-performing model, Claude-4.5-Sonnet, achieves only a 38.6% success rate
with 20.2 tool calling turns on average, while the top open-weights model
DeepSeek-V3.2-Exp reaches 20.1%. We expect Toolathlon to drive the development
of more capable language agents for real-world, long-horizon task execution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Website: https://toolathlon.xyz/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpreting LLMs as Credit Risk Classifiers: Do Their Feature
  Explanations Align with Classical ML? <span class="chip">CIKM 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25701v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25701v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saeed AlMarri, Kristof Juhasz, Mathieu Ravaut, Gautier Marti, Hamdan Al Ahbabi, Ibrahim Elfadel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are increasingly explored as flexible
alternatives to classical machine learning models for classification tasks
through zero-shot prompting. However, their suitability for structured tabular
data remains underexplored, especially in high-stakes financial applications
such as financial risk assessment. This study conducts a systematic comparison
between zero-shot LLM-based classifiers and LightGBM, a state-of-the-art
gradient-boosting model, on a real-world loan default prediction task. We
evaluate their predictive performance, analyze feature attributions using SHAP,
and assess the reliability of LLM-generated self-explanations. While LLMs are
able to identify key financial risk indicators, their feature importance
rankings diverge notably from LightGBM, and their self-explanations often fail
to align with empirical SHAP attributions. These findings highlight the
limitations of LLMs as standalone models for structured financial risk
prediction and raise concerns about the trustworthiness of their self-generated
explanations. Our results underscore the need for explainability audits,
baseline comparisons with interpretable models, and human-in-the-loop oversight
when deploying LLMs in risk-sensitive financial environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures, 3 tables, CIKM 2025 FinFAI workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Process-Level Trajectory Evaluation for Environment Configuration in
  Software Engineering Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25694v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25694v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayi Kuang, Yinghui Li, Xin Zhang, Yangning Li, Di Yin, Xing Sun, Ying Shen, Philip S. Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language model-based agents show promise for software engineering, but
environment configuration remains a bottleneck due to heavy manual effort and
scarce large-scale, high-quality datasets. Existing benchmarks assess only
end-to-end build/test success, obscuring where and why agents succeed or fail.
We introduce the Environment Configuration Diagnosis Benchmark, Enconda-bench,
which provides process-level trajectory assessment of fine-grained agent
capabilities during environment setup-planning, perception-driven error
diagnosis, feedback-driven repair, and action to execute final environment
configuration. Our task instances are automatically constructed by injecting
realistic README errors and are validated in Docker for scalable, high-quality
evaluation. Enconda-bench combines process-level analysis with end-to-end
executability to enable capability assessments beyond aggregate success rates.
Evaluations across state-of-the-art LLMs and agent frameworks show that while
agents can localize errors, they struggle to translate feedback into effective
corrections, limiting end-to-end performance. To our knowledge, Enconda-bench
is the first framework to provide process-level internal capability assessment
for environment configuration, offering actionable insights for improving
software engineering agents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PairUni: Pairwise Training for Unified Multimodal Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25682v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25682v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiani Zheng, Zhiyang Teng, Xiangtai Li, Anran Wang, Yu Tian, Kunpeng Qiu, Ye Tian, Haochen Wang, Zhuochen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unified vision-language models (UVLMs) must perform both understanding and
generation within a single architecture, but these tasks rely on heterogeneous
data and supervision, making it difficult to balance them during reinforcement
learning (RL). We propose PairUni, a unified framework that reorganizes data
into understanding-generation (UG) pairs and aligns optimization accordingly.
We first use GPT-o3 to augment single-task data, generating captions for
understanding samples and question-answer (QA) pairs for generation samples,
forming aligned pairs from the same instance. Additionally, for each generation
sample, we retrieve a semantically related understanding example to form a
retrieved pair, linking different but related data points. These paired
structures expose cross-task semantic correspondences and support consistent
policy learning. To leverage this structure, we present Pair-GPRO, a pair-aware
variant based on Group Relative Policy Optimization. It assigns a similarity
score to each pair to modulate the advantage, strengthening learning from
well-aligned examples and reducing task interference. We curate a high-quality
dataset of 16K UG pairs named PairUG for RL fine-tuning and evaluate PairUni on
the powerful Janus-Pro UVLMs. Our approach achieves balanced improvements on
various UVLMs, outperforming strong UVLM RL baselines. Code:
\href{https://github.com/Haochen-Wang409/PairUni}{github.com/Haochen-Wang409/PairUni}
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ZK-SenseLM: Verifiable Large-Model Wireless Sensing with Selective
  Abstention and Zero-Knowledge Attestation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25677v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25677v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hasan Akgul, Mari Eplik, Javier Rojas, Aina Binti Abdullah, Pieter van der Merwe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ZK-SenseLM is a secure and auditable wireless sensing framework that pairs a
large-model encoder for Wi-Fi channel state information (and optionally mmWave
radar or RFID) with a policy-grounded decision layer and end-to-end
zero-knowledge proofs of inference. The encoder uses masked spectral
pretraining with phase-consistency regularization, plus a light cross-modal
alignment that ties RF features to compact, human-interpretable policy tokens.
To reduce unsafe actions under distribution shift, we add a calibrated
selective-abstention head; the chosen risk-coverage operating point is
registered and bound into the proof. We implement a four-stage proving
pipeline: (C1) feature sanity and commitment, (C2) threshold and version
binding, (C3) time-window binding, and (C4) PLONK-style proofs that the
quantized network, given the committed window, produced the logged action and
confidence. Micro-batched proving amortizes cost across adjacent windows, and a
gateway option offloads proofs from low-power devices. The system integrates
with differentially private federated learning and on-device personalization
without weakening verifiability: model hashes and the registered threshold are
part of each public statement. Across activity, presence or intrusion,
respiratory proxy, and RF fingerprinting tasks, ZK-SenseLM improves macro-F1
and calibration, yields favorable coverage-risk curves under perturbations, and
rejects tamper and replay with compact proofs and fast verification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>45 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EHR-R1: A Reasoning-Enhanced Foundational Language Model for Electronic
  Health Record Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25628v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25628v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusheng Liao, Chaoyi Wu, Junwei Liu, Shuyang Jiang, Pengcheng Qiu, Haowen Wang, Yun Yue, Shuai Zhen, Jian Wang, Qianrui Fan, Jinjie Gu, Ya Zhang, Yanfeng Wang, Yu Wang, Weidi Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electronic Health Records (EHRs) contain rich yet complex information, and
their automated analysis is critical for clinical decision-making. Despite
recent advances of large language models (LLMs) in clinical workflows, their
ability to analyze EHRs remains limited due to narrow task coverage and lack of
EHR-oriented reasoning capabilities. This paper aims to bridge the gap,
specifically, we present EHR-Ins, a large-scale, comprehensive EHR reasoning
instruction dataset, comprising 300k high-quality reasoning cases and 4M
non-reasoning cases across 42 distinct EHR tasks. Its core innovation is a
thinking-graph-driven framework that enables to generate high-quality reasoning
data at scale. Based on it, we develop EHR-R1, a series of reasoning-enhanced
LLMs with up to 72B parameters tailored for EHR analysis. Through a multi-stage
training paradigm, including domain adaptation, reasoning enhancement, and
reinforcement learning, EHR-R1 systematically acquires domain knowledge and
diverse reasoning capabilities, enabling accurate and robust EHR analysis.
Lastly, we introduce EHR-Bench, a new benchmark curated from MIMIC-IV, spanning
42 tasks, to comprehensively assess reasoning and prediction across EHR
scenarios. In experiments, we show that the resulting EHR-R1 consistently
outperforms state-of-the-art commercial and open-source LLMs (including
DeepSeek-V3 and GPT-4o), surpassing GPT-4o by over 30 points on MIMIC-Bench and
achieving a 10\% higher zero-shot AUROC on EHRSHOT. Collectively, EHR-Ins,
EHR-R1, and EHR-Bench have significantly advanced the development for more
reliable and clinically relevant EHR analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are Language Models Efficient Reasoners? A Perspective from Logic
  Programming <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25626v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25626v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Opedal, Yanick Zengaffinen, Haruki Shirakami, Clemente Pasti, Mrinmaya Sachan, Abulhair Saparov, Ryan Cotterell, Bernhard Schölkopf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern language models (LMs) exhibit strong deductive reasoning capabilities,
yet standard evaluations emphasize correctness while overlooking a key aspect
of human-like reasoning: efficiency. In real-world reasoning scenarios, much of
the available information is irrelevant, and effective deductive inference
requires identifying and ignoring such distractions. We propose a framework for
assessing LM reasoning efficiency through the lens of logic programming,
introducing a simple method to align proofs written in natural language -- as
generated by an LM -- with shortest proofs found by executing the logic
program. Efficiency is quantified by measuring how well a model avoids
unnecessary inference. Empirically, we construct a dataset of math word
problems injected with various number of irrelevant axioms that vary in
semantic overlap with the goal theorem. We find that current LMs show marked
accuracy declines under such conditions -- even with minimal, domain-consistent
distractions -- and the proofs they generate frequently exhibit detours through
irrelevant inferences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating the Role of Verifiers in Test-Time Scaling for Legal
  Reasoning Tasks <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25623v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25623v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Davide Romano, Jonathan Schwarz, Daniele Giofré
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Test-time scaling (TTS) techniques can improve the performance of large
language models (LLMs) at the expense of additional computation and latency.
While TTS has proven effective in formal domains such as mathematics and
programming \citep{snell2024scaling, chen2024more}, its value in argumentative
domains such as law remains underexplored. We present an empirical study of
verifier-based TTS methods for legal multiple-choice QA (MCQA) across five
benchmarks. Using a family of 7 reward models, we evaluate both outcome-level
(Best-of-$N$) and process-level (tree search) verification under realistic
low-$N$ budgets. Our analysis systematically investigates how verifier utility
is affected by key properties such as domain specialization, model size, and
supervision type (process-supervised PRMs vs. outcome-only ORMs), even when
applied across different roles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP - NLLP Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FARSIQA: Faithful and Advanced RAG System for Islamic Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25621v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25621v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Aghajani Asl, Behrooz Minaei Bidgoli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of Large Language Models (LLMs) has revolutionized Natural
Language Processing, yet their application in high-stakes, specialized domains
like religious question answering is hindered by challenges like hallucination
and unfaithfulness to authoritative sources. This issue is particularly
critical for the Persian-speaking Muslim community, where accuracy and
trustworthiness are paramount. Existing Retrieval-Augmented Generation (RAG)
systems, relying on simplistic single-pass pipelines, fall short on complex,
multi-hop queries requiring multi-step reasoning and evidence aggregation. To
address this gap, we introduce FARSIQA, a novel, end-to-end system for Faithful
Advanced Question Answering in the Persian Islamic domain. FARSIQA is built
upon our innovative FAIR-RAG architecture: a Faithful, Adaptive, Iterative
Refinement framework for RAG. FAIR-RAG employs a dynamic, self-correcting
process: it adaptively decomposes complex queries, assesses evidence
sufficiency, and enters an iterative loop to generate sub-queries,
progressively filling information gaps. Operating on a curated knowledge base
of over one million authoritative Islamic documents, FARSIQA demonstrates
superior performance. Rigorous evaluation on the challenging IslamicPCQA
benchmark shows state-of-the-art performance: the system achieves a remarkable
97.0% in Negative Rejection - a 40-point improvement over baselines - and a
high Answer Correctness score of 74.3%. Our work establishes a new standard for
Persian Islamic QA and validates that our iterative, adaptive architecture is
crucial for building faithful, reliable AI systems in sensitive domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages, 5 figures, 10 tables. Keywords: Retrieval-Augmented
  Generation (RAG), Question Answering (QA), Islamic Knowledge Base, Faithful
  AI, Persian NLP, Multi-hop Reasoning, Large Language Models (LLMs)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Communication and Verification in LLM Agents towards Collaboration under
  Information Asymmetry <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25595v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25595v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Run Peng, Ziqiao Ma, Amy Pang, Sikai Li, Zhang Xi-Jia, Yingzhuo Yu, Cristian-Paul Bara, Joyce Chai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Large Language Model (LLM) agents are often approached from the angle
of action planning/generation to accomplish a goal (e.g., given by language
descriptions), their abilities to collaborate with each other to achieve a
joint goal are not well explored. To address this limitation, this paper
studies LLM agents in task collaboration, particularly under the condition of
information asymmetry, where agents have disparities in their knowledge and
skills and need to work together to complete a shared task. We extend Einstein
Puzzles, a classical symbolic puzzle, to a table-top game. In this game, two
LLM agents must reason, communicate, and act to satisfy spatial and relational
constraints required to solve the puzzle. We apply a fine-tuning-plus-verifier
framework in which LLM agents are equipped with various communication
strategies and verification signals from the environment. Empirical results
highlight the critical importance of aligned communication, especially when
agents possess both information-seeking and -providing capabilities.
Interestingly, agents without communication can still achieve high task
performance; however, further analysis reveals a lack of true rule
understanding and lower trust from human evaluators. Instead, by integrating an
environment-based verifier, we enhance agents' ability to comprehend task rules
and complete tasks, promoting both safer and more interpretable collaboration
in AI systems. https://github.com/Roihn/EinsteinPuzzles
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Workshop on Multi-Agent System @ ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lost in Phonation: Voice Quality Variation as an Evaluation Dimension
  for Speech Foundation Models <span class="chip">LREC 2026</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25577v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25577v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harm Lameris, Shree Harsha Bokkahalli Satish, Joakim Gustafson, Éva Székely
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in speech foundation models (SFMs) have enabled the direct
processing of spoken language from raw audio, bypassing intermediate textual
representations. This capability allows SFMs to be exposed to, and potentially
respond to, rich paralinguistic variations embedded in the input speech signal.
One under-explored dimension of paralinguistic variation is voice quality,
encompassing phonation types such as creaky and breathy voice. These phonation
types are known to influence how listeners infer affective state, stance and
social meaning in speech. Existing benchmarks for speech understanding largely
rely on multiple-choice question answering (MCQA) formats, which are prone to
failure and therefore unreliable in capturing the nuanced ways paralinguistic
features influence model behaviour. In this paper, we probe SFMs through
open-ended generation tasks and speech emotion recognition, evaluating whether
model behaviours are consistent across different phonation inputs. We introduce
a new parallel dataset featuring synthesized modifications to voice quality,
designed to evaluate SFM responses to creaky and breathy voice. Our work
provides the first examination of SFM sensitivity to these particular
non-lexical aspects of speech perception.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 3 figures, 4 tables, submitted to LREC 2026</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hybrid Quantum-Classical Recurrent Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25557v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25557v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenduan Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a hybrid quantum-classical recurrent neural network (QRNN)
architecture in which the entire recurrent core is realized as a parametrized
quantum circuit (PQC) controlled by a classical feedforward network. The hidden
state is the quantum state of an $n$-qubit PQC, residing in an exponentially
large Hilbert space $\mathbb{C}^{2^n}$. The PQC is unitary by construction,
making the hidden-state evolution norm-preserving without external constraints.
At each timestep, mid-circuit readouts are combined with the input embedding
and processed by the feedforward network, which provides explicit classical
nonlinearity. The outputs parametrize the PQC, which updates the hidden state
via unitary dynamics. The QRNN is compact and physically consistent, and it
unifies (i) unitary recurrence as a high-capacity memory, (ii) partial
observation via mid-circuit measurements, and (iii) nonlinear classical control
for input-conditioned parametrization. We evaluate the model in simulation with
up to 14 qubits on sentiment analysis, MNIST, permuted MNIST, copying memory,
and language modeling, adopting projective measurements as a limiting case to
obtain mid-circuit readouts while maintaining a coherent recurrent quantum
memory. We further devise a soft attention mechanism over the mid-circuit
readouts in a sequence-to-sequence model and show its effectiveness for machine
translation. To our knowledge, this is the first model (RNN or otherwise)
grounded in quantum operations to achieve competitive performance against
strong classical baselines across a broad class of sequence-learning tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TwinVoice: A Multi-dimensional Benchmark Towards Digital Twins via LLM
  Persona Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25536v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25536v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bangde Du, Minghao Guo, Songming He, Ziyi Ye, Xi Zhu, Weihang Su, Shuqi Zhu, Yujia Zhou, Yongfeng Zhang, Qingyao Ai, Yiqun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are exhibiting emergent human-like abilities and
are increasingly envisioned as the foundation for simulating an individual's
communication style, behavioral tendencies, and personality traits. However,
current evaluations of LLM-based persona simulation remain limited: most rely
on synthetic dialogues, lack systematic frameworks, and lack analysis of the
capability requirement. To address these limitations, we introduce TwinVoice, a
comprehensive benchmark for assessing persona simulation across diverse
real-world contexts. TwinVoice encompasses three dimensions: Social Persona
(public social interactions), Interpersonal Persona (private dialogues), and
Narrative Persona (role-based expression). It further decomposes the evaluation
of LLM performance into six fundamental capabilities, including opinion
consistency, memory recall, logical reasoning, lexical fidelity, persona tone,
and syntactic style. Experimental results reveal that while advanced models
achieve moderate accuracy in persona simulation, they still fall short of
capabilities such as syntactic style and memory recall. Consequently, the
average performance achieved by LLMs remains considerably below the human
baseline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Main paper: 11 pages, 3 figures, 6 tables. Appendix: 28 pages. Bangde
  Du and Minghao Guo contributed equally. Corresponding authors: Ziyi Ye
  (ziyiye@fudan.edu.cn), Qingyao Ai (aiqy@tsinghua.edu.cn)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fine-Tuned Language Models for Domain-Specific Summarization and Tagging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25460v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25460v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Wang, Fuming Lin, Yuyu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a pipeline integrating fine-tuned large language models
(LLMs) with named entity recognition (NER) for efficient domain-specific text
summarization and tagging. The authors address the challenge posed by rapidly
evolving sub-cultural languages and slang, which complicate automated
information extraction and law enforcement monitoring. By leveraging the LLaMA
Factory framework, the study fine-tunes LLMs on both generalpurpose and custom
domain-specific datasets, particularly in the political and security domains.
The models are evaluated using BLEU and ROUGE metrics, demonstrating that
instruction fine-tuning significantly enhances summarization and tagging
accuracy, especially for specialized corpora. Notably, the LLaMA3-8B-Instruct
model, despite its initial limitations in Chinese comprehension, outperforms
its Chinese-trained counterpart after domainspecific fine-tuning, suggesting
that underlying reasoning capabilities can transfer across languages. The
pipeline enables concise summaries and structured entity tagging, facilitating
rapid document categorization and distribution. This approach proves scalable
and adaptable for real-time applications, supporting efficient information
management and the ongoing need to capture emerging language trends. The
integration of LLMs and NER offers a robust solution for transforming
unstructured text into actionable insights, crucial for modern knowledge
management and security operations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Grounded in Reality: Learning and Deploying Proactive LLM from Offline
  Logs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25441v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25441v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fei Wei, Daoyuan Chen, Ce Wang, Yilun Huang, Yushuo Chen, Xuchen Pan, Yaliang Li, Bolin Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) excel as passive responders, but teaching them
to be proactive, goal-oriented partners, a critical capability in high-stakes
domains, remains a major challenge. Current paradigms either myopically
optimize single-turn attributes or rely on brittle, high-cost user simulators,
creating a persistent ``reality gap''. To bridge this gap, we introduce
\texttt{Learn-to-Ask}, a general, simulator-free framework for learning and
deploying proactive dialogue agents \textit{directly from offline expert data},
bypassing the need to model complex user dynamics. Our key insight is to
reframe the offline policy learning problem by leveraging the \textbf{observed
future} of each expert trajectory. This allows us to infer a dense,
turn-by-turn reward signal grounded in the expert's revealed strategy,
decomposing the intractable long-horizon problem into a series of supervised
learning tasks, and training a policy to output a structured \texttt{(action,
state_assessment)} tuple, governing both \textbf{what to ask} and, crucially,
\textbf{when to stop}. To ensure reward fidelity, our Automated Grader
Calibration pipeline systematically purges noise from the LLM-based reward
model with minimal human supervision. Empirically, we demonstrate the efficacy
of \texttt{Learn-to-Ask} in a real-world medical dataset, using LLMs of varying
sizes up to 32B. Our approach culminates in the successful deployment of LLMs
into a live, large-scale online AI service. In rigorous in-house evaluations,
our model was launched and achieved performance even superior to human experts,
proving our framework's ability to translate offline data into tangible,
real-world impact. We hope this work provides a practical and economically
viable blueprint for transforming passive LLMs into proactive, goal-oriented
LLM applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ More than a Moment: Towards Coherent Sequences of Audio Descriptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25440v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25440v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eshika Khandelwal, Junyu Xie, Tengda Han, Max Bain, Arsha Nagrani, Andrew Zisserman, Gül Varol, Makarand Tapaswi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio Descriptions (ADs) convey essential on-screen information, allowing
visually impaired audiences to follow videos. To be effective, ADs must form a
coherent sequence that helps listeners to visualise the unfolding scene, rather
than describing isolated moments. However, most automatic methods generate each
AD independently, often resulting in repetitive, incoherent descriptions. To
address this, we propose a training-free method, CoherentAD, that first
generates multiple candidate descriptions for each AD time interval, and then
performs auto-regressive selection across the sequence to form a coherent and
informative narrative. To evaluate AD sequences holistically, we introduce a
sequence-level metric, StoryRecall, which measures how well the predicted ADs
convey the ground truth narrative, alongside repetition metrics that capture
the redundancy across consecutive AD outputs. Our method produces coherent AD
sequences with enhanced narrative understanding, outperforming prior approaches
that rely on independent generations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Critical Study of Automatic Evaluation in Sign Language Translation <span class="chip">LREC 2026</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25434v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25434v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shakib Yazdani, Yasser Hamidullah, Cristina España-Bonet, Eleftherios Avramidis, Josef van Genabith
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic evaluation metrics are crucial for advancing sign language
translation (SLT). Current SLT evaluation metrics, such as BLEU and ROUGE, are
only text-based, and it remains unclear to what extent text-based metrics can
reliably capture the quality of SLT outputs. To address this gap, we
investigate the limitations of text-based SLT evaluation metrics by analyzing
six metrics, including BLEU, chrF, and ROUGE, as well as BLEURT on the one
hand, and large language model (LLM)-based evaluators such as G-Eval and GEMBA
zero-shot direct assessment on the other hand. Specifically, we assess the
consistency and robustness of these metrics under three controlled conditions:
paraphrasing, hallucinations in model outputs, and variations in sentence
length. Our analysis highlights the limitations of lexical overlap metrics and
demonstrates that while LLM-based evaluators better capture semantic
equivalence often missed by conventional metrics, they can also exhibit bias
toward LLM-paraphrased translations. Moreover, although all metrics are able to
detect hallucinations, BLEU tends to be overly sensitive, whereas BLEURT and
LLM-based evaluators are comparatively lenient toward subtle cases. This
motivates the need for multimodal evaluation frameworks that extend beyond
text-based metrics to enable a more holistic assessment of SLT outputs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to the LREC 2026 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Depth and Autonomy: A Framework for Evaluating LLM Applications in
  Social Science Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25432v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25432v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Sanaei, Ali Rajabzadeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are increasingly utilized by researchers across
a wide range of domains, and qualitative social science is no exception;
however, this adoption faces persistent challenges, including interpretive
bias, low reliability, and weak auditability. We introduce a framework that
situates LLM usage along two dimensions, interpretive depth and autonomy,
thereby offering a straightforward way to classify LLM applications in
qualitative research and to derive practical design recommendations. We present
the state of the literature with respect to these two dimensions, based on all
published social science papers available on Web of Science that use LLMs as a
tool and not strictly as the subject of study. Rather than granting models
expansive freedom, our approach encourages researchers to decompose tasks into
manageable segments, much as they would when delegating work to capable
undergraduate research assistants. By maintaining low levels of autonomy and
selectively increasing interpretive depth only where warranted and under
supervision, one can plausibly reap the benefits of LLMs while preserving
transparency and reliability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at the Annual Meeting of the American Political Science
  Association, Vancouver, BC, September 11--14 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RLMEval: Evaluating Research-Level Neural Theorem Proving <span class="chip">EMNLP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25427v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25427v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Auguste Poiroux, Antoine Bosselut, Viktor Kunčak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite impressive results on curated benchmarks, the practical impact of
large language models (LLMs) on research-level neural theorem proving and proof
autoformalization is still limited. We introduce RLMEval, an evaluation suite
for these tasks, focusing on research-level mathematics from real-world Lean
formalization projects. RLMEval targets the evaluation of neural theorem
proving and proof autoformalization on challenging research-level theorems by
leveraging real Lean Blueprint formalization projects. Our evaluation of
state-of-the-art models on RLMEval, comprising 613 theorems from 6 Lean
projects, reveals a significant gap: progress on existing benchmarks does not
readily translate to these more realistic settings, with the best model
achieving only a 10.3 % pass rate. RLMEval provides a new, challenging
benchmark designed to guide and accelerate progress in automated reasoning for
formal mathematics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2025 Findings. RLMEval benchmark released:
  https://github.com/augustepoiroux/RLMEval</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Implicature in Interaction: Understanding Implicature Improves Alignment
  in Human-LLM Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25426v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25426v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Asutosh Hota, Jussi P. P. Jokinen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of Large Language Models (LLMs) is positioning language
at the core of human-computer interaction (HCI). We argue that advancing HCI
requires attention to the linguistic foundations of interaction, particularly
implicature (meaning conveyed beyond explicit statements through shared
context) which is essential for human-AI (HAI) alignment. This study examines
LLMs' ability to infer user intent embedded in context-driven prompts and
whether understanding implicature improves response generation. Results show
that larger models approximate human interpretations more closely, while
smaller models struggle with implicature inference. Furthermore,
implicature-based prompts significantly enhance the perceived relevance and
quality of responses across models, with notable gains in smaller models.
Overall, 67.6% of participants preferred responses with implicature-embedded
prompts to literal ones, highlighting a clear preference for contextually
nuanced communication. Our work contributes to understanding how linguistic
theory can be used to address the alignment problem by making HAI interaction
more natural and contextually grounded.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The manuscript is approximately 7360 words and contains 12 figures
  and 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Seeing, Signing, and Saying: A Vision-Language Model-Assisted Pipeline
  for Sign Language Data Acquisition and Curation from Social Media 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25413v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25413v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shakib Yazdani, Yasser Hamidullah, Cristina España-Bonet, Josef van Genabith
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most existing sign language translation (SLT) datasets are limited in scale,
lack multilingual coverage, and are costly to curate due to their reliance on
expert annotation and controlled recording setup. Recently, Vision Language
Models (VLMs) have demonstrated strong capabilities as evaluators and real-time
assistants. Despite these advancements, their potential remains untapped in the
context of sign language dataset acquisition. To bridge this gap, we introduce
the first automated annotation and filtering framework that utilizes VLMs to
reduce reliance on manual effort while preserving data quality. Our method is
applied to TikTok videos across eight sign languages and to the already curated
YouTube-SL-25 dataset in German Sign Language for the purpose of additional
evaluation. Our VLM-based pipeline includes a face visibility detection, a sign
activity recognition, a text extraction from video content, and a judgment step
to validate alignment between video and text, implementing generic filtering,
annotation and validation steps. Using the resulting corpus, TikTok-SL-8, we
assess the performance of two off-the-shelf SLT models on our filtered dataset
for German and American Sign Languages, with the goal of establishing baselines
and evaluating the robustness of recent models on automatically extracted,
slightly noisy data. Our work enables scalable, weakly supervised pretraining
for SLT and facilitates data acquisition from social media.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by RANLP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Serve Programs, Not <span class="highlight-title">Prompt</span>s <span class="chip">SOSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25412v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25412v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        In Gim, Lin Zhong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current large language model (LLM) serving systems, primarily designed for
text completion, are neither efficient nor adaptable for increasingly complex
LLM applications due to their inflexible design. We propose a new LLM serving
system architecture that serves programs instead of prompts to address this
problem. These programs, called LLM Inference Programs (LIPs), allow users to
customize token prediction and KV cache management at runtime and to offload
parts of their application logic, such as tool execution, to the server. We
describe an example of this architecture through a system named Symphony, which
functions as an operating system for LIPs. Symphony exposes LLM model
computations via system calls and virtualizes KV cache with a dedicated file
system, while ensuring GPU efficiency with a two-level process scheduling
scheme. Symphony has the potential to open the door to a more efficient and
extensible ecosystem for LLM applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>HotOS 2025. Follow-up implementation work (SOSP 2025) is available at
  arXiv:2510.24051</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BhashaBench V1: A Comprehensive Benchmark for the Quadrant of Indic
  Domains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25409v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25409v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vijay Devane, Mohd Nauman, Bhargav Patel, Aniket Mahendra Wakchoure, Yogeshkumar Sant, Shyam Pawar, Viraj Thakur, Ananya Godse, Sunil Patra, Neha Maurya, Suraj Racha, Nitish Kamal Singh, Ajay Nagpal, Piyush Sawarkar, Kundeshwar Vijayrao Pundalik, Rohit Saluja, Ganesh Ramakrishnan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of large language models(LLMs) has intensified the need
for domain and culture specific evaluation. Existing benchmarks are largely
Anglocentric and domain-agnostic, limiting their applicability to India-centric
contexts. To address this gap, we introduce BhashaBench V1, the first
domain-specific, multi-task, bilingual benchmark focusing on critical Indic
knowledge systems. BhashaBench V1 contains 74,166 meticulously curated
question-answer pairs, with 52,494 in English and 21,672 in Hindi, sourced from
authentic government and domain-specific exams. It spans four major domains:
Agriculture, Legal, Finance, and Ayurveda, comprising 90+ subdomains and
covering 500+ topics, enabling fine-grained evaluation. Evaluation of 29+ LLMs
reveals significant domain and language specific performance gaps, with
especially large disparities in low-resource domains. For instance, GPT-4o
achieves 76.49% overall accuracy in Legal but only 59.74% in Ayurveda. Models
consistently perform better on English content compared to Hindi across all
domains. Subdomain-level analysis shows that areas such as Cyber Law,
International Finance perform relatively well, while Panchakarma, Seed Science,
and Human Rights remain notably weak. BhashaBench V1 provides a comprehensive
dataset for evaluating large language models across India's diverse knowledge
domains. It enables assessment of models' ability to integrate domain-specific
knowledge with bilingual understanding. All code, benchmarks, and resources are
publicly available to support open research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Roleplaying with Structure: Synthetic Therapist-Client Conversation
  Generation from Questionnaires 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25384v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25384v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Doan Nam Long Vu, Rui Tan, Lena Moench, Svenja Jule Francke, Daniel Woiwod, Florian Thomas-Odenthal, Sanna Stroth, Tilo Kircher, Christiane Hermann, Udo Dannlowski, Hamidreza Jamalabadi, Shaoxiong Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of AI for mental health is hindered by a lack of authentic
therapy dialogues, due to strict privacy regulations and the fact that clinical
sessions were historically rarely recorded. We present an LLM-driven pipeline
that generates synthetic counseling dialogues based on structured client
profiles and psychological questionnaires. Grounded on the principles of
Cognitive Behavioral Therapy (CBT), our method creates synthetic therapeutic
conversations for clinical disorders such as anxiety and depression. Our
framework, SQPsych (Structured Questionnaire-based Psychotherapy), converts
structured psychological input into natural language dialogues through
therapist-client simulations. Due to data governance policies and privacy
restrictions prohibiting the transmission of clinical questionnaire data to
third-party services, previous methodologies relying on proprietary models are
infeasible in our setting. We address this limitation by generating a
high-quality corpus using open-weight LLMs, validated through human expert
evaluation and LLM-based assessments. Our SQPsychLLM models fine-tuned on
SQPsychConv achieve strong performance on counseling benchmarks, surpassing
baselines in key therapeutic skills. Our findings highlight the potential of
synthetic data to enable scalable, data-secure, and clinically informed AI for
mental health support. We will release our code, models, and corpus at
https://ai-mh.github.io/SQPsych
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hallucinations in Bibliographic Recommendation: Citation Frequency as a
  Proxy for Training Data Redundancy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25378v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25378v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junichiro Niimi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have been increasingly applied to a wide range
of tasks, from natural language understanding to code generation. While they
have also been used to assist in bibliographic recommendation, the
hallucination of non-existent papers remains a major issue. Building on prior
studies, this study hypothesizes that an LLM's ability to correctly produce
bibliographic information depends on whether the underlying knowledge is
generated or memorized, with highly cited papers (i.e., more frequently appear
in the training corpus) showing lower hallucination rates. We therefore assume
citation count as a proxy for training data redundancy (i.e., the frequency
with which a given bibliographic record is repeatedly represented in the
pretraining corpus) and investigate how citation frequency affects hallucinated
references in LLM outputs. Using GPT-4.1, we generated and manually verified
100 bibliographic records across twenty computer-science domains, and measured
factual consistency via cosine similarity between generated and authentic
metadata. The results revealed that (i) hallucination rates vary across
research domains, (ii) citation count is strongly correlated with factual
accuracy, and (iii) bibliographic information becomes almost verbatimly
memorized beyond approximately 1,000 citations. These findings suggest that
highly cited papers are nearly verbatimly retained in the model, indicating a
threshold where generalization shifts into memorization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Monitoring Transformative Technological Convergence Through
  LLM-Extracted Semantic Entity Triple Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25370v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25370v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Sternfeld, Andrei Kucharavy, Dimitri Percia David, Alain Mermoud, Julian Jang-Jaccard, Nathan Monnet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Forecasting transformative technologies remains a critical but challenging
task, particularly in fast-evolving domains such as Information and
Communication Technologies (ICTs). Traditional expert-based methods struggle to
keep pace with short innovation cycles and ambiguous early-stage terminology.
In this work, we propose a novel, data-driven pipeline to monitor the emergence
of transformative technologies by identifying patterns of technological
convergence.
  Our approach leverages advances in Large Language Models (LLMs) to extract
semantic triples from unstructured text and construct a large-scale graph of
technology-related entities and relations. We introduce a new method for
grouping semantically similar technology terms (noun stapling) and develop
graph-based metrics to detect convergence signals. The pipeline includes
multi-stage filtering, domain-specific keyword clustering, and a temporal trend
analysis of topic co-occurence.
  We validate our methodology on two complementary datasets: 278,625 arXiv
preprints (2017--2024) to capture early scientific signals, and 9,793 USPTO
patent applications (2018-2024) to track downstream commercial developments.
Our results demonstrate that the proposed pipeline can identify both
established and emerging convergence patterns, offering a scalable and
generalizable framework for technology forecasting grounded in full-text
analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CLASS-IT: Conversational and Lecture-Aligned Small-Scale Instruction
  Tuning for BabyLMs <span class="chip">EMNLP2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25364v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25364v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Capone, Alessandro Bondielli, Alessandro Lenci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work investigates whether small-scale LMs can benefit from instruction
tuning. We compare conversational and question-answering instruction tuning
datasets, applied either in a merged or sequential curriculum, using
decoder-only models with 100M and 140M parameters. Evaluation spans both
fine-tuning (SuperGLUE) and zero-shot (BLiMP, EWoK, WUGs, entity tracking, and
psycholinguistic correlation) settings. Results show that instruction tuning
yields small but consistent gains in fine-tuning scenarios, with sequential
curricula outperforming merged data; however, improvements do not consistently
transfer to zero-shot tasks, suggesting a trade-off between interaction-focused
adaptation and broad linguistic generalization. These results highlight both
the potential and the constraints of adapting human-inspired learning
strategies to low-resource LMs, and point toward hybrid, curriculum-based
approaches for enhancing generalization under ecological training limits.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted for oral presentation at the BabyLM Challange 2025
  (EMNLP2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Not ready for the bench: LLM legal interpretation is unstable and out of
  step with human judgments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25356v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25356v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhishek Purushothama, Junghyun Min, Brandon Waldon, Nathan Schneider
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Legal interpretation frequently involves assessing how a legal text, as
understood by an 'ordinary' speaker of the language, applies to the set of
facts characterizing a legal dispute in the U.S. judicial system. Recent
scholarship has proposed that legal practitioners add large language models
(LLMs) to their interpretive toolkit. This work offers an empirical argument
against LLM interpretation as recently practiced by legal scholars and federal
judges. Our investigation in English shows that models do not provide stable
interpretive judgments: varying the question format can lead the model to
wildly different conclusions. Moreover, the models show weak to moderate
correlation with human judgment, with large variance across model and question
variant, suggesting that it is dangerous to give much credence to the
conclusions produced by generative AI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CRMWeaver: Building Powerful Business Agent via Agentic RL and Shared
  Memories 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25333v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25333v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilong Lai, Yipin Yang, Jialong Wu, Fengran Mo, Zhenglin Wang, Ting Liang, Jianguo Lin, Keping Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have witnessed the rapid development of LLM-based agents, which
shed light on using language agents to solve complex real-world problems. A
prominent application lies in business agents, which interact with databases
and internal knowledge bases via tool calls to fulfill diverse user
requirements. However, this domain is characterized by intricate data
relationships and a wide range of heterogeneous tasks, from statistical data
queries to knowledge-based question-answering. To address these challenges, we
propose CRMWeaver, a novel approach that enhances business agents in such
complex settings. To acclimate the agentic model to intricate business
environments, we employ a synthesis data generation and RL-based paradigm
during training, which significantly improves the model's ability to handle
complex data and varied tasks. During inference, a shared memories mechanism is
introduced, prompting the agent to learn from task guidelines in similar
problems, thereby further boosting its effectiveness and generalization,
especially in unseen scenarios. We validate the efficacy of our approach on the
CRMArena-Pro dataset, where our lightweight model achieves competitive results
in both B2B and B2C business scenarios, underscoring its practical value for
real-world applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GAP: Graph-Based Agent Planning with Parallel Tool Use and Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25320v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25320v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaqi Wu, Qinlao Zhao, Zefeng Chen, Kai Qin, Yifei Zhao, Xueqian Wang, Yuhang Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous agents powered by large language models (LLMs) have shown
impressive capabilities in tool manipulation for complex task-solving. However,
existing paradigms such as ReAct rely on sequential reasoning and execution,
failing to exploit the inherent parallelism among independent sub-tasks. This
sequential bottleneck leads to inefficient tool utilization and suboptimal
performance in multi-step reasoning scenarios. We introduce Graph-based Agent
Planning (GAP), a novel framework that explicitly models inter-task
dependencies through graph-based planning to enable adaptive parallel and
serial tool execution. Our approach trains agent foundation models to decompose
complex tasks into dependency-aware sub-task graphs, autonomously determining
which tools can be executed in parallel and which must follow sequential
dependencies. This dependency-aware orchestration achieves substantial
improvements in both execution efficiency and task accuracy. To train GAP, we
construct a high-quality dataset of graph-based planning traces derived from
the Multi-Hop Question Answering (MHQA) benchmark. We employ a two-stage
training strategy: supervised fine-tuning (SFT) on the curated dataset,
followed by reinforcement learning (RL) with a correctness-based reward
function on strategically sampled queries where tool-based reasoning provides
maximum value. Experimental results on MHQA datasets demonstrate that GAP
significantly outperforms traditional ReAct baselines, particularly on
multi-step retrieval tasks, while achieving dramatic improvements in tool
invocation efficiency through intelligent parallelization. The project page is
available at: https://github.com/WJQ7777/Graph-Agent-Planning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parrot: A Training Pipeline Enhances Both Program CoT and Natural
  Language CoT for Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25310v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25310v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Senjie Jin, Lu Chen, Zhiheng Xi, Yuhui Wang, Sirui Song, Yuhao Zhou, Xinbo Zhang, Peng Sun, Hong Lu, Tao Gui, Qi Zhang, Xuanjing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural language chain-of-thought (N-CoT) and Program chain-of-thought
(P-CoT) have emerged as two primary paradigms for large language models (LLMs)
to solve mathematical reasoning problems. Current research typically endeavors
to achieve unidirectional enhancement: P-CoT enhanced N-CoT or N-CoT enhanced
P-CoT. In this paper, we seek to fully unleash the two paradigms' strengths for
mutual enhancement and ultimately achieve simultaneous improvements. We conduct
a detailed analysis of the error types across two paradigms, based on which we
propose Parrot, a novel training pipeline for mathematical problems: 1) Three
target-designed subtasks integrate sequential P-CoT and N-CoT generation. 2) A
subtask hybrid training strategy to facilitate natural language semantic
transferability. 3) The converted N-CoT auxiliary reward is designed to
alleviate the sparse rewards in P-CoT optimization. Extensive experiments
demonstrate that Parrot significantly enhances both the performance of N-CoT
and P-CoT, especially on N-CoT. Using Parrot SFT, the N-CoT performance of
LLaMA2 and CodeLLaMA achieve gains of +21.87 and +21.48 on MathQA over the RL
baseline, which is resource-intensive.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Teaching Sarcasm: Few-Shot Multimodal Sarcasm Detection via Distillation
  to a Parameter-Efficient Student 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25303v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25303v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soumyadeep Jana, Sanasam Ranbir Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal sarcasm detection is challenging, especially in low-resource
settings where subtle image-text contradictions are hard to learn due to scarce
annotated data, which hinders the model's performance. Parameter-efficient
fine-tuning (PEFT) methods like adapters, LoRA, and prompt tuning reduce
overfitting but struggle to reach optimal performance due to limited
supervision from few-shot data. We propose PEKD, a unified framework that
enhances PEFT methods via distillation from an expert model trained on
large-scale sarcasm data, which acts as the teacher. To mitigate unreliable
signals from the teacher, we introduce an entropy-aware gating mechanism that
dynamically adjusts the distillation strength based on teacher confidence.
Experiments on two public datasets demonstrate that our PEKD framework enables
PEFT methods to outperform both prior parameter-efficient approaches and large
multimodal models, achieving strong results in the few-shot scenario. The
framework is modular and adaptable to a wide range of multimodal models and
tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adapting Small Language Models to Low-Resource Domains: A Case Study in
  Hindi Tourism QA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25273v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25273v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sandipan Majhi, Paheli Bhattacharya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain-specific question answering in low-resource languages faces two key
challenges: scarcity of annotated datasets and limited domain knowledge in
general-purpose language models. In this work, we present a multi-stage
finetuning strategy to adapt lightweight language models to the Hindi tourism
domain by leveraging both original and synthetic training data. Synthetic
question-answer pairs are generated using large LLMs (LLaMA-70B, Phi-14B) and
used to augment the limited original dataset. We explore several training
methodologies and analyse their impact on domain generalisation. Our results
demonstrate that large models can efficiently generate synthetic data, while
small models can effectively adapt to it, offering a scalable pathway for
low-resource, domain-specific QA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the Forum for Information Retrieval Evaluation 2025
  (VATIKA Track)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Medical Records to Diagnostic Dialogues: A Clinical-Grounded
  Approach and <span class="highlight-title">Dataset</span> for Psychiatric Comorbidity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25232v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25232v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianxi Wan, Jiaming Luo, Siyuan Chen, Kunyao Lan, Jianhua Chen, Haiyang Geng, Mengyue Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Psychiatric comorbidity is clinically significant yet challenging due to the
complexity of multiple co-occurring disorders. To address this, we develop a
novel approach integrating synthetic patient electronic medical record (EMR)
construction and multi-agent diagnostic dialogue generation. We create 502
synthetic EMRs for common comorbid conditions using a pipeline that ensures
clinical relevance and diversity. Our multi-agent framework transfers the
clinical interview protocol into a hierarchical state machine and context tree,
supporting over 130 diagnostic states while maintaining clinical standards.
Through this rigorous process, we construct PsyCoTalk, the first large-scale
dialogue dataset supporting comorbidity, containing 3,000 multi-turn diagnostic
dialogues validated by psychiatrists. This dataset enhances diagnostic accuracy
and treatment planning, offering a valuable resource for psychiatric
comorbidity research. Compared to real-world clinical transcripts, PsyCoTalk
exhibits high structural and linguistic fidelity in terms of dialogue length,
token distribution, and diagnostic reasoning strategies. Licensed psychiatrists
confirm the realism and diagnostic validity of the dialogues. This dataset
enables the development and evaluation of models capable of multi-disorder
psychiatric screening in a single conversational pass.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ProMediate: A Socio-cognitive framework for evaluating proactive agents
  in multi-party negotiation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25224v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25224v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyi Liu, Bahar Sarrafzadeh, Pei Zhou, Longqi Yang, Jieyu Zhao, Ashish Sharma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Large Language Models (LLMs) are increasingly used in agentic
frameworks to assist individual users, there is a growing need for agents that
can proactively manage complex, multi-party collaboration. Systematic
evaluation methods for such proactive agents remain scarce, limiting progress
in developing AI that can effectively support multiple people together.
Negotiation offers a demanding testbed for this challenge, requiring
socio-cognitive intelligence to navigate conflicting interests between multiple
participants and multiple topics and build consensus. Here, we present
ProMediate, the first framework for evaluating proactive AI mediator agents in
complex, multi-topic, multi-party negotiations. ProMediate consists of two core
components: (i) a simulation testbed based on realistic negotiation cases and
theory-driven difficulty levels (ProMediate-Easy, ProMediate-Medium, and
ProMediate-Hard), with a plug-and-play proactive AI mediator grounded in
socio-cognitive mediation theories, capable of flexibly deciding when and how
to intervene; and (ii) a socio-cognitive evaluation framework with a new suite
of metrics to measure consensus changes, intervention latency, mediator
effectiveness, and intelligence. Together, these components establish a
systematic framework for assessing the socio-cognitive intelligence of
proactive AI agents in multi-party settings. Our results show that a socially
intelligent mediator agent outperforms a generic baseline, via faster,
better-targeted interventions. In the ProMediate-Hard setting, our social
mediator increases consensus change by 3.6 percentage points compared to the
generic baseline (10.65\% vs 7.01\%) while being 77\% faster in response
(15.98s vs. 3.71s). In conclusion, ProMediate provides a rigorous,
theory-grounded testbed to advance the development of proactive, socially
intelligent agents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAVR: Reference-Answer-guided Variational Reasoning for Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25206v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25206v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianqianjin Lin, Xi Zhao, Xingyao Zhang, Rujiao Long, Yi Xu, Zhuoren Jiang, Wenbo Su, Bo Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) can refine the reasoning abilities of large
language models (LLMs), but critically depends on a key prerequisite: the LLM
can already generate high-utility reasoning paths with non-negligible
probability. For tasks beyond the LLM's current competence, such reasoning path
can be hard to sample, and learning risks reinforcing familiar but suboptimal
reasoning. We are motivated by the insight from cognitive science that Why is
this the answer is often an easier question than What is the answer, as it
avoids the heavy cognitive load of open-ended exploration, opting instead for
explanatory reconstruction-systematically retracing the reasoning that links a
question to its answer. We show that LLMs can similarly leverage answers to
derive high-quality reasoning paths. We formalize this phenomenon and prove
that conditioning on answer provably increases the expected utility of sampled
reasoning paths, thereby transforming intractable problems into learnable ones.
Building on this insight, we introduce RAVR (Reference-Answer-guided
Variational Reasoning), an end-to-end framework that uses answer-conditioned
reasoning as a variational surrogate for question-only reasoning. Experiments
in both general and math domains demonstrate consistent improvements over
strong baselines. We further analyze the reasoning behavior and find that RAVR
reduces hesitation, strengthens conclusion consolidation, and promotes
problem-specific strategies in reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Testing Cross-Lingual Text Comprehension In LLMs Using Next Sentence
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25187v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25187v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ritesh Sunil Chavan, Jack Mostow
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language models are trained on massive datasets, this data is
heavily skewed towards English. Does their impressive performance reflect
genuine ability or just this data advantage? To find out, we tested them in a
setting where they could not rely on data abundance: low-resource languages.
Building on prior work Agarwal et al. (2025) that used Next Sentence Prediction
(NSP) as a test, we created a large-scale benchmark with 10,000 questions each
for English (a high-resource language), Swahili (medium-resource), and Hausa
(low-resource). We then tested several top models, including GPT-4 Turbo,
Gemini 1.5 Flash, and LLaMA 3 70B, to see how their performance holds up. The
results painted a clear picture of how levels of language resources impact
outcomes. While all models excelled in English, their accuracy dropped in
Swahili and fell sharply in Hausa, with LLaMA 3 struggling the most. The story
became even more interesting when we introduced Chain-of-Thought (CoT)
prompting. For the struggling LLaMA 3, CoT acted as a helpful guide,
significantly boosting its accuracy. However, for the more capable GPT-4 and
Gemini, the same technique often backfired, leading to a kind of "overthinking"
that hurt their results in the cross-lingual context. This reveals that
Chain-of-Thought is not a universal solution; its effectiveness depends heavily
on the model's baseline capability and the specific context of the task. Our
framework pinpoints LLM weaknesses, highlights when CoT helps or hinders
cross-lingual NSP performance, and factors influencing their decisions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Model-Document Protocol for AI Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25160v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25160v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongjin Qian, Zheng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI search depends on linking large language models (LLMs) with vast external
knowledge sources. Yet web pages, PDF files, and other raw documents are not
inherently LLM-ready: they are long, noisy, and unstructured. Conventional
retrieval methods treat these documents as verbatim text and return raw
passages, leaving the burden of fragment assembly and contextual reasoning to
the LLM. This gap underscores the need for a new retrieval paradigm that
redefines how models interact with documents.
  We introduce the Model-Document Protocol (MDP), a general framework that
formalizes how raw text is bridged to LLMs through consumable knowledge
representations. Rather than treating retrieval as passage fetching, MDP
defines multiple pathways that transform unstructured documents into
task-specific, LLM-ready inputs. These include agentic reasoning, which curates
raw evidence into coherent context; memory grounding, which accumulates
reusable notes to enrich reasoning; and structured leveraging, which encodes
documents into formal representations such as graphs or key-value caches. All
three pathways share the same goal: ensuring that what reaches the LLM is not
raw fragments but compact, structured knowledge directly consumable for
reasoning.
  As an instantiation, we present MDP-Agent, which realizes the protocol
through an agentic process: constructing document-level gist memories for
global coverage, performing diffusion-based exploration with vertical
exploitation to uncover layered dependencies, and applying map-reduce style
synthesis to integrate large-scale evidence into compact yet sufficient
context. Experiments on information-seeking benchmarks demonstrate that
MDP-Agent outperforms baselines, validating both the soundness of the MDP
framework and the effectiveness of its agentic instantiation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explainable Disentanglement on Discrete Speech Representations for
  Noise-Robust ASR <span class="chip">SC 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25150v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25150v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shreyas Gopal, Ashutosh Anshul, Haoyang Li, Yue Heng Yeo, Hexin Liu, Eng Siong Chng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Discrete audio representations are gaining traction in speech modeling due to
their interpretability and compatibility with large language models, but are
not always optimized for noisy or real-world environments. Building on existing
works that quantize Whisper embeddings for speech-to-unit modeling, we propose
disentangling semantic speech content from background noise in the latent
space. Our end-to-end model separates clean speech in the form of codebook
tokens, while extracting interpretable noise vectors as quantization residue
which are supervised via a lightweight classifier. We show that our approach
improves alignment between clean/noisy speech and text, producing speech tokens
that display a high degree of noiseinvariance, and improves ASR performance.
Keeping Whisper frozen, we show an 82% reduction in error rate compared to
Whisper, and 35% improvement over baseline methods on the VBDemand test set.
Further analyses show that the learned token space generalizes well to both
seen and unseen acoustic conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Awarded Best Student Paper at APSIPA ASC 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on Unlearning in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25117v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25117v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruichen Qiu, Jiajun Tan, Jiayue Pu, Honglin Wang, Xiao-Shan Gao, Fei Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advancement of Large Language Models (LLMs) has revolutionized natural
language processing, yet their training on massive corpora poses significant
risks, including the memorization of sensitive personal data, copyrighted
material, and knowledge that could facilitate malicious activities. To mitigate
these issues and align with legal and ethical standards such as the "right to
be forgotten", machine unlearning has emerged as a critical technique to
selectively erase specific knowledge from LLMs without compromising their
overall performance. This survey provides a systematic review of over 180
papers on LLM unlearning published since 2021, focusing exclusively on
large-scale generative models. Distinct from prior surveys, we introduce novel
taxonomies for both unlearning methods and evaluations. We clearly categorize
methods into training-time, post-training, and inference-time based on the
training stage at which unlearning is applied. For evaluations, we not only
systematically compile existing datasets and metrics but also critically
analyze their advantages, disadvantages, and applicability, providing practical
guidance to the research community. In addition, we discuss key challenges and
promising future research directions. Our comprehensive overview aims to inform
and guide the ongoing development of secure and reliable LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Pretrain</span>ing Strategies using Monolingual and Parallel Data for
  Low-Resource Machine Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25116v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25116v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Idriss Nguepi Nguefack, Mara Finkelstein, Toadoum Sari Sakayo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research article examines the effectiveness of various pretraining
strategies for developing machine translation models tailored to low-resource
languages. Although this work considers several low-resource languages,
including Afrikaans, Swahili, and Zulu, the translation model is specifically
developed for Lingala, an under-resourced African language, building upon the
pretraining approach introduced by Reid and Artetxe (2021), originally designed
for high-resource languages. Through a series of comprehensive experiments, we
explore different pretraining methodologies, including the integration of
multiple languages and the use of both monolingual and parallel data during the
pretraining phase. Our findings indicate that pretraining on multiple languages
and leveraging both monolingual and parallel data significantly enhance
translation quality. This study offers valuable insights into effective
pretraining strategies for low-resource machine translation, helping to bridge
the performance gap between high-resource and low-resource languages. The
results contribute to the broader goal of developing more inclusive and
accurate NLP models for marginalized communities and underrepresented
populations. The code and datasets used in this study are publicly available to
facilitate further research and ensure reproducibility, with the exception of
certain data that may no longer be accessible due to changes in public
availability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 1. figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DEBATE: A Large-Scale Benchmark for Role-Playing LLM Agents in
  Multi-Agent, Long-Form Debates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25110v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25110v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yun-Shiuan Chuang, Ruixuan Tu, Chengtao Dai, Smit Vasani, Binwei Yao, Michael Henry Tessler, Sijia Yang, Dhavan Shah, Robert Hawkins, Junjie Hu, Timothy T. Rogers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately modeling opinion change through social interactions is crucial for
addressing issues like misinformation and polarization. While role-playing
large language models (LLMs) offer a promising way to simulate human-like
interactions, existing research shows that single-agent alignment does not
guarantee authentic multi-agent group dynamics. Current LLM role-play setups
often produce unnatural dynamics (e.g., premature convergence), without an
empirical benchmark to measure authentic human opinion trajectories. To bridge
this gap, we introduce DEBATE, the first large-scale empirical benchmark
explicitly designed to evaluate the authenticity of the interaction between
multi-agent role-playing LLMs. DEBATE contains 29,417 messages from multi-round
debate conversations among over 2,792 U.S.-based participants discussing 107
controversial topics, capturing both publicly-expressed messages and
privately-reported opinions. Using DEBATE, we systematically evaluate and
identify critical discrepancies between simulated and authentic group dynamics.
We further demonstrate DEBATE's utility for aligning LLMs with human behavior
through supervised fine-tuning, achieving improvements in surface-level metrics
(e.g., ROUGE-L and message length) while highlighting limitations in deeper
semantic alignment (e.g., semantic similarity). Our findings highlight both the
potential and current limitations of role-playing LLM agents for realistically
simulating human-like social dynamics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KnowCoder-A1: Incentivizing Agentic Reasoning Capability with Outcome
  Supervision for KBQA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25101v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25101v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuo Chen, Fei Wang, Zixuan Li, Zhao Zhang, Weiwei Ding, Chuanguang Yang, Yongjun Xu, Xiaolong Jin, Jiafeng Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge Base Question Answering (KBQA) aims to answer natural-language
questions over a structured Knowledge Base (KB). Recent work improves KBQA by
adopting an agentic reasoning paradigm, in which Large Language Models (LLMs)
iteratively decompose a question, generate its corresponding logical queries,
and interact with the KB to derive the answer. However, these methods typically
fine-tune LLMs on reasoning trajectories synthesized via process supervision,
which offers weak incentives for exploration and thus fails to strengthen the
agentic reasoning ability. In this paper, we propose KnowCoder-A1, an LLM that
can autonomously perform agentic reasoning on KBs to obtain answers. To
incentivize autonomous exploration, KnowCoder-A1 trains the LLM under
outcome-only supervision via a multi-stage curriculum reinforcement learning
with an easy-to-hard curriculum. To establish foundational agentic
capabilities, KnowCoder-A1 first fine-tunes the LLM on a small set of
high-quality trajectories obtained through outcome-based rejection sampling.
Then, to alleviate the reward sparsity inherent in outcome-only supervision, it
applies multi-stage curriculum RL with reward schedules that progress from easy
to hard. Trained with outcome-only supervision, KnowCoder-A1 exhibits powerful
reasoning behaviors and consistently outperforms prior approaches across three
mainstream datasets. Notably, on the zero-shot subset of GrailQA, KnowCoder-A1
achieves up to an 11.1% relative improvement while using only one-twelfth of
the training data, demonstrating strong agentic reasoning capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BioCoref: Benchmarking Biomedical Coreference Resolution with LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25087v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25087v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nourah M Salem, Elizabeth White, Michael Bada, Lawrence Hunter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Coreference resolution in biomedical texts presents unique challenges due to
complex domain-specific terminology, high ambiguity in mention forms, and
long-distance dependencies between coreferring expressions. In this work, we
present a comprehensive evaluation of generative large language models (LLMs)
for coreference resolution in the biomedical domain. Using the CRAFT corpus as
our benchmark, we assess the LLMs' performance with four prompting experiments
that vary in their use of local, contextual enrichment, and domain-specific
cues such as abbreviations and entity dictionaries. We benchmark these
approaches against a discriminative span-based encoder, SpanBERT, to compare
the efficacy of generative versus discriminative methods. Our results
demonstrate that while LLMs exhibit strong surface-level coreference
capabilities, especially when supplemented with domain-grounding prompts, their
performance remains sensitive to long-range context and mentions ambiguity.
Notably, the LLaMA 8B and 17B models show superior precision and F1 scores
under entity-augmented prompting, highlighting the potential of lightweight
prompt engineering for enhancing LLM utility in biomedical NLP tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TOPol: Capturing and Explaining Multidimensional Semantic Polarity
  Fields and Vectors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25069v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25069v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabin Taibi, Lucia Gomez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional approaches to semantic polarity in computational linguistics
treat sentiment as a unidimensional scale, overlooking the multidimensional
structure of language. This work introduces TOPol (Topic-Orientation POLarity),
a semi-unsupervised framework for reconstructing and interpreting
multidimensional narrative polarity fields under human-on-the-loop (HoTL)
defined contextual boundaries (CBs). The framework embeds documents using a
transformer-based large language model (tLLM), applies neighbor-tuned UMAP
projection, and segments topics via Leiden partitioning. Given a CB between
discourse regimes A and B, TOPol computes directional vectors between
corresponding topic-boundary centroids, yielding a polarity field that
quantifies fine-grained semantic displacement during regime shifts. This
vectorial representation enables assessing CB quality and detecting polarity
changes, guiding HoTL CB refinement. To interpret identified polarity vectors,
the tLLM compares their extreme points and produces contrastive labels with
estimated coverage. Robustness analyses show that only CB definitions (the main
HoTL-tunable parameter) significantly affect results, confirming methodological
stability. We evaluate TOPol on two corpora: (i) U.S. Central Bank speeches
around a macroeconomic breakpoint, capturing non-affective semantic shifts, and
(ii) Amazon product reviews across rating strata, where affective polarity
aligns with NRC valence. Results demonstrate that TOPol consistently captures
both affective and non-affective polarity transitions, providing a scalable,
generalizable, and interpretable framework for context-sensitive
multidimensional discourse analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 3 figures and 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can LLMs Estimate Cognitive Complexity of Reading Comprehension Items? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25064v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25064v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seonjeong Hwang, Hyounghun Kim, Gary Geunbae Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating the cognitive complexity of reading comprehension (RC) items is
crucial for assessing item difficulty before it is administered to learners.
Unlike syntactic and semantic features, such as passage length or semantic
similarity between options, cognitive features that arise during answer
reasoning are not readily extractable using existing NLP tools and have
traditionally relied on human annotation. In this study, we examine whether
large language models (LLMs) can estimate the cognitive complexity of RC items
by focusing on two dimensions-Evidence Scope and Transformation Level-that
indicate the degree of cognitive burden involved in reasoning about the answer.
Our experimental results demonstrate that LLMs can approximate the cognitive
complexity of items, indicating their potential as tools for prior difficulty
analysis. Further analysis reveals a gap between LLMs' reasoning ability and
their metacognitive awareness: even when they produce correct answers, they
sometimes fail to correctly identify the features underlying their own
reasoning process.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GAPMAP: Mapping Scientific Knowledge Gaps in Biomedical Literature Using
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25055v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25055v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nourah M Salem, Elizabeth White, Michael Bada, Lawrence Hunter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientific progress is driven by the deliberate articulation of what remains
unknown. This study investigates the ability of large language models (LLMs) to
identify research knowledge gaps in the biomedical literature. We define two
categories of knowledge gaps: explicit gaps, clear declarations of missing
knowledge; and implicit gaps, context-inferred missing knowledge. While prior
work has focused mainly on explicit gap detection, we extend this line of
research by addressing the novel task of inferring implicit gaps. We conducted
two experiments on almost 1500 documents across four datasets, including a
manually annotated corpus of biomedical articles. We benchmarked both
closed-weight models (from OpenAI) and open-weight models (Llama and Gemma 2)
under paragraph-level and full-paper settings. To address the reasoning of
implicit gaps inference, we introduce \textbf{\small TABI}, a Toulmin-Abductive
Bucketed Inference scheme that structures reasoning and buckets inferred
conclusion candidates for validation. Our results highlight the robust
capability of LLMs in identifying both explicit and implicit knowledge gaps.
This is true for both open- and closed-weight models, with larger variants
often performing better. This suggests a strong ability of LLMs for
systematically identifying candidate knowledge gaps, which can support
early-stage research formulation, policymakers, and funding decisions. We also
report observed failure modes and outline directions for robust deployment,
including domain adaptation, human-in-the-loop verification, and benchmarking
across open- and closed-weight models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Emotion Recognition in Spoken Language Models on Emotionally
  Incongruent Speech 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25054v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25054v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro Corrêa, João Lima, Victor Moreno, Paula Dornhofer Paro Costa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advancements in spoken language processing have driven the development of
spoken language models (SLMs), designed to achieve universal audio
understanding by jointly learning text and audio representations for a wide
range of tasks. Although promising results have been achieved, there is growing
discussion regarding these models' generalization capabilities and the extent
to which they truly integrate audio and text modalities in their internal
representations. In this work, we evaluate four SLMs on the task of speech
emotion recognition using a dataset of emotionally incongruent speech samples,
a condition under which the semantic content of the spoken utterance conveys
one emotion while speech expressiveness conveys another. Our results indicate
that SLMs rely predominantly on textual semantics rather than speech emotion to
perform the task, indicating that text-related representations largely dominate
over acoustic representations. We release both the code and the Emotionally
Incongruent Synthetic Speech dataset (EMIS) to the community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Cross-lingual Alignment: Balancing Transfer and Cultural
  Erasure in Multilingual LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26024v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26024v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        HyoJung Han, Sweta Agrawal, Eleftheria Briakou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-lingual alignment (CLA) aims to align multilingual representations,
enabling Large Language Models (LLMs) to seamlessly transfer knowledge across
languages. While intuitive, we hypothesize, this pursuit of representational
convergence can inadvertently cause "cultural erasure", the functional loss of
providing culturally-situated responses that should diverge based on the query
language. In this work, we systematically analyze this trade-off by introducing
a holistic evaluation framework, the transfer-localization plane, which
quantifies both desirable knowledge transfer and undesirable cultural erasure.
Using this framework, we re-evaluate recent CLA approaches and find that they
consistently improve factual transfer at the direct cost of cultural
localization across all six languages studied. Our investigation into the
internal representations of these models reveals a key insight: universal
factual transfer and culturally-specific knowledge are optimally steerable at
different model layers. Based on this finding, we propose Surgical Steering, a
novel inference-time method that disentangles these two objectives. By applying
targeted activation steering to distinct layers, our approach achieves a better
balance between the two competing dimensions, effectively overcoming the
limitations of current alignment techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PORTool: Tool-Use LLM Training with Rewarded Tree 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26020v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26020v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feijie Wu, Weiwu Zhu, Yuxiang Zhang, Soumya Chatterjee, Jiarong Zhu, Fan Mo, Rodin Luo, Jing Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current tool-use large language models (LLMs) are trained on static datasets,
enabling them to interact with external tools and perform multi-step,
tool-integrated reasoning, which produces tool-call trajectories. However,
these models imitate how a query is resolved in a generic tool-call routine,
thereby failing to explore possible solutions and demonstrating limited
performance in an evolved, dynamic tool-call environment. In this work, we
propose PORTool, a reinforcement learning (RL) method that encourages a
tool-use LLM to explore various trajectories yielding the correct answer.
Specifically, this method starts with generating multiple rollouts for a given
query, and some of them share the first few tool-call steps, thereby forming a
tree-like structure. Next, we assign rewards to each step, based on its ability
to produce a correct answer and make successful tool calls. A shared step
across different trajectories receives the same reward, while different steps
under the same fork receive different rewards. Finally, these step-wise rewards
are used to calculate fork-relative advantages, blended with
trajectory-relative advantages, to train the LLM for tool use. The experiments
utilize 17 tools to address user queries, covering both time-sensitive and
time-invariant topics. We conduct ablation studies to systematically justify
the necessity and the design robustness of step-wise rewards. Furthermore, we
compare the proposed PORTool with other training approaches and demonstrate
significant improvements in final accuracy and the number of tool-call steps.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CAVE: Detecting and Explaining Commonsense Anomalies in Visual
  Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26006v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26006v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rishika Bhagwatkar, Syrielle Montariol, Angelika Romanou, Beatriz Borges, Irina Rish, Antoine Bosselut
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans can naturally identify, reason about, and explain anomalies in their
environment. In computer vision, this long-standing challenge remains limited
to industrial defects or unrealistic, synthetically generated anomalies,
failing to capture the richness and unpredictability of real-world anomalies.
In this work, we introduce CAVE, the first benchmark of real-world visual
anomalies. CAVE supports three open-ended tasks: anomaly description,
explanation, and justification; with fine-grained annotations for visual
grounding and categorizing anomalies based on their visual manifestations,
their complexity, severity, and commonness. These annotations draw inspiration
from cognitive science research on how humans identify and resolve anomalies,
providing a comprehensive framework for evaluating Vision-Language Models
(VLMs) in detecting and understanding anomalies. We show that state-of-the-art
VLMs struggle with visual anomaly perception and commonsense reasoning, even
with advanced prompting strategies. By offering a realistic and cognitively
grounded benchmark, CAVE serves as a valuable resource for advancing research
in anomaly detection and commonsense reasoning in VLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Supervised Reinforcement Learning: From Expert Trajectories to Step-wise
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25992v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25992v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihe Deng, I-Hung Hsu, Jun Yan, Zifeng Wang, Rujun Han, Gufeng Zhang, Yanfei Chen, Wei Wang, Tomas Pfister, Chen-Yu Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) often struggle with problems that require
multi-step reasoning. For small-scale open-source models, Reinforcement
Learning with Verifiable Rewards (RLVR) fails when correct solutions are rarely
sampled even after many attempts, while Supervised Fine-Tuning (SFT) tends to
overfit long demonstrations through rigid token-by-token imitation. To address
this gap, we propose Supervised Reinforcement Learning (SRL), a framework that
reformulates problem solving as generating a sequence of logical "actions". SRL
trains the model to generate an internal reasoning monologue before committing
to each action. It provides smoother rewards based on the similarity between
the model's actions and expert actions extracted from the SFT dataset in a
step-wise manner. This supervision offers richer learning signals even when all
rollouts are incorrect, while encouraging flexible reasoning guided by expert
demonstrations. As a result, SRL enables small models to learn challenging
problems previously unlearnable by SFT or RLVR. Moreover, initializing training
with SRL before refining with RLVR yields the strongest overall performance.
Beyond reasoning benchmarks, SRL generalizes effectively to agentic software
engineering tasks, establishing it as a robust and versatile training framework
for reasoning-oriented LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AttnCache: Accelerating Self-Attention Inference for LLM Prefill via
  Attention Cache 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25979v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25979v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dinghong Song, Yuan Feng, Yiwei Wang, Shangye Chen, Cyril Guyot, Filip Blagojevic, Hyeran Jeon, Pengfei Su, Dong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are widely used in generative applications such
as chatting, code generation, and reasoning. However, many realworld workloads
such as classification, question answering, recommendation, and text embedding
rely solely on the prefill stage of inference, where the model encodes input
sequences without performing autoregressive decoding. In these prefill only
scenarios, the self-attention computation becomes the primary performance
bottleneck due to its quadratic complexity with respect to sequence length. In
this paper, we observe that semantically different sentences often produce
similar attention maps across layers and heads. Building on this insight, we
propose AttnCache, a framework that accelerates the prefill stage of LLM
inference by retrieving and reusing similar attention maps. Based on an
attention map memorization database, AttnCache employs efficient caching and
similarity search techniques to identify and reuse pre-cached attention maps
during inference, thereby reducing the computational overhead of
self-attention. Experimental results show that AttnCache achieves an average of
1.2x end-to-end and 2x attention speedup on CPU, and 1.6x end-to-end and 3x
attention speedup on GPU, with negligible accuracy degradation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures, submitted to Ninth Annual Conference on Machine
  Learning and Systems (MLSys'26)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeuronMM: High-Performance Matrix Multiplication for LLM Inference on
  AWS Trainium <span class="chip">EuroSys'26</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25977v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25977v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dinghong Song, Jierui Xu, Weichu Yang, Pengfei Su, Dong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI accelerators, customized to AI workloads, provide cost-effective and
high-performance solutions for training and inference. Trainium, an AI
accelerator recently developed by Amazon Web Services (AWS), provides an
attractive option for LLM training and inference through its heterogeneous
architecture. However, leveraging Trainium architecture for high performance
can be challenging because of its systolic array architecture and special
requirement on data layout. In this paper, we design high-performance matrix
multiplication (matmul), a critical compute kernel, for LLM inference on
Trainium. We introduce a series of techniques customized to Trainium based on
kernel fusion and novel caching strategies to reduce data movement across the
software-managed memory hierarchy, maximize SRAM bandwidth, and avoid expensive
matrix transpose. Evaluating with nine datasets and four recent LLMs, we show
that our system largely outperforms the state-of-the-art matmul implemented by
AWS on Trainium: at the level of matmul kernel, it achieves an average 1.35x
speedup (up to 2.22x), which translates to an average 1.66x speedup (up to
2.49x) for end-to-end LLM inference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 8 figures, submitted to the Proceedings of the Twenty-First
  European Conference on Computer Systems (EuroSys'26)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SymCode: A Neurosymbolic Approach to Mathematical Reasoning via
  Verifiable Code Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25975v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25975v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sina Bagheri Nezhad, Yao Li, Ameeta Agrawal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) often struggle with complex mathematical
reasoning, where prose-based generation leads to unverified and arithmetically
unsound solutions. Current prompting strategies like Chain of Thought still
operate within this unreliable medium, lacking a mechanism for deterministic
verification. To address these limitations, we introduce SymCode, a
neurosymbolic framework that reframes mathematical problem-solving as a task of
verifiable code generation using the SymPy library. We evaluate SymCode on
challenging benchmarks, including MATH-500 and OlympiadBench, demonstrating
significant accuracy improvements of up to 13.6 percentage points over
baselines. Our analysis shows that SymCode is not only more token-efficient but
also fundamentally shifts model failures from opaque logical fallacies towards
transparent, programmatic errors. By grounding LLM reasoning in a deterministic
symbolic engine, SymCode represents a key step towards more accurate and
trustworthy AI in formal domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantic Label Drift in Cross-Cultural Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25967v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25967v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohsinul Kabir, Tasnim Ahmed, Md Mezbaur Rahman, Polydoros Giannouris, Sophia Ananiadou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine Translation (MT) is widely employed to address resource scarcity in
low-resource languages by generating synthetic data from high-resource
counterparts. While sentiment preservation in translation has long been
studied, a critical but underexplored factor is the role of cultural alignment
between source and target languages. In this paper, we hypothesize that
semantic labels are drifted or altered during MT due to cultural divergence.
Through a series of experiments across culturally sensitive and neutral
domains, we establish three key findings: (1) MT systems, including modern
Large Language Models (LLMs), induce label drift during translation,
particularly in culturally sensitive domains; (2) unlike earlier statistical MT
tools, LLMs encode cultural knowledge, and leveraging this knowledge can
amplify label drift; and (3) cultural similarity or dissimilarity between
source and target languages is a crucial determinant of label preservation. Our
findings highlight that neglecting cultural factors in MT not only undermines
label fidelity but also risks misinterpretation and cultural conflict in
downstream applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting Multilingual Data Mixtures in Language Model <span class="highlight-title">Pretrain</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25947v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25947v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Negar Foroutan, Paul Teiletche, Ayush Kumar Tarun, Antoine Bosselut
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The impact of different multilingual data mixtures in pretraining large
language models (LLMs) has been a topic of ongoing debate, often raising
concerns about potential trade-offs between language coverage and model
performance (i.e., the curse of multilinguality). In this work, we investigate
these assumptions by training 1.1B and 3B parameter LLMs on diverse
multilingual corpora, varying the number of languages from 25 to 400. Our study
challenges common beliefs surrounding multilingual training. First, we find
that combining English and multilingual data does not necessarily degrade the
in-language performance of either group, provided that languages have a
sufficient number of tokens included in the pretraining corpus. Second, we
observe that using English as a pivot language (i.e., a high-resource language
that serves as a catalyst for multilingual generalization) yields benefits
across language families, and contrary to expectations, selecting a pivot
language from within a specific family does not consistently improve
performance for languages within that family. Lastly, we do not observe a
significant "curse of multilinguality" as the number of training languages
increases in models at this scale. Our findings suggest that multilingual data,
when balanced appropriately, can enhance language model capabilities without
compromising performance, even in low-resource settings
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RECAP: Reproducing Copyrighted Data from LLMs Training with an Agentic
  Pipeline 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25941v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25941v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        André V. Duarte, Xuying li, Bin Zeng, Arlindo L. Oliveira, Lei Li, Zhuo Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  If we cannot inspect the training data of a large language model (LLM), how
can we ever know what it has seen? We believe the most compelling evidence
arises when the model itself freely reproduces the target content. As such, we
propose RECAP, an agentic pipeline designed to elicit and verify memorized
training data from LLM outputs. At the heart of RECAP is a feedback-driven
loop, where an initial extraction attempt is evaluated by a secondary language
model, which compares the output against a reference passage and identifies
discrepancies. These are then translated into minimal correction hints, which
are fed back into the target model to guide subsequent generations. In
addition, to address alignment-induced refusals, RECAP includes a jailbreaking
module that detects and overcomes such barriers. We evaluate RECAP on
EchoTrace, a new benchmark spanning over 30 full books, and the results show
that RECAP leads to substantial gains over single-iteration approaches. For
instance, with GPT-4.1, the average ROUGE-L score for the copyrighted text
extraction improved from 0.38 to 0.47 - a nearly 24% increase.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FakeZero: Real-Time, Privacy-Preserving Misinformation Detection for
  Facebook and X 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25932v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25932v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soufiane Essahli, Oussama Sarsar, Imane Fouad, Anas Motii, Ahmed Bentajer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social platforms distribute information at unprecedented speed, which in turn
accelerates the spread of misinformation and threatens public discourse. We
present FakeZero, a fully client-side, cross-platform browser extension that
flags unreliable posts on Facebook and X (formerly Twitter) while the user
scrolls. All computation, DOM scraping, tokenisation, Transformer inference,
and UI rendering run locally through the Chromium messaging API, so no personal
data leaves the device.FakeZero employs a three-stage training curriculum:
baseline fine-tuning and domain-adaptive training enhanced with focal loss,
adversarial augmentation, and post-training quantisation. Evaluated on a
dataset of 239,000 posts, the DistilBERT-Quant model (67.6 MB) reaches 97.1%
macro-F1, 97.4% accuracy, and an AUROC of 0.996, with a median latency of
approximately 103 ms on a commodity laptop. A memory-efficient TinyBERT-Quant
variant retains 95.7% macro-F1 and 96.1% accuracy while shrinking the model to
14.7 MB and lowering latency to approximately 40 ms, showing that high-quality
fake-news detection is feasible under tight resource budgets with only modest
performance loss.By providing inline credibility cues, the extension can serve
as a valuable tool for policymakers seeking to curb the spread of
misinformation across social networks. With user consent, FakeZero also opens
the door for researchers to collect large-scale datasets of fake news in the
wild, enabling deeper analysis and the development of more robust detection
techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in the Proceedings of the 24th IEEE
  International Conference on Trust, Security and Privacy in Computing and
  Communications (TrustCom 2025) Privacy track, 11 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating the Impact of LLM-Assisted Annotation in a Perspectivized
  Setting: the Case of FrameNet Annotation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25904v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25904v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Frederico Belcavello, Ely Matos, Arthur Lorenzi, Lisandra Bonoto, Lívia Ruiz, Luiz Fernando Pereira, Victor Herbst, Yulla Navarro, Helen de Andrade Abreu, Lívia Dutra, Tiago Timponi Torrent
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The use of LLM-based applications as a means to accelerate and/or substitute
human labor in the creation of language resources and dataset is a reality.
Nonetheless, despite the potential of such tools for linguistic research,
comprehensive evaluation of their performance and impact on the creation of
annotated datasets, especially under a perspectivized approach to NLP, is still
missing. This paper contributes to reduction of this gap by reporting on an
extensive evaluation of the (semi-)automatization of FrameNet-like semantic
annotation by the use of an LLM-based semantic role labeler. The methodology
employed compares annotation time, coverage and diversity in three experimental
settings: manual, automatic and semi-automatic annotation. Results show that
the hybrid, semi-automatic annotation setting leads to increased frame
diversity and similar annotation coverage, when compared to the human-only
setting, while the automatic setting performs considerably worse in all
metrics, except for annotation time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Approximating Human Preferences Using a Multi-Judge Learned System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25884v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25884v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eitán Sprejer, Fernando Avalos, Augusto Bernardi, Jose Pedro Brito de Azevedo Faustino, Jacob Haimes, Narmeen Fatimah Oozeer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aligning LLM-based judges with human preferences is a significant challenge,
as they are difficult to calibrate and often suffer from rubric sensitivity,
bias, and instability. Overcoming this challenge advances key applications,
such as creating reliable reward models for Reinforcement Learning from Human
Feedback (RLHF) and building effective routing systems that select the
best-suited model for a given user query. In this work, we propose a framework
for modeling diverse, persona-based preferences by learning to aggregate
outputs from multiple rubric-conditioned judges. We investigate the performance
of this approach against naive baselines and assess its robustness through case
studies on both human and LLM-judges biases. Our primary contributions include
a persona-based method for synthesizing preference labels at scale and two
distinct implementations of our aggregator: Generalized Additive Model (GAM)
and a Multi-Layer Perceptron (MLP).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Through the Judge's Eyes: Inferred Thinking Traces Improve Reliability
  of LLM Raters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25860v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25860v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingjian Zhang, Tianhong Gao, Suliang Jin, Tianhao Wang, Teng Ye, Eytan Adar, Qiaozhu Mei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are increasingly used as raters for evaluation
tasks. However, their reliability is often limited for subjective tasks, when
human judgments involve subtle reasoning beyond annotation labels. Thinking
traces, the reasoning behind a judgment, are highly informative but challenging
to collect and curate. We present a human-LLM collaborative framework to infer
thinking traces from label-only annotations. The proposed framework uses a
simple and effective rejection sampling method to reconstruct these traces at
scale. These inferred thinking traces are applied to two complementary tasks:
(1) fine-tuning open LLM raters; and (2) synthesizing clearer annotation
guidelines for proprietary LLM raters. Across multiple datasets, our methods
lead to significantly improved LLM-human agreement. Additionally, the refined
annotation guidelines increase agreement among different LLM models. These
results suggest that LLMs can serve as practical proxies for otherwise
unrevealed human thinking traces, enabling label-only corpora to be extended
into thinking-trace-augmented resources that enhance the reliability of LLM
raters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bob's Confetti: Phonetic Memorization Attacks in Music and Video
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17937v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17937v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaechul Roh, Zachary Novack, Yuefeng Peng, Niloofar Mireshghallah, Taylor Berg-Kirkpatrick, Amir Houmansadr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative AI systems for music and video commonly use text-based filters to
prevent the regurgitation of copyrighted material. We expose a fundamental flaw
in this approach by introducing Adversarial PhoneTic Prompting (APT), a novel
attack that bypasses these safeguards by exploiting phonetic memorization. The
APT attack replaces iconic lyrics with homophonic but semantically unrelated
alternatives (e.g., "mom's spaghetti" becomes "Bob's confetti"), preserving
acoustic structure while altering meaning; we identify high-fidelity phonetic
matches using CMU pronouncing dictionary. We demonstrate that leading
Lyrics-to-Song (L2S) models like SUNO and YuE regenerate songs with striking
melodic and rhythmic similarity to their copyrighted originals when prompted
with these altered lyrics. More surprisingly, this vulnerability extends across
modalities. When prompted with phonetically modified lyrics from a song, a
Text-to-Video (T2V) model like Veo 3 reconstructs visual scenes from the
original music video-including specific settings and character
archetypes-despite the absence of any visual cues in the prompt. Our findings
reveal that models memorize deep, structural patterns tied to acoustics, not
just verbatim text. This phonetic-to-visual leakage represents a critical
vulnerability in transcript-conditioned generative models, rendering simple
copyright filters ineffective and raising urgent concerns about the secure
deployment of multimodal AI systems. Demo examples are available at our project
page (https://jrohsc.github.io/music_attack/).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLMs are Better Than You Think: Label-Guided In-Context Learning for
  Named Entity Recognition <span class="chip">EMNLP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.23722v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.23722v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Bai, Hamid Hassanzadeh, Ardavan Saeedi, Mark Dredze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context learning (ICL) enables large language models (LLMs) to perform new
tasks using only a few demonstrations. However, in Named Entity Recognition
(NER), existing ICL methods typically rely on task-agnostic semantic similarity
for demonstration retrieval, which often yields less relevant examples and
leads to inferior results. We introduce DEER, a training-free ICL approach that
enables LLMs to make more informed entity predictions through the use of
label-grounded statistics. DEER leverages token-level statistics from training
labels to identify tokens most informative for entity recognition, enabling
entity-focused demonstrations. It further uses these statistics to detect and
refine error-prone tokens through a targeted reflection step. Evaluated on five
NER datasets across four LLMs, DEER consistently outperforms existing ICL
methods and achieves performance comparable to supervised fine-tuning. Further
analyses demonstrate that DEER improves example retrieval, remains effective on
both seen and unseen entities, and exhibits strong robustness in low-resource
settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spontaneous Giving and Calculated Greed in Language Models <span class="chip">EMNLP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17720v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17720v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Li, Hirokazu Shirado
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models demonstrate strong problem-solving abilities through
reasoning techniques such as chain-of-thought prompting and reflection.
However, it remains unclear whether these reasoning capabilities extend to a
form of social intelligence: making effective decisions in cooperative
contexts. We examine this question using economic games that simulate social
dilemmas. First, we apply chain-of-thought and reflection prompting to GPT-4o
in a Public Goods Game. We then evaluate multiple off-the-shelf models across
six cooperation and punishment games, comparing those with and without explicit
reasoning mechanisms. We find that reasoning models consistently reduce
cooperation and norm enforcement, favoring individual rationality. In repeated
interactions, groups with more reasoning agents exhibit lower collective gains.
These behaviors mirror human patterns of "spontaneous giving and calculated
greed." Our findings underscore the need for LLM architectures that incorporate
social intelligence alongside reasoning, to help address--rather than
reinforce--the challenges of collective action.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2025 main conference and selected as an Oral
  Presentation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Precise In-Parameter Concept Erasure in Large Language Models <span class="chip">EMNLP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.22586v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.22586v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yoav Gur-Arieh, Clara Suslik, Yihuai Hong, Fazl Barez, Mor Geva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) often acquire knowledge during pretraining that
is undesirable in downstream deployments, e.g., sensitive information or
copyrighted content. Existing approaches for removing such knowledge rely on
fine-tuning, training low-rank adapters or fact-level editing, but these are
either too coarse, too shallow, or ineffective. In this work, we propose PISCES
(Precise In-parameter Suppression for Concept EraSure), a novel framework for
precisely erasing entire concepts from model parameters by directly editing
directions that encode them in parameter space. PISCES uses a disentangler
model to decompose MLP vectors into interpretable features, identifies those
associated with a target concept using automated interpretability techniques,
and removes them from model parameters. Experiments on Gemma 2 and Llama 3.1
over various concepts show that PISCES achieves modest gains in efficacy over
leading erasure methods, reducing accuracy on the target concept to as low as
7.7%, while dramatically improving erasure specificity (by up to 31%) and
robustness (by up to 38%). Overall, these results demonstrate that
feature-based in-parameter editing enables a more precise and reliable approach
for removing conceptual knowledge in language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2025 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SimulMEGA: MoE Routers are Advanced Policy Makers for Simultaneous
  Speech Translation <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.01200v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.01200v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyang Le, Bing Han, Jinshun Li, Songyong Chen, Yanmin Qian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simultaneous Speech Translation (SimulST) enables real-time cross-lingual
communication by jointly optimizing speech recognition and machine translation
under strict latency constraints. Existing systems struggle to balance
translation quality, latency, and semantic coherence, particularly in
multilingual many-to-many scenarios where divergent read and write policies
hinder unified strategy learning. In this paper, we present SimulMEGA
(Simultaneous Generation by Mixture-of-Experts Gating), an unsupervised policy
learning framework that combines prefix-based training with a
Mixture-of-Experts refiner to learn effective read and write decisions in an
implicit manner, without adding inference-time overhead. Our design requires
only minimal modifications to standard transformer architectures and
generalizes across both speech-to-text and text-to-speech streaming tasks.
Through comprehensive evaluation on six language pairs, our 500M parameter
speech-to-text model outperforms the Seamless baseline, achieving under 7
percent BLEU degradation at 1.5 seconds average lag and under 3 percent at 3
seconds. We further demonstrate the versatility of SimulMEGA by extending it to
streaming TTS with a unidirectional backbone, yielding superior latency quality
tradeoffs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025 poster</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SciReasoner: Laying the Scientific Reasoning Ground Across Disciplines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.21320v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.21320v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhou Wang, Chen Tang, Han Deng, Jiabei Xiao, Jiaqi Liu, Jianyu Wu, Jun Yao, Pengze Li, Encheng Su, Lintao Wang, Guohang Zhuang, Yuchen Ren, Ben Fei, Ming Hu, Xin Chen, Dongzhan Zhou, Junjun He, Xiangyu Yue, Zhenfei Yin, Jiamin Wu, Qihao Zheng, Yuhao Zhou, Huihui Xu, Chenglong Ma, Yan Lu, Wenlong Zhang, Chunfeng Song, Philip Torr, Shixiang Tang, Xinzhu Ma, Wanli Ouyang, Lei Bai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a scientific reasoning foundation model that aligns natural
language with heterogeneous scientific representations. The model is pretrained
on a 206B-token corpus spanning scientific text, pure sequences, and
sequence-text pairs, then aligned via SFT on 40M instructions, annealed
cold-start bootstrapping to elicit long-form chain-of-thought, and
reinforcement learning with task-specific reward shaping, which instills
deliberate scientific reasoning. It supports four capability families, covering
up to 103 tasks across workflows: (i) faithful translation between text and
scientific formats, (ii) text/knowledge extraction, (iii) property prediction,
(iv) property classification, (v) unconditional and conditional sequence
generation and design. Compared with specialist systems, our approach broadens
instruction coverage, improves cross-domain generalization, and enhances
fidelity. We detail data curation and training and show that cross-discipline
learning strengthens transfer and downstream reliability. The model, instruct
tuning datasets and the evaluation code are open-sourced at
https://huggingface.co/SciReason and
https://github.com/open-sciencelab/SciReason.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>technical report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OpenReward: Learning to Reward Long-form Agentic Tasks via Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24636v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24636v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyou Hu, Zhengliang Shi, Minghang Zhu, Haitao Li, Teng Sun, Pengjie Ren, Suzan Verberne, Zhaochun Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reward models (RMs) have become essential for aligning large language models
(LLMs), serving as scalable proxies for human evaluation in both training and
inference. However, existing RMs struggle on knowledge-intensive and long-form
tasks, where evaluating correctness requires grounding beyond the model's
internal knowledge. This limitation hinders them from reliably discriminating
subtle quality differences, especially when external evidence is necessary. To
address this, we introduce OpenRM, a tool-augmented long-form reward model that
systematically judges open-ended responses by invoking external tools to gather
relevant evidence. We train OpenRM with Group Relative Policy Optimization
(GRPO) on over 27K synthesized pairwise examples generated through a
controllable data synthesis framework. The training objective jointly
supervises intermediate tool usage and final outcome accuracy, incentivizing
our reward model to learn effective evidence-based judgment strategies.
Extensive experiments on three newly-collected datasets and two widely-used
benchmarks demonstrate that OpenRM substantially outperforms existing reward
modeling approaches. As a further step, we integrate OpenRM into both
inference-time response selection and training-time data selection. This yields
consistent gains in downstream LLM alignment tasks, highlighting the potential
of tool-augmented reward models for scaling reliable long-form evaluation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RLAIF-V: Open-Source AI Feedback Leads to Super <span class="highlight-title">GPT</span>-4V Trustworthiness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17220v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17220v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyu Yu, Haoye Zhang, Qiming Li, Qixin Xu, Yuan Yao, Da Chen, Xiaoman Lu, Ganqu Cui, Yunkai Dang, Taiwen He, Xiaocheng Feng, Jun Song, Bo Zheng, Zhiyuan Liu, Tat-Seng Chua, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional feedback learning for hallucination reduction relies on
labor-intensive manual labeling or expensive proprietary models. This leaves
the community without foundational knowledge about how to build high-quality
feedback with open-source MLLMs. In this work, we introduce RLAIF-V, a novel
framework that aligns MLLMs in a fully open-source paradigm. RLAIF-V maximally
explores open-source MLLMs from two perspectives, including high-quality
feedback data generation for preference learning and self-feedback guidance for
inference-time scaling. Extensive experiments on six benchmarks in both
automatic and human evaluation show that RLAIF-V substantially enhances the
trustworthiness of models at both preference learning and inference time.
RLAIF-V 7B reduces object hallucination by 80.7\% and overall hallucination by
33.7\%. Remarkably, RLAIF-V 12B further reveals the self-alignment potential of
open-source MLLMs, where the model can learn from feedback of itself to achieve
super GPT-4V trustworthiness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Website: https://github.com/RLHF-V/RLAIF-V</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust LLM Unlearning with MUDMAN: Meta-Unlearning with Disruption
  Masking And Normalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12484v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12484v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Filip Sondej, Yushi Yang, Mikołaj Kniejski, Marcel Windys
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models can retain dangerous knowledge and skills even after
extensive safety fine-tuning, posing both misuse and misalignment risks. Recent
studies show that even specialized unlearning methods can be easily reversed.
To address this, we systematically evaluate many existing and novel components
of unlearning methods and identify ones crucial for irreversible unlearning.
  We introduce Disruption Masking, a technique in which we only allow updating
weights, where the signs of the unlearning gradient and the retaining gradient
are the same. This ensures all updates are non-disruptive.
  Additionally, we identify the need for normalizing the unlearning gradients,
and also confirm the usefulness of meta-learning. We combine these insights
into MUDMAN (Meta-Unlearning with Disruption Masking and Normalization) and
validate its effectiveness at preventing the recovery of dangerous
capabilities. MUDMAN outperforms the prior TAR method by 40%, setting a new
state-of-the-art for robust unlearning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quantum <span class="highlight-title">Transformer</span>: Accelerating model inference via quantum linear
  algebra 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16714v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16714v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naixu Guo, Zhan Yu, Matthew Choi, Yizhan Han, Aman Agrawal, Kouhei Nakaji, Alán Aspuru-Guzik, Patrick Rebentrost
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Powerful generative artificial intelligence from large language models (LLMs)
harnesses extensive computational resources for inference. In this work, we
investigate the transformer architecture, a key component of these models,
under the lens of fault-tolerant quantum computing. We develop quantum
subroutines to construct the building blocks in the transformer, including the
self-attention, residual connection with layer normalization, and feed-forward
network. As an important subroutine, we show how to efficiently implement the
Hadamard product and element-wise functions of matrices on quantum computers.
Our algorithm prepares an amplitude encoding of the transformer output, which
can be measured for prediction or use in the next layer. We find that the
matrix norm of the input sequence plays a dominant role in the quantum
complexity. With numerical experiments on open-source LLMs, including for
bio-informatics applications, we demonstrate the potential of a quantum speedup
for transformer inference in practical regimes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>45 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GradeSQL: Test-Time Inference with Outcome Reward Models for Text-to-SQL
  Generation from Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.01308v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.01308v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mattia Tritto, Giuseppe Farano, Dario Di Palma, Gaetano Rossiello, Fedelucio Narducci, Dharmashankar Subramanian, Tommaso Di Noia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-SQL, the task of translating natural language questions into SQL
queries, has significantly advanced with the introduction of Large Language
Models (LLMs), broadening database accessibility for a wide range of users.
Despite substantial progress in generating valid SQL, current LLMs still
struggle with complex queries. To address this limitation, test-time strategies
such as Best-of-N (BoN) and Majority Voting (Maj) are often employed, based on
the assumption that LLMs can produce correct answers after multiple attempts.
However, these methods rely on surface-level heuristics, selecting the
syntactically correct query through execution-based BoN (ex-BoN) or the most
frequently generated one through Majority Voting. Recently, Outcome Reward
Models (ORMs), which assign utility scores to generated outputs based on
semantic correctness, have emerged as a promising reinforcement learning
approach for improving model alignment. We argue that ORMs could serve as an
effective new test-time heuristic, although their application in this context
remains largely underexplored.
  In this work, we propose a unified framework for training ORMs tailored to
the Text-to-SQL task and assess their effectiveness as a test-time heuristic
within the BoN strategy. We benchmark ORMs against ex-BoN and Maj across the
BIRD and Spider datasets, fine-tuning diverse open-source LLMs from the Qwen2,
Granite3, and Llama3 families. Results show that ORMs outperform ex-BoN and
Maj, achieving execution accuracy gains of +4.33% (BIRD) and +2.10% (Spider)
over ex-BoN, and +2.91% (BIRD) and +0.93% (Spider) over Maj. We further
demonstrate that finetuning models already aligned with SQL generation, such as
OmniSQL, yields superior ORM performance. Additionally, we observe that ORMs
achieve competitive results on simple queries and benefit more from an
increased number of candidates compared to ex-BoN and Maj.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reinforcement Learning Teachers of Test Time Scaling <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.08388v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.08388v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edoardo Cetin, Tianyu Zhao, Yujin Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training reasoning language models (LMs) with reinforcement learning (RL) for
one-hot correctness inherently relies on the LM being able to explore and solve
its task with some chance at initialization. Furthermore, a key use case of
reasoning LMs is to act as teachers for distilling new students and
cold-starting future RL iterations rather than being deployed themselves. From
these considerations, we introduce a new framework that avoids RL's exploration
challenge by training a new class of Reinforcement-Learned Teachers (RLTs)
focused on yielding the most effective downstream distillation. RLTs are
prompted with both the question and solution to each problem, and tasked to
simply "connect-the-dots" with detailed explanations tailored for their
students. We train RLTs with dense rewards obtained by feeding each explanation
to the student and testing its understanding of the problem's solution. In
practice, the raw outputs of a 7B RLT provide higher final performance on
competition and graduate-level tasks than existing distillation and
cold-starting pipelines that collect and postprocess the reasoning traces of
orders of magnitude larger LMs. Furthermore, RLTs maintain their effectiveness
when training larger students and when applied zero-shot to out-of-distribution
tasks, unlocking new levels of efficiency and re-usability for the RL reasoning
framework. Code available at: https://github.com/SakanaAI/RLT
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Many LLMs Are More Utilitarian Than One <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.00814v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.00814v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anita Keshmirian, Razan Baltaji, Babak Hemmatian, Hadi Asghari, Lav R. Varshney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Moral judgment is integral to large language models' (LLMs) social reasoning.
As multi-agent systems gain prominence, it becomes crucial to understand how
LLMs function when collaborating compared to operating as individual agents. In
human moral judgment, group deliberation leads to a Utilitarian Boost: a
tendency to endorse norm violations that inflict harm but maximize benefits for
the greatest number of people. We study whether a similar dynamic emerges in
multi-agent LLM systems. We test six models on well-established sets of moral
dilemmas across two conditions: (1) Solo, where models reason independently,
and (2) Group, where they engage in multi-turn discussions in pairs or triads.
In personal dilemmas, where agents decide whether to directly harm an
individual for the benefit of others, all models rated moral violations as more
acceptable when part of a group, demonstrating a Utilitarian Boost similar to
that observed in humans. However, the mechanism for the Boost in LLMs differed:
While humans in groups become more utilitarian due to heightened sensitivity to
decision outcomes, LLM groups showed either reduced sensitivity to norms or
enhanced impartiality. We report model differences in when and how strongly the
Boost manifests. We also discuss prompt and agent compositions that enhance or
mitigate the effect. We end with a discussion of the implications for AI
alignment, multi-agent design, and artificial moral reasoning. Code available
at: https://github.com/baltaci-r/MoralAgents
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the Conference on Neural Information Processing Systems
  (NeurIPS 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RoboOmni: Proactive Robot Manipulation in Omni-modal Context 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23763v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23763v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyin Wang, Jinlan Fu, Feihong Liu, Xinzhe He, Huangxuan Wu, Junhao Shi, Kexin Huang, Zhaoye Fei, Jingjing Gong, Zuxuan Wu, Yugang Jiang, See-Kiong Ng, Tat-Seng Chua, Xipeng Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Multimodal Large Language Models (MLLMs) have driven rapid
progress in Vision-Language-Action (VLA) models for robotic manipulation.
Although effective in many scenarios, current approaches largely rely on
explicit instructions, whereas in real-world interactions, humans rarely issue
instructions directly. Effective collaboration requires robots to infer user
intentions proactively. In this work, we introduce cross-modal contextual
instructions, a new setting where intent is derived from spoken dialogue,
environmental sounds, and visual cues rather than explicit commands. To address
this new setting, we present RoboOmni, a Perceiver-Thinker-Talker-Executor
framework based on end-to-end omni-modal LLMs that unifies intention
recognition, interaction confirmation, and action execution. RoboOmni fuses
auditory and visual signals spatiotemporally for robust intention recognition,
while supporting direct speech interaction. To address the absence of training
data for proactive intention recognition in robotic manipulation, we build
OmniAction, comprising 140k episodes, 5k+ speakers, 2.4k event sounds, 640
backgrounds, and six contextual instruction types. Experiments in simulation
and real-world settings show that RoboOmni surpasses text- and ASR-based
baselines in success rate, inference speed, intention recognition, and
proactive assistance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Multilingual, Large-Scale Study of the Interplay between LLM
  Safeguards, Personalisation, and Disinformation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.12993v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.12993v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        João A. Leite, Arnav Arora, Silvia Gargova, João Luz, Gustavo Sampaio, Ian Roberts, Carolina Scarton, Kalina Bontcheva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) can generate human-like disinformation, yet
their ability to personalise such content across languages and demographics
remains underexplored. This study presents the first large-scale, multilingual
analysis of persona-targeted disinformation generation by LLMs. Employing a red
teaming methodology, we prompt eight state-of-the-art LLMs with 324 false
narratives and 150 demographic personas (combinations of country, generation,
and political orientation) across four languages--English, Russian, Portuguese,
and Hindi--resulting in AI-TRAITS, a comprehensive dataset of 1.6 million
personalised disinformation texts. Results show that the use of even simple
personalisation prompts significantly increases the likelihood of jailbreaks
across all studied LLMs, up to 10 percentage points, and alters linguistic and
rhetorical patterns that enhance narrative persuasiveness. Models such as Grok
and GPT exhibited jailbreak rates and personalisation scores both exceeding
85%. These insights expose critical vulnerabilities in current state-of-the-art
LLMs and offer a foundation for improving safety alignment and detection
strategies in multilingual and cross-demographic contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reliable Evaluation and Benchmarks for Statement Autoformalization <span class="chip">EMNLP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.07222v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.07222v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Auguste Poiroux, Gail Weiss, Viktor Kunčak, Antoine Bosselut
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating statement autoformalization, translating natural language
mathematics into formal languages like Lean 4, remains a significant challenge,
with few metrics, datasets, and standards to robustly measure progress. In this
work, we present a comprehensive approach combining improved metrics, robust
benchmarks, and systematic evaluation, to fill this gap. First, we introduce
BEq+, an automated metric that correlates strongly with human judgment, along
with ProofNetVerif, a new dataset for assessing the quality of evaluation
metrics, containing 3,752 annotated examples. Second, we develop two new
autoformalization benchmarks: ProofNet#, a corrected version of ProofNet, and
RLM25, with 619 new pairs of research-level mathematics from six formalization
projects. Through systematic experimentation across these benchmarks, we find
that current techniques can achieve up to 45.1% accuracy on undergraduate
mathematics but struggle with research-level content without proper context.
Our work establishes a reliable foundation for evaluating and advancing
autoformalization systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2025. New benchmarks released, see
  https://github.com/augustepoiroux/RLMEval ,
  https://huggingface.co/datasets/PAug/ProofNetSharp , and
  https://huggingface.co/datasets/PAug/ProofNetVerif . For code, see
  https://github.com/augustepoiroux/LeanInteract</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When Models Outthink Their Safety: Mitigating Self-Jailbreak in Large
  Reasoning Models with Chain-of-Guardrails 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.21285v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.21285v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingzhi Mao, Chunkang Zhang, Junxiang Wang, Xinyan Guan, Boxi Cao, Yaojie Lu, Hongyu Lin, Xianpei Han, Le Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Reasoning Models (LRMs) demonstrate remarkable capabilities on complex
reasoning tasks but remain vulnerable to severe safety risks, including harmful
content generation and jailbreak attacks. Existing mitigation strategies rely
on injecting heuristic safety signals during training, which often suppress
reasoning ability and fail to resolve the safety-reasoning trade-off. To
systematically investigate this issue, we analyze the reasoning trajectories of
diverse LRMs and uncover a phenomenon we term Self-Jailbreak, where models
override their own risk assessments and justify responding to unsafe prompts.
This finding reveals that LRMs inherently possess the ability to reject unsafe
queries, but this ability is compromised, resulting in harmful outputs.
Building on these insights, we propose the Chain-of-Guardrail (CoG), a training
framework that recomposes or backtracks unsafe reasoning steps, steering the
model back onto safe trajectories while preserving valid reasoning chains.
Extensive experiments across multiple reasoning and safety benchmarks
demonstrate that CoG substantially improves the safety of current LRMs while
preserving comparable reasoning ability, significantly outperforming prior
methods that suffer from severe safety-reasoning trade-offs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>First two authors contributed equally. The main text is 10 pages,
  with an appendix of 19 pages. The paper contains 18 figures and 16 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Preference Optimization via Dynamic Target Margins <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03690v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03690v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Sun, Junkang Wu, Jiancan Wu, Zhibo Zhu, Xingyu Lu, Jun Zhou, Lintao Ma, Xiang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The alignment of Large Language Models (LLMs) is crucial for ensuring their
safety and reliability in practical applications. Direct Preference
Optimization (DPO) has emerged as an efficient method that directly optimizes
models using preference pairs, significantly reducing resource demands.
However, the effectiveness of DPO heavily depends on the data quality, which is
frequently compromised by noise. In this work, we propose $\gamma$-PO, a
dynamic target margin preference optimization algorithm that adjust reward
margins at the pairwise level. By introducing instance-specific margin
calibration, $\gamma$-PO strategically prioritizes high-confidence pairs (those
demonstrating higher reward margins) while suppressing potential noise from
ambiguous pairs. Moreover, $\gamma$-PO is a plug-and-play method, compatible
with variants of DPO that rely on reward margin between preference pairs.
Across benchmarks such as AlpacaEval2 and Arena-Hard, $\gamma$-PO achieves an
average 4.4\% improvement over other baselines, setting new benchmarks for
state-of-the-art performance. Additionally, $\gamma$-PO requires minimal code
changes and has a negligible impact on training efficiency, making it a robust
solution for enhancing LLMs alignment. Our codes are available at
\href{https://github.com/sunjie279/gammaPO}{https://github.com/sunjie279/gammaPO}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 6 figures, accepted to Findings of the 63rd Annual Meeting
  of the Association for Computational Linguistics (ACL 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differential Mamba <span class="chip">AACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.06204v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.06204v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nadav Schneider, Itamar Zimerman, Eliya Nachmani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequence models like Transformers and RNNs often overallocate attention to
irrelevant context, leading to noisy intermediate representations. This
degrades LLM capabilities by promoting hallucinations, weakening long-range and
retrieval abilities, and reducing robustness. Recent work has shown that
differential design can mitigate this issue in Transformers, improving their
effectiveness across various applications. In this paper, we explore whether
these techniques, originally developed for Transformers, can be applied to
Mamba, a recent architecture based on selective state-space layers that
achieves Transformer-level performance with greater efficiency. We show that a
naive adaptation of differential design to Mamba is insufficient and requires
careful architectural modifications. To address this, we introduce a novel
differential mechanism for Mamba, empirically validated on language modeling
benchmarks, demonstrating improved retrieval capabilities and superior
performance over vanilla Mamba. Finally, we conduct extensive ablation studies
and empirical analyses to justify our design choices and provide evidence that
our approach effectively mitigates the overallocation problem in Mamba-based
models. Our code is publicly available: https://github.com/NadavSc/Diff-Mamba
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AACL 2025. We provide the code at
  https://github.com/NadavSc/Diff-Mamba</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Are ASR foundation models generalized enough to capture features of
  regional dialects for low-resource languages? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23252v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23252v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tawsif Tashwar Dipto, Azmol Hossain, Rubayet Sabbir Faruque, Md. Rezuwan Hassan, Kanij Fatema, Tanmoy Shome, Ruwad Naswan, Md. Foriduzzaman Zihad, Mohaymen Ul Anam, Nazia Tasnim, Hasan Mahmud, Md Kamrul Hasan, Md. Mehedi Hasan Shawon, Farig Sadeque, Tahsin Reasat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional research on speech recognition modeling relies on the canonical
form for most low-resource languages while automatic speech recognition (ASR)
for regional dialects is treated as a fine-tuning task. To investigate the
effects of dialectal variations on ASR we develop a 78-hour annotated Bengali
Speech-to-Text (STT) corpus named Ben-10. Investigation from linguistic and
data-driven perspectives shows that speech foundation models struggle heavily
in regional dialect ASR, both in zero-shot and fine-tuned settings. We observe
that all deep learning methods struggle to model speech data under dialectal
variations but dialect specific model training alleviates the issue. Our
dataset also serves as a out of-distribution (OOD) resource for ASR modeling
under constrained resources in ASR algorithms. The dataset and code developed
for this project are publicly available
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The manuscript has to be withdrawn to address an authorship and
  intellectual property clarification</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ReSeek: A Self-Correcting Framework for Search Agents with Instructive
  Rewards 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.00568v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.00568v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiyu Li, Yang Tang, Yifan Wang, Peiming Li, Xi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Search agents powered by Large Language Models (LLMs) have demonstrated
significant potential in tackling knowledge-intensive tasks. Reinforcement
learning (RL) has emerged as a powerful paradigm for training these agents to
perform complex, multi-step reasoning. However, prior RL-based methods often
rely on sparse or rule-based rewards, which can lead agents to commit to
suboptimal or erroneous reasoning paths without the ability to recover. To
address these limitations, we propose ReSeek, a novel self-correcting framework
for training search agents. Our framework introduces a self-correction
mechanism that empowers the agent to dynamically identify and recover from
erroneous search paths during an episode. By invoking a special JUDGE action,
the agent can judge the information and re-plan its search strategy. To guide
this process, we design a dense, instructive process reward function, which
decomposes into a correctness reward for retrieving factual information and a
utility reward for finding information genuinely useful for the query.
Furthermore, to mitigate the risk of data contamination in existing datasets,
we introduce FictionalHot, a new and challenging benchmark with recently
curated questions requiring complex reasoning. Being intuitively reasonable and
practically simple, extensive experiments show that agents trained with ReSeek
significantly outperform SOTA baselines in task success rate and path
faithfulness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can LLMs Outshine Conventional Recommenders? A Comparative Evaluation <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.05493v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.05493v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qijiong Liu, Jieming Zhu, Lu Fan, Kun Wang, Hengchang Hu, Wei Guo, Yong Liu, Xiao-Ming Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, integrating large language models (LLMs) into recommender
systems has created new opportunities for improving recommendation quality.
However, a comprehensive benchmark is needed to thoroughly evaluate and compare
the recommendation capabilities of LLMs with traditional recommender systems.
In this paper, we introduce RecBench, which systematically investigates various
item representation forms (including unique identifier, text, semantic
embedding, and semantic identifier) and evaluates two primary recommendation
tasks, i.e., click-through rate prediction (CTR) and sequential recommendation
(SeqRec). Our extensive experiments cover up to 17 large models and are
conducted across five diverse datasets from fashion, news, video, books, and
music domains. Our findings indicate that LLM-based recommenders outperform
conventional recommenders, achieving up to a 5% AUC improvement in the CTR
scenario and up to a 170% NDCG@10 improvement in the SeqRec scenario. However,
these substantial performance gains come at the expense of significantly
reduced inference efficiency, rendering the LLM-as-RS paradigm impractical for
real-time recommendation environments. We aim for our findings to inspire
future research, including recommendation-specific model acceleration methods.
We will release our code, data, configurations, and platform to enable other
researchers to reproduce and build upon our experimental results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025 DB Track Accepted Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MAD-Fact: A Multi-Agent Debate Framework for Long-Form Factuality
  Evaluation in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.22967v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.22967v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yucheng Ning, Xixun Lin, Fang Fang, Yanan Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread adoption of Large Language Models (LLMs) raises critical
concerns about the factual accuracy of their outputs, especially in high-risk
domains such as biomedicine, law, and education. Existing evaluation methods
for short texts often fail on long-form content due to complex reasoning
chains, intertwined perspectives, and cumulative information. To address this,
we propose a systematic approach integrating large-scale long-form datasets,
multi-agent verification mechanisms, and weighted evaluation metrics. We
construct LongHalluQA, a Chinese long-form factuality dataset; and develop
MAD-Fact, a debate-based multi-agent verification system. We introduce a fact
importance hierarchy to capture the varying significance of claims in long-form
texts. Experiments on two benchmarks show that larger LLMs generally maintain
higher factual consistency, while domestic models excel on Chinese content. Our
work provides a structured framework for evaluating and enhancing factual
reliability in long-form LLM outputs, guiding their safe deployment in
sensitive domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The article has been accepted by Frontiers of Computer Science (FCS),
  with the DOI: {10.1007/s11704-025-51369-x}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NL-Debugging: Exploiting Natural Language as an Intermediate
  Representation for Code Debugging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.15356v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.15356v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiming Zhang, Qingyao Li, Xinyi Dai, Jizheng Chen, Kounianhua Du, Weiwen Liu, Yasheng Wang, Ruiming Tang, Yong Yu, Weinan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Debugging is a critical aspect of LLM's coding ability. Early debugging
efforts primarily focused on code-level analysis, which often falls short when
addressing complex programming errors that require a deeper understanding of
algorithmic logic. Recent advancements in large language models (LLMs) have
shifted attention toward leveraging natural language reasoning to enhance
code-related tasks. However, two fundamental questions remain unanswered: What
type of natural language format is most effective for debugging tasks? And what
specific benefits does natural language reasoning bring to the debugging
process? In this paper, we introduce NL-DEBUGGING, a novel framework that
employs natural language as an intermediate representation to improve code
debugging. By debugging at a natural language level, we demonstrate that
NL-DEBUGGING outperforms traditional debugging methods and enables a broader
modification space through direct refinement guided by execution feedback. Our
findings highlight the potential of natural language reasoning to advance
automated code debugging and address complex programming challenges.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WEST: LLM based Speech Toolkit for Speech Understanding, Generation, and
  Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.19902v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.19902v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Binbin Zhang, Chengdong Liang, Shuai Wang, Xuelong Geng, Zhao Guo, Haoyu Li, Hao Yin, Xipeng Yang, Pengshen Zhang, Changwei Ma, Lei Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present WEST(WE Speech Toolkit), a speech toolkit based on
a large language model (LLM) for speech understanding, generation, and
interaction. There are three key features of WEST: 1) Fully LLM-based: Standing
on the shoulders of giants by reusing mature architectures, ecosystems (e.g.,
Hugging Face), and methods (e.g., sequence packing) from large models. 2)
Full-stack: Supports tasks such as recognition, synthesis, understanding,
dialogue, and multimodal capabilities, with extensibility to incorporate
open-source models. 3) Simple and Stupid: A simple and stupid speech toolkit
that everyone can Touch. In addition, WEST provides two types of recipes,
models, and experimental results. The first is entirely based on open-source
models and open-source data, allowing users to fully reproduce the experiments
in this paper and serving as a verification system or minimal system baseline.
The second is trained on massive data, offering superior performance so the
user can directly apply it out of the box. WEST is publicly avilable at
https://github.com/wenet-e2e/west/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Think Twice Before You Judge: Mixture of Dual Reasoning Experts for
  Multimodal Sarcasm Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.04458v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.04458v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soumyadeep Jana, Abhrajyoti Kundu, Sanasam Ranbir Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal sarcasm detection has attracted growing interest due to the rise
of multimedia posts on social media. Understanding sarcastic image-text posts
often requires external contextual knowledge, such as cultural references or
commonsense reasoning. However, existing models struggle to capture the deeper
rationale behind sarcasm, relying mainly on shallow cues like image captions or
object-attribute pairs from images. To address this, we propose \textbf{MiDRE}
(\textbf{Mi}xture of \textbf{D}ual \textbf{R}easoning \textbf{E}xperts), which
integrates an internal reasoning expert for detecting incongruities within the
image-text pair and an external reasoning expert that utilizes structured
rationales generated via Chain-of-Thought prompting to a Large Vision-Language
Model. An adaptive gating mechanism dynamically weighs the two experts,
selecting the most relevant reasoning path. Unlike prior methods that treat
external knowledge as static input, MiDRE selectively adapts to when such
knowledge is beneficial, mitigating the risks of hallucinated or irrelevant
signals from large models. Experiments on two benchmark datasets show that
MiDRE achieves superior performance over baselines. Various qualitative
analyses highlight the crucial role of external rationales, revealing that even
when they are occasionally noisy, they provide valuable cues that guide the
model toward a better understanding of sarcasm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Blind Spot Navigation in Large Language Model Reasoning with Thought
  Space Explorer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24155v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24155v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinghan Zhang, Fengran Mo, Tharindu Cyril Weerasooriya, Xinyue Ye, Dongjie Wang, Yanjie Fu, Kunpeng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models have shown strong reasoning capabilities through
chain-structured methods such as Chain-of-Thought. Recent studies optimize
thought structures by generating parallel or tree-like structures, switching
between long and short reasoning modes, or aligning reasoning steps with task
performance. However, these approaches mainly rely on previously generated
logical directions of the chains, which ignore the unexplored regions of the
solution space. Such a phenomenon is defined as blind spots, which limit the
diversity and effectiveness of the reasoning process. To this end, we propose
the ``Thought Space Explorer'' (TSE), a framework for navigating and expanding
thought structures to overcome blind spots in LLM reasoning. Our TSE first
identifies key nodes with high impact, then generates new nodes by integrating
information from multiple chains. Finally, it extends new branches through
connection strategies. We conduct a series of experiments on math and QA
benchmarks. Compared with existing baseline methods, TSE improves the accuracy
of both the final answer and intermediate reasoning steps, while maintaining a
better effectiveness-efficiency trade-off for practical deployment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pass@K Policy Optimization: Solving Harder Reinforcement Learning
  Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.15201v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.15201v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Walder, Deep Karkhanis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning (RL) algorithms sample multiple n>1 solution attempts
for each problem and reward them independently. This optimizes for pass@1
performance and prioritizes the strength of isolated samples at the expense of
the diversity and collective utility of sets of samples. This under-utilizes
the sampling capacity, limiting exploration and eventual improvement on harder
examples. As a fix, we propose Pass-at-k Policy Optimization (PKPO), a
transformation on the final rewards which leads to direct optimization of
pass@k performance, thus optimizing for sets of samples that maximize reward
when considered jointly. Our contribution is to derive novel low variance
unbiased estimators for pass@k and its gradient, in both the binary and
continuous reward settings. We show optimization with our estimators reduces to
standard RL with rewards that have been jointly transformed by a stable and
efficient transformation function.
  While previous efforts are restricted to k=n, ours is the first to enable
robust optimization of pass@k for any arbitrary k <= n. Moreover, instead of
trading off pass@1 performance for pass@k gains, our method allows annealing k
during training, optimizing both metrics and often achieving strong pass@1
numbers alongside significant pass@k gains.
  We validate our reward transformations on toy experiments, which reveal the
variance reducing properties of our formulations. We also include real-world
examples using the open-source LLM, GEMMA-2. We find that our transformation
effectively optimizes for the target k. Furthermore, higher k values enable
solving more and harder problems, while annealing k boosts both the pass@1 and
pass@k . Crucially, for challenging task sets where conventional pass@1
optimization stalls, our pass@k approach unblocks learning, likely due to
better exploration by prioritizing joint utility over the utility of individual
samples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Augmenting Dialog with Think-Aloud Utterances for Modeling Individual
  Personality Traits by LLM <span class="chip">EMNLP2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.09158v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.09158v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seiya Ishikura, Hiroaki Yamada, Tatsuya Hiraoka, Hiroaki Yamada, Takenobu Tokunaga
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study proposes augmenting dialog data with think-aloud utterances (TAUs)
for modeling individual personalities in text chat by LLM. TAU is a
verbalization of a speaker's thought before articulating the utterance. We
expect "persona LLMs" trained with TAU-augmented data can mimic the speaker's
personality trait better. We tested whether the trained persona LLMs obtain the
human personality with respect to Big Five, a framework characterizing human
personality traits from five aspects. The results showed that LLMs trained with
TAU-augmented data more closely align to the speakers' Agreeableness and
Neuroticism of Big Five than those trained with original dialog data. We also
found that the quality of TAU-augmentation impacts persona LLM's performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 1 figure. Accepted at the First Workshop on Tailoring AI:
  Exploring Active and Passive LLM Personalization (PALS2025@EMNLP2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Landscape of Agentic Reinforcement Learning for LLMs: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.02547v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.02547v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guibin Zhang, Hejia Geng, Xiaohang Yu, Zhenfei Yin, Zaibin Zhang, Zelin Tan, Heng Zhou, Zhongzhi Li, Xiangyuan Xue, Yijiang Li, Yifan Zhou, Yang Chen, Chen Zhang, Yutao Fan, Zihu Wang, Songtao Huang, Francisco Piedrahita-Velez, Yue Liao, Hongru Wang, Mengyue Yang, Heng Ji, Michael Littman, Jun Wang, Shuicheng Yan, Philip Torr, Lei Bai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of agentic reinforcement learning (Agentic RL) marks a paradigm
shift from conventional reinforcement learning applied to large language models
(LLM RL), reframing LLMs from passive sequence generators into autonomous,
decision-making agents embedded in complex, dynamic worlds. This survey
formalizes this conceptual shift by contrasting the degenerate single-step
Markov Decision Processes (MDPs) of LLM-RL with the temporally extended,
partially observable Markov decision processes (POMDPs) that define Agentic RL.
Building on this foundation, we propose a comprehensive twofold taxonomy: one
organized around core agentic capabilities, including planning, tool use,
memory, reasoning, self-improvement, and perception, and the other around their
applications across diverse task domains. Central to our thesis is that
reinforcement learning serves as the critical mechanism for transforming these
capabilities from static, heuristic modules into adaptive, robust agentic
behavior. To support and accelerate future research, we consolidate the
landscape of open-source environments, benchmarks, and frameworks into a
practical compendium. By synthesizing over five hundred recent works, this
survey charts the contours of this rapidly evolving field and highlights the
opportunities and challenges that will shape the development of scalable,
general-purpose AI agents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lookahead Tree-Based Rollouts for Enhanced Trajectory-Level Exploration
  in Reinforcement Learning with Verifiable Rewards 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24302v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24302v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shangyu Xing, Siyuan Wang, Chenyuan Yang, Xinyu Dai, Xiang Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning with Verifiable Rewards (RLVR), particularly with
algorithms like Group Relative Policy Optimization (GRPO), has proven highly
effective in enhancing the reasoning capabilities of large language models.
However, a critical bottleneck in current pipelines lies in the limited
diversity of sampled trajectories during group rollouts. Homogeneous
trajectories and their associated rewards would diminish the return signals for
policy updates, thereby hindering effective policy learning. This lack of
diversity stems primarily from token-level stochastic sampling, where local
variations are likely to collapse into near-identical reasoning paths. To
address this limitation, we propose Lookahead Tree-Based Rollouts (LATR), a
novel rollout strategy designed to explicitly promotes trajectory-level
diversity by enforcing branching into different candidate tokens likely to
yield distinct continuations. Specifically, LATR iteratively operates in three
stages: (1) branching at high-uncertainty generation steps, (2) performing
lookahead simulation for each new branch, and (3) pruning branches that
exhibits prolonged similarity during simulation. Compared with stochastic
Sampling, LATR accelerates policy learning by 131% on average and improves
final pass@1 performance by 4.2% on both GRPO and Dynamic sAmpling Policy
Optimization (DAPO) algorithms across different reasoning tasks. Our code and
data are publicly available at https://github.com/starreeze/latr.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adapter-state Sharing CLIP for Parameter-efficient Multimodal Sarcasm
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.04508v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.04508v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soumyadeep Jana, Sahil Danayak, Sanasam Ranbir Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing prevalence of multimodal image-text sarcasm on social media poses
challenges for opinion mining systems. Existing approaches rely on full
fine-tuning of large models, making them unsuitable to adapt under
resource-constrained settings. While recent parameter-efficient fine-tuning
(PEFT) methods offer promise, their off-the-shelf use underperforms on complex
tasks like sarcasm detection. We propose AdS-CLIP (Adapter-state Sharing in
CLIP), a lightweight framework built on CLIP that inserts adapters only in the
upper layers to preserve low-level unimodal representations in the lower layers
and introduces a novel adapter-state sharing mechanism, where textual adapters
guide visual ones to promote efficient cross-modal learning in the upper
layers. Experiments on two public benchmarks demonstrate that AdS-CLIP not only
outperforms standard PEFT methods but also existing multimodal baselines with
significantly fewer trainable parameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Face the Facts! Evaluating RAG-based Pipelines for Professional
  Fact-Checking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15189v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15189v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Russo, Stefano Menini, Jacopo Staiano, Marco Guerini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural Language Processing and Generation systems have recently shown the
potential to complement and streamline the costly and time-consuming job of
professional fact-checkers. In this work, we lift several constraints of
current state-of-the-art pipelines for automated fact-checking based on the
Retrieval-Augmented Generation (RAG) paradigm. Our goal is to benchmark,
following professional fact-checking practices, RAG-based methods for the
generation of verdicts - i.e., short texts discussing the veracity of a claim -
evaluating them on stylistically complex claims and heterogeneous, yet
reliable, knowledge bases. Our findings show a complex landscape, where, for
example, LLM-based retrievers outperform other retrieval techniques, though
they still struggle with heterogeneous knowledge bases; larger models excel in
verdict faithfulness, while smaller models provide better context adherence,
with human evaluations favouring zero-shot and one-shot approaches for
informativeness, and fine-tuned models for emotional alignment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and data at https://github.com/drusso98/face-the-facts -
  Accepted for publication at INLG 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ S'MoRE: Structural Mixture of Residual Experts for Parameter-Efficient
  LLM Fine-tuning <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.06426v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.06426v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanqing Zeng, Yinglong Xia, Zhuokai Zhao, Chuan Jiang, Qiang Zhang, Jiayi Liu, Qunshu Zhang, Lizhu Zhang, Xiangjun Fan, Benyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning pre-trained large language models (LLMs) presents a dual
challenge of balancing parameter efficiency and model capacity. Existing
methods like low-rank adaptations (LoRA) are efficient but lack flexibility,
while Mixture-of-Experts (MoE) enhance model capacity at the cost of more &
under-utilized parameters. To address these limitations, we propose Structural
Mixture of Residual Experts (S'MoRE), a novel framework that seamlessly
integrates the efficiency of LoRA with the flexibility of MoE. Conceptually,
S'MoRE employs hierarchical low-rank decomposition of expert weights, yielding
residuals of varying orders interconnected in a multi-layer structure. By
routing input tokens through sub-trees of residuals, S'MoRE emulates the
capacity of numerous experts by instantiating and assembling just a few
low-rank matrices. We craft the inter-layer propagation of S'MoRE's residuals
as a special type of Graph Neural Network (GNN), and prove that under similar
parameter budget, S'MoRE improves structural flexibility of traditional MoE (or
Mixture-of-LoRA) by exponential order. Comprehensive theoretical analysis and
empirical results demonstrate that S'MoRE achieves superior fine-tuning
performance, offering a transformative approach for efficient LLM adaptation.
Our implementation is available at: https://github.com/ZimpleX/SMoRE-LLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Steering Information Utility in Key-Value Memory for Language Model
  Post-Training <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.05158v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.05158v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunyuan Deng, Ruidi Chang, Hanjie Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in language models (LMs) have marked a shift toward the
growing importance of post-training. Yet, post-training approaches such as
supervised fine-tuning (SFT) do not guarantee the effective use of knowledge
acquired during pretraining. We therefore introduce InfoSteer, a lightweight
method that encourages parametric information utilization in LMs during
post-training. Specifically, InfoSteer treats the feed-forward network (FFN)
layer as associate key-value memory and promotes the use of stored memory
vectors via forward-pass interventions or regularization during
backpropagation. This simple guidance during post-training phase yields
consistent performance improvements across diverse model families -- including
Qwen, Gemma and Llama -- spanning 15 downstream tasks in both in-distribution
(ID) and out-of-distribution (OOD) evaluations. Beyond performance gains, we
also find that steered LMs can adaptively allocate information by placing more
emphasis on generating semantically meaningful tokens, while using fewer
resources on simple transition ones (e.g., `\texttt{,}' or `\texttt{and}'). Our
work underscores that vanilla post-training does not fully exploit the
potential gained during pre-training, and that steering LMs in latent
representation space offers a promising approach to enhance both performance
and interpretability. The code is available at:
https://github.com/chili-lab/InfoSteer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OpenGuardrails: A Configurable, Unified, and Scalable Guardrails
  Platform for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.19169v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.19169v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Wang, Haowen Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) are increasingly integrated into real-world
applications, ensuring their safety, robustness, and privacy compliance has
become critical. We present OpenGuardrails, the first fully open-source
platform that unifies large-model-based safety detection, manipulation defense,
and deployable guardrail infrastructure. OpenGuardrails protects against three
major classes of risks: (1) content-safety violations such as harmful or
explicit text generation, (2) model-manipulation attacks including prompt
injection, jailbreaks, and code-interpreter abuse, and (3) data leakage
involving sensitive or private information. Unlike prior modular or rule-based
frameworks, OpenGuardrails introduces three core innovations: (1) a
Configurable Policy Adaptation mechanism that allows per-request customization
of unsafe categories and sensitivity thresholds; (2) a Unified LLM-based Guard
Architecture that performs both content-safety and manipulation detection
within a single model; and (3) a Quantized, Scalable Model Design that
compresses a 14B dense base model to 3.3B via GPTQ while preserving over 98 of
benchmark accuracy. The system supports 119 languages, achieves
state-of-the-art performance across multilingual safety benchmarks, and can be
deployed as a secure gateway or API-based service for enterprise use. All
models, datasets, and deployment scripts are released under the Apache 2.0
license.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AMAS: Adaptively Determining Communication Topology for LLM-based
  Multi-Agent System <span class="chip">EMNLP-2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.01617v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.01617v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui Yi Leong, Yuheng Li, Yuqing Wu, Wenwen Ouyang, Wei Zhu, Jiechao Gao, Wei Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although large language models (LLMs) have revolutionized natural language
processing capabilities, their practical implementation as autonomous
multi-agent systems (MAS) for industrial problem-solving encounters persistent
barriers. Conventional MAS architectures are fundamentally restricted by
inflexible, hand-crafted graph topologies that lack contextual responsiveness,
resulting in diminished efficacy across varied academic and commercial
workloads. To surmount these constraints, we introduce AMAS, a
paradigm-shifting framework that redefines LLM-based MAS through a novel
dynamic graph designer. This component autonomously identifies task-specific
optimal graph configurations via lightweight LLM adaptation, eliminating the
reliance on monolithic, universally applied structural templates. Instead, AMAS
exploits the intrinsic properties of individual inputs to intelligently direct
query trajectories through task-optimized agent pathways. Rigorous validation
across question answering, mathematical deduction, and code generation
benchmarks confirms that AMAS systematically exceeds state-of-the-art
single-agent and multi-agent approaches across diverse LLM architectures. Our
investigation establishes that context-sensitive structural adaptability
constitutes a foundational requirement for high-performance LLM MAS
deployments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP-2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FT-MDT: Extracting Decision Trees from Medical Texts via a Novel
  Low-rank Adaptation Method <span class="chip">EMNLP-2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.04655v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.04655v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuheng Li, Jiechao Gao, Wei Han, Wenwen Ouyang, Wei Zhu, Hui Yi Leong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge of the medical decision process, which can be modeled as medical
decision trees (MDTs), is critical to building clinical decision support
systems. However, current MDT construction methods rely heavily on
time-consuming and laborious manual annotation. To address this challenge, we
propose PI-LoRA (Path-Integrated LoRA), a novel low-rank adaptation method for
automatically extracting MDTs from clinical guidelines and textbooks. We
integrate gradient path information to capture synergistic effects between
different modules, enabling more effective and reliable rank allocation. This
framework ensures that the most critical modules receive appropriate rank
allocations while less important ones are pruned, resulting in a more efficient
and accurate model for extracting medical decision trees from clinical texts.
Extensive experiments on medical guideline datasets demonstrate that our
PI-LoRA method significantly outperforms existing parameter-efficient
fine-tuning approaches for the Text2MDT task, achieving better accuracy with
substantially reduced model complexity. The proposed method achieves
state-of-the-art results while maintaining a lightweight architecture, making
it particularly suitable for clinical decision support systems where
computational resources may be limited.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP-2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Consistency of Responses and Continuations Generated by Large Language
  Models on Social Media <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08102v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08102v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenlu Fan, Yuqi Zhu, Bin Wang, Wentao Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) demonstrate remarkable capabilities in text
generation, yet their emotional consistency and semantic coherence in social
media contexts remain insufficiently understood. This study investigates how
LLMs handle emotional content and maintain semantic relationships through
continuation and response tasks using three open-source models: Gemma, Llama3
and Llama3.3 and one commercial Model:Claude. By analyzing climate change
discussions from Twitter and Reddit, we examine emotional transitions,
intensity patterns, and semantic consistency between human-authored and
LLM-generated content. Our findings reveal that while both models maintain high
semantic coherence, they exhibit distinct emotional patterns: these models show
a strong tendency to moderate negative emotions. When the input text carries
negative emotions such as anger, disgust, fear, or sadness, LLM tends to
generate content with more neutral emotions, or even convert them into positive
emotions such as joy or surprise. At the same time, we compared the
LLM-generated content with human-authored content. The four models
systematically generated responses with reduced emotional intensity and showed
a preference for neutral rational emotions in the response task. In addition,
these models all maintained a high semantic similarity with the original text,
although their performance in the continuation task and the response task was
different. These findings provide deep insights into the emotion and semantic
processing capabilities of LLM, which are of great significance for its
deployment in social media environments and human-computer interaction design.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by the International AAAI Conference on
  Web and Social Media (ICWSM) 2026 (Los Angeles, California, U.S.)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PatientSim: A Persona-Driven Simulator for Realistic Doctor-Patient
  Interactions <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.17818v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.17818v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daeun Kyung, Hyunseung Chung, Seongsu Bae, Jiho Kim, Jae Ho Sohn, Taerim Kim, Soo Kyung Kim, Edward Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Doctor-patient consultations require multi-turn, context-aware communication
tailored to diverse patient personas. Training or evaluating doctor LLMs in
such settings requires realistic patient interaction systems. However, existing
simulators often fail to reflect the full range of personas seen in clinical
practice. To address this, we introduce PatientSim, a patient simulator that
generates realistic and diverse patient personas for clinical scenarios,
grounded in medical expertise. PatientSim operates using: 1) clinical profiles,
including symptoms and medical history, derived from real-world data in the
MIMIC-ED and MIMIC-IV datasets, and 2) personas defined by four axes:
personality, language proficiency, medical history recall level, and cognitive
confusion level, resulting in 37 unique combinations. We evaluate eight LLMs
for factual accuracy and persona consistency. The top-performing open-source
model, Llama 3.3 70B, is validated by four clinicians to confirm the robustness
of our framework. As an open-source, customizable platform, PatientSim provides
a reproducible and scalable solution that can be customized for specific
training needs. Offering a privacy-compliant environment, it serves as a robust
testbed for evaluating medical dialogue systems across diverse patient
presentations and shows promise as an educational tool for healthcare. The code
is available at https://github.com/dek924/PatientSim.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a Spotlight at NeurIPS 2025 Datasets and Benchmarks Track
  (10 pages for main text, 4 pages for references, 36 pages for supplementary
  materials)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DPRF: A Generalizable Dynamic Persona Refinement Framework for
  Optimizing Behavior Alignment Between Personalized LLM Role-Playing Agents
  and Humans 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.14205v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.14205v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bingsheng Yao, Bo Sun, Yuanzhe Dong, Yuxuan Lu, Dakuo Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emerging large language model role-playing agents (LLM RPAs) aim to
simulate individual human behaviors, but the persona fidelity is often
undermined by manually-created profiles (e.g., cherry-picked information and
personality characteristics) without validating the alignment with the target
individuals. To address this limitation, our work introduces the Dynamic
Persona Refinement Framework (DPRF). DPRF aims to optimize the alignment of LLM
RPAs' behaviors with those of target individuals by iteratively identifying the
cognitive divergence, either through free-form or theory-grounded, structured
analysis, between generated behaviors and human ground truth, and refining the
persona profile to mitigate these divergences. We evaluate DPRF with five LLMs
on four diverse behavior-prediction scenarios: formal debates, social media
posts with mental health issues, public interviews, and movie reviews. DPRF can
consistently improve behavioral alignment considerably over baseline personas
and generalizes across models and scenarios. Our work provides a robust
methodology for creating high-fidelity persona profiles and enhancing the
validity of downstream applications, such as user simulation, social studies,
and personalized AI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ConsistencyAI: A Benchmark to Assess LLMs' Factual Consistency When
  Responding to Different Demographic Groups 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.13852v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.13852v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Banyas, Shristi Sharma, Alistair Simmons, Atharva Vispute
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Is an LLM telling you different facts than it's telling me? This paper
introduces ConsistencyAI, an independent benchmark for measuring the factual
consistency of large language models (LLMs) for different personas.
ConsistencyAI tests whether, when users of different demographics ask identical
questions, the model responds with factually inconsistent answers. Designed
without involvement from LLM providers, this benchmark offers impartial
evaluation and accountability. In our experiment, we queried 19 LLMs with
prompts that requested 5 facts for each of 15 topics. We repeated this query
100 times for each LLM, each time adding prompt context from a different
persona selected from a subset of personas modeling the general population. We
processed the responses into sentence embeddings, computed cross-persona cosine
similarity, and computed the weighted average of cross-persona cosine
similarity to calculate factual consistency scores. In 100-persona experiments,
scores ranged from 0.9065 to 0.7896, and the mean was 0.8656, which we adopt as
a benchmark threshold. xAI's Grok-3 is most consistent, while several
lightweight models rank lowest. Consistency varies by topic: the job market is
least consistent, G7 world leaders most consistent, and issues like vaccines or
the Israeli-Palestinian conflict diverge by provider. These results show that
both the provider and the topic shape the factual consistency. We release our
code and interactive demo to support reproducible evaluation and encourage
persona-invariant prompting strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>For associated code repository, see
  http://github.com/banyasp/consistencyAI For user-friendly web app, see
  http://v0-llm-comparison-webapp.vercel.app/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AutoLibra: Agent Metric Induction from Open-Ended Human Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.02820v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.02820v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Zhu, Phil Cuvin, Xinkai Yu, Charlotte Ka Yee Yan, Jason Zhang, Diyi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Agents are predominantly evaluated and optimized via task success metrics,
which are coarse, rely on manual design from experts, and fail to reward
intermediate emergent behaviors. We propose **AutoLibra**, a framework for
agent evaluation, that transforms open-ended human feedback *e.g.* "If you find
that the button is disabled, don't click it again", or "This agent has too much
autonomy to decide what to do on its own" into metrics for evaluating
fine-grained behaviors in agent trajectories. AutoLibra accomplishes this by
grounding feedback to an agent's behavior, clustering similar positive and
negative behaviors, and creating concrete metrics with clear definitions and
concrete examples, which can be used for prompting LLM-as-a-Judge as
evaluators. We further propose two meta metrics to evaluate the alignment of a
set of (induced) metrics with open feedback: "coverage" and "redundancy".
Through optimizing these meta-metrics, we experimentally demonstrate
AutoLibra's ability to induce more concrete agent evaluation metrics than the
ones proposed in previous agent evaluation benchmarks and discover new metrics
to analyze agents. We also present two applications of AutoLibra in agent
improvement: First, we show that AutoLibra serve human prompt engineers for
diagonalize agent failures and improve prompts iterative. Moreover, we find
that AutoLibra can induce metrics for automatic optimization for agents, which
makes agents improve through self-regulation. Our results suggest that
AutoLibra is a powerful task-agnostic tool for evaluating and improving
language agents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/Open-Social-World/autolibra</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reward Collapse in Aligning Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.17608v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.17608v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziang Song, Tianle Cai, Jason D. Lee, Weijie J. Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The extraordinary capabilities of large language models (LLMs) such as
ChatGPT and GPT-4 are in part unleashed by aligning them with reward models
that are trained on human preferences, which are often represented as rankings
of responses to prompts. In this paper, we document the phenomenon of
\textit{reward collapse}, an empirical observation where the prevailing
ranking-based approach results in an \textit{identical} reward distribution
\textit{regardless} of the prompts during the terminal phase of training. This
outcome is undesirable as open-ended prompts like ``write a short story about
your best friend'' should yield a continuous range of rewards for their
completions, while specific prompts like ``what is the capital of New Zealand''
should generate either high or low rewards. Our theoretical investigation
reveals that reward collapse is primarily due to the insufficiency of the
ranking-based objective function to incorporate prompt-related information
during optimization. This insight allows us to derive closed-form expressions
for the reward distribution associated with a set of utility functions in an
asymptotic regime. To overcome reward collapse, we introduce a prompt-aware
optimization scheme that provably admits a prompt-dependent reward distribution
within the interpolating regime. Our experimental results suggest that our
proposed prompt-aware utility functions significantly alleviate reward collapse
during the training of reward models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in the Journal of Data Science (JDS),
  reference JDS1201</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vision-and-Language Training Helps Deploy Taxonomic Knowledge but Does
  Not Fundamentally Alter It 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.13328v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.13328v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yulu Qin, Dheeraj Varghese, Adam Dahlgren Lindström, Lucia Donatelli, Kanishka Misra, Najoung Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Does vision-and-language (VL) training change the linguistic representations
of language models in meaningful ways? Most results in the literature have
shown inconsistent or marginal differences, both behaviorally and
representationally. In this work, we start from the hypothesis that the domain
in which VL training could have a significant effect is lexical-conceptual
knowledge, in particular its taxonomic organization. Through comparing minimal
pairs of text-only LMs and their VL-trained counterparts, we first show that
the VL models often outperform their text-only counterparts on a text-only
question-answering task that requires taxonomic understanding of concepts
mentioned in the questions. Using an array of targeted behavioral and
representational analyses, we show that the LMs and VLMs do not differ
significantly in terms of their taxonomic knowledge itself, but they differ in
how they represent questions that contain concepts in a taxonomic relation vs.
a non-taxonomic relation. This implies that the taxonomic knowledge itself does
not change substantially through additional VL training, but VL training does
improve the deployment of this knowledge in the context of a specific task,
even when the presentation of the task is purely linguistic.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language Models can Self-Improve at State-Value Estimation for Better
  Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02878v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02878v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ethan Mendes, Alan Ritter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collecting ground-truth rewards or human demonstrations for multi-step
reasoning tasks is often prohibitively expensive, particularly in interactive
domains such as web tasks. We introduce Self-Taught Lookahead (STL), a
reward-free framework that improves language model-based value functions by
reasoning explicitly about state transitions. STL can be viewed as a
chain-of-thought analogue of the value iteration algorithm: instead of
regressing directly on numeric values, a value LLM is trained to simulate a
step of lookahead in natural language - predicting the next action, resulting
state, and rationale for its value, thereby refining value estimates without
any labeled data. This self-supervised procedure yields more accurate
state-value predictions, which in turn enable lightweight search algorithms to
expand fewer states while maintaining strong performance. Empirically,
STL-trained value models built on moderately sized (8B parameter) open-weight
LLMs boost web agent success rates by 39%, achieving comparable performance
with proprietary models. STL also generalizes to multi-hop QA and math puzzles.
We find that STL enables small open-source models to guide efficient search,
reducing inference costs by integrating explicit reasoning with value learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TabSTAR: A Tabular Foundation Model for Tabular Data with Text Fields <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.18125v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.18125v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alan Arazi, Eilam Shapira, Roi Reichart
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While deep learning has achieved remarkable success across many domains, it
has historically underperformed on tabular learning tasks, which remain
dominated by gradient boosting decision trees. However, recent advancements are
paving the way for Tabular Foundation Models, which can leverage real-world
knowledge and generalize across diverse datasets, particularly when the data
contains free-text. Although incorporating language model capabilities into
tabular tasks has been explored, most existing methods utilize static,
target-agnostic textual representations, limiting their effectiveness. We
introduce TabSTAR: a Tabular Foundation Model with Semantically Target-Aware
Representations. TabSTAR is designed to enable transfer learning on tabular
data with textual features, with an architecture free of dataset-specific
parameters. It unfreezes a pretrained text encoder and takes as input target
tokens, which provide the model with the context needed to learn task-specific
embeddings. TabSTAR achieves state-of-the-art performance for both medium- and
large-sized datasets across known benchmarks of classification tasks with text
features, and its pretraining phase exhibits scaling laws in the number of
datasets, offering a pathway for further performance improvements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ This Candidate is [MASK]. <span class="highlight-title">Prompt</span>-based Sentiment Extraction and
  Reference Letters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16325v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16325v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabian Slonimczyk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  I propose a relatively simple way to deploy pre-trained large language models
(LLMs) in order to extract sentiment and other useful features from text data.
The method, which I refer to as prompt-based sentiment extraction, offers
multiple advantages over other methods used in economics and finance. In
particular, it accepts the text input as is (without pre-processing) and
produces a sentiment score that has a probability interpretation. Unlike other
LLM-based approaches, it does not require any fine-tuning or labeled data. I
apply my prompt-based strategy to a hand-collected corpus of confidential
reference letters (RLs). I show that the sentiment contents of RLs are clearly
reflected in job market outcomes. Candidates with higher average sentiment in
their RLs perform markedly better regardless of the measure of success chosen.
Moreover, I show that sentiment dispersion among letter writers negatively
affects the job market candidate's performance. I compare my sentiment
extraction approach to other commonly used methods for sentiment analysis:
`bag-of-words' approaches, fine-tuned language models, and querying advanced
chatbots. No other method can fully reproduce the results obtained by
prompt-based sentiment extraction. Finally, I slightly modify the method to
obtain `gendered' sentiment scores (as in Eberhardt et al., 2023). I show that
RLs written for female candidates emphasize `grindstone' personality traits,
whereas male candidates' letters emphasize `standout' traits. These gender
differences negatively affect women's job market outcomes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ M-Prometheus: A Suite of Open Multilingual LLM Judges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.04953v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.04953v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        José Pombal, Dongkeun Yoon, Patrick Fernandes, Ian Wu, Seungone Kim, Ricardo Rei, Graham Neubig, André F. T. Martins
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The use of language models for automatically evaluating long-form text
(LLM-as-a-judge) is becoming increasingly common, yet most LLM judges are
optimized exclusively for English, with strategies for enhancing their
multilingual evaluation capabilities remaining largely unexplored in the
current literature. This has created a disparity in the quality of automatic
evaluation methods for non-English languages, ultimately hindering the
development of models with better multilingual capabilities. To bridge this
gap, we introduce M-Prometheus, a suite of open-weight LLM judges ranging from
3B to 14B parameters that can provide both direct assessment and pairwise
comparison feedback on multilingual outputs. M-Prometheus models outperform
state-of-the-art open LLM judges on multilingual reward benchmarks spanning
more than 20 languages, as well as on literary machine translation (MT)
evaluation covering 4 language pairs. Furthermore, M-Prometheus models can be
leveraged at decoding time to significantly improve generated outputs across
all 3 tested languages, showcasing their utility for the development of better
multilingual models. Lastly, through extensive ablations, we identify the key
factors for obtaining an effective multilingual judge, including backbone model
selection and training on synthetic multilingual feedback data instead of
translated data. We release our models, training dataset, and code.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Zero-shot Benchmarking: A Framework for Flexible and Scalable Automatic
  Evaluation of Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.01001v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.01001v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        José Pombal, Nuno M. Guerreiro, Ricardo Rei, André F. T. Martins
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As language models improve and become capable of performing more complex
tasks across modalities, evaluating them automatically becomes increasingly
challenging. Developing strong and robust task-specific automatic metrics gets
harder, and human-annotated test sets -- which are expensive to create --
saturate more quickly. A compelling alternative is to design reliable
strategies to automate the creation of test data and evaluation, but previous
attempts either rely on pre-existing data, or focus solely on individual tasks.
We present Zero-shot Benchmarking (ZSB), a framework for creating high-quality
benchmarks for any task by leveraging language models for both synthetic test
data creation and evaluation. ZSB is simple and flexible: it requires only the
creation of a prompt for data generation and one for evaluation; it is scalable
to tasks and languages where collecting real-world data is costly or
impractical; it is model-agnostic, allowing the creation of increasingly
challenging benchmarks as models improve. To assess the effectiveness of our
framework, we create benchmarks for five text-only tasks and a multi-modal one:
general capabilities in four languages (English, Chinese, French, and Korean),
translation, and general vision-language capabilities in English. We then rank
a broad range of open and closed systems on our benchmarks. ZSB rankings
consistently correlate strongly with human rankings, outperforming
widely-adopted standard benchmarks. Through ablations, we find that strong
benchmarks can be created with open models, and that judge model size and
dataset variety are crucial drivers of performance. We release all our
benchmarks, and code to reproduce our experiments and to produce new
benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unveiling the Learning Mind of Language Models: A Cognitive Framework
  and Empirical Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13464v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13464v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengyu Hu, Jianxun Lian, Zheyuan Xiao, Seraphina Zhang, Tianfu Wang, Nicholas Jing Yuan, Xing Xie, Hui Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown impressive capabilities across tasks
such as mathematics, coding, and reasoning, yet their learning ability, which
is crucial for adapting to dynamic environments and acquiring new knowledge,
remains underexplored. In this work, we address this gap by introducing a
framework inspired by cognitive psychology and education. Specifically, we
decompose general learning ability into three distinct, complementary
dimensions: Learning from Instructor (acquiring knowledge via explicit
guidance), Learning from Concept (internalizing abstract structures and
generalizing to new contexts), and Learning from Experience (adapting through
accumulated exploration and feedback). We conduct a comprehensive empirical
study across the three learning dimensions and identify several insightful
findings, such as (i) interaction improves learning; (ii) conceptual
understanding is scale-emergent and benefits larger models; and (iii) LLMs are
effective few-shot learners but not many-shot learners. Based on our framework
and empirical findings, we introduce a benchmark that provides a unified and
realistic evaluation of LLMs' general learning abilities across three learning
cognition dimensions. It enables diagnostic insights and supports evaluation
and development of more adaptive and human-like models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The LSCD Benchmark: a Testbed for Diachronic Word Meaning Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.00176v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.00176v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominik Schlechtweg, Sachin Yadav, Nikolay Arefyev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lexical Semantic Change Detection (LSCD) is a complex, lemma-level task,
which is usually operationalized based on two subsequently applied usage-level
tasks: First, Word-in-Context (WiC) labels are derived for pairs of usages.
Then, these labels are represented in a graph on which Word Sense Induction
(WSI) is applied to derive sense clusters. Finally, LSCD labels are derived by
comparing sense clusters over time. This modularity is reflected in most LSCD
datasets and models. It also leads to a large heterogeneity in modeling options
and task definitions, which is exacerbated by a variety of dataset versions,
preprocessing options and evaluation metrics. This heterogeneity makes it
difficult to evaluate models under comparable conditions, to choose optimal
model combinations or to reproduce results. Hence, we provide a benchmark
repository standardizing LSCD evaluation. Through transparent implementation
results become easily reproducible and by standardization different components
can be freely combined. The repository reflects the task's modularity by
allowing model evaluation for WiC, WSI and LSCD. This allows for careful
evaluation of increasingly complex model components providing new ways of model
optimization. We use the implemented benchmark to conduct a number of
experiments with recent models and systematically improve the state-of-the-art.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GradEscape: A Gradient-Based Evader Against AI-Generated Text Detectors <span class="chip">USENIX Security'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.08188v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.08188v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenlong Meng, Shuguo Fan, Chengkun Wei, Min Chen, Yuwei Li, Yuanchao Zhang, Zhikun Zhang, Wenzhi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce GradEscape, the first gradient-based evader
designed to attack AI-generated text (AIGT) detectors. GradEscape overcomes the
undifferentiable computation problem, caused by the discrete nature of text, by
introducing a novel approach to construct weighted embeddings for the detector
input. It then updates the evader model parameters using feedback from victim
detectors, achieving high attack success with minimal text modification. To
address the issue of tokenizer mismatch between the evader and the detector, we
introduce a warm-started evader method, enabling GradEscape to adapt to
detectors across any language model architecture. Moreover, we employ novel
tokenizer inference and model extraction techniques, facilitating effective
evasion even in query-only access.
  We evaluate GradEscape on four datasets and three widely-used language
models, benchmarking it against four state-of-the-art AIGT evaders.
Experimental results demonstrate that GradEscape outperforms existing evaders
in various scenarios, including with an 11B paraphrase model, while utilizing
only 139M parameters. We have successfully applied GradEscape to two real-world
commercial AIGT detectors. Our analysis reveals that the primary vulnerability
stems from disparity in text expression styles within the training data. We
also propose a potential defense strategy to mitigate the threat of AIGT
evaders. We open-source our GradEscape for developing more robust AIGT
detectors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by USENIX Security'25; Update badges and Artifact Appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VC4VG: Optimizing Video Captions for Text-to-Video Generation <span class="chip">EMNLP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24134v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24134v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Du, Zhuoran Lin, Kaiqiang Song, Biao Wang, Zhicheng Zheng, Tiezheng Ge, Bo Zheng, Qin Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in text-to-video (T2V) generation highlight the critical role
of high-quality video-text pairs in training models capable of producing
coherent and instruction-aligned videos. However, strategies for optimizing
video captions specifically for T2V training remain underexplored. In this
paper, we introduce VC4VG (Video Captioning for Video Generation), a
comprehensive caption optimization framework tailored to the needs of T2V
models. We begin by analyzing caption content from a T2V perspective,
decomposing the essential elements required for video reconstruction into
multiple dimensions, and proposing a principled caption design methodology. To
support evaluation, we construct VC4VG-Bench, a new benchmark featuring
fine-grained, multi-dimensional, and necessity-graded metrics aligned with
T2V-specific requirements. Extensive T2V fine-tuning experiments demonstrate a
strong correlation between improved caption quality and video generation
performance, validating the effectiveness of our approach. We release all
benchmark tools and code at https://github.com/alimama-creative/VC4VG to
support further research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adversarial Paraphrasing: A Universal Attack for Humanizing AI-Generated
  Text <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.07001v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.07001v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yize Cheng, Vinu Sankar Sadasivan, Mehrdad Saberi, Shoumik Saha, Soheil Feizi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing capabilities of Large Language Models (LLMs) have raised
concerns about their misuse in AI-generated plagiarism and social engineering.
While various AI-generated text detectors have been proposed to mitigate these
risks, many remain vulnerable to simple evasion techniques such as
paraphrasing. However, recent detectors have shown greater robustness against
such basic attacks. In this work, we introduce Adversarial Paraphrasing, a
training-free attack framework that universally humanizes any AI-generated text
to evade detection more effectively. Our approach leverages an off-the-shelf
instruction-following LLM to paraphrase AI-generated content under the guidance
of an AI text detector, producing adversarial examples that are specifically
optimized to bypass detection. Extensive experiments show that our attack is
both broadly effective and highly transferable across several detection
systems. For instance, compared to simple paraphrasing attack--which,
ironically, increases the true positive at 1% false positive (T@1%F) by 8.57%
on RADAR and 15.03% on Fast-DetectGPT--adversarial paraphrasing, guided by
OpenAI-RoBERTa-Large, reduces T@1%F by 64.49% on RADAR and a striking 98.96% on
Fast-DetectGPT. Across a diverse set of detectors--including neural
network-based, watermark-based, and zero-shot approaches--our attack achieves
an average T@1%F reduction of 87.88% under the guidance of
OpenAI-RoBERTa-Large. We also analyze the tradeoff between text quality and
attack success to find that our method can significantly reduce detection
rates, with mostly a slight degradation in text quality. Our adversarial setup
highlights the need for more robust and resilient detection strategies in the
light of increasingly sophisticated evasion techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Latent Chain-of-Thought for Visual Reasoning <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23925v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23925v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guohao Sun, Hang Hua, Jian Wang, Jiebo Luo, Sohail Dianat, Majid Rabbani, Raghuveer Rao, Zhiqiang Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chain-of-thought (CoT) reasoning is critical for improving the
interpretability and reliability of Large Vision-Language Models (LVLMs).
However, existing training algorithms such as SFT, PPO, and GRPO may not
generalize well across unseen reasoning tasks and heavily rely on a biased
reward model. To address this challenge, we reformulate reasoning in LVLMs as
posterior inference and propose a scalable training algorithm based on
amortized variational inference. By leveraging diversity-seeking reinforcement
learning algorithms, we introduce a novel sparse reward function for
token-level learning signals that encourage diverse, high-likelihood latent
CoT, overcoming deterministic sampling limitations and avoiding reward hacking.
Additionally, we implement a Bayesian inference-scaling strategy that replaces
costly Best-of-N and Beam Search with a marginal likelihood to efficiently rank
optimal rationales and answers. We empirically demonstrate that the proposed
method enhances the state-of-the-art LVLMs on seven reasoning benchmarks, in
terms of effectiveness, generalization, and interpretability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AI Debate Aids Assessment of Controversial Claims 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.02175v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.02175v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Salman Rahman, Sheriff Issaka, Ashima Suvarna, Genglin Liu, James Shiffer, Jaeyoung Lee, Md Rizwan Parvez, Hamid Palangi, Shi Feng, Nanyun Peng, Yejin Choi, Julian Michael, Liwei Jiang, Saadia Gabriel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As AI grows more powerful, it will increasingly shape how we understand the
world. But with this influence comes the risk of amplifying misinformation and
deepening social divides-especially on consequential topics where factual
accuracy directly impacts well-being. Scalable Oversight aims to ensure AI
systems remain truthful even when their capabilities exceed those of their
evaluators. Yet when humans serve as evaluators, their own beliefs and biases
can impair judgment. We study whether AI debate can guide biased judges toward
the truth by having two AI systems debate opposing sides of controversial
factuality claims on COVID-19 and climate change where people hold strong prior
beliefs. We conduct two studies. Study I recruits human judges with either
mainstream or skeptical beliefs who evaluate claims through two protocols:
debate (interaction with two AI advisors arguing opposing sides) or consultancy
(interaction with a single AI advisor). Study II uses AI judges with and
without human-like personas to evaluate the same protocols. In Study I, debate
consistently improves human judgment accuracy and confidence calibration,
outperforming consultancy by 4-10% across COVID-19 and climate change claims.
The improvement is most significant for judges with mainstream beliefs (up to
+15.2% accuracy on COVID-19 claims), though debate also helps skeptical judges
who initially misjudge claims move toward accurate views (+4.7% accuracy). In
Study II, AI judges with human-like personas achieve even higher accuracy
(78.5%) than human judges (70.1%) and default AI judges without personas
(69.8%), suggesting their potential for supervising frontier AI models. These
findings highlight AI debate as a promising path toward scalable,
bias-resilient oversight in contested domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fuzzy, Symbolic, and Contextual: Enhancing LLM Instruction via Cognitive
  Scaffolding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.21204v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.21204v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vanessa Figueiredo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study how prompt-level inductive biases influence the cognitive behavior
of large language models (LLMs) in instructional dialogue. We introduce a
symbolic scaffolding method paired with a short-term memory schema designed to
promote adaptive, structured reasoning in Socratic tutoring. Using controlled
ablation across five system variants, we evaluate model outputs via
expert-designed rubrics covering scaffolding, responsiveness, symbolic
reasoning, and conversational memory. We present preliminary results using an
LLM-based evaluation framework aligned to a cognitively grounded rubric. This
enables scalable, systematic comparisons across architectural variants in
early-stage experimentation. The preliminary results show that our full system
consistently outperforms baseline variants. Analysis reveals that removing
memory or symbolic structure degrades key cognitive behaviors, including
abstraction, adaptive probing, and conceptual continuity. These findings
support a processing-level account in which prompt-level cognitive scaffolds
can reliably shape emergent instructional strategies in LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">118</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VFXMaster: Unlocking Dynamic Visual Effect Generation via In-Context
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25772v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25772v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baolu Li, Yiming Zhang, Qinghe Wang, Liqian Ma, Xiaoyu Shi, Xintao Wang, Pengfei Wan, Zhenfei Yin, Yunzhi Zhuge, Huchuan Lu, Xu Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual effects (VFX) are crucial to the expressive power of digital media,
yet their creation remains a major challenge for generative AI. Prevailing
methods often rely on the one-LoRA-per-effect paradigm, which is
resource-intensive and fundamentally incapable of generalizing to unseen
effects, thus limiting scalability and creation. To address this challenge, we
introduce VFXMaster, the first unified, reference-based framework for VFX video
generation. It recasts effect generation as an in-context learning task,
enabling it to reproduce diverse dynamic effects from a reference video onto
target content. In addition, it demonstrates remarkable generalization to
unseen effect categories. Specifically, we design an in-context conditioning
strategy that prompts the model with a reference example. An in-context
attention mask is designed to precisely decouple and inject the essential
effect attributes, allowing a single unified model to master the effect
imitation without information leakage. In addition, we propose an efficient
one-shot effect adaptation mechanism to boost generalization capability on
tough unseen effects from a single user-provided video rapidly. Extensive
experiments demonstrate that our method effectively imitates various categories
of effect information and exhibits outstanding generalization to out-of-domain
effects. To foster future research, we will release our code, models, and a
comprehensive dataset to the community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page URL:https://libaolu312.github.io/VFXMaster/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FreeArt3D: Training-Free Articulated Object Generation using 3D
  Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25765v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25765v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuhao Chen, Isabella Liu, Xinyue Wei, Hao Su, Minghua Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Articulated 3D objects are central to many applications in robotics, AR/VR,
and animation. Recent approaches to modeling such objects either rely on
optimization-based reconstruction pipelines that require dense-view supervision
or on feed-forward generative models that produce coarse geometric
approximations and often overlook surface texture. In contrast, open-world 3D
generation of static objects has achieved remarkable success, especially with
the advent of native 3D diffusion models such as Trellis. However, extending
these methods to articulated objects by training native 3D diffusion models
poses significant challenges. In this work, we present FreeArt3D, a
training-free framework for articulated 3D object generation. Instead of
training a new model on limited articulated data, FreeArt3D repurposes a
pre-trained static 3D diffusion model (e.g., Trellis) as a powerful shape
prior. It extends Score Distillation Sampling (SDS) into the 3D-to-4D domain by
treating articulation as an additional generative dimension. Given a few images
captured in different articulation states, FreeArt3D jointly optimizes the
object's geometry, texture, and articulation parameters without requiring
task-specific training or access to large-scale articulated datasets. Our
method generates high-fidelity geometry and textures, accurately predicts
underlying kinematic structures, and generalizes well across diverse object
categories. Despite following a per-instance optimization paradigm, FreeArt3D
completes in minutes and significantly outperforms prior state-of-the-art
approaches in both quality and versatility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Spatial Reasoning in the Large Model Era: A <span class="highlight-title">Survey</span> and
  Benchmarks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25760v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25760v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xu Zheng, Zihao Dongfang, Lutao Jiang, Boyuan Zheng, Yulong Guo, Zhenquan Zhang, Giuliano Albanese, Runyi Yang, Mengjiao Ma, Zixin Zhang, Chenfei Liao, Dingcheng Zhen, Yuanhuiyi Lyu, Yuqian Fu, Bin Ren, Linfeng Zhang, Danda Pani Paudel, Nicu Sebe, Luc Van Gool, Xuming Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans possess spatial reasoning abilities that enable them to understand
spaces through multimodal observations, such as vision and sound. Large
multimodal reasoning models extend these abilities by learning to perceive and
reason, showing promising performance across diverse spatial tasks. However,
systematic reviews and publicly available benchmarks for these models remain
limited. In this survey, we provide a comprehensive review of multimodal
spatial reasoning tasks with large models, categorizing recent progress in
multimodal large language models (MLLMs) and introducing open benchmarks for
evaluation. We begin by outlining general spatial reasoning, focusing on
post-training techniques, explainability, and architecture. Beyond classical 2D
tasks, we examine spatial relationship reasoning, scene and layout
understanding, as well as visual question answering and grounding in 3D space.
We also review advances in embodied AI, including vision-language navigation
and action models. Additionally, we consider emerging modalities such as audio
and egocentric video, which contribute to novel spatial understanding through
new sensors. We believe this survey establishes a solid foundation and offers
insights into the growing field of multimodal spatial reasoning. Updated
information about this survey, codes and implementation of the open benchmarks
can be found at https://github.com/zhengxuJosh/Awesome-Spatial-Reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hawk: Leveraging Spatial Context for Faster Autoregressive Text-to-Image
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25739v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25739v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhi-Kai Chen, Jun-Peng Jiang, Han-Jia Ye, De-Chuan Zhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autoregressive (AR) image generation models are capable of producing
high-fidelity images but often suffer from slow inference due to their
inherently sequential, token-by-token decoding process. Speculative decoding,
which employs a lightweight draft model to approximate the output of a larger
AR model, has shown promise in accelerating text generation without
compromising quality. However, its application to image generation remains
largely underexplored. The challenges stem from a significantly larger sampling
space, which complicates the alignment between the draft and target model
outputs, coupled with the inadequate use of the two-dimensional spatial
structure inherent in images, thereby limiting the modeling of local
dependencies. To overcome these challenges, we introduce Hawk, a new approach
that harnesses the spatial structure of images to guide the speculative model
toward more accurate and efficient predictions. Experimental results on
multiple text-to-image benchmarks demonstrate a 1.71x speedup over standard AR
models, while preserving both image fidelity and diversity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Feedback Alignment Meets Low-Rank Manifolds: A Structured Recipe for
  Local Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25594v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25594v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arani Roy, Marco P. Apolinario, Shristi Das Biswas, Kaushik Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training deep neural networks (DNNs) with backpropagation (BP) achieves
state-of-the-art accuracy but requires global error propagation and full
parameterization, leading to substantial memory and computational overhead.
Direct Feedback Alignment (DFA) enables local, parallelizable updates with
lower memory requirements but is limited by unstructured feedback and poor
scalability in deeper architectures, specially convolutional neural networks.
To address these limitations, we propose a structured local learning framework
that operates directly on low-rank manifolds defined by the Singular Value
Decomposition (SVD) of weight matrices. Each layer is trained in its decomposed
form, with updates applied to the SVD components using a composite loss that
integrates cross-entropy, subspace alignment, and orthogonality regularization.
Feedback matrices are constructed to match the SVD structure, ensuring
consistent alignment between forward and feedback pathways. Our method reduces
the number of trainable parameters relative to the original DFA model, without
relying on pruning or post hoc compression. Experiments on CIFAR-10, CIFAR-100,
and ImageNet show that our method achieves accuracy comparable to that of BP.
Ablation studies confirm the importance of each loss term in the low-rank
setting. These results establish local learning on low-rank manifolds as a
principled and scalable alternative to full-rank gradient-based training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RegionE: Adaptive Region-Aware Generation for Efficient Image Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25590v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25590v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengtao Chen, Xianfang Zeng, Maosen Zhao, Mingzhu Shen, Peng Ye, Bangyin Xiang, Zhibo Wang, Wei Cheng, Gang Yu, Tao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, instruction-based image editing (IIE) has received widespread
attention. In practice, IIE often modifies only specific regions of an image,
while the remaining areas largely remain unchanged. Although these two types of
regions differ significantly in generation difficulty and computational
redundancy, existing IIE models do not account for this distinction, instead
applying a uniform generation process across the entire image. This motivates
us to propose RegionE, an adaptive, region-aware generation framework that
accelerates IIE tasks without additional training. Specifically, the RegionE
framework consists of three main components: 1) Adaptive Region Partition. We
observed that the trajectory of unedited regions is straight, allowing for
multi-step denoised predictions to be inferred in a single step. Therefore, in
the early denoising stages, we partition the image into edited and unedited
regions based on the difference between the final estimated result and the
reference image. 2) Region-Aware Generation. After distinguishing the regions,
we replace multi-step denoising with one-step prediction for unedited areas.
For edited regions, the trajectory is curved, requiring local iterative
denoising. To improve the efficiency and quality of local iterative generation,
we propose the Region-Instruction KV Cache, which reduces computational cost
while incorporating global information. 3) Adaptive Velocity Decay Cache.
Observing that adjacent timesteps in edited regions exhibit strong velocity
similarity, we further propose an adaptive velocity decay cache to accelerate
the local denoising process. We applied RegionE to state-of-the-art IIE base
models, including Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit. RegionE
achieved acceleration factors of 2.57, 2.41, and 2.06. Evaluations by GPT-4o
confirmed that semantic and perceptual fidelity were well preserved.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 10 figures, 18 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparative Study of UNet-based Architectures for Liver Tumor
  Segmentation in Multi-Phase Contrast-Enhanced Computed Tomography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25522v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25522v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Doan-Van-Anh Ly, Thi-Thu-Hien Pham, Thanh-Hai Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Segmentation of liver structures in multi-phase contrast-enhanced computed
tomography (CECT) plays a crucial role in computer-aided diagnosis and
treatment planning for liver diseases, including tumor detection. In this
study, we investigate the performance of UNet-based architectures for liver
tumor segmentation, starting from the original UNet and extending to UNet3+
with various backbone networks. We evaluate ResNet, Transformer-based, and
State-space (Mamba) backbones, all initialized with pretrained weights.
Surprisingly, despite the advances in modern architecture, ResNet-based models
consistently outperform Transformer- and Mamba-based alternatives across
multiple evaluation metrics. To further improve segmentation quality, we
introduce attention mechanisms into the backbone and observe that incorporating
the Convolutional Block Attention Module (CBAM) yields the best performance.
ResNetUNet3+ with CBAM module not only produced the best overlap metrics with a
Dice score of 0.755 and IoU of 0.662, but also achieved the most precise
boundary delineation, evidenced by the lowest HD95 distance of 77.911. The
model's superiority was further cemented by its leading overall accuracy of
0.925 and specificity of 0.926, showcasing its robust capability in accurately
identifying both lesion and healthy tissue. To further enhance
interpretability, Grad-CAM visualizations were employed to highlight the
region's most influential predictions, providing insights into its
decision-making process. These findings demonstrate that classical ResNet
architecture, when combined with modern attention modules, remain highly
competitive for medical image segmentation tasks, offering a promising
direction for liver tumor detection in clinical practice.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FaCT: Faithful Concept Traces for Explaining Neural Network Decisions <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25512v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25512v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amin Parchami-Araghi, Sukrut Rao, Jonas Fischer, Bernt Schiele
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep networks have shown remarkable performance across a wide range of tasks,
yet getting a global concept-level understanding of how they function remains a
key challenge. Many post-hoc concept-based approaches have been introduced to
understand their workings, yet they are not always faithful to the model.
Further, they make restrictive assumptions on the concepts a model learns, such
as class-specificity, small spatial extent, or alignment to human expectations.
In this work, we put emphasis on the faithfulness of such concept-based
explanations and propose a new model with model-inherent mechanistic
concept-explanations. Our concepts are shared across classes and, from any
layer, their contribution to the logit and their input-visualization can be
faithfully traced. We also leverage foundation models to propose a new
concept-consistency metric, C$^2$-Score, that can be used to evaluate
concept-based methods. We show that, compared to prior work, our concepts are
quantitatively more consistent and users find our concepts to be more
interpretable, all while retaining competitive ImageNet performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2025; Code is available at
  https://github.com/m-parchami/FaCT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SPADE: Sparsity Adaptive Depth Estimator for Zero-Shot, Real-Time,
  Monocular Depth Estimation in Underwater Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25463v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25463v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongjie Zhang, Gideon Billings, Stefan B. Williams
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Underwater infrastructure requires frequent inspection and maintenance due to
harsh marine conditions. Current reliance on human divers or remotely operated
vehicles is limited by perceptual and operational challenges, especially around
complex structures or in turbid water. Enhancing the spatial awareness of
underwater vehicles is key to reducing piloting risks and enabling greater
autonomy. To address these challenges, we present SPADE: SParsity Adaptive
Depth Estimator, a monocular depth estimation pipeline that combines
pre-trained relative depth estimator with sparse depth priors to produce dense,
metric scale depth maps. Our two-stage approach first scales the relative depth
map with the sparse depth points, then refines the final metric prediction with
our proposed Cascade Conv-Deformable Transformer blocks. Our approach achieves
improved accuracy and generalisation over state-of-the-art baselines and runs
efficiently at over 15 FPS on embedded hardware, promising to support practical
underwater inspection and intervention. This work has been submitted to IEEE
Journal of Oceanic Engineering Special Issue of AUV 2026.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ More than a Moment: Towards Coherent Sequences of Audio Descriptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25440v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25440v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eshika Khandelwal, Junyu Xie, Tengda Han, Max Bain, Arsha Nagrani, Andrew Zisserman, Gül Varol, Makarand Tapaswi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio Descriptions (ADs) convey essential on-screen information, allowing
visually impaired audiences to follow videos. To be effective, ADs must form a
coherent sequence that helps listeners to visualise the unfolding scene, rather
than describing isolated moments. However, most automatic methods generate each
AD independently, often resulting in repetitive, incoherent descriptions. To
address this, we propose a training-free method, CoherentAD, that first
generates multiple candidate descriptions for each AD time interval, and then
performs auto-regressive selection across the sequence to form a coherent and
informative narrative. To evaluate AD sequences holistically, we introduce a
sequence-level metric, StoryRecall, which measures how well the predicted ADs
convey the ground truth narrative, alongside repetition metrics that capture
the redundancy across consecutive AD outputs. Our method produces coherent AD
sequences with enhanced narrative understanding, outperforming prior approaches
that rely on independent generations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Instance-Level Composed Image Retrieval <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25387v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25387v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bill Psomas, George Retsinas, Nikos Efthymiadis, Panagiotis Filntisis, Yannis Avrithis, Petros Maragos, Ondrej Chum, Giorgos Tolias
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The progress of composed image retrieval (CIR), a popular research direction
in image retrieval, where a combined visual and textual query is used, is held
back by the absence of high-quality training and evaluation data. We introduce
a new evaluation dataset, i-CIR, which, unlike existing datasets, focuses on an
instance-level class definition. The goal is to retrieve images that contain
the same particular object as the visual query, presented under a variety of
modifications defined by textual queries. Its design and curation process keep
the dataset compact to facilitate future research, while maintaining its
challenge-comparable to retrieval among more than 40M random
distractors-through a semi-automated selection of hard negatives.
  To overcome the challenge of obtaining clean, diverse, and suitable training
data, we leverage pre-trained vision-and-language models (VLMs) in a
training-free approach called BASIC. The method separately estimates
query-image-to-image and query-text-to-image similarities, performing late
fusion to upweight images that satisfy both queries, while down-weighting those
that exhibit high similarity with only one of the two. Each individual
similarity is further improved by a set of components that are simple and
intuitive. BASIC sets a new state of the art on i-CIR but also on existing CIR
datasets that follow a semantic-level class definition. Project page:
https://vrg.fel.cvut.cz/icir/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Prompt</span> Estimation from Prototypes for Federated <span class="highlight-title">Prompt</span> Tuning of Vision
  <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25372v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25372v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        M Yashwanth, Sharannya Ghosh, Aditay Tripathi, Anirban Chakraborty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Prompt Tuning (VPT) of pre-trained Vision Transformers (ViTs) has
proven highly effective as a parameter-efficient fine-tuning technique for
adapting large models to downstream tasks with limited data. Its parameter
efficiency makes it particularly suitable for Federated Learning (FL), where
both communication and computation budgets are often constrained. However,
global prompt tuning struggles to generalize across heterogeneous clients,
while personalized tuning overfits to local data and lacks generalization. We
propose PEP-FedPT (Prompt Estimation from Prototypes for Federated Prompt
Tuning), a unified framework designed to achieve both generalization and
personalization in federated prompt tuning of ViTs. Within this framework, we
introduce the novel Class-Contextualized Mixed Prompt (CCMP) - based on
class-specific prompts maintained alongside a globally shared prompt. For each
input, CCMP adaptively combines class-specific prompts using weights derived
from global class prototypes and client class priors. This approach enables
per-sample prompt personalization without storing client-dependent trainable
parameters. The prompts are collaboratively optimized via traditional federated
averaging technique on the same. Comprehensive evaluations on CIFAR-100,
TinyImageNet, DomainNet, and iNaturalist datasets demonstrate that PEP-FedPT
consistently surpasses the state-of-the-art baselines under diverse data
heterogeneity scenarios, establishing a strong foundation for efficient and
generalizable federated prompt tuning of Vision Transformers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D CT-Based Coronary Calcium Assessment: A Feature-Driven Machine
  Learning Framework <span class="chip">MICCAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25347v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25347v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayman Abaid, Gianpiero Guidone, Sara Alsubai, Foziyah Alquahtani, Talha Iqbal, Ruth Sharif, Hesham Elzomor, Emiliano Bianchini, Naeif Almagal, Michael G. Madden, Faisal Sharif, Ihsan Ullah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Coronary artery calcium (CAC) scoring plays a crucial role in the early
detection and risk stratification of coronary artery disease (CAD). In this
study, we focus on non-contrast coronary computed tomography angiography (CCTA)
scans, which are commonly used for early calcification detection in clinical
settings. To address the challenge of limited annotated data, we propose a
radiomics-based pipeline that leverages pseudo-labeling to generate training
labels, thereby eliminating the need for expert-defined segmentations.
Additionally, we explore the use of pretrained foundation models, specifically
CT-FM and RadImageNet, to extract image features, which are then used with
traditional classifiers. We compare the performance of these deep learning
features with that of radiomics features. Evaluation is conducted on a clinical
CCTA dataset comprising 182 patients, where individuals are classified into two
groups: zero versus non-zero calcium scores. We further investigate the impact
of training on non-contrast datasets versus combined contrast and non-contrast
datasets, with testing performed only on non contrast scans. Results show that
radiomics-based models significantly outperform CNN-derived embeddings from
foundation models (achieving 84% accuracy and p<0.05), despite the
unavailability of expert annotations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 2 Figures, MICCAI AMAI 2025 workshop, to be published in
  Volume 16206 of the Lecture Notes in Computer Science series</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Informative Sample Selection Model for Skeleton-based Action Recognition
  with Limited Training Samples 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25345v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25345v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhigang Tu, Zhengbo Zhang, Jia Gong, Junsong Yuan, Bo Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Skeleton-based human action recognition aims to classify human skeletal
sequences, which are spatiotemporal representations of actions, into predefined
categories. To reduce the reliance on costly annotations of skeletal sequences
while maintaining competitive recognition accuracy, the task of 3D Action
Recognition with Limited Training Samples, also known as semi-supervised 3D
Action Recognition, has been proposed. In addition, active learning, which aims
to proactively select the most informative unlabeled samples for annotation,
has been explored in semi-supervised 3D Action Recognition for training sample
selection. Specifically, researchers adopt an encoder-decoder framework to
embed skeleton sequences into a latent space, where clustering information,
combined with a margin-based selection strategy using a multi-head mechanism,
is utilized to identify the most informative sequences in the unlabeled set for
annotation. However, the most representative skeleton sequences may not
necessarily be the most informative for the action recognizer, as the model may
have already acquired similar knowledge from previously seen skeleton samples.
To solve it, we reformulate Semi-supervised 3D action recognition via active
learning from a novel perspective by casting it as a Markov Decision Process
(MDP). Built upon the MDP framework and its training paradigm, we train an
informative sample selection model to intelligently guide the selection of
skeleton sequences for annotation. To enhance the representational capacity of
the factors in the state-action pairs within our method, we project them from
Euclidean space to hyperbolic space. Furthermore, we introduce a meta tuning
strategy to accelerate the deployment of our method in real-world scenarios.
Extensive experiments on three 3D action recognition benchmarks demonstrate the
effectiveness of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Image Processing (TIP), 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ StreamingCoT: A <span class="highlight-title">Dataset</span> for Temporal Dynamics and Multimodal
  Chain-of-Thought Reasoning in Streaming VideoQA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25332v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25332v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhang Hu, Zhenyu Yang, Shihan Wang, Shengsheng Qian, Bin Wen, Fan Yang, Tingting Gao, Changsheng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid growth of streaming video applications demands multimodal models
with enhanced capabilities for temporal dynamics understanding and complex
reasoning. However, current Video Question Answering (VideoQA) datasets suffer
from two critical limitations: 1) Static annotation mechanisms fail to capture
the evolving nature of answers in temporal video streams, and 2) The absence of
explicit reasoning process annotations restricts model interpretability and
logical deduction capabilities. To address these challenges, We introduce
StreamingCoT, the first dataset explicitly designed for temporally evolving
reasoning in streaming VideoQA and multimodal Chain-of-Thought (CoT) tasks. Our
framework first establishes a dynamic hierarchical annotation architecture that
generates per-second dense descriptions and constructs temporally-dependent
semantic segments through similarity fusion, paired with question-answer sets
constrained by temporal evolution patterns. We further propose an explicit
reasoning chain generation paradigm that extracts spatiotemporal objects via
keyframe semantic alignment, derives object state transition-based reasoning
paths using large language models, and ensures logical coherence through
human-verified validation. This dataset establishes a foundation for advancing
research in streaming video understanding, complex temporal reasoning, and
multimodal inference. Our StreamingCoT and its construction toolkit can be
accessed at https://github.com/Fleeting-hyh/StreamingCoT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MMEdge: Accelerating On-device Multimodal Inference via Pipelined
  Sensing and Encoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25327v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25327v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runxi Huang, Mingxuan Yu, Mingyu Tsoi, Xiaomin Ouyang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-time multimodal inference on resource-constrained edge devices is
essential for applications such as autonomous driving, human-computer
interaction, and mobile health. However, prior work often overlooks the tight
coupling between sensing dynamics and model execution, as well as the complex
inter-modality dependencies. In this paper, we propose MMEdge, an new on-device
multi-modal inference framework based on pipelined sensing and encoding.
Instead of waiting for complete sensor inputs, MMEdge decomposes the entire
inference process into a sequence of fine-grained sensing and encoding units,
allowing computation to proceed incrementally as data arrive. MMEdge also
introduces a lightweight but effective temporal aggregation module that
captures rich temporal dynamics across different pipelined units to maintain
accuracy performance. Such pipelined design also opens up opportunities for
fine-grained cross-modal optimization and early decision-making during
inference. To further enhance system performance under resource variability and
input data complexity, MMEdge incorporates an adaptive multimodal configuration
optimizer that dynamically selects optimal sensing and model configurations for
each modality under latency constraints, and a cross-modal speculative skipping
mechanism that bypasses future units of slower modalities when early
predictions reach sufficient confidence. We evaluate MMEdge using two public
multimodal datasets and deploy it on a real-world unmanned aerial vehicle
(UAV)-based multimodal testbed. The results show that MMEdge significantly
reduces end-to-end latency while maintaining high task accuracy across various
system and data dynamics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SenSys 2026</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prototype-Driven Adaptation for Few-Shot Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25318v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25318v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yushen Huang, Zhiming Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot object detection (FSOD) often suffers from base-class bias and
unstable calibration when only a few novel samples are available. We propose
Prototype-Driven Alignment (PDA), a lightweight, plug-in metric head for DeFRCN
that provides a prototype-based "second opinion" complementary to the linear
classifier. PDA maintains support-only prototypes in a learnable
identity-initialized projection space and optionally applies
prototype-conditioned RoI alignment to reduce geometric mismatch. During
fine-tuning, prototypes can be adapted via exponential moving average(EMA)
updates on labeled foreground RoIs-without introducing class-specific
parameters-and are frozen at inference to ensure strict protocol compliance.
PDA employs a best-of-K matching scheme to capture intra-class multi-modality
and temperature-scaled fusion to combine metric similarities with detector
logits. Experiments on VOC FSOD and GFSOD benchmarks show that PDA consistently
improves novel-class performance with minimal impact on base classes and
negligible computational overhead.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages,1 figure,2 tables,Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Seeing Clearly and Deeply: An RGBD Imaging Approach with a Bio-inspired
  Monocentric Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25314v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25314v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zongxi Yu, Xiaolong Qian, Shaohua Gao, Qi Jiang, Yao Gao, Kailun Yang, Kaiwei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Achieving high-fidelity, compact RGBD imaging presents a dual challenge:
conventional compact optics struggle with RGB sharpness across the entire
depth-of-field, while software-only Monocular Depth Estimation (MDE) is an
ill-posed problem reliant on unreliable semantic priors. While deep optics with
elements like DOEs can encode depth, they introduce trade-offs in fabrication
complexity and chromatic aberrations, compromising simplicity. To address this,
we first introduce a novel bio-inspired all-spherical monocentric lens, around
which we build the Bionic Monocentric Imaging (BMI) framework, a holistic
co-design. This optical design naturally encodes depth into its depth-varying
Point Spread Functions (PSFs) without requiring complex diffractive or freeform
elements. We establish a rigorous physically-based forward model to generate a
synthetic dataset by precisely simulating the optical degradation process. This
simulation pipeline is co-designed with a dual-head, multi-scale reconstruction
network that employs a shared encoder to jointly recover a high-fidelity
All-in-Focus (AiF) image and a precise depth map from a single coded capture.
Extensive experiments validate the state-of-the-art performance of the proposed
framework. In depth estimation, the method attains an Abs Rel of 0.026 and an
RMSE of 0.130, markedly outperforming leading software-only approaches and
other deep optics systems. For image restoration, the system achieves an SSIM
of 0.960 and a perceptual LPIPS score of 0.082, thereby confirming a superior
balance between image fidelity and depth accuracy. This study illustrates that
the integration of bio-inspired, fully spherical optics with a joint
reconstruction algorithm constitutes an effective strategy for addressing the
intrinsic challenges in high-performance compact RGBD imaging. Source code will
be publicly available at https://github.com/ZongxiYu-ZJU/BMI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The source code will be publicly available at
  https://github.com/ZongxiYu-ZJU/BMI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GaTector+: A Unified Head-free Framework for Gaze Object and Gaze
  Following Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25301v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25301v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Jin, Guangyu Guo, Binglu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gaze object detection and gaze following are fundamental tasks for
interpreting human gaze behavior or intent. However, most previous methods
usually solve these two tasks separately, and their prediction of gaze objects
and gaze following typically depend on head-related prior knowledge during both
the training phase and real-world deployment. This dependency necessitates an
auxiliary network to extract head location, thus precluding joint optimization
across the entire system and constraining the practical applicability. To this
end, we propose GaTector+, a unified framework for gaze object detection and
gaze following, which eliminates the dependence on the head-related priors
during inference. Specifically, GaTector+ uses an expanded
specific-general-specific feature extractor that leverages a shared backbone,
which extracts general features for gaze following and object detection using
the shared backbone while using specific blocks before and after the shared
backbone to better consider the specificity of each sub-task. To obtain
head-related knowledge without prior information, we first embed a head
detection branch to predict the head of each person. Then, before regressing
the gaze point, a head-based attention mechanism is proposed to fuse the sense
feature and gaze feature with the help of head location. Since the
suboptimization of the gaze point heatmap leads to the performance bottleneck,
we propose an attention supervision mechanism to accelerate the learning of the
gaze heatmap. Finally, we propose a novel evaluation metric, mean Similarity
over Candidates (mSoC), for gaze object detection, which is more sensitive to
variations between bounding boxes. The experimental results on multiple
benchmark datasets demonstrate the effectiveness of our model in both gaze
object detection and gaze following tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusion-Driven Progressive Target Manipulation for Source-Free Domain
  Adaptation <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25279v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25279v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuyang Huang, Yabo Chen, Junyu Zhou, Wenrui Dai, Xiaopeng Zhang, Junni Zou, Hongkai Xiong, Qi Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Source-free domain adaptation (SFDA) is a challenging task that tackles
domain shifts using only a pre-trained source model and unlabeled target data.
Existing SFDA methods are restricted by the fundamental limitation of
source-target domain discrepancy. Non-generation SFDA methods suffer from
unreliable pseudo-labels in challenging scenarios with large domain
discrepancies, while generation-based SFDA methods are evidently degraded due
to enlarged domain discrepancies in creating pseudo-source data. To address
this limitation, we propose a novel generation-based framework named
Diffusion-Driven Progressive Target Manipulation (DPTM) that leverages
unlabeled target data as references to reliably generate and progressively
refine a pseudo-target domain for SFDA. Specifically, we divide the target
samples into a trust set and a non-trust set based on the reliability of
pseudo-labels to sufficiently and reliably exploit their information. For
samples from the non-trust set, we develop a manipulation strategy to
semantically transform them into the newly assigned categories, while
simultaneously maintaining them in the target distribution via a latent
diffusion model. Furthermore, we design a progressive refinement mechanism that
progressively reduces the domain discrepancy between the pseudo-target domain
and the real target domain via iterative refinement. Experimental results
demonstrate that DPTM outperforms existing methods by a large margin and
achieves state-of-the-art performance on four prevailing SFDA benchmark
datasets with different scales. Remarkably, DPTM can significantly enhance the
performance by up to 18.6% in scenarios with large source-target gaps.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SynHLMA:Synthesizing Hand Language Manipulation for Articulated Object
  with Discrete Human Object Interaction Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25268v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25268v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wang zhi, Yuyan Liu, Liu Liu, Li Zhang, Ruixuan Lu, Dan Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating hand grasps with language instructions is a widely studied topic
that benefits from embodied AI and VR/AR applications. While transferring into
hand articulatied object interaction (HAOI), the hand grasps synthesis requires
not only object functionality but also long-term manipulation sequence along
the object deformation. This paper proposes a novel HAOI sequence generation
framework SynHLMA, to synthesize hand language manipulation for articulated
objects. Given a complete point cloud of an articulated object, we utilize a
discrete HAOI representation to model each hand object interaction frame. Along
with the natural language embeddings, the representations are trained by an
HAOI manipulation language model to align the grasping process with its
language description in a shared representation space. A joint-aware loss is
employed to ensure hand grasps follow the dynamic variations of articulated
object joints. In this way, our SynHLMA achieves three typical hand
manipulation tasks for articulated objects of HAOI generation, HAOI prediction
and HAOI interpolation. We evaluate SynHLMA on our built HAOI-lang dataset and
experimental results demonstrate the superior hand grasp sequence generation
performance comparing with state-of-the-art. We also show a robotics grasp
application that enables dexterous grasps execution from imitation learning
using the manipulation sequence provided by our SynHLMA. Our codes and datasets
will be made publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LangHOPS: Language Grounded Hierarchical Open-Vocabulary Part
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25263v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25263v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Miao, Jan-Nico Zaech, Xi Wang, Fabien Despinoy, Danda Pani Paudel, Luc Van Gool
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose LangHOPS, the first Multimodal Large Language Model (MLLM) based
framework for open-vocabulary object-part instance segmentation. Given an
image, LangHOPS can jointly detect and segment hierarchical object and part
instances from open-vocabulary candidate categories. Unlike prior approaches
that rely on heuristic or learnable visual grouping, our approach grounds
object-part hierarchies in language space. It integrates the MLLM into the
object-part parsing pipeline to leverage its rich knowledge and reasoning
capabilities, and link multi-granularity concepts within the hierarchies. We
evaluate LangHOPS across multiple challenging scenarios, including in-domain
and cross-dataset object-part instance segmentation, and zero-shot semantic
segmentation. LangHOPS achieves state-of-the-art results, surpassing previous
methods by 5.5% Average Precision (AP) (in-domain) and 4.8% (cross-dataset) on
the PartImageNet dataset and by 2.5% mIOU on unseen object parts in ADE20K
(zero-shot). Ablation studies further validate the effectiveness of the
language-grounded hierarchy and MLLM driven part query refinement strategy. The
code will be released here.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures, 14 tables, Neurips 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RT-DETRv4: Painlessly Furthering Real-Time Object Detection with Vision
  Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25257v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25257v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijun Liao, Yian Zhao, Xin Shan, Yu Yan, Chang Liu, Lei Lu, Xiangyang Ji, Jie Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-time object detection has achieved substantial progress through
meticulously designed architectures and optimization strategies. However, the
pursuit of high-speed inference via lightweight network designs often leads to
degraded feature representation, which hinders further performance improvements
and practical on-device deployment. In this paper, we propose a cost-effective
and highly adaptable distillation framework that harnesses the rapidly evolving
capabilities of Vision Foundation Models (VFMs) to enhance lightweight object
detectors. Given the significant architectural and learning objective
disparities between VFMs and resource-constrained detectors, achieving stable
and task-aligned semantic transfer is challenging. To address this, on one
hand, we introduce a Deep Semantic Injector (DSI) module that facilitates the
integration of high-level representations from VFMs into the deep layers of the
detector. On the other hand, we devise a Gradient-guided Adaptive Modulation
(GAM) strategy, which dynamically adjusts the intensity of semantic transfer
based on gradient norm ratios. Without increasing deployment and inference
overhead, our approach painlessly delivers striking and consistent performance
gains across diverse DETR-based models, underscoring its practical utility for
real-time detection. Our new model family, RT-DETRv4, achieves state-of-the-art
results on COCO, attaining AP scores of 49.7/53.5/55.4/57.0 at corresponding
speeds of 273/169/124/78 FPS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mapping and Classification of Trees Outside Forests using Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25239v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25239v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moritz Lucas, Hamid Ebrahimy, Viacheslav Barkov, Ralf Pecenka, Kai-Uwe Kühnberger, Björn Waske
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trees Outside Forests (TOF) play an important role in agricultural landscapes
by supporting biodiversity, sequestering carbon, and regulating microclimates.
Yet, most studies have treated TOF as a single class or relied on rigid
rule-based thresholds, limiting ecological interpretation and adaptability
across regions. To address this, we evaluate deep learning for TOF
classification using a newly generated dataset and high-resolution aerial
imagery from four agricultural landscapes in Germany. Specifically, we compare
convolutional neural networks (CNNs), vision transformers, and hybrid
CNN-transformer models across six semantic segmentation architectures (ABCNet,
LSKNet, FT-UNetFormer, DC-Swin, BANet, and U-Net) to map four categories of
woody vegetation: Forest, Patch, Linear, and Tree, derived from previous
studies and governmental products. Overall, the models achieved good
classification accuracy across the four landscapes, with the FT-UNetFormer
performing best (mean Intersection-over-Union 0.74; mean F1 score 0.84),
underscoring the importance of spatial context understanding in TOF mapping and
classification. Our results show good results for Forest and Linear class and
reveal challenges particularly in classifying complex structures with high edge
density, notably the Patch and Tree class. Our generalization experiments
highlight the need for regionally diverse training data to ensure reliable
large-scale mapping. The dataset and code are openly available at
https://github.com/Moerizzy/TOFMapper
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VADB: A Large-Scale Video Aesthetic Database with Professional and
  Multi-Dimensional Annotations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25238v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25238v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianqian Qiao, DanDan Zheng, Yihang Bo, Bao Peng, Heng Huang, Longteng Jiang, Huaye Wang, Jingdong Chen, Jun Zhou, Xin Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video aesthetic assessment, a vital area in multimedia computing, integrates
computer vision with human cognition. Its progress is limited by the lack of
standardized datasets and robust models, as the temporal dynamics of video and
multimodal fusion challenges hinder direct application of image-based methods.
This study introduces VADB, the largest video aesthetic database with 10,490
diverse videos annotated by 37 professionals across multiple aesthetic
dimensions, including overall and attribute-specific aesthetic scores, rich
language comments and objective tags. We propose VADB-Net, a dual-modal
pre-training framework with a two-stage training strategy, which outperforms
existing video quality assessment models in scoring tasks and supports
downstream video aesthetic assessment tasks. The dataset and source code are
available at https://github.com/BestiVictory/VADB.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepShield: Fortifying Deepfake Video Detection with Local and Global
  Forgery Analysis <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25237v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25237v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinqi Cai, Jichang Li, Zhaolun Li, Weikai Chen, Rushi Lan, Xi Xie, Xiaonan Luo, Guanbin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in deep generative models have made it easier to manipulate
face videos, raising significant concerns about their potential misuse for
fraud and misinformation. Existing detectors often perform well in in-domain
scenarios but fail to generalize across diverse manipulation techniques due to
their reliance on forgery-specific artifacts. In this work, we introduce
DeepShield, a novel deepfake detection framework that balances local
sensitivity and global generalization to improve robustness across unseen
forgeries. DeepShield enhances the CLIP-ViT encoder through two key components:
Local Patch Guidance (LPG) and Global Forgery Diversification (GFD). LPG
applies spatiotemporal artifact modeling and patch-wise supervision to capture
fine-grained inconsistencies often overlooked by global models. GFD introduces
domain feature augmentation, leveraging domain-bridging and boundary-expanding
feature generation to synthesize diverse forgeries, mitigating overfitting and
enhancing cross-domain adaptability. Through the integration of novel local and
global analysis for deepfake detection, DeepShield outperforms state-of-the-art
methods in cross-dataset and cross-manipulation evaluations, achieving superior
robustness against unseen deepfake attacks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Disentangled Speech- and Expression-Driven Blendshapes for 3D
  Talking Face Animation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25234v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25234v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxiang Mao, Zhijie Zhang, Zhiheng Zhang, Jiawei Liu, Chen Zeng, Shihong Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Expressions are fundamental to conveying human emotions. With the rapid
advancement of AI-generated content (AIGC), realistic and expressive 3D facial
animation has become increasingly crucial. Despite recent progress in
speech-driven lip-sync for talking-face animation, generating emotionally
expressive talking faces remains underexplored. A major obstacle is the
scarcity of real emotional 3D talking-face datasets due to the high cost of
data capture. To address this, we model facial animation driven by both speech
and emotion as a linear additive problem. Leveraging a 3D talking-face dataset
with neutral expressions (VOCAset) and a dataset of 3D expression sequences
(Florence4D), we jointly learn a set of blendshapes driven by speech and
emotion. We introduce a sparsity constraint loss to encourage disentanglement
between the two types of blendshapes while allowing the model to capture
inherent secondary cross-domain deformations present in the training data. The
learned blendshapes can be further mapped to the expression and jaw pose
parameters of the FLAME model, enabling the animation of 3D Gaussian avatars.
Qualitative and quantitative experiments demonstrate that our method naturally
generates talking faces with specified expressions while maintaining accurate
lip synchronization. Perceptual studies further show that our approach achieves
superior emotional expressivity compared to existing methods, without
compromising lip-sync quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 6 figures, accepted to ICXR 2025 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Balanced conic rectified flow <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25229v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25229v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kim Shin Seong, Mingi Kwon, Jaeseok Jeong, Youngjung Uh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rectified flow is a generative model that learns smooth transport mappings
between two distributions through an ordinary differential equation (ODE).
Unlike diffusion-based generative models, which require costly numerical
integration of a generative ODE to sample images with state-of-the-art quality,
rectified flow uses an iterative process called reflow to learn smooth and
straight ODE paths. This allows for relatively simple and efficient generation
of high-quality images. However, rectified flow still faces several challenges.
1) The reflow process requires a large number of generative pairs to preserve
the target distribution, leading to significant computational costs. 2) Since
the model is typically trained using only generated image pairs, its
performance heavily depends on the 1-rectified flow model, causing it to become
biased towards the generated data.
  In this work, we experimentally expose the limitations of the original
rectified flow and propose a novel approach that incorporates real images into
the training process. By preserving the ODE paths for real images, our method
effectively reduces reliance on large amounts of generated data. Instead, we
demonstrate that the reflow process can be conducted efficiently using a much
smaller set of generated and real images. In CIFAR-10, we achieved
significantly better FID scores, not only in one-step generation but also in
full-step simulations, while using only of the generative pairs compared to the
original method. Furthermore, our approach induces straighter paths and avoids
saturation on generated images during reflow, leading to more robust ODE
learning while preserving the distribution of real images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Main paper: 10 pages (total 40 pages including appendix), 5 figures.
  Accepted at NeurIPS 2025 (Poster). Acknowledgment: Supported by the NRF of
  Korea (RS-2023-00223062) and IITP grants (RS-2020-II201361, RS-2024-00439762)
  funded by the Korean government (MSIT)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aligning What You Separate: Denoised Patch Mixing for Source-Free Domain
  Adaptation in Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25227v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25227v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quang-Khai Bui-Tran, Thanh-Huy Nguyen, Hoang-Thien Nguyen, Ba-Thinh Lam, Nguyen Lan Vi Vu, Phat K. Huynh, Ulas Bagci, Min Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Source-Free Domain Adaptation (SFDA) is emerging as a compelling solution for
medical image segmentation under privacy constraints, yet current approaches
often ignore sample difficulty and struggle with noisy supervision under domain
shift. We present a new SFDA framework that leverages Hard Sample Selection and
Denoised Patch Mixing to progressively align target distributions. First,
unlabeled images are partitioned into reliable and unreliable subsets through
entropy-similarity analysis, allowing adaptation to start from easy samples and
gradually incorporate harder ones. Next, pseudo-labels are refined via Monte
Carlo-based denoising masks, which suppress unreliable pixels and stabilize
training. Finally, intra- and inter-domain objectives mix patches between
subsets, transferring reliable semantics while mitigating noise. Experiments on
benchmark datasets show consistent gains over prior SFDA and UDA methods,
delivering more accurate boundary delineation and achieving state-of-the-art
Dice and ASSD scores. Our study highlights the importance of progressive
adaptation and denoised supervision for robust segmentation under domain shift.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MSF-Net: Multi-Stage Feature Extraction and Fusion for Robust
  Photometric Stereo 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25221v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25221v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiyu Qin, Zhihao Cai, Kaixuan Wang, Lin Qi, Junyu Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Photometric stereo is a technique aimed at determining surface normals
through the utilization of shading cues derived from images taken under
different lighting conditions. However, existing learning-based approaches
often fail to accurately capture features at multiple stages and do not
adequately promote interaction between these features. Consequently, these
models tend to extract redundant features, especially in areas with intricate
details such as wrinkles and edges. To tackle these issues, we propose MSF-Net,
a novel framework for extracting information at multiple stages, paired with
selective update strategy, aiming to extract high-quality feature information,
which is critical for accurate normal construction. Additionally, we have
developed a feature fusion module to improve the interplay among different
features. Experimental results on the DiLiGenT benchmark show that our proposed
MSF-Net significantly surpasses previous state-of-the-art methods in the
accuracy of surface normal estimation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ U-CAN: Unsupervised Point Cloud Denoising with Consistency-Aware
  Noise2Noise Matching <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25210v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25210v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junsheng Zhou, Xingyu Shi, Haichuan Song, Yi Fang, Yu-Shen Liu, Zhizhong Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Point clouds captured by scanning sensors are often perturbed by noise, which
have a highly negative impact on downstream tasks (e.g. surface reconstruction
and shape understanding). Previous works mostly focus on training neural
networks with noisy-clean point cloud pairs for learning denoising priors,
which requires extensively manual efforts. In this work, we introduce U-CAN, an
Unsupervised framework for point cloud denoising with Consistency-Aware
Noise2Noise matching. Specifically, we leverage a neural network to infer a
multi-step denoising path for each point of a shape or scene with a noise to
noise matching scheme. We achieve this by a novel loss which enables
statistical reasoning on multiple noisy point cloud observations. We further
introduce a novel constraint on the denoised geometry consistency for learning
consistency-aware denoising patterns. We justify that the proposed constraint
is a general term which is not limited to 3D domain and can also contribute to
the area of 2D image denoising. Our evaluations under the widely used
benchmarks in point cloud denoising, upsampling and image denoising show
significant improvement over the state-of-the-art unsupervised methods, where
U-CAN also produces comparable results with the supervised methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2025. Project page:
  https://gloriasze.github.io/U-CAN/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI-Powered Early Detection of Critical Diseases using Image Processing
  and Audio Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25199v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25199v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manisha More, Kavya Bhand, Kaustubh Mukdam, Kavya Sharma, Manas Kawtikwar, Hridayansh Kaware, Prajwal Kavhar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Early diagnosis of critical diseases can significantly improve patient
survival and reduce treatment costs. However, existing diagnostic techniques
are often costly, invasive, and inaccessible in low-resource regions. This
paper presents a multimodal artificial intelligence (AI) diagnostic framework
integrating image analysis, thermal imaging, and audio signal processing for
early detection of three major health conditions: skin cancer, vascular blood
clots, and cardiopulmonary abnormalities. A fine-tuned MobileNetV2
convolutional neural network was trained on the ISIC 2019 dataset for skin
lesion classification, achieving 89.3% accuracy, 91.6% sensitivity, and 88.2%
specificity. A support vector machine (SVM) with handcrafted features was
employed for thermal clot detection, achieving 86.4% accuracy (AUC = 0.89) on
synthetic and clinical data. For cardiopulmonary analysis, lung and heart sound
datasets from PhysioNet and Pascal were processed using Mel-Frequency Cepstral
Coefficients (MFCC) and classified via Random Forest, reaching 87.2% accuracy
and 85.7% sensitivity. Comparative evaluation against state-of-the-art models
demonstrates that the proposed system achieves competitive results while
remaining lightweight and deployable on low-cost devices. The framework
provides a promising step toward scalable, real-time, and accessible AI-based
pre-diagnostic healthcare solutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mask-Robust Face Verification for Online Learning via YOLOv5 and
  Residual Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25184v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25184v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhifeng Wang, Minghui Wang, Chunyan Zeng, Jialong Yao, Yang Yang, Hongmin Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the contemporary landscape, the fusion of information technology and the
rapid advancement of artificial intelligence have ushered school education into
a transformative phase characterized by digitization and heightened
intelligence. Concurrently, the global paradigm shift caused by the Covid-19
pandemic has catalyzed the evolution of e-learning, accentuating its
significance. Amidst these developments, one pivotal facet of the online
education paradigm that warrants attention is the authentication of identities
within the digital learning sphere. Within this context, our study delves into
a solution for online learning authentication, utilizing an enhanced
convolutional neural network architecture, specifically the residual network
model. By harnessing the power of deep learning, this technological approach
aims to galvanize the ongoing progress of online education, while concurrently
bolstering its security and stability. Such fortification is imperative in
enabling online education to seamlessly align with the swift evolution of the
educational landscape. This paper's focal proposition involves the deployment
of the YOLOv5 network, meticulously trained on our proprietary dataset. This
network is tasked with identifying individuals' faces culled from images
captured by students' open online cameras. The resultant facial information is
then channeled into the residual network to extract intricate features at a
deeper level. Subsequently, a comparative analysis of Euclidean distances
against students' face databases is performed, effectively ascertaining the
identity of each student.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Test-Time Adaptive Object Detection with Foundation Model <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25175v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25175v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingjie Gao, Yanan Zhang, Zhi Cai, Di Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, test-time adaptive object detection has attracted increasing
attention due to its unique advantages in online domain adaptation, which
aligns more closely with real-world application scenarios. However, existing
approaches heavily rely on source-derived statistical characteristics while
making the strong assumption that the source and target domains share an
identical category space. In this paper, we propose the first foundation
model-powered test-time adaptive object detection method that eliminates the
need for source data entirely and overcomes traditional closed-set limitations.
Specifically, we design a Multi-modal Prompt-based Mean-Teacher framework for
vision-language detector-driven test-time adaptation, which incorporates text
and visual prompt tuning to adapt both language and vision representation
spaces on the test data in a parameter-efficient manner. Correspondingly, we
propose a Test-time Warm-start strategy tailored for the visual prompts to
effectively preserve the representation capability of the vision branch.
Furthermore, to guarantee high-quality pseudo-labels in every test batch, we
maintain an Instance Dynamic Memory (IDM) module that stores high-quality
pseudo-labels from previous test samples, and propose two novel
strategies-Memory Enhancement and Memory Hallucination-to leverage IDM's
high-quality instances for enhancing original predictions and hallucinating
images without available pseudo-labels, respectively. Extensive experiments on
cross-corruption and cross-dataset benchmarks demonstrate that our method
consistently outperforms previous state-of-the-art methods, and can adapt to
arbitrary cross-domain and cross-category target data. Code is available at
https://github.com/gaoyingjay/ttaod_foundation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Classifier Enhancement Using Extended Context and Domain Experts for
  Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25174v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25174v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huadong Tang, Youpeng Zhao, Min Xu, Jun Wang, Qiang Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prevalent semantic segmentation methods generally adopt a vanilla classifier
to categorize each pixel into specific classes.
  Although such a classifier learns global information from the training data,
this information is represented by a set of fixed parameters (weights and
biases).
  However, each image has a different class distribution, which prevents the
classifier from addressing the unique characteristics of individual images.
  At the dataset level, class imbalance leads to segmentation results being
biased towards majority classes, limiting the model's effectiveness in
identifying and segmenting minority class regions.
  In this paper, we propose an Extended Context-Aware Classifier (ECAC) that
dynamically adjusts the classifier using global (dataset-level) and local
(image-level) contextual information.
  Specifically, we leverage a memory bank to learn dataset-level contextual
information of each class, incorporating the class-specific contextual
information from the current image to improve the classifier for precise pixel
labeling.
  Additionally, a teacher-student network paradigm is adopted, where the domain
expert (teacher network) dynamically adjusts contextual information with ground
truth and transfers knowledge to the student network.
  Comprehensive experiments illustrate that the proposed ECAC can achieve
state-of-the-art performance across several datasets, including ADE20K,
COCO-Stuff10K, and Pascal-Context.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IEEE TRANSACTIONS ON MULTIMEDIA (TMM)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ $D^2GS$: Dense Depth Regularization for LiDAR-free Urban Scene
  Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25173v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25173v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kejing Xia, Jidong Jia, Ke Jin, Yucai Bai, Li Sun, Dacheng Tao, Youjian Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Gaussian Splatting (GS) has shown great potential for urban scene
reconstruction in the field of autonomous driving. However, current urban scene
reconstruction methods often depend on multimodal sensors as inputs,
\textit{i.e.} LiDAR and images. Though the geometry prior provided by LiDAR
point clouds can largely mitigate ill-posedness in reconstruction, acquiring
such accurate LiDAR data is still challenging in practice: i) precise
spatiotemporal calibration between LiDAR and other sensors is required, as they
may not capture data simultaneously; ii) reprojection errors arise from spatial
misalignment when LiDAR and cameras are mounted at different locations. To
avoid the difficulty of acquiring accurate LiDAR depth, we propose $D^2GS$, a
LiDAR-free urban scene reconstruction framework. In this work, we obtain
geometry priors that are as effective as LiDAR while being denser and more
accurate. $\textbf{First}$, we initialize a dense point cloud by
back-projecting multi-view metric depth predictions. This point cloud is then
optimized by a Progressive Pruning strategy to improve the global consistency.
$\textbf{Second}$, we jointly refine Gaussian geometry and predicted dense
metric depth via a Depth Enhancer. Specifically, we leverage diffusion priors
from a depth foundation model to enhance the depth maps rendered by Gaussians.
In turn, the enhanced depths provide stronger geometric constraints during
Gaussian training. $\textbf{Finally}$, we improve the accuracy of ground
geometry by constraining the shape and normal attributes of Gaussians within
road regions. Extensive experiments on the Waymo dataset demonstrate that our
method consistently outperforms state-of-the-art methods, producing more
accurate geometry even when compared with those using ground-truth LiDAR data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Study on Inference Latency for Vision <span class="highlight-title">Transformer</span>s on Mobile Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25166v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25166v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuojin Li, Marco Paolieri, Leana Golubchik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the significant advances in machine learning techniques on mobile
devices, particularly in the domain of computer vision, in this work we
quantitatively study the performance characteristics of 190 real-world vision
transformers (ViTs) on mobile devices. Through a comparison with 102 real-world
convolutional neural networks (CNNs), we provide insights into the factors that
influence the latency of ViT architectures on mobile devices. Based on these
insights, we develop a dataset including measured latencies of 1000 synthetic
ViTs with representative building blocks and state-of-the-art architectures
from two machine learning frameworks and six mobile platforms. Using this
dataset, we show that inference latency of new ViTs can be predicted with
sufficient accuracy for real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in Springer LNICST, volume 663, Proceedings of VALUETOOLS
  2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Transformer</span>s in Medicine: Improving Vision-Language Alignment for
  Medical Image Captioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25164v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25164v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yogesh Thakku Suresh, Vishwajeet Shivaji Hogale, Luca-Alexandru Zamfira, Anandavardhana Hegde
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a transformer-based multimodal framework for generating clinically
relevant captions for MRI scans. Our system combines a DEiT-Small vision
transformer as an image encoder, MediCareBERT for caption embedding, and a
custom LSTM-based decoder. The architecture is designed to semantically align
image and textual embeddings, using hybrid cosine-MSE loss and contrastive
inference via vector similarity. We benchmark our method on the MultiCaRe
dataset, comparing performance on filtered brain-only MRIs versus general MRI
images against state-of-the-art medical image captioning methods including
BLIP, R2GenGPT, and recent transformer-based approaches. Results show that
focusing on domain-specific data improves caption accuracy and semantic
alignment. Our work proposes a scalable, interpretable solution for automated
medical image reporting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work is to appear in the Proceedings of MICAD 2025, the 6th
  International Conference on Medical Imaging and Computer-Aided Diagnosis</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Target-Guided Bayesian Flow Networks for Quantitatively Constrained CAD
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25163v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25163v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Zheng, Chenwei Sun, Wenbo Zhang, Jiancheng Lv, Xianggen Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep generative models, such as diffusion models, have shown promising
progress in image generation and audio generation via simplified continuity
assumptions. However, the development of generative modeling techniques for
generating multi-modal data, such as parametric CAD sequences, still lags
behind due to the challenges in addressing long-range constraints and parameter
sensitivity. In this work, we propose a novel framework for quantitatively
constrained CAD generation, termed Target-Guided Bayesian Flow Network (TGBFN).
For the first time, TGBFN handles the multi-modality of CAD sequences (i.e.,
discrete commands and continuous parameters) in a unified continuous and
differentiable parameter space rather than in the discrete data space. In
addition, TGBFN penetrates the parameter update kernel and introduces a guided
Bayesian flow to control the CAD properties. To evaluate TGBFN, we construct a
new dataset for quantitatively constrained CAD generation. Extensive
comparisons across single-condition and multi-condition constrained generation
tasks demonstrate that TGBFN achieves state-of-the-art performance in
generating high-fidelity, condition-aware CAD sequences. The code is available
at https://github.com/scu-zwh/TGBFN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Real-Time Inference of Thin Liquid Film Thickness Profiles from
  Interference Patterns Using Vision <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25157v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25157v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gautam A. Viruthagiri, Arnuv Tandon, Gerald G. Fuller, Vinny Chandran Suja
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Thin film interferometry is a powerful technique for non-invasively measuring
liquid film thickness with applications in ophthalmology, but its clinical
translation is hindered by the challenges in reconstructing thickness profiles
from interference patterns - an ill-posed inverse problem complicated by phase
periodicity, imaging noise and ambient artifacts. Traditional reconstruction
methods are either computationally intensive, sensitive to noise, or require
manual expert analysis, which is impractical for real-time diagnostics. To
address this challenge, here we present a vision transformer-based approach for
real-time inference of thin liquid film thickness profiles directly from
isolated interferograms. Trained on a hybrid dataset combining
physiologically-relevant synthetic and experimental tear film data, our model
leverages long-range spatial correlations to resolve phase ambiguities and
reconstruct temporally coherent thickness profiles in a single forward pass
from dynamic interferograms acquired in vivo and ex vivo. The network
demonstrates state-of-the-art performance on noisy, rapidly-evolving films with
motion artifacts, overcoming limitations of conventional phase-unwrapping and
iterative fitting methods. Our data-driven approach enables automated,
consistent thickness reconstruction at real-time speeds on consumer hardware,
opening new possibilities for continuous monitoring of pre-lens ocular tear
films and non-invasive diagnosis of conditions such as the dry eye disease.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 figures, will be updated</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EA3D: Online Open-World 3D Object Extraction from Streaming Videos <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25146v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25146v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyu Zhou, Jingqi Wang, Yuang Jia, Yongtao Wang, Deqing Sun, Ming-Hsuan Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current 3D scene understanding methods are limited by offline-collected
multi-view data or pre-constructed 3D geometry. In this paper, we present
ExtractAnything3D (EA3D), a unified online framework for open-world 3D object
extraction that enables simultaneous geometric reconstruction and holistic
scene understanding. Given a streaming video, EA3D dynamically interprets each
frame using vision-language and 2D vision foundation encoders to extract
object-level knowledge. This knowledge is integrated and embedded into a
Gaussian feature map via a feed-forward online update strategy. We then
iteratively estimate visual odometry from historical frames and incrementally
update online Gaussian features with new observations. A recurrent joint
optimization module directs the model's attention to regions of interest,
simultaneously enhancing both geometric reconstruction and semantic
understanding. Extensive experiments across diverse benchmarks and tasks,
including photo-realistic rendering, semantic and instance segmentation, 3D
bounding box and semantic occupancy estimation, and 3D mesh generation,
demonstrate the effectiveness of EA3D. Our method establishes a unified and
efficient framework for joint online 3D reconstruction and holistic scene
understanding, enabling a broad range of downstream tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The Thirty-Ninth Annual Conference on Neural Information Processing
  Systems(NeurIPS 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting Reconstruction-based AI-generated Image Detection: A
  Geometric Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25141v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25141v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wan Jiang, Jing Yan, Ruixuan Zhang, Xiaojing Chen, Changtao Miao, Zhe Li, Chenhao Lin, Yunfeng Diao, Richang Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of generative Artificial Intelligence (AI) has made detecting
AI-generated images a critical challenge for ensuring authenticity. Existing
reconstruction-based methods lack theoretical foundations and on empirical
heuristics, limiting interpretability and reliability. In this paper, we
introduce the Jacobian-Spectral Lower Bound for reconstruction error from a
geometric perspective, showing that real images off the reconstruction manifold
exhibit a non-trivial error lower bound, while generated images on the manifold
have near-zero error. Furthermore, we reveal the limitations of existing
methods that rely on static reconstruction error from a single pass. These
methods often fail when some real images exhibit lower error than generated
ones. This counterintuitive behavior reduces detection accuracy and requires
data-specific threshold tuning, limiting their applicability in real-world
scenarios. To address these challenges, we propose ReGap, a training-free
method that computes dynamic reconstruction error by leveraging structured
editing operations to introduce controlled perturbations. This enables
measuring error changes before and after editing, improving detection accuracy
by enhancing error separation. Experimental results show that our method
outperforms existing baselines, exhibits robustness to common post-processing
operations and generalizes effectively across diverse conditions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DINO-YOLO: <span class="highlight-title">Self-Supervised</span> <span class="highlight-title">Pre-train</span>ing for Data-Efficient Object
  Detection in Civil Engineering Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25140v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25140v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Malaisree P, Youwai S, Kitkobsin T, Janrungautai S, Amorndechaphon D, Rojanavasu P
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object detection in civil engineering applications is constrained by limited
annotated data in specialized domains. We introduce DINO-YOLO, a hybrid
architecture combining YOLOv12 with DINOv3 self-supervised vision transformers
for data-efficient detection. DINOv3 features are strategically integrated at
two locations: input preprocessing (P0) and mid-backbone enhancement (P3).
Experimental validation demonstrates substantial improvements: Tunnel Segment
Crack detection (648 images) achieves 12.4% improvement, Construction PPE (1K
images) gains 13.7%, and KITTI (7K images) shows 88.6% improvement, while
maintaining real-time inference (30-47 FPS). Systematic ablation across five
YOLO scales and nine DINOv3 variants reveals that Medium-scale architectures
achieve optimal performance with DualP0P3 integration (55.77% mAP@0.5), while
Small-scale requires Triple Integration (53.63%). The 2-4x inference overhead
(21-33ms versus 8-16ms baseline) remains acceptable for field deployment on
NVIDIA RTX 5090. DINO-YOLO establishes state-of-the-art performance for civil
engineering datasets (<10K images) while preserving computational efficiency,
providing practical solutions for construction safety monitoring and
infrastructure inspection in data-constrained environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Region-CAM: Towards Accurate Object Regions in Class Activation Maps for
  Weakly Supervised Learning Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25134v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25134v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingdong Cai, Charith Abhayaratne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class Activation Mapping (CAM) methods are widely applied in weakly
supervised learning tasks due to their ability to highlight object regions.
However, conventional CAM methods highlight only the most discriminative
regions of the target. These highlighted regions often fail to cover the entire
object and are frequently misaligned with object boundaries, thereby limiting
the performance of downstream weakly supervised learning tasks, particularly
Weakly Supervised Semantic Segmentation (WSSS), which demands pixel-wise
accurate activation maps to get the best results. To alleviate the above
problems, we propose a novel activation method, Region-CAM. Distinct from
network feature weighting approaches, Region-CAM generates activation maps by
extracting semantic information maps (SIMs) and performing semantic information
propagation (SIP) by considering both gradients and features in each of the
stages of the baseline classification model. Our approach highlights a greater
proportion of object regions while ensuring activation maps to have precise
boundaries that align closely with object edges. Region-CAM achieves 60.12% and
58.43% mean intersection over union (mIoU) using the baseline model on the
PASCAL VOC training and validation datasets, respectively, which are
improvements of 13.61% and 13.13% over the original CAM (46.51% and 45.30%). On
the MS COCO validation set, Region-CAM achieves 36.38%, a 16.23% improvement
over the original CAM (20.15%). We also demonstrate the superiority of
Region-CAM in object localization tasks, using the ILSVRC2012 validation set.
Region-CAM achieves 51.7% in Top-1 Localization accuracy Loc1. Compared with
LayerCAM, an activation method designed for weakly supervised object
localization, Region-CAM achieves 4.5% better performance in Loc1.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint for journal paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AtlasGS: Atlanta-world Guided Surface Reconstruction with Implicit
  Structured Gaussians <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25129v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25129v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiyu Zhang, Chong Bao, Yipeng Chen, Hongjia Zhai, Yitong Dong, Hujun Bao, Zhaopeng Cui, Guofeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D reconstruction of indoor and urban environments is a prominent research
topic with various downstream applications. However, existing geometric priors
for addressing low-texture regions in indoor and urban settings often lack
global consistency. Moreover, Gaussian Splatting and implicit SDF fields often
suffer from discontinuities or exhibit computational inefficiencies, resulting
in a loss of detail. To address these issues, we propose an Atlanta-world
guided implicit-structured Gaussian Splatting that achieves smooth indoor and
urban scene reconstruction while preserving high-frequency details and
rendering efficiency. By leveraging the Atlanta-world model, we ensure the
accurate surface reconstruction for low-texture regions, while the proposed
novel implicit-structured GS representations provide smoothness without
sacrificing efficiency and high-frequency details. Specifically, we propose a
semantic GS representation to predict the probability of all semantic regions
and deploy a structure plane regularization with learnable plane indicators for
global accurate surface reconstruction. Extensive experiments demonstrate that
our method outperforms state-of-the-art approaches in both indoor and urban
scenes, delivering superior surface reconstruction quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 11 figures. NeurIPS 2025; Project page:
  https://zju3dv.github.io/AtlasGS/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visual Diversity and Region-aware <span class="highlight-title">Prompt</span> Learning for Zero-shot HOI
  Detection <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25094v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25094v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chanhyeong Yang, Taehoon Song, Jihwan Park, Hyunwoo J. Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Zero-shot Human-Object Interaction detection aims to localize humans and
objects in an image and recognize their interaction, even when specific
verb-object pairs are unseen during training. Recent works have shown promising
results using prompt learning with pretrained vision-language models such as
CLIP, which align natural language prompts with visual features in a shared
embedding space. However, existing approaches still fail to handle the visual
complexity of interaction, including (1) intra-class visual diversity, where
instances of the same verb appear in diverse poses and contexts, and (2)
inter-class visual entanglement, where distinct verbs yield visually similar
patterns. To address these challenges, we propose VDRP, a framework for Visual
Diversity and Region-aware Prompt learning. First, we introduce a visual
diversity-aware prompt learning strategy that injects group-wise visual
variance into the context embedding. We further apply Gaussian perturbation to
encourage the prompts to capture diverse visual variations of a verb. Second,
we retrieve region-specific concepts from the human, object, and union regions.
These are used to augment the diversity-aware prompt embeddings, yielding
region-aware prompts that enhance verb-level discrimination. Experiments on the
HICO-DET benchmark demonstrate that our method achieves state-of-the-art
performance under four zero-shot evaluation settings, effectively addressing
both intra-class diversity and inter-class visual entanglement. Code is
available at https://github.com/mlvlab/VDRP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PSTF-AttControl: Per-Subject-Tuning-Free Personalized Image Generation
  with Controllable Face Attributes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25084v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25084v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang liu, Zhaoxiang Liu, Huan Hu, Zipeng Wang, Ping Chen, Zezhou Chen, Kai Wang, Shiguo Lian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in personalized image generation have significantly
improved facial identity preservation, particularly in fields such as
entertainment and social media. However, existing methods still struggle to
achieve precise control over facial attributes in a per-subject-tuning-free
(PSTF) way. Tuning-based techniques like PreciseControl have shown promise by
providing fine-grained control over facial features, but they often require
extensive technical expertise and additional training data, limiting their
accessibility. In contrast, PSTF approaches simplify the process by enabling
image generation from a single facial input, but they lack precise control over
facial attributes. In this paper, we introduce a novel, PSTF method that
enables both precise control over facial attributes and high-fidelity
preservation of facial identity. Our approach utilizes a face recognition model
to extract facial identity features, which are then mapped into the $W^+$
latent space of StyleGAN2 using the e4e encoder. We further enhance the model
with a Triplet-Decoupled Cross-Attention module, which integrates facial
identity, attribute features, and text embeddings into the UNet architecture,
ensuring clean separation of identity and attribute information. Trained on the
FFHQ dataset, our method allows for the generation of personalized images with
fine-grained control over facial attributes, while without requiring additional
fine-tuning or training data for individual identities. We demonstrate that our
approach successfully balances personalization with precise facial attribute
control, offering a more efficient and user-friendly solution for high-quality,
adaptable facial image synthesis. The code is publicly available at
https://github.com/UnicomAI/PSTF-AttControl.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Image and Vision Computing (18 pages, 8 figures)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neighborhood Feature Pooling for Remote Sensing Image Classification <span class="chip">WACV 2026</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25077v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25077v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fahimeh Orvati Nia, Amirmohammad Mohammadi, Salim Al Kharsa, Pragati Naikare, Zigfried Hampel-Arias, Joshua Peeples
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we propose neighborhood feature pooling (NFP) as a novel
texture feature extraction method for remote sensing image classification. The
NFP layer captures relationships between neighboring inputs and efficiently
aggregates local similarities across feature dimensions. Implemented using
convolutional layers, NFP can be seamlessly integrated into any network.
Results comparing the baseline models and the NFP method indicate that NFP
consistently improves performance across diverse datasets and architectures
while maintaining minimal parameter overhead.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures. Accepted to WACV 2026 (Winter Conference on
  Applications of Computer Vision)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vision-Language Integration for Zero-Shot Scene Understanding in
  Real-World Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25070v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25070v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manjunath Prasad Holenarasipura Rajiv, B. M. Vidyavathi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Zero-shot scene understanding in real-world settings presents major
challenges due to the complexity and variability of natural scenes, where
models must recognize new objects, actions, and contexts without prior labeled
examples. This work proposes a vision-language integration framework that
unifies pre-trained visual encoders (e.g., CLIP, ViT) and large language models
(e.g., GPT-based architectures) to achieve semantic alignment between visual
and textual modalities. The goal is to enable robust zero-shot comprehension of
scenes by leveraging natural language as a bridge to generalize over unseen
categories and contexts. Our approach develops a unified model that embeds
visual inputs and textual prompts into a shared space, followed by multimodal
fusion and reasoning layers for contextual interpretation. Experiments on
Visual Genome, COCO, ADE20K, and custom real-world datasets demonstrate
significant gains over state-of-the-art zero-shot models in object recognition,
activity detection, and scene captioning. The proposed system achieves up to
18% improvement in top-1 accuracy and notable gains in semantic coherence
metrics, highlighting the effectiveness of cross-modal alignment and language
grounding in enhancing generalization for real-world scene understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint under review at IEEE Transactions on Pattern Analysis and
  Machine Intelligence (TPAMI), 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DRIP: Dynamic patch Reduction via Interpretable Pooling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25067v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25067v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusen Peng, Sachin Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, the advances in vision-language models, including contrastive
pretraining and instruction tuning, have greatly pushed the frontier of
multimodal AI. However, owing to the large-scale and hence expensive
pretraining, the efficiency concern has discouraged researchers from attempting
to pretrain a vision language model from scratch. In this work, we propose
Dynamic patch Reduction via Interpretable Pooling (DRIP), which adapts to the
input images and dynamically merges tokens in the deeper layers of a visual
encoder. Our results on both ImageNet training from scratch and CLIP
contrastive pretraining demonstrate a significant GFLOP reduction while
maintaining comparable classification/zero-shot performance. To further
validate our proposed method, we conduct continual pretraining on a large
biology dataset, extending its impact into scientific domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Auto3DSeg for Brain Tumor Segmentation from 3D MRI in BraTS 2023
  Challenge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25058v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25058v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andriy Myronenko, Dong Yang, Yufan He, Daguang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we describe our solution to the BraTS 2023 cluster of
challenges using Auto3DSeg from MONAI. We participated in all 5 segmentation
challenges, and achieved the 1st place results in three of them: Brain
Metastasis, Brain Meningioma, BraTS-Africa challenges, and the 2nd place
results in the remaining two: Adult and Pediatic Glioma challenges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>BraTS23 winner</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Breast Cancer VLMs: Clinically Practical Vision-Language Train-Inference
  Models <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25051v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25051v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shunjie-Fabian Zheng, Hyeonjun Lee, Thijs Kooi, Ali Diba
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Breast cancer remains the most commonly diagnosed malignancy among women in
the developed world. Early detection through mammography screening plays a
pivotal role in reducing mortality rates. While computer-aided diagnosis (CAD)
systems have shown promise in assisting radiologists, existing approaches face
critical limitations in clinical deployment - particularly in handling the
nuanced interpretation of multi-modal data and feasibility due to the
requirement of prior clinical history. This study introduces a novel framework
that synergistically combines visual features from 2D mammograms with
structured textual descriptors derived from easily accessible clinical metadata
and synthesized radiological reports through innovative tokenization modules.
Our proposed methods in this study demonstrate that strategic integration of
convolutional neural networks (ConvNets) with language representations achieves
superior performance to vision transformer-based models while handling
high-resolution images and enabling practical deployment across diverse
populations. By evaluating it on multi-national cohort screening mammograms,
our multi-modal approach achieves superior performance in cancer detection and
calcification identification compared to unimodal baselines, with particular
improvements. The proposed method establishes a new paradigm for developing
clinically viable VLM-based CAD systems that effectively leverage imaging data
and contextual patient information through effective fusion mechanisms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Computer Vision for Automated Medical Diagnosis (CVAMD)
  Workshop at ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Temporal Understanding in Video-LLMs through Stacked Temporal
  Attention in Vision Encoders <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26027v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26027v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Rasekh, Erfan Bagheri Soula, Omid Daliran, Simon Gottschalk, Mohsen Fayyaz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite significant advances in Multimodal Large Language Models (MLLMs),
understanding complex temporal dynamics in videos remains a major challenge.
Our experiments show that current Video Large Language Model (Video-LLM)
architectures have critical limitations in temporal understanding, struggling
with tasks that require detailed comprehension of action sequences and temporal
progression. In this work, we propose a Video-LLM architecture that introduces
stacked temporal attention modules directly within the vision encoder. This
design incorporates a temporal attention in vision encoder, enabling the model
to better capture the progression of actions and the relationships between
frames before passing visual tokens to the LLM. Our results show that this
approach significantly improves temporal reasoning and outperforms existing
models in video question answering tasks, specifically in action recognition.
We improve on benchmarks including VITATECS, MVBench, and Video-MME by up to
+5.5%. By enhancing the vision encoder with temporal structure, we address a
critical gap in video understanding for Video-LLMs. Project page and code are
available at: https://alirasekh.github.io/STAVEQ2/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Groupwise Registration with Physics-Informed Test-Time Adaptation on
  Multi-parametric Cardiac MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26022v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26022v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinqi Li, Yi Zhang, Li-Ting Huang, Hsiao-Huang Chang, Thoralf Niendorf, Min-Chi Ku, Qian Tao, Hsin-Jung Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multiparametric mapping MRI has become a viable tool for myocardial tissue
characterization. However, misalignment between multiparametric maps makes
pixel-wise analysis challenging. To address this challenge, we developed a
generalizable physics-informed deep-learning model using test-time adaptation
to enable group image registration across contrast weighted images acquired
from multiple physical models (e.g., a T1 mapping model and T2 mapping model).
The physics-informed adaptation utilized the synthetic images from specific
physics model as registration reference, allows for transductive learning for
various tissue contrast. We validated the model in healthy volunteers with
various MRI sequences, demonstrating its improvement for multi-modal
registration with a wide range of image contrast variability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Climate Adaptation-Aware Flood Prediction for Coastal Cities Using Deep
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26017v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26017v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bilal Hassan, Areg Karapetyan, Aaron Chung Hin Chow, Samer Madanat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Climate change and sea-level rise (SLR) pose escalating threats to coastal
cities, intensifying the need for efficient and accurate methods to predict
potential flood hazards. Traditional physics-based hydrodynamic simulators,
although precise, are computationally expensive and impractical for city-scale
coastal planning applications. Deep Learning (DL) techniques offer promising
alternatives, however, they are often constrained by challenges such as data
scarcity and high-dimensional output requirements. Leveraging a recently
proposed vision-based, low-resource DL framework, we develop a novel,
lightweight Convolutional Neural Network (CNN)-based model designed to predict
coastal flooding under variable SLR projections and shoreline adaptation
scenarios. Furthermore, we demonstrate the ability of the model to generalize
across diverse geographical contexts by utilizing datasets from two distinct
regions: Abu Dhabi and San Francisco. Our findings demonstrate that the
proposed model significantly outperforms state-of-the-art methods, reducing the
mean absolute error (MAE) in predicted flood depth maps on average by nearly
20%. These results highlight the potential of our approach to serve as a
scalable and practical tool for coastal flood management, empowering
decision-makers to develop effective mitigation strategies in response to the
growing impacts of climate change. Project Page: https://caspiannet.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Hydrology and Earth System Sciences</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CAVE: Detecting and Explaining Commonsense Anomalies in Visual
  Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26006v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26006v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rishika Bhagwatkar, Syrielle Montariol, Angelika Romanou, Beatriz Borges, Irina Rish, Antoine Bosselut
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans can naturally identify, reason about, and explain anomalies in their
environment. In computer vision, this long-standing challenge remains limited
to industrial defects or unrealistic, synthetically generated anomalies,
failing to capture the richness and unpredictability of real-world anomalies.
In this work, we introduce CAVE, the first benchmark of real-world visual
anomalies. CAVE supports three open-ended tasks: anomaly description,
explanation, and justification; with fine-grained annotations for visual
grounding and categorizing anomalies based on their visual manifestations,
their complexity, severity, and commonness. These annotations draw inspiration
from cognitive science research on how humans identify and resolve anomalies,
providing a comprehensive framework for evaluating Vision-Language Models
(VLMs) in detecting and understanding anomalies. We show that state-of-the-art
VLMs struggle with visual anomaly perception and commonsense reasoning, even
with advanced prompting strategies. By offering a realistic and cognitively
grounded benchmark, CAVE serves as a valuable resource for advancing research
in anomaly detection and commonsense reasoning in VLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DARTS: A Drone-Based AI-Powered Real-Time Traffic Incident Detection
  System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26004v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26004v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bai Li, Achilleas Kourtellis, Rong Cao, Joseph Post, Brian Porter, Yu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rapid and reliable incident detection is critical for reducing crash-related
fatalities, injuries, and congestion. However, conventional methods, such as
closed-circuit television, dashcam footage, and sensor-based detection,
separate detection from verification, suffer from limited flexibility, and
require dense infrastructure or high penetration rates, restricting
adaptability and scalability to shifting incident hotspots. To overcome these
challenges, we developed DARTS, a drone-based, AI-powered real-time traffic
incident detection system. DARTS integrates drones' high mobility and aerial
perspective for adaptive surveillance, thermal imaging for better
low-visibility performance and privacy protection, and a lightweight deep
learning framework for real-time vehicle trajectory extraction and incident
detection. The system achieved 99% detection accuracy on a self-collected
dataset and supports simultaneous online visual verification, severity
assessment, and incident-induced congestion propagation monitoring via a
web-based interface. In a field test on Interstate 75 in Florida, DARTS
detected and verified a rear-end collision 12 minutes earlier than the local
transportation management center and monitored incident-induced congestion
propagation, suggesting potential to support faster emergency response and
enable proactive traffic control to reduce congestion and secondary crash risk.
Crucially, DARTS's flexible deployment architecture reduces dependence on
frequent physical patrols, indicating potential scalability and
cost-effectiveness for use in remote areas and resource-constrained settings.
This study presents a promising step toward a more flexible and integrated
real-time traffic incident detection system, with significant implications for
the operational efficiency and responsiveness of modern transportation
management.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint version. This manuscript is currently under review at
  Transportation Research Part C: Emerging Technologies. The PDF corresponds to
  the version submitted in June 2025. The main findings of this work were
  recognized with the Best Intelligent Transportation Systems Paper Award at
  the 2025 TRB Annual Meeting</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Larger Hausdorff Dimension in Scanning Pattern Facilitates Mamba-Based
  Methods in Low-Light Image Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26001v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26001v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinhua Wang, Caibo Feng, Xiangjun Fu, Chunxiao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose an innovative enhancement to the Mamba framework by increasing the
Hausdorff dimension of its scanning pattern through a novel Hilbert Selective
Scan mechanism. This mechanism explores the feature space more effectively,
capturing intricate fine-scale details and improving overall coverage. As a
result, it mitigates information inconsistencies while refining spatial
locality to better capture subtle local interactions without sacrificing the
model's ability to handle long-range dependencies. Extensive experiments on
publicly available benchmarks demonstrate that our approach significantly
improves both the quantitative metrics and qualitative visual fidelity of
existing Mamba-based low-light image enhancement methods, all while reducing
computational resource consumption and shortening inference time. We believe
that this refined strategy not only advances the state-of-the-art in low-light
image enhancement but also holds promise for broader applications in fields
that leverage Mamba-based techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fine-tuning Segment Anything for Real-Time Tumor Tracking in Cine-MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25990v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25990v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valentin Boussot, Cédric Hémon, Jean-Claude Nunes, Jean-Louis Dillenseger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we address the TrackRAD2025 challenge of real-time tumor
tracking in cine-MRI sequences of the thoracic and abdominal regions under
strong data scarcity constraints. Two complementary strategies were explored:
(i) unsupervised registration with the IMPACT similarity metric and (ii)
foundation model-based segmentation leveraging SAM 2.1 and its recent variants
through prompt-based interaction. Due to the one-second runtime constraint, the
SAM-based method was ultimately selected. The final configuration used SAM2.1
b+ with mask-based prompts from the first annotated slice, fine-tuned solely on
the small labeled subset from TrackRAD2025. Training was configured to minimize
overfitting, using 1024x1024 patches (batch size 1), standard augmentations,
and a balanced Dice + IoU loss. A low uniform learning rate (0.0001) was
applied to all modules (prompt encoder, decoder, Hiera backbone) to preserve
generalization while adapting to annotator-specific styles. Training lasted 300
epochs (~12h on RTX A6000, 48GB). The same inference strategy was consistently
applied across all anatomical sites and MRI field strengths. Test-time
augmentation was considered but ultimately discarded due to negligible
performance gains. The final model was selected based on the highest Dice
Similarity Coefficient achieved on the validation set after fine-tuning. On the
hidden test set, the model reached a Dice score of 0.8794, ranking 6th overall
in the TrackRAD2025 challenge. These results highlight the strong potential of
foundation models for accurate and real-time tumor tracking in MRI-guided
radiotherapy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper for the Trackrad2025 challenge, Team BreizhTrack</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Brain-IT: Image Reconstruction from fMRI via Brain-Interaction
  <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25976v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25976v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roman Beliy, Amit Zalcher, Jonathan Kogman, Navve Wasserman, Michal Irani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing images seen by people from their fMRI brain recordings
provides a non-invasive window into the human brain. Despite recent progress
enabled by diffusion models, current methods often lack faithfulness to the
actual seen images. We present "Brain-IT", a brain-inspired approach that
addresses this challenge through a Brain Interaction Transformer (BIT),
allowing effective interactions between clusters of functionally-similar
brain-voxels. These functional-clusters are shared by all subjects, serving as
building blocks for integrating information both within and across brains. All
model components are shared by all clusters & subjects, allowing efficient
training with a limited amount of data. To guide the image reconstruction, BIT
predicts two complementary localized patch-level image features: (i)high-level
semantic features which steer the diffusion model toward the correct semantic
content of the image; and (ii)low-level structural features which help to
initialize the diffusion process with the correct coarse layout of the image.
BIT's design enables direct flow of information from brain-voxel clusters to
localized image features. Through these principles, our method achieves image
reconstructions from fMRI that faithfully reconstruct the seen images, and
surpass current SotA approaches both visually and by standard objective
metrics. Moreover, with only 1-hour of fMRI data from a new subject, we achieve
results comparable to current methods trained on full 40-hour recordings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SplitFlow: Flow Decomposition for Inversion-Free Text-to-Image Editing <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25970v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25970v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sung-Hoon Yoon, Minghan Li, Gaspard Beaudouin, Congcong Wen, Muhammad Rafay Azhar, Mengyu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rectified flow models have become a de facto standard in image generation due
to their stable sampling trajectories and high-fidelity outputs. Despite their
strong generative capabilities, they face critical limitations in image editing
tasks: inaccurate inversion processes for mapping real images back into the
latent space, and gradient entanglement issues during editing often result in
outputs that do not faithfully reflect the target prompt. Recent efforts have
attempted to directly map source and target distributions via ODE-based
approaches without inversion; however,these methods still yield suboptimal
editing quality. In this work, we propose a flow decomposition-and-aggregation
framework built upon an inversion-free formulation to address these
limitations. Specifically, we semantically decompose the target prompt into
multiple sub-prompts, compute an independent flow for each, and aggregate them
to form a unified editing trajectory. While we empirically observe that
decomposing the original flow enhances diversity in the target space,
generating semantically aligned outputs still requires consistent guidance
toward the full target prompt. To this end, we design a projection and
soft-aggregation mechanism for flow, inspired by gradient conflict resolution
in multi-task learning. This approach adaptively weights the sub-target
velocity fields, suppressing semantic redundancy while emphasizing distinct
directions, thereby preserving both diversity and consistency in the final
edited output. Experimental results demonstrate that our method outperforms
existing zero-shot editing approaches in terms of semantic fidelity and
attribute disentanglement. The code is available at
https://github.com/Harvard-AI-and-Robotics-Lab/SplitFlow.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-ready version for NeurIPS 2025, 10 pages (main paper)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative Image Restoration and Super-Resolution using Physics-Informed
  Synthetic Data for Scanning Tunneling Microscopy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25921v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25921v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikola L. Kolev, Tommaso Rodani, Neil J. Curson, Taylor J. Z. Stock, Alberto Cazzaniga
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scanning tunnelling microscopy (STM) enables atomic-resolution imaging and
atom manipulation, but its utility is often limited by tip degradation and slow
serial data acquisition. Fabrication adds another layer of complexity since the
tip is often subjected to large voltages, which may alter the shape of its
apex, requiring it to be conditioned. Here, we propose a machine learning (ML)
approach for image repair and super-resolution to alleviate both challenges.
Using a dataset of only 36 pristine experimental images of Si(001):H, we
demonstrate that a physics-informed synthetic data generation pipeline can be
used to train several state-of-the-art flow-matching and diffusion models.
Quantitative evaluation with metrics such as the CLIP Maximum Mean Discrepancy
(CMMD) score and structural similarity demonstrates that our models are able to
effectively restore images and offer a two- to fourfold reduction in image
acquisition time by accurately reconstructing images from sparsely sampled
data. Our framework has the potential to significantly increase STM
experimental throughput by offering a route to reducing the frequency of
tip-conditioning procedures and to enhancing frame rates in existing high-speed
STM systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GENRE-CMR: Generalizable Deep Learning for Diverse Multi-Domain Cardiac
  MRI Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.20600v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.20600v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kian Anvari Hamedani, Narges Razizadeh, Shahabedin Nabavi, Mohsen Ebrahimi Moghaddam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accelerated Cardiovascular Magnetic Resonance (CMR) image reconstruction
remains a critical challenge due to the trade-off between scan time and image
quality, particularly when generalizing across diverse acquisition settings. We
propose GENRE-CMR, a generative adversarial network (GAN)-based architecture
employing a residual deep unrolled reconstruction framework to enhance
reconstruction fidelity and generalization. The architecture unrolls iterative
optimization into a cascade of convolutional subnetworks, enriched with
residual connections to enable progressive feature propagation from shallow to
deeper stages. To further improve performance, we integrate two loss functions:
(1) an Edge-Aware Region (EAR) loss, which guides the network to focus on
structurally informative regions and helps prevent common reconstruction
blurriness; and (2) a Statistical Distribution Alignment (SDA) loss, which
regularizes the feature space across diverse data distributions via a symmetric
KL divergence formulation. Extensive experiments confirm that GENRE-CMR
surpasses state-of-the-art methods on training and unseen data, achieving
0.9552 SSIM and 38.90 dB PSNR on unseen distributions across various
acceleration factors and sampling trajectories. Ablation studies confirm the
contribution of each proposed component to reconstruction quality and
generalization. Our framework presents a unified and robust solution for
high-quality CMR reconstruction, paving the way for clinically adaptable
deployment across heterogeneous acquisition protocols.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MILo: Mesh-In-the-Loop Gaussian Splatting for Detailed and Efficient
  Surface Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.24096v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.24096v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antoine Guédon, Diego Gomez, Nissim Maruani, Bingchen Gong, George Drettakis, Maks Ovsjanikov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While recent advances in Gaussian Splatting have enabled fast reconstruction
of high-quality 3D scenes from images, extracting accurate surface meshes
remains a challenge. Current approaches extract the surface through costly
post-processing steps, resulting in the loss of fine geometric details or
requiring significant time and leading to very dense meshes with millions of
vertices. More fundamentally, the a posteriori conversion from a volumetric to
a surface representation limits the ability of the final mesh to preserve all
geometric structures captured during training. We present MILo, a novel
Gaussian Splatting framework that bridges the gap between volumetric and
surface representations by differentiably extracting a mesh from the 3D
Gaussians. We design a fully differentiable procedure that constructs the
mesh-including both vertex locations and connectivity-at every iteration
directly from the parameters of the Gaussians, which are the only quantities
optimized during training. Our method introduces three key technical
contributions: a bidirectional consistency framework ensuring both
representations-Gaussians and the extracted mesh-capture the same underlying
geometry during training; an adaptive mesh extraction process performed at each
training iteration, which uses Gaussians as differentiable pivots for Delaunay
triangulation; a novel method for computing signed distance values from the 3D
Gaussians that enables precise surface extraction while avoiding geometric
erosion. Our approach can reconstruct complete scenes, including backgrounds,
with state-of-the-art quality while requiring an order of magnitude fewer mesh
vertices than previous methods. Due to their light weight and empty interior,
our meshes are well suited for downstream applications such as physics
simulations or animation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages. A presentation video of our approach is available at
  https://youtu.be/_SGNhhNz0fE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeepVideo-R1: Video Reinforcement Fine-Tuning via Difficulty-aware
  Regressive GRPO <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.07464v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.07464v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinyoung Park, Jeehye Na, Jinyoung Kim, Hyunwoo J. Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works have demonstrated the effectiveness of reinforcement learning
(RL)-based post-training for enhancing the reasoning capabilities of large
language models (LLMs). In particular, Group Relative Policy Optimization
(GRPO) has shown impressive success using a PPO-style reinforcement algorithm
with group-normalized rewards. However, the effectiveness of GRPO in Video
Large Language Models (VideoLLMs) has still been less studyed. In this paper,
we explore GRPO and identify two problems that deteriorate the effective
learning: (1) reliance on safeguards, and (2) vanishing advantage. To mitigate
these challenges, we propose DeepVideo-R1, a video large language model trained
with Reg-GRPO (Regressive GRPO) and difficulty-aware data augmentation.
Reg-GRPO reformulates the GRPO loss function into a regression task that
directly predicts the advantage in GRPO, eliminating the need for safeguards
such as the clipping and min functions. It directly aligns the model with
advantages, providing guidance to prefer better ones. The difficulty-aware data
augmentation strategy augments input prompts/videos to locate the difficulty of
samples at solvable difficulty levels, enabling diverse reward signals. Our
experimental results show that our approach significantly improves video
reasoning performance across multiple benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RL-I2IT: Image-to-Image Translation with Deep Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.13672v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.13672v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Hu, Chengming Feng, Shu Hu, Ming-Ching Chang, Xin Li, Xi Wu, Xin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most existing Image-to-Image Translation (I2IT) methods generate images in a
single run of a deep learning (DL) model. However, designing such a single-step
model is always challenging, requiring a huge number of parameters and easily
falling into bad global minimums and overfitting. In this work, we reformulate
I2IT as a step-wise decision-making problem via deep reinforcement learning
(DRL) and propose a novel framework that performs RL-based I2IT (RL-I2IT). The
key feature in the RL-I2IT framework is to decompose a monolithic learning
process into small steps with a lightweight model to progressively transform a
source image successively to a target image. Considering that it is challenging
to handle high dimensional continuous state and action spaces in the
conventional RL framework, we introduce meta policy with a new concept Plan to
the standard Actor-Critic model, which is of a lower dimension than the
original image and can facilitate the actor to generate a tractable high
dimensional action. In the RL-I2IT framework, we also employ a task-specific
auxiliary learning strategy to stabilize the training process and improve the
performance of the corresponding task. Experiments on several I2IT tasks
demonstrate the effectiveness and robustness of the proposed method when facing
high-dimensional continuous action space problems. Our implementation of the
RL-I2IT framework is available at
https://github.com/Algolzw/SPAC-Deformable-Registration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quantizing Space and Time: Fusing Time Series and Images for Earth
  Observation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23118v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23118v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gianfranco Basile, Johannes Jakubik, Benedikt Blumenstiel, Thomas Brunschwiler, Juan Bernabe Moreno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a task-agnostic framework for multimodal fusion of time series and
single timestamp images, enabling cross-modal generation and robust downstream
performance. Our approach explores deterministic and learned strategies for
time series quantization and then leverages a masked correlation learning
objective, aligning discrete image and time series tokens in a unified
representation space. Instantiated in the Earth observation domain, the
pretrained model generates consistent global temperature profiles from
satellite imagery and is validated through counterfactual experiments. Across
downstream tasks, our task-agnostic pretraining outperforms task-specific
fusion by 6% in R^2 and 2% in RMSE on average, and exceeds baseline methods by
50% in R^2 and 12% in RMSE. Finally, we analyze gradient sensitivity across
modalities, providing insights into model robustness. Code, data, and weights
will be released under a permissive license.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual
  Question Answering <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21710v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21710v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liangyu Zhong, Fabio Rosenthal, Joachim Sicking, Fabian Hüger, Thorsten Bagdonat, Hanno Gottschalk, Leo Schwinn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Multimodal Large Language Models (MLLMs) offer strong perception and
reasoning capabilities for image-text input, Visual Question Answering (VQA)
focusing on small image details still remains a challenge. Although visual
cropping techniques seem promising, recent approaches have several limitations:
the need for task-specific fine-tuning, low efficiency due to uninformed
exhaustive search, or incompatibility with efficient attention implementations.
We address these shortcomings by proposing a training-free visual cropping
method, dubbed FOCUS, that leverages MLLM-internal representations to guide the
search for the most relevant image region. This is accomplished in four steps:
first, we identify the target object(s) in the VQA prompt; second, we compute
an object relevance map using the key-value (KV) cache; third, we propose and
rank relevant image regions based on the map; and finally, we perform the
fine-grained VQA task using the top-ranked region. As a result of this informed
search strategy, FOCUS achieves strong performance across four fine-grained VQA
datasets and three types of MLLMs. It outperforms three popular visual cropping
methods in both accuracy and efficiency, and matches the best-performing
baseline, ZoomEye, while requiring 3 - 6.5 x less compute.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2025 - main track. Project page:
  https://focus-mllm-vqa.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RoboOmni: Proactive Robot Manipulation in Omni-modal Context 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23763v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23763v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyin Wang, Jinlan Fu, Feihong Liu, Xinzhe He, Huangxuan Wu, Junhao Shi, Kexin Huang, Zhaoye Fei, Jingjing Gong, Zuxuan Wu, Yugang Jiang, See-Kiong Ng, Tat-Seng Chua, Xipeng Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Multimodal Large Language Models (MLLMs) have driven rapid
progress in Vision-Language-Action (VLA) models for robotic manipulation.
Although effective in many scenarios, current approaches largely rely on
explicit instructions, whereas in real-world interactions, humans rarely issue
instructions directly. Effective collaboration requires robots to infer user
intentions proactively. In this work, we introduce cross-modal contextual
instructions, a new setting where intent is derived from spoken dialogue,
environmental sounds, and visual cues rather than explicit commands. To address
this new setting, we present RoboOmni, a Perceiver-Thinker-Talker-Executor
framework based on end-to-end omni-modal LLMs that unifies intention
recognition, interaction confirmation, and action execution. RoboOmni fuses
auditory and visual signals spatiotemporally for robust intention recognition,
while supporting direct speech interaction. To address the absence of training
data for proactive intention recognition in robotic manipulation, we build
OmniAction, comprising 140k episodes, 5k+ speakers, 2.4k event sounds, 640
backgrounds, and six contextual instruction types. Experiments in simulation
and real-world settings show that RoboOmni surpasses text- and ASR-based
baselines in success rate, inference speed, intention recognition, and
proactive assistance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ScribbleVS: Scribble-Supervised Medical Image Segmentation via Dynamic
  Competitive Pseudo Label Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.10237v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.10237v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Wang, Xinlin Zhang, Zhenxuan Zhang, Yuanbo Zhou, Yuanbin Chen, Longxuan Zhao, Chaohui Xu, Shun Chen, Guang Yang, Tong Tong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In clinical medicine, precise image segmentation can provide substantial
support to clinicians. However, obtaining high-quality segmentation typically
demands extensive pixel-level annotations, which are labor-intensive and
expensive. Scribble annotations offer a more cost-effective alternative by
improving labeling efficiency. Nonetheless, using such sparse supervision for
training reliable medical image segmentation models remains a significant
challenge. Some studies employ pseudo-labeling to enhance supervision, but
these methods are susceptible to noise interference. To address these
challenges, we introduce ScribbleVS, a framework designed to learn from
scribble annotations. We introduce a Regional Pseudo Labels Diffusion Module to
expand the scope of supervision and reduce the impact of noise present in
pseudo labels. Additionally, we introduce a Dynamic Competitive Selection
module for enhanced refinement in selecting pseudo labels. Experiments
conducted on the ACDC, MSCMRseg, WORD, and BraTS2020 datasets demonstrate
promising results, achieving segmentation precision comparable to fully
supervised models. The codes of this study are available at
https://github.com/ortonwang/ScribbleVS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HAIF-GS: Hierarchical and Induced Flow-Guided Gaussian Splatting for
  Dynamic Scene <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.09518v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.09518v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianing Chen, Zehao Li, Yujun Cai, Hao Jiang, Chengxuan Qian, Juyuan Kang, Shuqin Gao, Honglong Zhao, Tianlu Mao, Yucheng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing dynamic 3D scenes from monocular videos remains a fundamental
challenge in 3D vision. While 3D Gaussian Splatting (3DGS) achieves real-time
rendering in static settings, extending it to dynamic scenes is challenging due
to the difficulty of learning structured and temporally consistent motion
representations. This challenge often manifests as three limitations in
existing methods: redundant Gaussian updates, insufficient motion supervision,
and weak modeling of complex non-rigid deformations. These issues collectively
hinder coherent and efficient dynamic reconstruction. To address these
limitations, we propose HAIF-GS, a unified framework that enables structured
and consistent dynamic modeling through sparse anchor-driven deformation. It
first identifies motion-relevant regions via an Anchor Filter to suppress
redundant updates in static areas. A self-supervised Induced Flow-Guided
Deformation module induces anchor motion using multi-frame feature aggregation,
eliminating the need for explicit flow labels. To further handle fine-grained
deformations, a Hierarchical Anchor Propagation mechanism increases anchor
resolution based on motion complexity and propagates multi-level
transformations. Extensive experiments on synthetic and real-world benchmarks
validate that HAIF-GS significantly outperforms prior dynamic 3DGS methods in
rendering quality, temporal coherence, and reconstruction efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2025. Project page:
  https://echopickle.github.io/HAIF-GS.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Simulating Automotive Radar with Lidar and Camera Inputs <span class="chip">IROS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.08068v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.08068v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peili Song, Dezhen Song, Yifan Yang, Enfan Lan, Jingtai Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-cost millimeter automotive radar has received more and more attention due
to its ability to handle adverse weather and lighting conditions in autonomous
driving. However, the lack of quality datasets hinders research and
development. We report a new method that is able to simulate 4D millimeter wave
radar signals including pitch, yaw, range, and Doppler velocity along with
radar signal strength (RSS) using camera image, light detection and ranging
(lidar) point cloud, and ego-velocity. The method is based on two new neural
networks: 1) DIS-Net, which estimates the spatial distribution and number of
radar signals, and 2) RSS-Net, which predicts the RSS of the signal based on
appearance and geometric information. We have implemented and tested our method
using open datasets from 3 different models of commercial automotive radar. The
experimental results show that our method can successfully generate
high-fidelity radar signals. Moreover, we have trained a popular object
detection neural network with data augmented by our synthesized radar. The
network outperforms the counterpart trained only on raw radar data, a promising
result to facilitate future radar-based research and development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IROS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for
  Autonomous Driving <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.17685v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.17685v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuang Zeng, Xinyuan Chang, Mengwei Xie, Xinran Liu, Yifan Bai, Zheng Pan, Mu Xu, Xing Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language-Action (VLA) models are increasingly used for end-to-end
driving due to their world knowledge and reasoning ability. Most prior work,
however, inserts textual chains-of-thought (CoT) as intermediate steps tailored
to the current scene. Such symbolic compressions can blur spatio-temporal
relations and discard fine visual cues, creating a cross-modal gap between
perception and planning. We propose FSDrive, a visual spatio-temporal CoT
framework that enables VLAs to think in images. The model first acts as a world
model to generate a unified future frame that overlays coarse but
physically-plausible priors-future lane dividers and 3D boxes-on the predicted
future image. This unified frame serves as the visual CoT, capturing both
spatial structure and temporal evolution. The same VLA then functions as an
inverse-dynamics model, planning trajectories from current observations and the
visual CoT. To equip VLAs with image generation while preserving understanding,
we introduce a unified pre-training paradigm that expands the vocabulary to
include visual tokens and jointly optimizes VQA (for semantics) and
future-frame prediction (for dynamics). A progressive easy-to-hard scheme first
predicts lane/box priors to enforce physical constraints, then completes full
future frames for fine details. On nuScenes and NAVSIM, FSDrive improves
trajectory accuracy and reduces collisions under both ST-P3 and UniAD metrics,
and attains competitive FID for future-frame generation despite using
lightweight autoregression. It also advances scene understanding on DriveLM.
Together, these results indicate that visual CoT narrows the cross-modal gap
and yields safer, more anticipatory planning. Code is available at
https://github.com/MIV-XJTU/FSDrive.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2025 as Spotlight Presentation. Code:
  https://github.com/MIV-XJTU/FSDrive</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multimodal Recurrent Ensembles for Predicting Brain Responses to
  Naturalistic Movies (Algonauts 2025) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17897v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17897v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Semih Eren, Deniz Kucukahmetler, Nico Scherf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately predicting distributed cortical responses to naturalistic stimuli
requires models that integrate visual, auditory and semantic information over
time. We present a hierarchical multimodal recurrent ensemble that maps
pretrained video, audio, and language embeddings to fMRI time series recorded
while four subjects watched almost 80 hours of movies provided by the Algonauts
2025 challenge. Modality-specific bidirectional RNNs encode temporal dynamics;
their hidden states are fused and passed to a second recurrent layer, and
lightweight subject-specific heads output responses for 1000 cortical parcels.
Training relies on a composite MSE-correlation loss and a curriculum that
gradually shifts emphasis from early sensory to late association regions.
Averaging 100 model variants further boosts robustness. The resulting system
ranked third on the competition leaderboard, achieving an overall Pearson r =
0.2094 and the highest single-parcel peak score (mean r = 0.63) among all
participants, with particularly strong gains for the most challenging subject
(Subject 5). The approach establishes a simple, extensible baseline for future
multimodal brain-encoding benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 2 figures, 1 table. Invited report, CCN 2025 Algonauts
  Project session (3rd-place team). Code:
  https://github.com/erensemih/Algonauts2025_ModalityRNN v3: Added equal
  contribution footnote to author list. Corrected reference list</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Classification of Driver Behaviour Using External Observation Techniques
  for Autonomous Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.09349v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.09349v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ian Nell, Shane Gilroy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Road traffic accidents remain a significant global concern, with human error,
particularly distracted and impaired driving, among the leading causes. This
study introduces a novel driver behaviour classification system that uses
external observation techniques to detect indicators of distraction and
impairment. The proposed framework employs advanced computer vision
methodologies, including real-time object tracking, lateral displacement
analysis, and lane position monitoring. The system identifies unsafe driving
behaviours such as excessive lateral movement and erratic trajectory patterns
by implementing the YOLO object detection model and custom lane estimation
algorithms. Unlike systems reliant on inter-vehicular communication, this
vision-based approach enables behavioural analysis of non-connected vehicles.
Experimental evaluations on diverse video datasets demonstrate the framework's
reliability and adaptability across varying road and environmental conditions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ U-DECN: End-to-End Underwater Object Detection ConvNet with Improved
  DeNoising Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.05780v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.05780v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoyan Liu, Bo Wang, Bing Wang, Ye Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Underwater object detection has higher requirements of running speed and
deployment efficiency for the detector due to its specific environmental
challenges. NMS of two- or one-stage object detectors and transformer
architecture of query-based end-to-end object detectors are not conducive to
deployment on underwater embedded devices with limited processing power. As for
the detrimental effect of underwater color cast noise, recent underwater object
detectors make network architecture or training complex, which also hinders
their application and deployment on unmanned underwater vehicles. In this
paper, we propose the Underwater DECO with improved deNoising training
(U-DECN), the query-based end-to-end object detector (with ConvNet
encoder-decoder architecture) for underwater color cast noise that addresses
the above problems. We integrate advanced technologies from DETR variants into
DECO and design optimization methods specifically for the ConvNet architecture,
including Deformable Convolution in SIM and Separate Contrastive DeNoising
Forward methods. To address the underwater color cast noise issue, we propose
an Underwater Color DeNoising Query method to improve the generalization of the
model for the biased object feature information by different color cast noise.
Our U-DECN, with ResNet-50 backbone, achieves the best 64.0 AP on DUO and the
best 58.1 AP on RUOD, and 21 FPS (5 times faster than Deformable DETR and DINO
4 FPS) on NVIDIA AGX Orin by TensorRT FP16, outperforming the other
state-of-the-art query-based end-to-end object detectors. The code is available
at https://github.com/LEFTeyex/U-DECN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures, 7 tables, accepted by IEEE TGRS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Probabilistic Kernel Function for Fast Angle Testing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.20274v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.20274v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kejing Lu, Chuan Xiao, Yoshiharu Ishikawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we study the angle testing problem in the context of
similarity search in high-dimensional Euclidean spaces and propose two
projection-based probabilistic kernel functions, one designed for angle
comparison and the other for angle thresholding. Unlike existing approaches
that rely on random projection vectors drawn from Gaussian distributions, our
approach leverages reference angles and employs a deterministic structure for
the projection vectors. Notably, our kernel functions do not require asymptotic
assumptions, such as the number of projection vectors tending to infinity, and
can be both theoretically and experimentally shown to outperform
Gaussian-distribution-based kernel functions. We apply the proposed kernel
function to Approximate Nearest Neighbor Search (ANNS) and demonstrate that our
approach achieves a 2.5X ~ 3X higher query-per-second (QPS) throughput compared
to the widely-used graph-based search algorithm HNSW.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FastJAM: a Fast Joint Alignment Model for Images <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.22842v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.22842v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omri Hirsch, Ron Shapira Weber, Shira Ifergane, Oren Freifeld
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Joint Alignment (JA) of images aims to align a collection of images into a
unified coordinate frame, such that semantically-similar features appear at
corresponding spatial locations. Most existing approaches often require long
training times, large-capacity models, and extensive hyperparameter tuning. We
introduce FastJAM, a rapid, graph-based method that drastically reduces the
computational complexity of joint alignment tasks. FastJAM leverages pairwise
matches computed by an off-the-shelf image matcher, together with a rapid
nonparametric clustering, to construct a graph representing intra- and
inter-image keypoint relations. A graph neural network propagates and
aggregates these correspondences, efficiently predicting per-image homography
parameters via image-level pooling. Utilizing an inverse-compositional loss,
that eliminates the need for a regularization term over the predicted
transformations (and thus also obviates the hyperparameter tuning associated
with such terms), FastJAM performs image JA quickly and effectively.
Experimental results on several benchmarks demonstrate that FastJAM achieves
results better than existing modern JA methods in terms of alignment quality,
while reducing computation time from hours or minutes to mere seconds. Our code
is available at our project webpage, https://bgu-cs-vil.github.io/FastJAM/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2025. Pages 1-10 are the Main Paper. Pages 23-31
  are Supplemental Material. FastJAM website -
  https://bgu-cs-vil.github.io/FastJAM/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Open3D-VQA: A Benchmark for Comprehensive Spatial Reasoning with
  Multimodal Large Language Model in Open Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.11094v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.11094v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weichen Zhang, Zile Zhou, Xin Zeng, Xuchen Liu, Jianjie Fang, Chen Gao, Yong Li, Jinqiang Cui, Xinlei Chen, Xiao-Ping Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatial reasoning is a fundamental capability of multimodal large language
models (MLLMs), yet their performance in open aerial environments remains
underexplored. In this work, we present Open3D-VQA, a novel benchmark for
evaluating MLLMs' ability to reason about complex spatial relationships from an
aerial perspective. The benchmark comprises 73k QA pairs spanning 7 general
spatial reasoning tasks, including multiple-choice, true/false, and
short-answer formats, and supports both visual and point cloud modalities. The
questions are automatically generated from spatial relations extracted from
both real-world and simulated aerial scenes. Evaluation on 13 popular MLLMs
reveals that: 1) Models are generally better at answering questions about
relative spatial relations than absolute distances, 2) 3D LLMs fail to
demonstrate significant advantages over 2D LLMs, and 3) Fine-tuning solely on
the simulated dataset can significantly improve the model's spatial reasoning
performance in real-world scenarios. We release our benchmark, data generation
pipeline, and evaluation toolkit to support further research:
https://github.com/EmbodiedCity/Open3D-VQA.code.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Activation Matching for Explanation Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.23051v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.23051v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pirzada Suhail, Aditya Anand, Amit Sethi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we introduce an activation-matching--based approach to generate
minimal, faithful explanations for the decision-making of a pretrained
classifier on any given image. Given an input image $x$ and a frozen model $f$,
we train a lightweight autoencoder to output a binary mask $m$ such that the
explanation $e = m \odot x$ preserves both the model's prediction and the
intermediate activations of \(x\). Our objective combines: (i) multi-layer
activation matching with KL divergence to align distributions and cross-entropy
to retain the top-1 label for both the image and the explanation; (ii) mask
priors -- L1 area for minimality, a binarization penalty for crisp 0/1 masks,
and total variation for compactness; and (iii) abductive constraints for
faithfulness and necessity. Together, these objectives yield small,
human-interpretable masks that retain classifier behavior while discarding
irrelevant input regions, providing practical and faithful minimalist
explanations for the decision making of the underlying model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Single Image Estimation of Cell Migration Direction by Deep Circular
  Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19162v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19162v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lennart Bruns, Lucas Lamparter, Milos Galic, Xiaoyi Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we address the problem of estimating the migration direction
of cells based on a single image. A solution to this problem lays the
foundation for a variety of applications that were previously not possible. To
our knowledge, there is only one related work that employs a classification CNN
with four classes (quadrants). However, this approach does not allow for
detailed directional resolution. We tackle the single image estimation problem
using deep circular regression, with a particular focus on cycle-sensitive
methods. On two common datasets, we achieve a mean estimation error of
$\sim\!17^\circ$, representing a significant improvement over previous work,
which reported estimation error of $30^\circ$ and $34^\circ$, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When are radiology reports useful for training medical image
  classifiers? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24385v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24385v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Herman Bergström, Zhongqi Yue, Fredrik D. Johansson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical images used to train machine learning models are often accompanied by
radiology reports containing rich expert annotations. However, relying on these
reports as inputs for clinical prediction requires the timely manual work of a
trained radiologist. This raises a natural question: when can radiology reports
be leveraged during training to improve image-only classification? Prior works
are limited to evaluating pre-trained image representations by fine-tuning them
to predict diagnostic labels, often extracted from reports, ignoring tasks with
labels that are weakly associated with the text. To address this gap, we
conduct a systematic study of how radiology reports can be used during both
pre-training and fine-tuning, across diagnostic and prognostic tasks (e.g.,
12-month readmission), and under varying training set sizes. Our findings
reveal that: (1) Leveraging reports during pre-training is beneficial for
downstream classification tasks where the label is well-represented in the
text; however, pre-training through explicit image-text alignment can be
detrimental in settings where it's not; (2) Fine-tuning with reports can lead
to significant improvements and even have a larger impact than the pre-training
method in certain settings. These results provide actionable insights into when
and how to leverage privileged text data to train medical image classifiers
while highlighting gaps in current research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MagicPortrait: Temporally Consistent Face Reenactment with 3D Geometric
  Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.21497v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.21497v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengting Wei, Yante Li, Tuomas Varanka, Yan Jiang, Guoying Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we propose a method for video face reenactment that integrates
a 3D face parametric model into a latent diffusion framework, aiming to improve
shape consistency and motion control in existing video-based face generation
approaches. Our approach employs the FLAME (Faces Learned with an Articulated
Model and Expressions) model as the 3D face parametric representation,
providing a unified framework for modeling face expressions and head pose. This
not only enables precise extraction of motion features from driving videos, but
also contributes to the faithful preservation of face shape and geometry.
Specifically, we enhance the latent diffusion model with rich 3D expression and
detailed pose information by incorporating depth maps, normal maps, and
rendering maps derived from FLAME sequences. These maps serve as motion
guidance and are encoded into the denoising UNet through a specifically
designed Geometric Guidance Encoder (GGE). A multi-layer feature fusion module
with integrated self-attention mechanisms is used to combine facial appearance
and motion latent features within the spatial domain. By utilizing the 3D face
parametric model as motion guidance, our method enables parametric alignment of
face identity between the reference image and the motion captured from the
driving video. Experimental results on benchmark datasets show that our method
excels at generating high-quality face animations with precise expression and
head pose variation modeling. In addition, it demonstrates strong
generalization performance on out-of-domain images. Code is publicly available
at https://github.com/weimengting/MagicPortrait.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HyperET: Efficient Training in Hyperbolic Space for Multi-modal Large
  Language Models <span class="chip">NeurIPS2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.20322v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.20322v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zelin Peng, Zhengqin Xu, Qingyang Liu, Xiaokang Yang, Wei Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal large language models (MLLMs) have emerged as a transformative
approach for aligning visual and textual understanding. They typically require
extremely high computational resources (e.g., thousands of GPUs) for training
to achieve cross-modal alignment at multi-granularity levels. We argue that a
key source of this inefficiency lies in the vision encoders they widely equip
with, e.g., CLIP and SAM, which lack the alignment with language at
multi-granularity levels. To address this issue, in this paper, we leverage
hyperbolic space, which inherently models hierarchical levels and thus provides
a principled framework for bridging the granularity gap between visual and
textual modalities at an arbitrary granularity level. Concretely, we propose an
efficient training paradigm for MLLMs, dubbed as HyperET, which can optimize
visual representations to align with their textual counterparts at an arbitrary
granularity level through dynamic hyperbolic radius adjustment in hyperbolic
space. HyperET employs learnable matrices with M\"{o}bius multiplication
operations, implemented via three effective configurations: diagonal scaling
matrices, block-diagonal matrices, and banded matrices, providing a flexible
yet efficient parametrization strategy. Comprehensive experiments across
multiple MLLM benchmarks demonstrate that HyperET consistently improves both
existing pre-training and fine-tuning MLLMs clearly with less than 1\%
additional parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS2025 (Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DGTRSD & DGTRS-CLIP: A Dual-Granularity Remote Sensing Image-Text
  <span class="highlight-title">Dataset</span> and Vision Language Foundation Model for Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.19311v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.19311v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weizhi Chen, Yupeng Deng, Jin Wei, Jingbo Chen, Jiansheng Chen, Yuman Feng, Zhihao Xi, Diyou Liu, Kai Li, Yu Meng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Language Foundation Models based on CLIP architecture for remote
sensing primarily rely on short text captions, which often result in incomplete
semantic representations. Although longer captions convey richer information,
existing models struggle to process them effectively because of limited
text-encoding capacity, and there remains a shortage of resources that align
remote sensing images with both short text and long text captions. To address
this gap, we introduce DGTRSD, a dual-granularity remote sensing image-text
dataset, where each image is paired with both a short text caption and a long
text description, providing a solid foundation for dual-granularity semantic
modeling. Based on this, we further propose DGTRS-CLIP, a dual-granularity
curriculum learning framework that combines short text and long text
supervision to achieve dual-granularity semantic alignment. Extensive
experiments on four typical zero-shot tasks: long text cross-modal retrieval,
short text cross-modal retrieval, image classification, and semantic
localization demonstrate that DGTRS-CLIP consistently outperforms existing
methods across all tasks. The code has been open-sourced and is available at
https://github.com/MitsuiChen14/DGTRS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ XY-Cut++: Advanced Layout Ordering via Hierarchical Mask Mechanism on a
  Novel Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.10258v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.10258v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuai Liu, Youmeng Li, Jizeng Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Document Reading Order Recovery is a fundamental task in document image
understanding, playing a pivotal role in enhancing Retrieval-Augmented
Generation (RAG) and serving as a critical preprocessing step for large
language models (LLMs). Existing methods often struggle with complex
layouts(e.g., multi-column newspapers), high-overhead interactions between
cross-modal elements (visual regions and textual semantics), and a lack of
robust evaluation benchmarks. We introduce XY-Cut++, an advanced layout
ordering method that integrates pre-mask processing, multi-granularity
segmentation, and cross-modal matching to address these challenges. Our method
significantly enhances layout ordering accuracy compared to traditional XY-Cut
techniques. Specifically, XY-Cut++ achieves state-of-the-art performance (98.8
BLEU overall) while maintaining simplicity and efficiency. It outperforms
existing baselines by up to 24\% and demonstrates consistent accuracy across
simple and complex layouts on the newly introduced DocBench-100 dataset. This
advancement establishes a reliable foundation for document structure recovery,
setting a new standard for layout ordering tasks and facilitating more
effective RAG and LLM preprocessing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NoisyGRPO: Incentivizing Multimodal CoT Reasoning via Noise Injection
  and Bayesian Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.21122v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.21122v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Longtian Qiu, Shan Ning, Jiaxuan Sun, Xuming He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) has shown promise in enhancing the general
Chain-of-Thought (CoT) reasoning capabilities of multimodal large language
models (MLLMs). However, when applied to improve general CoT reasoning,
existing RL frameworks often struggle to generalize beyond the training
distribution. To address this, we propose NoisyGRPO, a systematic multimodal RL
framework that introduces controllable noise into visual inputs for enhanced
exploration and explicitly models the advantage estimation process via a
Bayesian framework. Specifically, NoisyGRPO improves RL training by: (1)
Noise-Injected Exploration Policy: Perturbing visual inputs with Gaussian noise
to encourage exploration across a wider range of visual scenarios; and (2)
Bayesian Advantage Estimation: Formulating advantage estimation as a principled
Bayesian inference problem, where the injected noise level serves as a prior
and the observed trajectory reward as the likelihood. This Bayesian modeling
fuses both sources of information to compute a robust posterior estimate of
trajectory advantage, effectively guiding MLLMs to prefer visually grounded
trajectories over noisy ones. Experiments on standard CoT quality, general
capability, and hallucination benchmarks demonstrate that NoisyGRPO
substantially improves generalization and robustness, especially in RL settings
with small-scale MLLMs such as Qwen2.5-VL 3B. The project page is available at
https://artanic30.github.io/project_pages/NoisyGRPO/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Neurips2025, Project page at at
  https://artanic30.github.io/project_pages/NoisyGRPO/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InstDrive: Instance-Aware 3D Gaussian Splatting for Driving Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.12015v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.12015v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyuan Liu, Haochen Yu, Bochao Zou, Jianfei Jiang, Qiankun Liu, Jiansheng Chen, Huimin Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing dynamic driving scenes from dashcam videos has attracted
increasing attention due to its significance in autonomous driving and scene
understanding. While recent advances have made impressive progress, most
methods still unify all background elements into a single representation,
hindering both instance-level understanding and flexible scene editing. Some
approaches attempt to lift 2D segmentation into 3D space, but often rely on
pre-processed instance IDs or complex pipelines to map continuous features to
discrete identities. Moreover, these methods are typically designed for indoor
scenes with rich viewpoints, making them less applicable to outdoor driving
scenarios. In this paper, we present InstDrive, an instance-aware 3D Gaussian
Splatting framework tailored for the interactive reconstruction of dynamic
driving scene. We use masks generated by SAM as pseudo ground-truth to guide 2D
feature learning via contrastive loss and pseudo-supervised objectives. At the
3D level, we introduce regularization to implicitly encode instance identities
and enforce consistency through a voxel-based loss. A lightweight static
codebook further bridges continuous features and discrete identities without
requiring data pre-processing or complex optimization. Quantitative and
qualitative experiments demonstrate the effectiveness of InstDrive, and to the
best of our knowledge, it is the first framework to achieve 3D instance
segmentation in dynamic, open-world driving scenes.More visualizations are
available at our project page.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DPMambaIR: All-in-One Image Restoration via Degradation-Aware <span class="highlight-title">Prompt</span>
  State Space Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.17732v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.17732v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhanwen Liu, Sai Zhou, Yuchao Dai, Yang Wang, Yisheng An, Xiangmo Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  All-in-One image restoration aims to address multiple image degradation
problems using a single model, offering a more practical and versatile solution
compared to designing dedicated models for each degradation type. Existing
approaches typically rely on Degradation-specific models or coarse-grained
degradation prompts to guide image restoration. However, they lack fine-grained
modeling of degradation information and face limitations in balancing
multi-task conflicts. To overcome these limitations, we propose DPMambaIR, a
novel All-in-One image restoration framework that introduces a fine-grained
degradation extractor and a Degradation-Aware Prompt State Space Model
(DP-SSM). The DP-SSM leverages the fine-grained degradation features captured
by the extractor as dynamic prompts, which are then incorporated into the state
space modeling process. This enhances the model's adaptability to diverse
degradation types, while a complementary High-Frequency Enhancement Block (HEB)
recovers local high-frequency details. Extensive experiments on a mixed dataset
containing seven degradation types show that DPMambaIR achieves the best
performance, with 27.69dB and 0.893 in PSNR and SSIM, respectively. These
results highlight the potential and superiority of DPMambaIR as a unified
solution for All-in-One image restoration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vision-Centric 4D Occupancy Forecasting and Planning via Implicit
  Residual World Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.16729v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.16729v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianbiao Mei, Yu Yang, Xuemeng Yang, Licheng Wen, Jiajun Lv, Botian Shi, Yong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end autonomous driving systems increasingly rely on vision-centric
world models to understand and predict their environment. However, a common
ineffectiveness in these models is the full reconstruction of future scenes,
which expends significant capacity on redundantly modeling static backgrounds.
To address this, we propose IR-WM, an Implicit Residual World Model that
focuses on modeling the current state and evolution of the world. IR-WM first
establishes a robust bird's-eye-view representation of the current state from
the visual observation. It then leverages the BEV features from the previous
timestep as a strong temporal prior and predicts only the "residual", i.e., the
changes conditioned on the ego-vehicle's actions and scene context. To
alleviate error accumulation over time, we further apply an alignment module to
calibrate semantic and dynamic misalignments. Moreover, we investigate
different forecasting-planning coupling schemes and demonstrate that the
implicit future state generated by world models substantially improves planning
accuracy. On the nuScenes benchmark, IR-WM achieves top performance in both 4D
occupancy forecasting and trajectory planning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InfoChartQA: A Benchmark for Multimodal Question Answering on
  Infographic Charts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.19028v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.19028v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianchi Xie, Minzhi Lin, Mengchen Liu, Yilin Ye, Changjian Chen, Shixia Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding infographic charts with design-driven visual elements (e.g.,
pictograms, icons) requires both visual recognition and reasoning, posing
challenges for multimodal large language models (MLLMs). However, existing
visual-question answering benchmarks fall short in evaluating these
capabilities of MLLMs due to the lack of paired plain charts and
visual-element-based questions. To bridge this gap, we introduce InfoChartQA, a
benchmark for evaluating MLLMs on infographic chart understanding. It includes
5,642 pairs of infographic and plain charts, each sharing the same underlying
data but differing in visual presentations. We further design
visual-element-based questions to capture their unique visual designs and
communicative intent. Evaluation of 20 MLLMs reveals a substantial performance
decline on infographic charts, particularly for visual-element-based questions
related to metaphors. The paired infographic and plain charts enable
fine-grained error analysis and ablation studies, which highlight new
opportunities for advancing MLLMs in infographic chart understanding. We
release InfoChartQA at https://github.com/CoolDawnAnt/InfoChartQA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and
  Semantic Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.19638v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.19638v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ming Meng, Qi Dong, Jiajie Li, Zhe Zhu, Xingyu Wang, Zhaoxin Fan, Wei Zhao, Wenjun Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Virtual try-on technology has become increasingly important in the fashion
and retail industries, enabling the generation of high-fidelity garment images
that adapt seamlessly to target human models. While existing methods have
achieved notable progress, they still face significant challenges in
maintaining consistency across different poses. Specifically, geometric
distortions lead to a lack of spatial consistency, mismatches in garment
structure and texture across poses result in semantic inconsistency, and the
loss or distortion of fine-grained details diminishes visual fidelity. To
address these challenges, we propose HF-VTON, a novel framework that ensures
high-fidelity virtual try-on performance across diverse poses. HF-VTON consists
of three key modules: (1) the Appearance-Preserving Warp Alignment Module
(APWAM), which aligns garments to human poses, addressing geometric
deformations and ensuring spatial consistency; (2) the Semantic Representation
and Comprehension Module (SRCM), which captures fine-grained garment attributes
and multi-pose data to enhance semantic representation, maintaining structural,
textural, and pattern consistency; and (3) the Multimodal Prior-Guided
Appearance Generation Module (MPAGM), which integrates multimodal features and
prior knowledge from pre-trained models to optimize appearance generation,
ensuring both semantic and geometric consistency. Additionally, to overcome
data limitations in existing benchmarks, we introduce the SAMP-VTONS dataset,
featuring multi-pose pairs and rich textual annotations for a more
comprehensive evaluation. Experimental results demonstrate that HF-VTON
outperforms state-of-the-art methods on both VITON-HD and SAMP-VTONS, excelling
in visual fidelity, semantic consistency, and detail preservation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>After the publication of the paper, we discovered some significant
  errors/omissions that need to be corrected and improved</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diverse Teaching and Label Propagation for Generic Semi-Supervised
  Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.08549v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.08549v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Li, Pengcheng Zhou, Linye Ma, Wenyi Zhao, Huihua Yang, Yuchen Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Both limited annotation and domain shift are significant challenges
frequently encountered in medical image segmentation, leading to derivative
scenarios like semi-supervised medical (SSMIS), semi-supervised medical domain
generalization (Semi-MDG) and unsupervised medical domain adaptation (UMDA).
Conventional methods are generally tailored to specific tasks in isolation, the
error accumulation hinders the effective utilization of unlabeled data and
limits further improvements, resulting in suboptimal performance when these
issues occur. In this paper, we aim to develop a generic framework that masters
all three tasks. We found that the key to solving the problem lies in how to
generate reliable pseudo labels for the unlabeled data in the presence of
domain shift with labeled data and increasing the diversity of the model. To
tackle this issue, we employ a Diverse Teaching and Label Propagation Network
(DTLP-Net) to boosting the Generic Semi-Supervised Medical Image Segmentation.
Our DTLP-Net involves a single student model and two diverse teacher models,
which can generate reliable pseudo-labels for the student model. The first
teacher model decouple the training process with labeled and unlabeled data,
The second teacher is momentum-updated periodically, thus generating reliable
yet divers pseudo-labels. To fully utilize the information within the data, we
adopt inter-sample and intra-sample data augmentation to learn the global and
local knowledge. In addition, to further capture the voxel-level correlations,
we propose label propagation to enhance the model robust. We evaluate our
proposed framework on five benchmark datasets for SSMIS, UMDA, and Semi-MDG
tasks. The results showcase notable improvements compared to state-of-the-art
methods across all five settings, indicating the potential of our framework to
tackle more challenging SSL scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluation of Safety Cognition Capability in Vision-Language Models for
  Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.06497v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.06497v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enming Zhang, Peizhe Gong, Xingyuan Dai, Min Huang, Yisheng Lv, Qinghai Miao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring the safety of vision-language models (VLMs) in autonomous driving
systems is of paramount importance, yet existing research has largely focused
on conventional benchmarks rather than safety-critical evaluation. In this
work, we present SCD-Bench (Safety Cognition Driving Benchmark) a novel
framework specifically designed to assess the safety cognition capabilities of
VLMs within interactive driving scenarios. To address the scalability challenge
of data annotation, we introduce ADA (Autonomous Driving Annotation), a
semi-automated labeling system, further refined through expert review by
professionals with domain-specific knowledge in autonomous driving. To
facilitate scalable and consistent evaluation, we also propose an automated
assessment pipeline leveraging large language models, which demonstrates over
98% agreement with human expert judgments. In addressing the broader challenge
of aligning VLMs with safety cognition in driving environments, we construct
SCD-Training, the first large-scale dataset tailored for this task, comprising
324.35K high-quality samples. Through extensive experiments, we show that
models trained on SCD-Training exhibit marked improvements not only on
SCD-Bench, but also on general and domain-specific benchmarks, offering a new
perspective on enhancing safety-aware interactions in vision-language systems
for autonomous driving.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Depth-Aware Super-Resolution via Distance-Adaptive Variational
  Formulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.05746v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.05746v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianhao Guo, Bingjie Lu, Feng Wang, Zhengyang Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Single image super-resolution traditionally assumes spatially-invariant
degradation models, yet real-world imaging systems exhibit complex
distance-dependent effects including atmospheric scattering, depth-of-field
variations, and perspective distortions. This fundamental limitation
necessitates spatially-adaptive reconstruction strategies that explicitly
incorporate geometric scene understanding for optimal performance. We propose a
rigorous variational framework that characterizes super-resolution as a
spatially-varying inverse problem, formulating the degradation operator as a
pseudodifferential operator with distance-dependent spectral characteristics
that enable theoretical analysis of reconstruction limits across depth ranges.
Our neural architecture implements discrete gradient flow dynamics through
cascaded residual blocks with depth-conditional convolution kernels, ensuring
convergence to stationary points of the theoretical energy functional while
incorporating learned distance-adaptive regularization terms that dynamically
adjust smoothness constraints based on local geometric structure. Spectral
constraints derived from atmospheric scattering theory prevent bandwidth
violations and noise amplification in far-field regions, while adaptive kernel
generation networks learn continuous mappings from depth to reconstruction
filters. Comprehensive evaluation across five benchmark datasets demonstrates
state-of-the-art performance, achieving 36.89/0.9516 and 30.54/0.8721 PSNR/SSIM
at 2 and 4 scales on KITTI outdoor scenes, outperforming existing methods by
0.44dB and 0.36dB respectively. This work establishes the first
theoretically-grounded distance-adaptive super-resolution framework and
demonstrates significant improvements on depth-variant scenarios while
maintaining competitive performance across traditional benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Why Foundation Models in Pathology Are Failing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23807v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23807v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamid R. Tizhoosh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In non-medical domains, foundation models (FMs) have revolutionized computer
vision and language processing through large-scale self-supervised and
multimodal learning. Consequently, their rapid adoption in computational
pathology was expected to deliver comparable breakthroughs in cancer diagnosis,
prognostication, and multimodal retrieval. However, recent systematic
evaluations reveal fundamental weaknesses: low diagnostic accuracy, poor
robustness, geometric instability, heavy computational demands, and concerning
safety vulnerabilities. This short paper examines these shortcomings and argues
that they stem from deeper conceptual mismatches between the assumptions
underlying generic foundation modeling in mainstream AI and the intrinsic
complexity of human tissue. Seven interrelated causes are identified:
biological complexity, ineffective self-supervision, overgeneralization,
excessive architectural complexity, lack of domain-specific innovation,
insufficient data, and a fundamental design flaw related to tissue patch size.
These findings suggest that current pathology foundation models remain
conceptually misaligned with the nature of tissue morphology and call for a
fundamental rethinking of the paradigm itself.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LightBagel: A Light-weighted, Double Fusion Framework for Unified
  Multimodal Understanding and Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.22946v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.22946v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyu Wang, Zilong Chen, Chenhui Gou, Feng Li, Chaorui Deng, Deyao Zhu, Kunchang Li, Weihao Yu, Haoqin Tu, Haoqi Fan, Cihang Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unified multimodal models have recently shown remarkable gains in both
capability and versatility, yet most leading systems are still trained from
scratch and require substantial computational resources. In this paper, we show
that competitive performance can be obtained far more efficiently by
strategically fusing publicly available models specialized for either
generation or understanding. Our key design is to retain the original blocks
while additionally interleaving multimodal self-attention blocks throughout the
networks. This double fusion mechanism (1) effectively enables rich multi-modal
fusion while largely preserving the original strengths of the base models, and
(2) catalyzes synergistic fusion of high-level semantic representations from
the understanding encoder with low-level spatial signals from the generation
encoder. By training with only ~ 35B tokens, this approach achieves strong
results across multiple benchmarks: 0.91 on GenEval for compositional
text-to-image generation, 82.16 on DPG-Bench for complex text-to-image
generation, 6.06 on GEditBench, and 3.77 on ImgEdit-Bench for image editing. By
fully releasing the entire suite of code, model weights, and datasets, we hope
to support future research on unified multimodal modeling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Withdrawn because the submission was premature and not agreed by all
  parties in collaboration</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unified Multimodal Chain-of-Thought Reward Model through Reinforcement
  Fine-Tuning <span class="chip">NeurIPS2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.03318v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.03318v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yibin Wang, Zhimin Li, Yuhang Zang, Chunyu Wang, Qinglin Lu, Cheng Jin, Jiaqi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in multimodal Reward Models (RMs) have shown significant
promise in delivering reward signals to align vision models with human
preferences. However, current RMs are generally restricted to providing direct
responses or engaging in shallow reasoning processes with limited depth, often
leading to inaccurate reward signals. We posit that incorporating explicit long
chains of thought (CoT) into the reward reasoning process can significantly
strengthen their reliability and robustness. Furthermore, we believe that once
RMs internalize CoT reasoning, their direct response accuracy can also be
improved through implicit reasoning capabilities. To this end, this paper
proposes UnifiedReward-Think, the first unified multimodal CoT-based reward
model, capable of multi-dimensional, step-by-step long-chain reasoning for both
visual understanding and generation reward tasks. Specifically, we adopt an
exploration-driven reinforcement fine-tuning approach to elicit and incentivize
the model's latent complex reasoning ability: (1) We first use a small amount
of image generation preference data to distill the reasoning process of GPT-4o,
which is then used for the model's cold start to learn the format and structure
of CoT reasoning. (2) Subsequently, by leveraging the model's prior knowledge
and generalization capabilities, we prepare large-scale unified multimodal
preference data to elicit the model's reasoning process across various vision
tasks. During this phase, correct reasoning outputs are retained for rejection
sampling to refine the model (3) while incorrect predicted samples are finally
used for Group Relative Policy Optimization (GRPO) based reinforcement
fine-tuning, enabling the model to explore diverse reasoning paths and optimize
for correct and robust solutions. Extensive experiments across various vision
reward tasks demonstrate the superiority of our model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>[NeurIPS2025] Project Page:
  https://codegoat24.github.io/UnifiedReward/think</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RoboCerebra: A Large-scale Benchmark for Long-horizon Robotic
  Manipulation Evaluation <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.06677v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.06677v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songhao Han, Boxiang Qiu, Yue Liao, Siyuan Huang, Chen Gao, Shuicheng Yan, Si Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in vision-language models (VLMs) have enabled
instruction-conditioned robotic systems with improved generalization. However,
most existing work focuses on reactive System 1 policies, underutilizing VLMs'
strengths in semantic reasoning and long-horizon planning. These System 2
capabilities-characterized by deliberative, goal-directed thinking-remain under
explored due to the limited temporal scale and structural complexity of current
benchmarks. To address this gap, we introduce RoboCerebra, a benchmark for
evaluating high-level reasoning in long-horizon robotic manipulation.
RoboCerebra includes: (1) a large-scale simulation dataset with extended task
horizons and diverse subtask sequences in household environments; (2) a
hierarchical framework combining a high-level VLM planner with a low-level
vision-language-action (VLA) controller; and (3) an evaluation protocol
targeting planning, reflection, and memory through structured System 1-System 2
interaction. The dataset is constructed via a top-down pipeline, where GPT
generates task instructions and decomposes them into subtask sequences. Human
operators execute the subtasks in simulation, yielding high-quality
trajectories with dynamic object variations. Compared to prior benchmarks,
RoboCerebra features significantly longer action sequences and denser
annotations. We further benchmark state-of-the-art VLMs as System 2 modules and
analyze their performance across key cognitive dimensions, advancing the
development of more capable and generalizable robotic planners.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 18 figures, Accepted by NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ L2RSI: Cross-view LiDAR-based Place Recognition for Large-scale Urban
  Scenes via Remote Sensing Imagery <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.11245v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.11245v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziwei Shi, Xiaoran Zhang, Wenjing Xu, Yan Xia, Yu Zang, Siqi Shen, Cheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We tackle the challenge of LiDAR-based place recognition, which traditionally
depends on costly and time-consuming prior 3D maps. To overcome this, we first
construct LiRSI-XA dataset, which encompasses approximately $110,000$ remote
sensing submaps and $13,000$ LiDAR point cloud submaps captured in urban
scenes, and propose a novel method, L2RSI, for cross-view LiDAR place
recognition using high-resolution Remote Sensing Imagery. This approach enables
large-scale localization capabilities at a reduced cost by leveraging readily
available overhead images as map proxies. L2RSI addresses the dual challenges
of cross-view and cross-modal place recognition by learning feature alignment
between point cloud submaps and remote sensing submaps in the semantic domain.
Additionally, we introduce a novel probability propagation method based on
particle estimation to refine position predictions, effectively leveraging
temporal and spatial information. This approach enables large-scale retrieval
and cross-scene generalization without fine-tuning. Extensive experiments on
LiRSI-XA demonstrate that, within a $100km^2$ retrieval range, L2RSI accurately
localizes $83.27\%$ of point cloud submaps within a $30m$ radius for top-$1$
retrieved location. Our project page is publicly available at
https://shizw695.github.io/L2RSI/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 7 figures, NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph-Theoretic Consistency for Robust and Topology-Aware
  Semi-Supervised Histopathology Segmentation <span class="chip">AAAI 2026</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.22689v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.22689v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ha-Hieu Pham, Minh Le, Han Huynh, Nguyen Quoc Khanh Le, Huy-Hieu Pham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised semantic segmentation (SSSS) is vital in computational
pathology, where dense annotations are costly and limited. Existing methods
often rely on pixel-level consistency, which propagates noisy pseudo-labels and
produces fragmented or topologically invalid masks. We propose Topology Graph
Consistency (TGC), a framework that integrates graph-theoretic constraints by
aligning Laplacian spectra, component counts, and adjacency statistics between
prediction graphs and references. This enforces global topology and improves
segmentation accuracy. Experiments on GlaS and CRAG demonstrate that TGC
achieves state-of-the-art performance under 5-10% supervision and significantly
narrows the gap to full supervision.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the AAAI 2026 Student Abstract and Poster Program</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AI in Lung Health: Benchmarking Detection and Diagnostic Models Across
  Multiple CT Scan <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.04605v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.04605v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fakrul Islam Tushar, Avivah Wang, Lavsen Dahal, Ehsan Samei, Michael R. Harowicz, Jayashree Kalpathy-Cramer, Kyle J. Lafata, Tina D. Tailor, Cynthia Rudin, Joseph Y. Lo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Background: Development of artificial intelligence (AI) models for lung
cancer screening requires large, well-annotated low-dose computed tomography
(CT) datasets and rigorous performance benchmarks. Purpose: To create a
reproducible benchmarking resource leveraging the Duke Lung Cancer Screening
(DLCS) and multiple public datasets to develop and evaluate models for nodule
detection and classification. Materials & Methods: This retrospective study
uses the DLCS dataset (1,613 patients; 2,487 nodules) and external datasets
including LUNA16, LUNA25, and NLST-3D. For detection, MONAI RetinaNet models
were trained on DLCS (DLCS-De) and LUNA16 (LUNA16-De) and evaluated using the
Competition Performance Metric (CPM). For nodule-level classification, we
compare five strategies: pretrained models (Models Genesis, Med3D), a
self-supervised foundation model (FMCB), and ResNet50 with random
initialization versus Strategic Warm-Start (ResNet50-SWS) pretrained with
detection-derived candidate patches stratified by confidence. Results: For
detection on the DLCS test set, DLCS-De achieved sensitivity 0.82 at 2 false
positives/scan (CPM 0.63) versus LUNA16-De (0.62, CPM 0.45). For external
validation on NLST-3D, DLCS-De (sensitivity 0.72, CPM 0.58) also outperformed
LUNA16-De (sensitivity 0.64, CPM 0.49). For classification across multiple
datasets, ResNet50-SWS attained AUCs of 0.71 (DLCS; 95% CI, 0.61-0.81), 0.90
(LUNA16; 0.87-0.93), 0.81 (NLST-3D; 0.79-0.82), and 0.80 (LUNA25; 0.78-0.82),
matching or exceeding pretrained/self-supervised baselines. Performance
differences reflected dataset label standards. Conclusion: This work
establishes a standardized benchmarking resource for lung cancer AI research,
supporting model development, validation, and translation. All code, models,
and data are publicly released to promote reproducibility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2 tables, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Caption-Driven Explainability: Probing CNNs for Bias via CLIP <span class="chip">ICIP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.22035v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.22035v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrick Koller, Amil V. Dravid, Guido M. Schuster, Aggelos K. Katsaggelos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robustness has become one of the most critical problems in machine learning
(ML). The science of interpreting ML models to understand their behavior and
improve their robustness is referred to as explainable artificial intelligence
(XAI). One of the state-of-the-art XAI methods for computer vision problems is
to generate saliency maps. A saliency map highlights the pixel space of an
image that excites the ML model the most. However, this property could be
misleading if spurious and salient features are present in overlapping pixel
spaces. In this paper, we propose a caption-based XAI method, which integrates
a standalone model to be explained into the contrastive language-image
pre-training (CLIP) model using a novel network surgery approach. The resulting
caption-based XAI model identifies the dominant concept that contributes the
most to the models prediction. This explanation minimizes the risk of the
standalone model falling for a covariate shift and contributes significantly
towards developing robust ML models. Our code is available at
https://github.com/patch0816/caption-driven-xai
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted and presented at the IEEE ICIP 2025 Satellite Workshop
  "Generative AI for World Simulations and Communications & Celebrating 40
  Years of Excellence in Education: Honoring Professor Aggelos Katsaggelos",
  Anchorage, Alaska, USA, September 14, 2025. Camera-ready preprint; the
  official IEEE Xplore publication will follow. Code:
  https://github.com/patch0816/caption-driven-xai</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pixel-Perfect Depth with Semantics-<span class="highlight-title">Prompt</span>ed Diffusion <span class="highlight-title">Transformer</span>s <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.07316v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.07316v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gangwei Xu, Haotong Lin, Hongcheng Luo, Xianqi Wang, Jingfeng Yao, Lianghui Zhu, Yuechuan Pu, Cheng Chi, Haiyang Sun, Bing Wang, Guang Chen, Hangjun Ye, Sida Peng, Xin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents Pixel-Perfect Depth, a monocular depth estimation model
based on pixel-space diffusion generation that produces high-quality,
flying-pixel-free point clouds from estimated depth maps. Current generative
depth estimation models fine-tune Stable Diffusion and achieve impressive
performance. However, they require a VAE to compress depth maps into latent
space, which inevitably introduces \textit{flying pixels} at edges and details.
Our model addresses this challenge by directly performing diffusion generation
in the pixel space, avoiding VAE-induced artifacts. To overcome the high
complexity associated with pixel-space generation, we introduce two novel
designs: 1) Semantics-Prompted Diffusion Transformers (SP-DiT), which
incorporate semantic representations from vision foundation models into DiT to
prompt the diffusion process, thereby preserving global semantic consistency
while enhancing fine-grained visual details; and 2) Cascade DiT Design that
progressively increases the number of tokens to further enhance efficiency and
accuracy. Our model achieves the best performance among all published
generative models across five benchmarks, and significantly outperforms all
other models in edge-aware point cloud evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025. Project page: https://pixel-perfect-depth.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WaMaIR: Image Restoration via Multiscale Wavelet Convolutions and
  Mamba-based Channel Modeling with Texture Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.16765v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.16765v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengyu Zhu, Congyi Fan, Fuxuan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image restoration is a fundamental and challenging task in computer vision,
where CNN-based frameworks demonstrate significant computational efficiency.
However, previous CNN-based methods often face challenges in adequately
restoring fine texture details, which are limited by the small receptive field
of CNN structures and the lack of channel feature modeling. In this paper, we
propose WaMaIR, which is a novel framework with a large receptive field for
image perception and improves the reconstruction of texture details in restored
images. Specifically, we introduce the Global Multiscale Wavelet Transform
Convolutions (GMWTConvs) for expandding the receptive field to extract image
features, preserving and enriching texture features in model inputs. Meanwhile,
we propose the Mamba-Based Channel-Aware Module (MCAM), explicitly designed to
capture long-range dependencies within feature channels, which enhancing the
model sensitivity to color, edges, and texture information. Additionally, we
propose Multiscale Texture Enhancement Loss (MTELoss) for image restoration to
guide the model in preserving detailed texture structures effectively.
Extensive experiments confirm that WaMaIR outperforms state-of-the-art methods,
achieving better image restoration and efficient computational performance of
the model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Chinese Conference on Pattern Recognition and Computer Vision (PRCV),
  Oral</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Think or Not? Selective Reasoning via Reinforcement Learning for
  Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.16854v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.16854v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaqi Wang, Kevin Qinghong Lin, James Cheng, Mike Zheng Shou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning (RL) has proven to be an effective post-training
strategy for enhancing reasoning in vision-language models (VLMs). Group
Relative Policy Optimization (GRPO) is a recent prominent method that
encourages models to generate complete reasoning traces before answering,
leading to increased token usage and computational cost. Inspired by the
human-like thinking process-where people skip reasoning for easy questions but
think carefully when needed-we explore how to enable VLMs to first decide when
reasoning is necessary. To realize this, we propose TON, a two-stage training
strategy: (i) a supervised fine-tuning (SFT) stage with a simple yet effective
'thought dropout' operation, where reasoning traces are randomly replaced with
empty thoughts. This introduces a think-or-not format that serves as a cold
start for selective reasoning; (ii) a GRPO stage that enables the model to
freely explore when to think or not, while maximizing task-aware outcome
rewards. Experimental results show that TON can reduce the completion length by
up to 90% compared to vanilla GRPO, without sacrificing performance or even
improving it. Further evaluations across LLM (GSM8K), VLM (CLEVR, Super-CLEVR,
GeoQA), and Agentic (AITZ) tasks-covering a range of reasoning difficulties
under both 3B and 7B models-consistently reveal that the model progressively
learns to bypass unnecessary reasoning steps as training advances. These
findings shed light on the path toward human-like reasoning patterns in RL
approaches. Our code is available at https://github.com/kokolerk/TON.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>camera ready revision</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ditch the Denoiser: Emergence of Noise Robustness in <span class="highlight-title">Self-Supervised</span>
  Learning from Data Curriculum <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12191v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12191v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenquan Lu, Jiaqi Zhang, Hugues Van Assel, Randall Balestriero
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-Supervised Learning (SSL) has become a powerful solution to extract rich
representations from unlabeled data. Yet, SSL research is mostly focused on
clean, curated and high-quality datasets. As a result, applying SSL on noisy
data remains a challenge, despite being crucial to applications such as
astrophysics, medical imaging, geophysics or finance. In this work, we present
a fully self-supervised framework that enables noise-robust representation
learning without requiring a denoiser at inference or downstream fine-tuning.
Our method first trains an SSL denoiser on noisy data, then uses it to
construct a denoised-to-noisy data curriculum (i.e., training first on
denoised, then noisy samples) for pretraining a SSL backbone (e.g., DINOv2),
combined with a teacher-guided regularization that anchors noisy embeddings to
their denoised counterparts. This process encourages the model to internalize
noise robustness. Notably, the denoiser can be discarded after pretraining,
simplifying deployment. On ImageNet-1k with ViT-B under extreme Gaussian noise
($\sigma=255$, SNR = 0.72 dB), our method improves linear probing accuracy by
4.8% over DINOv2, demonstrating that denoiser-free robustness can emerge from
noise-aware pretraining. The code is available at
https://github.com/wenquanlu/noisy_dinov2.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic Traceback Learning for Medical Report Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.13267v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.13267v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuchang Ye, Mingyuan Meng, Mingjian Li, Dagan Feng, Usman Naseem, Jinman Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated medical report generation has demonstrated the potential to
significantly reduce the workload associated with time-consuming medical
reporting. Recent generative representation learning methods have shown promise
in integrating vision and language modalities for medical report generation.
However, when trained end-to-end and applied directly to medical image-to-text
generation, they face two significant challenges: i) difficulty in accurately
capturing subtle yet crucial pathological details, and ii) reliance on both
visual and textual inputs during inference, leading to performance degradation
in zero-shot inference when only images are available. To address these
challenges, this study proposes a novel multimodal dynamic traceback learning
framework (DTrace). Specifically, we introduce a traceback mechanism to
supervise the semantic validity of generated content and a dynamic learning
strategy to adapt to various proportions of image and text input, enabling text
generation without strong reliance on the input from both modalities during
inference. The learning of cross-modal knowledge is enhanced by supervising the
model to recover masked semantic information from a complementary counterpart.
Extensive experiments conducted on two benchmark datasets, IU-Xray and
MIMIC-CXR, demonstrate that the proposed DTrace framework outperforms
state-of-the-art methods for medical report generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE Transactions on Multimedia (TMM)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning World Models for Interactive Video Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.21996v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.21996v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taiye Chen, Xun Hu, Zihan Ding, Chi Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundational world models must be both interactive and preserve
spatiotemporal coherence for effective future planning with action choices.
However, present models for long video generation have limited inherent world
modeling capabilities due to two main challenges: compounding errors and
insufficient memory mechanisms. We enhance image-to-video models with
interactive capabilities through additional action conditioning and
autoregressive framework, and reveal that compounding error is inherently
irreducible in autoregressive video generation, while insufficient memory
mechanism leads to incoherence of world models. We propose video retrieval
augmented generation (VRAG) with explicit global state conditioning, which
significantly reduces long-term compounding errors and increases spatiotemporal
consistency of world models. In contrast, naive autoregressive generation with
extended context windows and retrieval-augmented generation prove less
effective for video generation, primarily due to the limited in-context
learning capabilities of current video models. Our work illuminates the
fundamental challenges in video world models and establishes a comprehensive
benchmark for improving video generation models with internal world modeling
capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://sites.google.com/view/vrag</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OnlyFlow: Optical Flow based Motion Conditioning for Video Diffusion
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.10501v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.10501v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mathis Koroglu, Hugo Caselles-Dupré, Guillaume Jeanneret Sanmiguel, Matthieu Cord
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of text-to-video generation tasks with precise
control for various applications such as camera movement control and
video-to-video editing. Most methods tacking this problem rely on providing
user-defined controls, such as binary masks or camera movement embeddings. In
our approach we propose OnlyFlow, an approach leveraging the optical flow
firstly extracted from an input video to condition the motion of generated
videos. Using a text prompt and an input video, OnlyFlow allows the user to
generate videos that respect the motion of the input video as well as the text
prompt. This is implemented through an optical flow estimation model applied on
the input video, which is then fed to a trainable optical flow encoder. The
output feature maps are then injected into the text-to-video backbone model. We
perform quantitative, qualitative and user preference studies to show that
OnlyFlow positively compares to state-of-the-art methods on a wide range of
tasks, even though OnlyFlow was not specifically trained for such tasks.
OnlyFlow thus constitutes a versatile, lightweight yet efficient method for
controlling motion in text-to-video generation. Models and code will be made
available on GitHub and HuggingFace.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 1 supplementary page, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quality-Aware Prototype Memory for Face Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.07734v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.07734v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Evgeny Smirnov, Vasiliy Galyuk, Evgeny Lukyanets
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prototype Memory is a powerful model for face representation learning. It
enables training face recognition models on datasets of any size by generating
prototypes (classifier weights) on the fly and efficiently utilizing them.
Prototype Memory demonstrated strong results in many face recognition
benchmarks. However, the algorithm of prototype generation, used in it, is
prone to the problems of imperfectly calculated prototypes in case of
low-quality or poorly recognizable faces in the images, selected for the
prototype creation. All images of the same person presented in the mini-batch
are used with equal weights, and the resulting averaged prototype can be
contaminated by imperfect embeddings of low-quality face images. This may lead
to misleading training signals and degrade the performance of the trained
models. In this paper, we propose a simple and effective way to improve
Prototype Memory with quality-aware prototype generation. Quality-Aware
Prototype Memory uses different weights for images of different quality in the
process of prototype generation. With this improvement, prototypes receive more
informative signals from high-quality images and are less affected by
low-quality ones. We propose and compare several methods of quality estimation
and usage, perform extensive experiments on the different face recognition
benchmarks and demonstrate the advantages of the proposed model compared to the
basic version of Prototype Memory.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GenIR: Generative Visual Feedback for Mental Image Retrieval <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.06220v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.06220v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Diji Yang, Minghao Liu, Chung-Hsiang Lo, Yi Zhang, James Davis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models (VLMs) have shown strong performance on text-to-image
retrieval benchmarks. However, bridging this success to real-world applications
remains a challenge. In practice, human search behavior is rarely a one-shot
action. Instead, it is often a multi-round process guided by clues in mind.
That is, a mental image ranging from vague recollections to vivid mental
representations of the target image. Motivated by this gap, we study the task
of Mental Image Retrieval (MIR), which targets the realistic yet underexplored
setting where users refine their search for a mentally envisioned image through
multi-round interactions with an image search engine. Central to successful
interactive retrieval is the capability of machines to provide users with
clear, actionable feedback; however, existing methods rely on indirect or
abstract verbal feedback, which can be ambiguous, misleading, or ineffective
for users to refine the query. To overcome this, we propose GenIR, a generative
multi-round retrieval paradigm leveraging diffusion-based image generation to
explicitly reify the AI system's understanding at each round. These synthetic
visual representations provide clear, interpretable feedback, enabling users to
refine their queries intuitively and effectively. We further introduce a fully
automated pipeline to generate a high-quality multi-round MIR dataset.
Experimental results demonstrate that GenIR significantly outperforms existing
interactive methods in the MIR scenario. This work establishes a new task with
a dataset and an effective generative retrieval method, providing a foundation
for future research in this direction
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MoralCLIP: Contrastive Alignment of Vision-and-Language Representations
  with Moral Foundations Theory <span class="chip">ACM MM '25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05696v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05696v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ana Carolina Condez, Diogo Tavares, João Magalhães
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in vision-language models have enabled rich semantic
understanding across modalities. However, these encoding methods lack the
ability to interpret or reason about the moral dimensions of content-a crucial
aspect of human cognition. In this paper, we address this gap by introducing
MoralCLIP, a novel embedding representation method that extends multimodal
learning with explicit moral grounding based on Moral Foundations Theory (MFT).
Our approach integrates visual and textual moral cues into a unified embedding
space, enabling cross-modal moral alignment. MoralCLIP is grounded on the
multi-label dataset Social-Moral Image Database to identify co-occurring moral
foundations in visual content. For MoralCLIP training, we design a moral data
augmentation strategy to scale our annotated dataset to 15,000 image-text pairs
labeled with MFT-aligned dimensions. Our results demonstrate that explicit
moral supervision improves both unimodal and multimodal understanding of moral
content, establishing a foundation for morally-aware AI systems capable of
recognizing and aligning with human moral values.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updated version: corresponds to the ACM MM '25 published paper and
  includes full appendix material</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CAUSAL3D: A Comprehensive Benchmark for Causal Learning from Visual Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04852v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04852v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Disheng Liu, Yiran Qiao, Wuche Liu, Yiren Lu, Yunlai Zhou, Tuo Liang, Yu Yin, Jing Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  True intelligence hinges on the ability to uncover and leverage hidden causal
relations. Despite significant progress in AI and computer vision (CV), there
remains a lack of benchmarks for assessing models' abilities to infer latent
causality from complex visual data. In this paper, we introduce
\textsc{\textbf{Causal3D}}, a novel and comprehensive benchmark that integrates
structured data (tables) with corresponding visual representations (images) to
evaluate causal reasoning. Designed within a systematic framework, Causal3D
comprises 19 3D-scene datasets capturing diverse causal relations, views, and
backgrounds, enabling evaluations across scenes of varying complexity. We
assess multiple state-of-the-art methods, including classical causal discovery,
causal representation learning, and large/vision-language models (LLMs/VLMs).
Our experiments show that as causal structures grow more complex without prior
knowledge, performance declines significantly, highlighting the challenges even
advanced methods face in complex causal scenarios. Causal3D serves as a vital
resource for advancing causal reasoning in CV and fostering trustworthy AI in
critical domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Datasets link:
  https://huggingface.co/datasets/LLDDSS/Causal3D_Dataset</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LODGE: Level-of-Detail Large-Scale Gaussian Splatting with Efficient
  Rendering <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.23158v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.23158v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonas Kulhanek, Marie-Julie Rakotosaona, Fabian Manhardt, Christina Tsalicoglou, Michael Niemeyer, Torsten Sattler, Songyou Peng, Federico Tombari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we present a novel level-of-detail (LOD) method for 3D Gaussian
Splatting that enables real-time rendering of large-scale scenes on
memory-constrained devices. Our approach introduces a hierarchical LOD
representation that iteratively selects optimal subsets of Gaussians based on
camera distance, thus largely reducing both rendering time and GPU memory
usage. We construct each LOD level by applying a depth-aware 3D smoothing
filter, followed by importance-based pruning and fine-tuning to maintain visual
fidelity. To further reduce memory overhead, we partition the scene into
spatial chunks and dynamically load only relevant Gaussians during rendering,
employing an opacity-blending mechanism to avoid visual artifacts at chunk
boundaries. Our method achieves state-of-the-art performance on both outdoor
(Hierarchical 3DGS) and indoor (Zip-NeRF) datasets, delivering high-quality
renderings with reduced latency and memory requirements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025; Web: https://lodge-gs.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NerfBaselines: Consistent and Reproducible Evaluation of Novel View
  Synthesis Methods <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17345v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17345v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonas Kulhanek, Torsten Sattler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Novel view synthesis is an important problem with many applications,
including AR/VR, gaming, and robotic simulations. With the recent rapid
development of Neural Radiance Fields (NeRFs) and 3D Gaussian Splatting (3DGS)
methods, it is becoming difficult to keep track of the current state of the art
(SoTA) due to methods using different evaluation protocols, codebases being
difficult to install and use, and methods not generalizing well to novel 3D
scenes. In our experiments, we show that even tiny differences in the
evaluation protocols of various methods can artificially boost the performance
of these methods. This raises questions about the validity of quantitative
comparisons performed in the literature. To address these questions, we propose
NerfBaselines, an evaluation framework which provides consistent benchmarking
tools, ensures reproducibility, and simplifies the installation and use of
various methods. We validate our implementation experimentally by reproducing
the numbers reported in the original papers. For improved accessibility, we
release a web platform that compares commonly used methods on standard
benchmarks. We strongly believe NerfBaselines is a valuable contribution to the
community as it ensures that quantitative results are comparable and thus truly
measure progress in the field of novel view synthesis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025 D&B; Web: https://jkulhanek.com/nerfbaselines</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VC4VG: Optimizing Video Captions for Text-to-Video Generation <span class="chip">EMNLP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24134v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24134v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Du, Zhuoran Lin, Kaiqiang Song, Biao Wang, Zhicheng Zheng, Tiezheng Ge, Bo Zheng, Qin Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in text-to-video (T2V) generation highlight the critical role
of high-quality video-text pairs in training models capable of producing
coherent and instruction-aligned videos. However, strategies for optimizing
video captions specifically for T2V training remain underexplored. In this
paper, we introduce VC4VG (Video Captioning for Video Generation), a
comprehensive caption optimization framework tailored to the needs of T2V
models. We begin by analyzing caption content from a T2V perspective,
decomposing the essential elements required for video reconstruction into
multiple dimensions, and proposing a principled caption design methodology. To
support evaluation, we construct VC4VG-Bench, a new benchmark featuring
fine-grained, multi-dimensional, and necessity-graded metrics aligned with
T2V-specific requirements. Extensive T2V fine-tuning experiments demonstrate a
strong correlation between improved caption quality and video generation
performance, validating the effectiveness of our approach. We release all
benchmark tools and code at https://github.com/alimama-creative/VC4VG to
support further research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ KARMA: Efficient Structural Defect Segmentation via Kolmogorov-Arnold
  Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.08186v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.08186v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Meftahul Ferdaus, Mahdi Abdelguerfi, Elias Ioup, Steven Sloan, Kendall N. Niles, Ken Pathak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic segmentation of structural defects in civil infrastructure remains
challenging due to variable defect appearances, harsh imaging conditions, and
significant class imbalance. Current deep learning methods, despite their
effectiveness, typically require millions of parameters, rendering them
impractical for real-time inspection systems. We introduce KARMA
(Kolmogorov-Arnold Representation Mapping Architecture), a highly efficient
semantic segmentation framework that models complex defect patterns through
compositions of one-dimensional functions rather than conventional
convolutions. KARMA features three technical innovations: (1) a
parameter-efficient Tiny Kolmogorov-Arnold Network (TiKAN) module leveraging
low-rank factorization for KAN-based feature transformation; (2) an optimized
feature pyramid structure with separable convolutions for multi-scale defect
analysis; and (3) a static-dynamic prototype mechanism that enhances feature
representation for imbalanced classes. Extensive experiments on benchmark
infrastructure inspection datasets demonstrate that KARMA achieves competitive
or superior mean IoU performance compared to state-of-the-art approaches, while
using significantly fewer parameters (0.959M vs. 31.04M, a 97% reduction).
Operating at 0.264 GFLOPS, KARMA maintains inference speeds suitable for
real-time deployment, enabling practical automated infrastructure inspection
systems without compromising accuracy. The source code can be accessed at the
following URL: https://github.com/faeyelab/karma.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">19</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Retrieval-Augmented Search for Large-Scale Map Collections with ColPali 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25718v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25718v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jamie Mahowald, Benjamin Charles Germain Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal approaches have shown great promise for searching and navigating
digital collections held by libraries, archives, and museums. In this paper, we
introduce map-RAS: a retrieval-augmented search system for historic maps. In
addition to introducing our framework, we detail our publicly-hosted demo for
searching 101,233 map images held by the Library of Congress. With our system,
users can multimodally query the map collection via ColPali, summarize search
results using Llama 3.2, and upload their own collections to perform
inter-collection search. We articulate potential use cases for archivists,
curators, and end-users, as well as future work with our system in both machine
learning and the digital humanities. Our demo can be viewed at:
http://www.mapras.com.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MMQ-v2: Align, Denoise, and Amplify: Adaptive Behavior Mining for
  Semantic IDs Learning in Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25622v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25622v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Xu, Moyu Zhang, Chaofan Fan, Jinxin Hu, Xiaochen Li, Yu Zhang, Xiaoyi Zeng, Jing Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Industrial recommender systems rely on unique Item Identifiers (ItemIDs).
However, this method struggles with scalability and generalization in large,
dynamic datasets that have sparse long-tail data.Content-based Semantic IDs
(SIDs) address this by sharing knowledge through content quantization. However,
by ignoring dynamic behavioral properties, purely content-based SIDs have
limited expressive power. Existing methods attempt to incorporate behavioral
information but overlook a critical distinction: unlike relatively uniform
content features, user-item interactions are highly skewed and diverse,
creating a vast information gap in quality and quantity between popular and
long-tail items. This oversight leads to two critical limitations: (1) Noise
Corruption: Indiscriminate behavior-content alignment allows collaborative
noise from long-tail items to corrupt their content representations, leading to
the loss of critical multimodal information. (2)Signal Obscurity: The
equal-weighting scheme for SIDs fails to reflect the varying importance of
different behavioral signals, making it difficult for downstream tasks to
distinguish important SIDs from uninformative ones. To tackle these issues, we
propose a mixture-of-quantization framework, MMQ-v2, to adaptively Align,
Denoise, and Amplify multimodal information from content and behavior
modalities for semantic IDs learning. The semantic IDs generated by this
framework named ADA-SID. It introduces two innovations: an adaptive
behavior-content alignment that is aware of information richness to shield
representations from noise, and a dynamic behavioral router to amplify critical
signals by applying different weights to SIDs. Extensive experiments on public
and large-scale industrial datasets demonstrate ADA-SID's significant
superiority in both generative and discriminative recommendation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FARSIQA: Faithful and Advanced RAG System for Islamic Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25621v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25621v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Aghajani Asl, Behrooz Minaei Bidgoli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of Large Language Models (LLMs) has revolutionized Natural
Language Processing, yet their application in high-stakes, specialized domains
like religious question answering is hindered by challenges like hallucination
and unfaithfulness to authoritative sources. This issue is particularly
critical for the Persian-speaking Muslim community, where accuracy and
trustworthiness are paramount. Existing Retrieval-Augmented Generation (RAG)
systems, relying on simplistic single-pass pipelines, fall short on complex,
multi-hop queries requiring multi-step reasoning and evidence aggregation. To
address this gap, we introduce FARSIQA, a novel, end-to-end system for Faithful
Advanced Question Answering in the Persian Islamic domain. FARSIQA is built
upon our innovative FAIR-RAG architecture: a Faithful, Adaptive, Iterative
Refinement framework for RAG. FAIR-RAG employs a dynamic, self-correcting
process: it adaptively decomposes complex queries, assesses evidence
sufficiency, and enters an iterative loop to generate sub-queries,
progressively filling information gaps. Operating on a curated knowledge base
of over one million authoritative Islamic documents, FARSIQA demonstrates
superior performance. Rigorous evaluation on the challenging IslamicPCQA
benchmark shows state-of-the-art performance: the system achieves a remarkable
97.0% in Negative Rejection - a 40-point improvement over baselines - and a
high Answer Correctness score of 74.3%. Our work establishes a new standard for
Persian Islamic QA and validates that our iterative, adaptive architecture is
crucial for building faithful, reliable AI systems in sensitive domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages, 5 figures, 10 tables. Keywords: Retrieval-Augmented
  Generation (RAG), Question Answering (QA), Islamic Knowledge Base, Faithful
  AI, Persian NLP, Multi-hop Reasoning, Large Language Models (LLMs)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalized Pseudo-Relevance Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25488v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25488v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiteng Tu, Weihang Su, Yujia Zhou, Yiqun Liu, Fen Lin, Qin Liu, Qingyao Ai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Query rewriting is a fundamental technique in information retrieval (IR). It
typically employs the retrieval result as relevance feedback to refine the
query and thereby addresses the vocabulary mismatch between user queries and
relevant documents. Traditional pseudo-relevance feedback (PRF) and its
vector-based extension (VPRF) improve retrieval performance by leveraging
top-retrieved documents as relevance feedback. However, they are constructed
based on two major hypotheses: the relevance assumption (top documents are
relevant) and the model assumption (rewriting methods need to be designed
specifically for particular model architectures). While recent large language
models (LLMs)-based generative relevance feedback (GRF) enables model-free
query reformulation, it either suffers from severe LLM hallucination or, again,
relies on the relevance assumption to guarantee the effectiveness of rewriting
quality. To overcome these limitations, we introduce an assumption-relaxed
framework: \textit{Generalized Pseudo Relevance Feedback} (GPRF), which
performs model-free, natural language rewriting based on retrieved documents,
not only eliminating the model assumption but also reducing dependence on the
relevance assumption. Specifically, we design a utility-oriented training
pipeline with reinforcement learning to ensure robustness against noisy
feedback. Extensive experiments across multiple benchmarks and retrievers
demonstrate that GPRF consistently outperforms strong baselines, establishing
it as an effective and generalizable framework for query rewriting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Alibaba International E-commerce Product Search Competition DcuRAGONs
  Team Technical Report <span class="chip">CIKM
  2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25428v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25428v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thang-Long Nguyen-Ho, Minh-Khoi Pham, Hoang-Bao Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This report details our methodology and results developed for the
Multilingual E-commerce Search Competition. The problem aims to recognize
relevance between user queries versus product items in a multilingual context
and improve recommendation performance on e-commerce platforms. Utilizing Large
Language Models (LLMs) and their capabilities in other tasks, our data-centric
method achieved the highest score compared to other solutions during the
competition. Final leaderboard is publised at
https://alibaba-international-cikm2025.github.io. The source code for our
project is published at https://github.com/nhtlongcs/e-commerce-product-search.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Alibaba International E-commerce Product Search Competition @ CIKM
  2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Automated Quality Assurance of Patent Specifications: A
  Multi-Dimensional LLM Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25402v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25402v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuqian Chai, Chaochao Wang, Weilei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the surge in patent applications and emergence of AI drafting tools,
systematic evaluation of patent content quality has received limited research
attention. To address this gap, We propose to evaluate patents using regulatory
compliance, technical coherence, and figure-reference consistency detection
modules, and then generate improvement suggestions via an integration module.
The framework is validated on a comprehensive dataset comprising 80
human-authored and 80 AI-generated patents from two patent drafting tools.
Experimental results show balanced accuracies of 99.74\%, 82.12\%, and 91.2\%
respectively across the three detection modules when validated against expert
annotations. Additional analysis was conducted to examine defect distributions
across patent sections, technical domains, and authoring sources. Section-based
analysis indicates that figure-text consistency and technical detail precision
require particular attention. Mechanical Engineering and Construction show more
claim-specification inconsistencies due to complex technical documentation
requirements. AI-generated patents show a significant gap compared to
human-authored ones. While human-authored patents primarily contain
surface-level errors like typos, AI-generated patents exhibit more structural
defects in figure-text alignment and cross-references.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting scalable sequential recommendation with Multi-Embedding
  Approach and Mixture-of-Experts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25285v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25285v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiushi Pan, Hao Wang, Guoyuan An, Luankang Zhang, Wei Guo, Yong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recommendation systems, how to effectively scale up recommendation models
has been an essential research topic. While significant progress has been made
in developing advanced and scalable architectures for sequential
recommendation(SR) models, there are still challenges due to items'
multi-faceted characteristics and dynamic item relevance in the user context.
To address these issues, we propose Fuxi-MME, a framework that integrates a
multi-embedding strategy with a Mixture-of-Experts (MoE) architecture.
Specifically, to efficiently capture diverse item characteristics in a
decoupled manner, we decompose the conventional single embedding matrix into
several lower-dimensional embedding matrices. Additionally, by substituting
relevant parameters in the Fuxi Block with an MoE layer, our model achieves
adaptive and specialized transformation of the enriched representations.
Empirical results on public datasets show that our proposed framework
outperforms several competitive baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Measuring the Research Output and Performance of the University of
  Ibadan from 2014 to 2023: A Scientometric Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25283v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25283v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muneer Ahmad, Undie Felicia Nkatv
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study employs scientometric methods to assess the research output and
performance of the University of Ibadan from 2014 to 2023. By analyzing
publication trends, citation patterns, and collaboration networks, the research
aims to comprehensively evaluate the university's research productivity,
impact, and disciplinary focus. This article's endeavors are characterized by
innovation, interdisciplinary collaboration, and commitment to excellence,
making the University of Ibadan a significant hub for cutting-edge research in
Nigeria and beyond. The goal of the current study is to ascertain the influence
of the university's research output and publication patterns between 2014 and
2023. The study focuses on the departments at the University of Ibadan that
contribute the most, the best journals for publishing, the nations that
collaborate, the impact of citations both locally and globally, well-known
authors and their total production, and the research output broken down by
year. According to the university's ten-year publication data, 7159 papers with
an h-index of 75 were published between 2014 and 2023, garnering 218572
citations. Furthermore, the VOSviewer software mapping approach is used to
illustrate the stenographical mapping of data through graphs. The findings of
this study will contribute to understanding the university's research
strengths, weaknesses, and potential areas for improvement. Additionally, the
results will inform evidence-based decision-making for enhancing research
strategies and policies at the University of Ibadan.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 5 figures, Research Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TV-Rec: Time-Variant Convolutional Filter for Sequential Recommendation <span class="chip">NeurIPS
  2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25259v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25259v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yehjin Shin, Jeongwhan Choi, Seojin Kim, Noseong Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, convolutional filters have been increasingly adopted in sequential
recommendation for their ability to capture local sequential patterns. However,
most of these models complement convolutional filters with self-attention. This
is because convolutional filters alone, generally fixed filters, struggle to
capture global interactions necessary for accurate recommendation. We propose
Time-Variant Convolutional Filters for Sequential Recommendation (TV-Rec), a
model inspired by graph signal processing, where time-variant graph filters
capture position-dependent temporal variations in user sequences. By replacing
both fixed kernels and self-attention with time-variant filters, TV-Rec
achieves higher expressive power and better captures complex interaction
patterns in user behavior. This design not only eliminates the need for
self-attention but also reduces computation while accelerating inference.
Extensive experiments on six public benchmarks show that TV-Rec outperforms
state-of-the-art baselines by an average of 7.49%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The 39th Conference on Neural Information Processing Systems (NeurIPS
  2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GReF: A Unified Generative Framework for Efficient Reranking via Ordered
  Multi-token Prediction <span class="chip">CIKM 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25220v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25220v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhijie Lin, Zhuofeng Li, Chenglei Dai, Wentian Bao, Shuai Lin, Enyun Yu, Haoxiang Zhang, Liang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In a multi-stage recommendation system, reranking plays a crucial role in
modeling intra-list correlations among items. A key challenge lies in exploring
optimal sequences within the combinatorial space of permutations. Recent
research follows a two-stage (generator-evaluator) paradigm, where a generator
produces multiple feasible sequences, and an evaluator selects the best one. In
practice, the generator is typically implemented as an autoregressive model.
However, these two-stage methods face two main challenges. First, the
separation of the generator and evaluator hinders end-to-end training. Second,
autoregressive generators suffer from inference efficiency. In this work, we
propose a Unified Generative Efficient Reranking Framework (GReF) to address
the two primary challenges. Specifically, we introduce Gen-Reranker, an
autoregressive generator featuring a bidirectional encoder and a dynamic
autoregressive decoder to generate causal reranking sequences. Subsequently, we
pre-train Gen-Reranker on the item exposure order for high-quality parameter
initialization. To eliminate the need for the evaluator while integrating
sequence-level evaluation during training for end-to-end optimization, we
propose post-training the model through Rerank-DPO. Moreover, for efficient
autoregressive inference, we introduce ordered multi-token prediction (OMTP),
which trains Gen-Reranker to simultaneously generate multiple future items
while preserving their order, ensuring practical deployment in real-time
recommender systems. Extensive offline experiments demonstrate that GReF
outperforms state-of-the-art reranking methods while achieving latency that is
nearly comparable to non-autoregressive models. Additionally, GReF has also
been deployed in a real-world video app Kuaishou with over 300 million daily
active users, significantly improving online recommendation quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CIKM 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Model-Document Protocol for AI Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25160v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25160v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongjin Qian, Zheng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI search depends on linking large language models (LLMs) with vast external
knowledge sources. Yet web pages, PDF files, and other raw documents are not
inherently LLM-ready: they are long, noisy, and unstructured. Conventional
retrieval methods treat these documents as verbatim text and return raw
passages, leaving the burden of fragment assembly and contextual reasoning to
the LLM. This gap underscores the need for a new retrieval paradigm that
redefines how models interact with documents.
  We introduce the Model-Document Protocol (MDP), a general framework that
formalizes how raw text is bridged to LLMs through consumable knowledge
representations. Rather than treating retrieval as passage fetching, MDP
defines multiple pathways that transform unstructured documents into
task-specific, LLM-ready inputs. These include agentic reasoning, which curates
raw evidence into coherent context; memory grounding, which accumulates
reusable notes to enrich reasoning; and structured leveraging, which encodes
documents into formal representations such as graphs or key-value caches. All
three pathways share the same goal: ensuring that what reaches the LLM is not
raw fragments but compact, structured knowledge directly consumable for
reasoning.
  As an instantiation, we present MDP-Agent, which realizes the protocol
through an agentic process: constructing document-level gist memories for
global coverage, performing diffusion-based exploration with vertical
exploitation to uncover layered dependencies, and applying map-reduce style
synthesis to integrate large-scale evidence into compact yet sufficient
context. Experiments on information-seeking benchmarks demonstrate that
MDP-Agent outperforms baselines, validating both the soundness of the MDP
framework and the effectiveness of its agentic instantiation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continual Low-Rank Adapters for LLM-based Generative Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25093v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25093v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyunsik Yoo, Ting-Wei Li, SeongKu Kang, Zhining Liu, Charlie Xu, Qilin Qi, Hanghang Tong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language models (LLMs) achieve strong performance in
recommendation, they face challenges in continual learning as users, items, and
user preferences evolve over time. Existing LoRA-based continual methods
primarily focus on preserving performance on previous tasks, but this overlooks
the unique nature of recommendation: the goal is not to predict past
preferences, and outdated preferences can even harm performance when current
interests shift significantly. To address this, we propose PESO (Proximally
rEgularized Single evolving lOra, a continual adaptation method for LoRA in
recommendation. PESO introduces a proximal regularizer that anchors the current
adapter to its most recent frozen state, enabling the model to flexibly balance
adaptation and preservation, and to better capture recent user behaviors.
Theoretically, we show that this proximal design provides data-aware,
direction-wise guidance in the LoRA subspace. Empirically, PESO consistently
outperforms existing LoRA-based continual learning methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Quest for Reliable Metrics of Responsible AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.26007v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.26007v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Theresia Veronika Rampisela, Maria Maistro, Tuukka Ruotsalo, Christina Lioma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of Artificial Intelligence (AI), including AI in Science
(AIS), should be done following the principles of responsible AI. Progress in
responsible AI is often quantified through evaluation metrics, yet there has
been less work on assessing the robustness and reliability of the metrics
themselves. We reflect on prior work that examines the robustness of fairness
metrics for recommender systems as a type of AI application and summarise their
key takeaways into a set of non-exhaustive guidelines for developing reliable
metrics of responsible AI. Our guidelines apply to a broad spectrum of AI
applications, including AIS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for presentation at the AI in Science Summit 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HyMiRec: A Hybrid Multi-interest Learning Framework for LLM-based
  Sequential Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.13738v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.13738v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyi Zhou, Cheng Chen, Kai Zuo, Manjie Xu, Zhendong Fu, Yibo Chen, Xu Tang, Yao Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have recently demonstrated strong potential for
sequential recommendation. However, current LLM-based approaches face critical
limitations in modeling users' long-term and diverse interests. First, due to
inference latency and feature fetching bandwidth constraints, existing methods
typically truncate user behavior sequences to include only the most recent
interactions, resulting in the loss of valuable long-range preference signals.
Second, most current methods rely on next-item prediction with a single
predicted embedding, overlooking the multifaceted nature of user interests and
limiting recommendation diversity. To address these challenges, we propose
HyMiRec, a hybrid multi-interest sequential recommendation framework, which
leverages a lightweight recommender to extracts coarse interest embeddings from
long user sequences and an LLM-based recommender to captures refined interest
embeddings. To alleviate the overhead of fetching features, we introduce a
residual codebook based on cosine similarity, enabling efficient compression
and reuse of user history embeddings. To model the diverse preferences of
users, we design a disentangled multi-interest learning module, which leverages
multiple interest queries to learn disentangles multiple interest signals
adaptively, allowing the model to capture different facets of user intent.
Extensive experiments are conducted on both benchmark datasets and a collected
industrial dataset, demonstrating our effectiveness over existing
state-of-the-art methods. Furthermore, online A/B testing shows that HyMiRec
brings consistent improvements in real-world recommendation systems. Code is
available at https://github.com/FireRedTeam/FireRedSeqRec.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Who You Are Matters: Bridging Topics and Social Roles via LLM-Enhanced
  Logical Recommendation <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10940v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10940v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qing Yu, Xiaobei Wang, Shuchang Liu, Yandong Bai, Xiaoyu Yang, Xueliang Wang, Chang Meng, Shanshan Wu, Hailan Yang, Huihui Xiao, Xiang Li, Fan Yang, Xiaoqiang Feng, Lantao Hu, Han Li, Kun Gai, Lixin Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems filter contents/items valuable to users by inferring
preferences from user features and historical behaviors. Mainstream approaches
follow the learning-to-rank paradigm, which focus on discovering and modeling
item topics (e.g., categories), and capturing user preferences on these topics
based on historical interactions. However, this paradigm often neglects the
modeling of user characteristics and their social roles, which are logical
confounders influencing the correlated interest and user preference transition.
To bridge this gap, we introduce the user role identification task and the
behavioral logic modeling task that aim to explicitly model user roles and
learn the logical relations between item topics and user social roles. We show
that it is possible to explicitly solve these tasks through an efficient
integration framework of Large Language Model (LLM) and recommendation systems,
for which we propose TagCF. On the one hand, TagCF exploits the (Multi-modal)
LLM's world knowledge and logic inference ability to extract realistic
tag-based virtual logic graphs that reveal dynamic and expressive knowledge of
users, refining our understanding of user behaviors. On the other hand, TagCF
presents empirically effective integration modules that take advantage of the
extracted tag-logic information, augmenting the recommendation performance. We
conduct both online experiments and offline experiments with industrial and
public datasets as verification of TagCF's effectiveness, and we empirically
show that the user role modeling strategy is potentially a better choice than
the modeling of item topics. Additionally, we provide evidence that the
extracted logic graphs are empirically a general and transferable knowledge
that can benefit a wide range of recommendation tasks. Our code is available in
https://github.com/Code2Q/TagCF.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to be published in NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can LLMs Outshine Conventional Recommenders? A Comparative Evaluation <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.05493v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.05493v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qijiong Liu, Jieming Zhu, Lu Fan, Kun Wang, Hengchang Hu, Wei Guo, Yong Liu, Xiao-Ming Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, integrating large language models (LLMs) into recommender
systems has created new opportunities for improving recommendation quality.
However, a comprehensive benchmark is needed to thoroughly evaluate and compare
the recommendation capabilities of LLMs with traditional recommender systems.
In this paper, we introduce RecBench, which systematically investigates various
item representation forms (including unique identifier, text, semantic
embedding, and semantic identifier) and evaluates two primary recommendation
tasks, i.e., click-through rate prediction (CTR) and sequential recommendation
(SeqRec). Our extensive experiments cover up to 17 large models and are
conducted across five diverse datasets from fashion, news, video, books, and
music domains. Our findings indicate that LLM-based recommenders outperform
conventional recommenders, achieving up to a 5% AUC improvement in the CTR
scenario and up to a 170% NDCG@10 improvement in the SeqRec scenario. However,
these substantial performance gains come at the expense of significantly
reduced inference efficiency, rendering the LLM-as-RS paradigm impractical for
real-time recommendation environments. We aim for our findings to inspire
future research, including recommendation-specific model acceleration methods.
We will release our code, data, configurations, and platform to enable other
researchers to reproduce and build upon our experimental results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025 DB Track Accepted Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Capturing User Interests from Data Streams for Continual Sequential
  Recommendation <span class="chip">WSDM'26</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.07466v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.07466v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gyuseok Lee, Hyunsik Yoo, Junyoung Hwang, SeongKu Kang, Hwanjo Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based sequential recommendation (SR) models excel at modeling
long-range dependencies in user behavior via self-attention. However, updating
them with continuously arriving behavior sequences incurs high computational
costs or leads to catastrophic forgetting. Although continual learning, a
standard approach for non-stationary data streams, has recently been applied to
recommendation, existing methods gradually forget long-term user preferences
and remain underexplored in SR. In this paper, we introduce Continual
Sequential Transformer for Recommendation (CSTRec). CSTRec is designed to
effectively adapt to current interests by leveraging well-preserved historical
ones, thus capturing the trajectory of user interests over time. The core of
CSTRec is Continual Sequential Attention (CSA), a linear attention tailored for
continual SR, which enables CSTRec to partially retain historical knowledge
without direct access to prior data. CSA has two key components: (1)
Cauchy-Schwarz Normalization that stabilizes learning over time under uneven
user interaction frequencies; (2) Collaborative Interest Enrichment that
alleviates forgetting through shared, learnable interest pools. In addition, we
introduce a new technique to facilitate the adaptation of new users by
transferring historical knowledge from existing users with similar interests.
Extensive experiments on three real-world datasets show that CSTRec outperforms
state-of-the-art models in both knowledge retention and acquisition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WSDM'26</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can We Hide Machines in the Crowd? Quantifying Equivalence in
  LLM-in-the-loop Annotation Tasks <span class="chip">SIGIR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.06658v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.06658v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaman He, Zikang Leng, Dana McKay, Damiano Spina, Johanne R. Trippas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many evaluations of large language models (LLMs) in text annotation focus
primarily on the correctness of the output, typically comparing model-generated
labels to human-annotated ``ground truth'' using standard performance metrics.
In contrast, our study moves beyond effectiveness alone. We aim to explore how
labeling decisions -- by both humans and LLMs -- can be statistically evaluated
across individuals. Rather than treating LLMs purely as annotation systems, we
approach LLMs as an alternative annotation mechanism that may be capable of
mimicking the subjective judgments made by humans. To assess this, we develop a
statistical evaluation method based on Krippendorff's $\alpha$, paired
bootstrapping, and the Two One-Sided t-Tests (TOST) equivalence test procedure.
This evaluation method tests whether an LLM can blend into a group of human
annotators without being distinguishable.
  We apply this approach to two datasets -- MovieLens 100K and PolitiFact --
and find that the LLM is statistically indistinguishable from a human annotator
in the former ($p = 0.004$), but not in the latter ($p = 0.155$), highlighting
task-dependent differences. It also enables early evaluation on a small sample
of human data to inform whether LLMs are suitable for large-scale annotation in
a given application.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at SIGIR-AP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Models for Few-Shot Named Entity Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1810.06818v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1810.06818v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yufei Zhao, Xiaoshi Zhong, Erik Cambria, Jagath C. Rajapakse
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Named entity recognition (NER) is a fundamental task in numerous downstream
applications. Recently, researchers have employed pre-trained language models
(PLMs) and large language models (LLMs) to address this task. However, fully
leveraging the capabilities of PLMs and LLMs with minimal human effort remains
challenging. In this paper, we propose GPT4NER, a method that prompts LLMs to
resolve the few-shot NER task. GPT4NER constructs effective prompts using three
key components: entity definition, few-shot examples, and chain-of-thought. By
prompting LLMs with these effective prompts, GPT4NER transforms few-shot NER,
which is traditionally considered as a sequence-labeling problem, into a
sequence-generation problem. We conduct experiments on two benchmark datasets,
CoNLL2003 and OntoNotes5.0, and compare the performance of GPT4NER to
representative state-of-the-art models in both few-shot and fully supervised
settings. Experimental results demonstrate that GPT4NER achieves the $F_1$ of
83.15\% on CoNLL2003 and 70.37\% on OntoNotes5.0, significantly outperforming
few-shot baselines by an average margin of 7 points. Compared to
fully-supervised baselines, GPT4NER achieves 87.9\% of their best performance
on CoNLL2003 and 76.4\% of their best performance on OntoNotes5.0. We also
utilize a relaxed-match metric for evaluation and report performance in the
sub-task of named entity extraction (NEE), and experiments demonstrate their
usefulness to help better understand model behaviors in the NER task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 2 figures. Accepted by AI, Computer Science and Robotics
  Technology (ACRT)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">100</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ E-Scores for (In)Correctness Assessment of Generative Model Outputs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25770v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25770v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guneet S. Dhillon, Javier González, Teodora Pandeva, Alicia Curth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While generative models, especially large language models (LLMs), are
ubiquitous in today's world, principled mechanisms to assess their
(in)correctness are limited. Using the conformal prediction framework, previous
works construct sets of LLM responses where the probability of including an
incorrect response, or error, is capped at a desired user-defined tolerance
level. However, since these methods are based on p-values, they are susceptible
to p-hacking, i.e., choosing the tolerance level post-hoc can invalidate the
guarantees. We therefore leverage e-values to complement generative model
outputs with e-scores as a measure of incorrectness. In addition to achieving
the same statistical guarantees as before, e-scores provide users flexibility
in adaptively choosing tolerance levels after observing the e-scores
themselves, by upper bounding a post-hoc notion of error called size
distortion. We experimentally demonstrate their efficacy in assessing LLM
outputs for different correctness types: mathematical factuality and property
constraints satisfaction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Stochastic Flows: Solver-Free Modelling and Inference for SDE
  Solutions <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25769v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25769v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naoki Kiyohara, Edward Johns, Yingzhen Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stochastic differential equations (SDEs) are well suited to modelling noisy
and irregularly sampled time series found in finance, physics, and machine
learning. Traditional approaches require costly numerical solvers to sample
between arbitrary time points. We introduce Neural Stochastic Flows (NSFs) and
their latent variants, which directly learn (latent) SDE transition laws using
conditional normalising flows with architectural constraints that preserve
properties inherited from stochastic flows. This enables one-shot sampling
between arbitrary states and yields up to two orders of magnitude speed-ups at
large time gaps. Experiments on synthetic SDE simulations and on real-world
tracking and video data show that NSFs maintain distributional accuracy
comparable to numerical approaches while dramatically reducing computation for
arbitrary time-point sampling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025 (poster). Project page:
  https://nkiyohara.github.io/nsf-neurips2025/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synthetic Data Reveals Generalization Gaps in Correlated Multiple
  Instance Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25759v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25759v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ethan Harvey, Dennis Johan Loevlie, Michael C. Hughes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multiple instance learning (MIL) is often used in medical imaging to classify
high-resolution 2D images by processing patches or classify 3D volumes by
processing slices. However, conventional MIL approaches treat instances
separately, ignoring contextual relationships such as the appearance of nearby
patches or slices that can be essential in real applications. We design a
synthetic classification task where accounting for adjacent instance features
is crucial for accurate prediction. We demonstrate the limitations of
off-the-shelf MIL approaches by quantifying their performance compared to the
optimal Bayes estimator for this task, which is available in closed-form. We
empirically show that newer correlated MIL methods still struggle to generalize
as well as possible when trained from scratch on tens of thousands of
instances.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MLPrE -- A tool for preprocessing and exploratory data analysis prior to
  machine learning model construction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25755v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25755v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David S Maxwell, Michael Darkoh, Sidharth R Samudrala, Caroline Chung, Stephanie T Schmidt, Bissan Al-Lazikani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the recent growth of Deep Learning for AI, there is a need for tools to
meet the demand of data flowing into those models. In some cases, source data
may exist in multiple formats, and therefore the source data must be
investigated and properly engineered for a Machine Learning model or graph
database. Overhead and lack of scalability with existing workflows limit
integration within a larger processing pipeline such as Apache Airflow, driving
the need for a robust, extensible, and lightweight tool to preprocess arbitrary
datasets that scales with data type and size. To address this, we present
Machine Learning Preprocessing and Exploratory Data Analysis, MLPrE, in which
SparkDataFrames were utilized to hold data during processing and ensure
scalability. A generalizable JSON input file format was utilized to describe
stepwise changes to that DataFrame. Stages were implemented for input and
output, filtering, basic statistics, feature engineering, and exploratory data
analysis. A total of 69 stages were implemented into MLPrE, of which we
highlight and demonstrate key stages using six diverse datasets. We further
highlight MLPrE's ability to independently process multiple fields in flat
files and recombine them, otherwise requiring an additional pipeline, using a
UniProt glossary term dataset. Building on this advantage, we demonstrated the
clustering stage with available wine quality data. Lastly, we demonstrate the
preparation of data for a graph database in the final stages of MLPrE using
phosphosite kinase data. Overall, our MLPrE tool offers a generalizable and
scalable tool for preprocessing and early data analysis, filling a critical
need for such a tool given the ever expanding use of machine learning. This
tool serves to accelerate and simplify early stage development in larger
workflows.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Data Mixing Shapes In-Context Learning: Asymptotic Equivalence for
  <span class="highlight-title">Transformer</span>s with MLPs <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25753v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25753v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samet Demir, Zafer Dogan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretrained Transformers demonstrate remarkable in-context learning (ICL)
capabilities, enabling them to adapt to new tasks from demonstrations without
parameter updates. However, theoretical studies often rely on simplified
architectures (e.g., omitting MLPs), data models (e.g., linear regression with
isotropic inputs), and single-source training, limiting their relevance to
realistic settings. In this work, we study ICL in pretrained Transformers with
nonlinear MLP heads on nonlinear tasks drawn from multiple data sources with
heterogeneous input, task, and noise distributions. We analyze a model where
the MLP comprises two layers, with the first layer trained via a single
gradient step and the second layer fully optimized. Under high-dimensional
asymptotics, we prove that such models are equivalent in ICL error to
structured polynomial predictors, leveraging results from the theory of
Gaussian universality and orthogonal polynomials. This equivalence reveals that
nonlinear MLPs meaningfully enhance ICL performance, particularly on nonlinear
tasks, compared to linear baselines. It also enables a precise analysis of data
mixing effects: we identify key properties of high-quality data sources (low
noise, structured covariances) and show that feature learning emerges only when
the task covariance exhibits sufficient structure. These results are validated
empirically across various activation functions, model sizes, and data
distributions. Finally, we experiment with a real-world scenario involving
multilingual sentiment analysis where each language is treated as a different
source. Our experimental results for this case exemplify how our findings
extend to real-world cases. Overall, our work advances the theoretical
foundations of ICL in Transformers and provides actionable insight into the
role of architecture and data in ICL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025, 24 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Meshless solutions of PDE inverse problems on irregular geometries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25752v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25752v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        James V. Roggeveen, Michael P. Brenner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Solving inverse and optimization problems over solutions of nonlinear partial
differential equations (PDEs) on complex spatial domains is a long-standing
challenge. Here we introduce a method that parameterizes the solution using
spectral bases on arbitrary spatiotemporal domains, whereby the basis is
defined on a hyperrectangle containing the true domain. We find the
coefficients of the basis expansion by solving an optimization problem whereby
both the equations, the boundary conditions and any optimization targets are
enforced by a loss function, building on a key idea from Physics-Informed
Neural Networks (PINNs). Since the representation of the function natively has
exponential convergence, so does the solution of the optimization problem, as
long as it can be solved efficiently. We find empirically that the optimization
protocols developed for machine learning find solutions with exponential
convergence on a wide range of equations. The method naturally allows for the
incorporation of data assimilation by including additional terms in the loss
function, and for the efficient solution of optimization problems over the PDE
solutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hawk: Leveraging Spatial Context for Faster Autoregressive Text-to-Image
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25739v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25739v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhi-Kai Chen, Jun-Peng Jiang, Han-Jia Ye, De-Chuan Zhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autoregressive (AR) image generation models are capable of producing
high-fidelity images but often suffer from slow inference due to their
inherently sequential, token-by-token decoding process. Speculative decoding,
which employs a lightweight draft model to approximate the output of a larger
AR model, has shown promise in accelerating text generation without
compromising quality. However, its application to image generation remains
largely underexplored. The challenges stem from a significantly larger sampling
space, which complicates the alignment between the draft and target model
outputs, coupled with the inadequate use of the two-dimensional spatial
structure inherent in images, thereby limiting the modeling of local
dependencies. To overcome these challenges, we introduce Hawk, a new approach
that harnesses the spatial structure of images to guide the speculative model
toward more accurate and efficient predictions. Experimental results on
multiple text-to-image benchmarks demonstrate a 1.71x speedup over standard AR
models, while preserving both image fidelity and diversity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LieSolver: A PDE-constrained solver for IBVPs using Lie symmetries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25731v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25731v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        René P. Klausen, Ivan Timofeev, Johannes Frank, Jonas Naujoks, Thomas Wiegand, Sebastian Lapuschkin, Wojciech Samek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a method for efficiently solving initial-boundary value problems
(IBVPs) that uses Lie symmetries to enforce the associated partial differential
equation (PDE) exactly by construction. By leveraging symmetry transformations,
the model inherently incorporates the physical laws and learns solutions from
initial and boundary data. As a result, the loss directly measures the model's
accuracy, leading to improved convergence. Moreover, for well-posed IBVPs, our
method enables rigorous error estimation. The approach yields compact models,
facilitating an efficient optimization. We implement LieSolver and demonstrate
its application to linear homogeneous PDEs with a range of initial conditions,
showing that it is faster and more accurate than physics-informed neural
networks (PINNs). Overall, our method improves both computational efficiency
and the reliability of predictions for PDE-constrained problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physics-Guided Conditional Diffusion Networks for Microwave Image
  Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25729v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25729v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shirin Chehelgami, Joe LoVetri, Vahab Khoshdel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A conditional latent-diffusion based framework for solving the
electromagnetic inverse scattering problem associated with microwave imaging is
introduced. This generative machine-learning model explicitly mirrors the
non-uniqueness of the ill-posed inverse problem. Unlike existing inverse
solvers utilizing deterministic machine learning techniques that produce a
single reconstruction, the proposed latent-diffusion model generates multiple
plausible permittivity maps conditioned on measured scattered-field data,
thereby generating several potential instances in the range-space of the
non-unique inverse mapping. A forward electromagnetic solver is integrated into
the reconstruction pipeline as a physics-based evaluation mechanism. The space
of candidate reconstructions form a distribution of possibilities consistent
with the conditioning data and the member of this space yielding the lowest
scattered-field data discrepancy between the predicted and measured scattered
fields is reported as the final solution. Synthetic and experimental labeled
datasets are used for training and evaluation of the model. An innovative
labeled synthetic dataset is created that exemplifies a varied set of
scattering features. Training of the model using this new dataset produces high
quality permittivity reconstructions achieving improved generalization with
excellent fidelity to shape recognition. The results highlight the potential of
hybrid generative physics frameworks as a promising direction for robust,
data-driven microwave imaging.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling flow-based approaches for topology sampling in $\mathrm{SU}(3)$
  gauge theory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25704v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25704v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Claudio Bonanno, Andrea Bulgarelli, Elia Cellini, Alessandro Nada, Dario Panfalone, Davide Vadacchino, Lorenzo Verzichelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop a methodology based on out-of-equilibrium simulations to mitigate
topological freezing when approaching the continuum limit of lattice gauge
theories. We reduce the autocorrelation of the topological charge employing
open boundary conditions, while removing exactly their unphysical effects using
a non-equilibrium Monte Carlo approach in which periodic boundary conditions
are gradually switched on. We perform a detailed analysis of the computational
costs of this strategy in the case of the four-dimensional $\mathrm{SU}(3)$
Yang-Mills theory. After achieving full control of the scaling, we outline a
clear strategy to sample topology efficiently in the continuum limit, which we
check at lattice spacings as small as $0.045$ fm. We also generalize this
approach by designing a customized Stochastic Normalizing Flow for evolutions
in the boundary conditions, obtaining superior performances with respect to the
purely stochastic non-equilibrium approach, and paving the way for more
efficient future flow-based solutions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>1+39 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Convolutional Spiking-based GRU Cell for Spatio-temporal Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25696v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25696v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yesmine Abdennadher, Eleonora Cicciarella, Michele Rossi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spike-based temporal messaging enables SNNs to efficiently process both
purely temporal and spatio-temporal time-series or event-driven data. Combining
SNNs with Gated Recurrent Units (GRUs), a variant of recurrent neural networks,
gives rise to a robust framework for sequential data processing; however,
traditional RNNs often lose local details when handling long sequences.
Previous approaches, such as SpikGRU, fail to capture fine-grained local
dependencies in event-based spatio-temporal data. In this paper, we introduce
the Convolutional Spiking GRU (CS-GRU) cell, which leverages convolutional
operations to preserve local structure and dependencies while integrating the
temporal precision of spiking neurons with the efficient gating mechanisms of
GRUs. This versatile architecture excels on both temporal datasets (NTIDIGITS,
SHD) and spatio-temporal benchmarks (MNIST, DVSGesture, CIFAR10DVS). Our
experiments show that CS-GRU outperforms state-of-the-art GRU variants by an
average of 4.35%, achieving over 90% accuracy on sequential tasks and up to
99.31% on MNIST. It is worth noting that our solution achieves 69% higher
efficiency compared to SpikGRU. The code is available at:
https://github.com/YesmineAbdennadher/CS-GRU.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 1 figure. Published in 2025 IEEE International Workshop On
  Machine Learning for Signal Processing, Aug. 31-Sep. 3, 2025, Istanbul,
  Turkey</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PyDPF: A Python Package for Differentiable Particle Filtering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25693v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25693v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John-Joseph Brady, Benjamin Cox, Víctor Elvira, Yunpeng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-space models (SSMs) are a widely used tool in time series analysis. In
the complex systems that arise from real-world data, it is common to employ
particle filtering (PF), an efficient Monte Carlo method for estimating the
hidden state corresponding to a sequence of observations. Applying particle
filtering requires specifying both the parametric form and the parameters of
the system, which are often unknown and must be estimated. Gradient-based
optimisation techniques cannot be applied directly to standard particle
filters, as the filters themselves are not differentiable. However, several
recently proposed methods modify the resampling step to make particle filtering
differentiable. In this paper, we present an implementation of several such
differentiable particle filters (DPFs) with a unified API built on the popular
PyTorch framework. Our implementation makes these algorithms easily accessible
to a broader research community and facilitates straightforward comparison
between them. We validate our framework by reproducing experiments from several
existing studies and demonstrate how DPFs can be applied to address several
common challenges with state space modelling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>42 pages, 0 figures, under review at the Journal of Statistical
  Software, the python package can be found at https://pypi.org/project/pydpf/
  , the full documentation at
  https://python-dpf.readthedocs.io/en/latest/#documentation-index , and the
  source code including experiments and replication material at
  https://github.com/John-JoB/pydpf</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Configuration-First Framework for Reproducible, Low-Code Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25692v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25692v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Strnad, Blaž Bertalanič, Carolina Fortuna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning is increasingly permeating radio-based localization
services. To keep results credible and comparable, everyday workflows should
make rigorous experiment specification and exact repeatability the default,
without blocking advanced experimentation. However, in practice, researchers
face a three-way gap that could be filled by a framework that offers (i) low
coding effort for end-to-end studies, (ii) reproducibility by default including
versioned code, data, and configurations, controlled randomness, isolated runs,
and recorded artifacts, and (iii) built-in extensibility so new models,
metrics, and stages can be added with minimal integration effort. Existing
tools rarely deliver all three for machine learning in general and localization
workflows in particular. In this paper we introduce LOCALIZE, a low-code,
configuration-first framework for radio localization in which experiments are
declared in human-readable configuration, a workflow orchestrator runs
standardized pipelines from data preparation to reporting, and all artifacts,
such as datasets, models, metrics, and reports, are versioned. The
preconfigured, versioned datasets reduce initial setup and boilerplate,
speeding up model development and evaluation. The design, with clear extension
points, allows experts to add components without reworking the infrastructure.
In a qualitative comparison and a head-to-head study against a plain Jupyter
notebook baseline, we show that the framework reduces authoring effort while
maintaining comparable runtime and memory behavior. Furthermore, using a
Bluetooth Low Energy dataset, we show that scaling across training data (1x to
10x) keeps orchestration overheads bounded as data grows. Overall, the
framework makes reproducible machine-learning-based localization
experimentation practical, accessible, and extensible.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 7 figures. Preprint submitted to ACM Transactions on
  Software Engineering and Methodology (TOSEM), 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Model Inversion Attacks Meet Cryptographic Fuzzy Extractors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25687v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25687v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mallika Prabhakar, Louise Xu, Prateek Saxena
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model inversion attacks pose an open challenge to privacy-sensitive
applications that use machine learning (ML) models. For example, face
authentication systems use modern ML models to compute embedding vectors from
face images of the enrolled users and store them. If leaked, inversion attacks
can accurately reconstruct user faces from the leaked vectors. There is no
systematic characterization of properties needed in an ideal defense against
model inversion, even for the canonical example application of a face
authentication system susceptible to data breaches, despite a decade of
best-effort solutions.
  In this paper, we formalize the desired properties of a provably strong
defense against model inversion and connect it, for the first time, to the
cryptographic concept of fuzzy extractors. We further show that existing fuzzy
extractors are insecure for use in ML-based face authentication. We do so
through a new model inversion attack called PIPE, which achieves a success rate
of over 89% in most cases against prior schemes. We then propose L2FE-Hash, the
first candidate fuzzy extractor which supports standard Euclidean distance
comparators as needed in many ML-based applications, including face
authentication. We formally characterize its computational security guarantees,
even in the extreme threat model of full breach of stored secrets, and
empirically show its usable accuracy in face authentication for practical face
distributions. It offers attack-agnostic security without requiring any
re-training of the ML model it protects. Empirically, it nullifies both prior
state-of-the-art inversion attacks as well as our new PIPE attack.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph Network-based Structural Simulator: Graph Neural Networks for
  Structural Dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25683v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25683v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessandro Lucchetti, Francesco Cadini, Marco Giglio, Luca Lomazzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have recently been explored as surrogate models
for numerical simulations. While their applications in computational fluid
dynamics have been investigated, little attention has been given to structural
problems, especially for dynamic cases. To address this gap, we introduce the
Graph Network-based Structural Simulator (GNSS), a GNN framework for surrogate
modeling of dynamic structural problems.
  GNSS follows the encode-process-decode paradigm typical of GNN-based machine
learning models, and its design makes it particularly suited for dynamic
simulations thanks to three key features: (i) expressing node kinematics in
node-fixed local frames, which avoids catastrophic cancellation in
finite-difference velocities; (ii) employing a sign-aware regression loss,
which reduces phase errors in long rollouts; and (iii) using a
wavelength-informed connectivity radius, which optimizes graph construction.
  We evaluate GNSS on a case study involving a beam excited by a 50kHz
Hanning-modulated pulse. The results show that GNSS accurately reproduces the
physics of the problem over hundreds of timesteps and generalizes to unseen
loading conditions, where existing GNNs fail to converge or deliver meaningful
predictions.
  Compared with explicit finite element baselines, GNSS achieves substantial
inference speedups while preserving spatial and temporal fidelity. These
findings demonstrate that locality-preserving GNNs with physics-consistent
update rules are a competitive alternative for dynamic, wave-dominated
structural simulations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mechanistic Interpretability of RNNs emulating Hidden Markov Models <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25674v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25674v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elia Torre, Michele Viscione, Lucas Pompe, Benjamin F Grewe, Valerio Mante
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recurrent neural networks (RNNs) provide a powerful approach in neuroscience
to infer latent dynamics in neural populations and to generate hypotheses about
the neural computations underlying behavior. However, past work has focused on
relatively simple, input-driven, and largely deterministic behaviors - little
is known about the mechanisms that would allow RNNs to generate the richer,
spontaneous, and potentially stochastic behaviors observed in natural settings.
Modeling with Hidden Markov Models (HMMs) has revealed a segmentation of
natural behaviors into discrete latent states with stochastic transitions
between them, a type of dynamics that may appear at odds with the continuous
state spaces implemented by RNNs. Here we first show that RNNs can replicate
HMM emission statistics and then reverse-engineer the trained networks to
uncover the mechanisms they implement. In the absence of inputs, the activity
of trained RNNs collapses towards a single fixed point. When driven by
stochastic input, trajectories instead exhibit noise-sustained dynamics along
closed orbits. Rotation along these orbits modulates the emission probabilities
and is governed by transitions between regions of slow, noise-driven dynamics
connected by fast, deterministic transitions. The trained RNNs develop highly
structured connectivity, with a small set of "kick neurons" initiating
transitions between these regions. This mechanism emerges during training as
the network shifts into a regime of stochastic resonance, enabling it to
perform probabilistic computations. Analyses across multiple HMM architectures
- fully connected, cyclic, and linear-chain - reveal that this solution
generalizes through the modular reuse of the same dynamical motif, suggesting a
compositional principle by which RNNs can emulate complex discrete latent
dynamics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spectral Perturbation Bounds for Low-Rank Approximation with
  Applications to Privacy <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25670v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25670v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Phuc Tran, Nisheeth K. Vishnoi, Van H. Vu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A central challenge in machine learning is to understand how noise or
measurement errors affect low-rank approximations, particularly in the spectral
norm. This question is especially important in differentially private low-rank
approximation, where one aims to preserve the top-$p$ structure of a
data-derived matrix while ensuring privacy. Prior work often analyzes Frobenius
norm error or changes in reconstruction quality, but these metrics can over- or
under-estimate true subspace distortion. The spectral norm, by contrast,
captures worst-case directional error and provides the strongest utility
guarantees. We establish new high-probability spectral-norm perturbation bounds
for symmetric matrices that refine the classical Eckart--Young--Mirsky theorem
and explicitly capture interactions between a matrix $A \in \mathbb{R}^{n
\times n}$ and an arbitrary symmetric perturbation $E$. Under mild eigengap and
norm conditions, our bounds yield sharp estimates for $\|(A + E)_p - A_p\|$,
where $A_p$ is the best rank-$p$ approximation of $A$, with improvements of up
to a factor of $\sqrt{n}$. As an application, we derive improved utility
guarantees for differentially private PCA, resolving an open problem in the
literature. Our analysis relies on a novel contour bootstrapping method from
complex analysis and extends it to a broad class of spectral functionals,
including polynomials and matrix exponentials. Empirical results on real-world
datasets confirm that our bounds closely track the actual spectral error under
diverse perturbation regimes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Subgraph Federated Learning via Spectral Methods <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25657v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25657v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Javad Aliakbari, Johan Östman, Ashkan Panahi, Alexandre Graell i Amat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of federated learning (FL) with graph-structured data
distributed across multiple clients. In particular, we address the prevalent
scenario of interconnected subgraphs, where interconnections between clients
significantly influence the learning process. Existing approaches suffer from
critical limitations, either requiring the exchange of sensitive node
embeddings, thereby posing privacy risks, or relying on
computationally-intensive steps, which hinders scalability. To tackle these
challenges, we propose FedLap, a novel framework that leverages global
structure information via Laplacian smoothing in the spectral domain to
effectively capture inter-node dependencies while ensuring privacy and
scalability. We provide a formal analysis of the privacy of FedLap,
demonstrating that it preserves privacy. Notably, FedLap is the first subgraph
FL scheme with strong privacy guarantees. Extensive experiments on benchmark
datasets demonstrate that FedLap achieves competitive or superior utility
compared to existing techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be presented at The Annual Conference on Neural Information
  Processing Systems (NeurIPS) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continuous subsurface property retrieval from sparse radar observations
  using physics informed neural networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25648v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25648v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ishfaq Aziz, Mohamad Alipour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating subsurface dielectric properties is essential for applications
ranging from environmental surveys of soils to nondestructive evaluation of
concrete in infrastructure. Conventional wave inversion methods typically
assume few discrete homogeneous layers and require dense measurements or strong
prior knowledge of material boundaries, limiting scalability and accuracy in
realistic settings where properties vary continuously. We present a physics
informed machine learning framework that reconstructs subsurface permittivity
as a fully neural, continuous function of depth, trained to satisfy both
measurement data and Maxwells equations. We validate the framework with both
simulations and custom built radar experiments on multilayered natural
materials. Results show close agreement with in-situ permittivity measurements
(R^2=0.93), with sensitivity to even subtle variations (Delta eps_r=2).
Parametric analysis reveals that accurate profiles can be recovered with as few
as three strategically placed sensors in two layer systems. This approach
reframes subsurface inversion from boundary-driven to continuous property
estimation, enabling accurate characterization of smooth permittivity
variations and advancing electromagnetic imaging using low cost radar systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 9 main text figures + 2 supplementary figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are Language Models Efficient Reasoners? A Perspective from Logic
  Programming <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25626v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25626v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Opedal, Yanick Zengaffinen, Haruki Shirakami, Clemente Pasti, Mrinmaya Sachan, Abulhair Saparov, Ryan Cotterell, Bernhard Schölkopf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern language models (LMs) exhibit strong deductive reasoning capabilities,
yet standard evaluations emphasize correctness while overlooking a key aspect
of human-like reasoning: efficiency. In real-world reasoning scenarios, much of
the available information is irrelevant, and effective deductive inference
requires identifying and ignoring such distractions. We propose a framework for
assessing LM reasoning efficiency through the lens of logic programming,
introducing a simple method to align proofs written in natural language -- as
generated by an LM -- with shortest proofs found by executing the logic
program. Efficiency is quantified by measuring how well a model avoids
unnecessary inference. Empirically, we construct a dataset of math word
problems injected with various number of irrelevant axioms that vary in
semantic overlap with the goal theorem. We find that current LMs show marked
accuracy declines under such conditions -- even with minimal, domain-consistent
distractions -- and the proofs they generate frequently exhibit detours through
irrelevant inferences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Don't Blind Your VLA: Aligning Visual Representations for OOD
  Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25616v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25616v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikita Kachaev, Mikhail Kolosov, Daniil Zelezetsky, Alexey K. Kovalev, Aleksandr I. Panov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing success of Vision-Language-Action (VLA) models stems from the
promise that pretrained Vision-Language Models (VLMs) can endow agents with
transferable world knowledge and vision-language (VL) grounding, laying a
foundation for action models with broader generalization. Yet when these VLMs
are adapted to the action modality, it remains unclear to what extent their
original VL representations and knowledge are preserved. In this work, we
conduct a systematic study of representation retention during VLA fine-tuning,
showing that naive action fine-tuning leads to degradation of visual
representations. To characterize and measure these effects, we probe VLA's
hidden representations and analyze attention maps, further, we design a set of
targeted tasks and methods that contrast VLA models with their counterpart
VLMs, isolating changes in VL capabilities induced by action fine-tuning. We
further evaluate a range of strategies for aligning visual representations and
introduce a simple yet effective method that mitigates degradation and yields
improved generalization to out-of-distribution (OOD) scenarios. Taken together,
our analysis clarifies the trade-off between action fine-tuning and the
degradation of VL representations and highlights practical approaches to
recover inherited VL capabilities. Code is publicly available:
https://blind-vla-paper.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BOLT-GAN: Bayes-Optimal Loss for Stable GAN Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25609v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25609v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammadreza Tavasoli Naeini, Ali Bereyhi, Morteza Noshad, Ben Liang, Alfred O. Hero III
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce BOLT-GAN, a simple yet effective modification of the WGAN
framework inspired by the Bayes Optimal Learning Threshold (BOLT). We show that
with a Lipschitz continuous discriminator, BOLT-GAN implicitly minimizes a
different metric distance than the Earth Mover (Wasserstein) distance and
achieves better training stability. Empirical evaluations on four standard
image generation benchmarks (CIFAR-10, CelebA-64, LSUN Bedroom-64, and LSUN
Church-64) show that BOLT-GAN consistently outperforms WGAN, achieving 10-60%
lower Frechet Inception Distance (FID). Our results suggest that BOLT is a
broadly applicable principle for enhancing GAN training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization
  Formats 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25602v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25602v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengzhao Chen, Meng Wu, Hui Jin, Zhihang Yuan, Jing Liu, Chaoyi Zhang, Yunshui Li, Jie Huang, Jin Ma, Zeyue Xue, Zhiheng Liu, Xingyan Bin, Ping Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern AI hardware, such as Nvidia's Blackwell architecture, is increasingly
embracing low-precision floating-point (FP) formats to handle the pervasive
activation outliers in Large Language Models (LLMs). Despite this industry
trend, a unified comparison of FP and integer (INT) quantization across varying
granularities has been missing, leaving algorithm and hardware co-design
without clear guidance. This paper fills that gap by systematically
investigating the trade-offs between FP and INT formats. We reveal a critical
performance crossover: while FP excels in coarse-grained quantization, the
comparison at fine-grained (block-wise) levels is more nuanced. Our
comprehensive comparison demonstrates that for popular 8-bit fine-grained
formats (e.g., MX with block size 32), MXINT8 is superior to its FP counterpart
in both algorithmic accuracy and hardware efficiency. However, for 4-bit
formats, FP (e.g., MXFP4, NVFP4) often holds an accuracy advantage , though we
show that NVINT4 can surpass NVFP4 when outlier-mitigation techniques like
Hadamard rotation are applied. We also introduce a symmetric clipping method
that resolves gradient bias in fine-grained low-bit INT training, enabling
nearly lossless performance for MXINT8 training. These findings challenge the
current hardware trajectory, demonstrating that a one-size-fits-all FP approach
is suboptimal and advocating that fine-grained INT formats, particularly
MXINT8, offer a better balance of accuracy, power, and efficiency for future AI
accelerators.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty Quantification for Regression: A Unified Framework based on
  kernel scores 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25599v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25599v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher Bülte, Yusuf Sale, Gitta Kutyniok, Eyke Hüllermeier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Regression tasks, notably in safety-critical domains, require proper
uncertainty quantification, yet the literature remains largely
classification-focused. In this light, we introduce a family of measures for
total, aleatoric, and epistemic uncertainty based on proper scoring rules, with
a particular emphasis on kernel scores. The framework unifies several
well-known measures and provides a principled recipe for designing new ones
whose behavior, such as tail sensitivity, robustness, and out-of-distribution
responsiveness, is governed by the choice of kernel. We prove explicit
correspondences between kernel-score characteristics and downstream behavior,
yielding concrete design guidelines for task-specific measures. Extensive
experiments demonstrate that these measures are effective in downstream tasks
and reveal clear trade-offs among instantiations, including robustness and
out-of-distribution detection performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Feedback Alignment Meets Low-Rank Manifolds: A Structured Recipe for
  Local Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25594v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25594v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arani Roy, Marco P. Apolinario, Shristi Das Biswas, Kaushik Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training deep neural networks (DNNs) with backpropagation (BP) achieves
state-of-the-art accuracy but requires global error propagation and full
parameterization, leading to substantial memory and computational overhead.
Direct Feedback Alignment (DFA) enables local, parallelizable updates with
lower memory requirements but is limited by unstructured feedback and poor
scalability in deeper architectures, specially convolutional neural networks.
To address these limitations, we propose a structured local learning framework
that operates directly on low-rank manifolds defined by the Singular Value
Decomposition (SVD) of weight matrices. Each layer is trained in its decomposed
form, with updates applied to the SVD components using a composite loss that
integrates cross-entropy, subspace alignment, and orthogonality regularization.
Feedback matrices are constructed to match the SVD structure, ensuring
consistent alignment between forward and feedback pathways. Our method reduces
the number of trainable parameters relative to the original DFA model, without
relying on pruning or post hoc compression. Experiments on CIFAR-10, CIFAR-100,
and ImageNet show that our method achieves accuracy comparable to that of BP.
Ablation studies confirm the importance of each loss term in the low-rank
setting. These results establish local learning on low-rank manifolds as a
principled and scalable alternative to full-rank gradient-based training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalized Sobolev IPM for Graph-Based Measures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25591v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25591v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tam Le, Truyen Nguyen, Hideitsu Hino, Kenji Fukumizu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the Sobolev IPM problem for measures supported on a graph metric
space, where critic function is constrained to lie within the unit ball defined
by Sobolev norm. While Le et al. (2025) achieved scalable computation by
relating Sobolev norm to weighted $L^p$-norm, the resulting framework remains
intrinsically bound to $L^p$ geometric structure, limiting its ability to
incorporate alternative structural priors beyond the $L^p$ geometry paradigm.
To overcome this limitation, we propose to generalize Sobolev IPM through the
lens of \emph{Orlicz geometric structure}, which employs convex functions to
capture nuanced geometric relationships, building upon recent advances in
optimal transport theory -- particularly Orlicz-Wasserstein (OW) and
generalized Sobolev transport -- that have proven instrumental in advancing
machine learning methodologies. This generalization encompasses classical
Sobolev IPM as a special case while accommodating diverse geometric priors
beyond traditional $L^p$ structure. It however brings up significant
computational hurdles that compound those already inherent in Sobolev IPM. To
address these challenges, we establish a novel theoretical connection between
Orlicz-Sobolev norm and Musielak norm which facilitates a novel regularization
for the generalized Sobolev IPM (GSI). By further exploiting the underlying
graph structure, we show that GSI with Musielak regularization (GSI-M) reduces
to a simple \emph{univariate optimization} problem, achieving remarkably
computational efficiency. Empirically, GSI-M is several-order faster than the
popular OW in computation, and demonstrates its practical advantages in
comparing probability measures on a given graph for document classification and
several tasks in topological data analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning-Augmented Online Bidding in Stochastic Settings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25582v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25582v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Spyros Angelopoulos, Bertrand Simon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online bidding is a classic optimization problem, with several applications
in online decision-making, the design of interruptible systems, and the
analysis of approximation algorithms. In this work, we study online bidding
under learning-augmented settings that incorporate stochasticity, in either the
prediction oracle or the algorithm itself. In the first part, we study bidding
under distributional predictions, and find Pareto-optimal algorithms that offer
the best-possible tradeoff between the consistency and the robustness of the
algorithm. In the second part, we study the power and limitations of randomized
bidding algorithms, by presenting upper and lower bounds on the
consistency/robustness tradeoffs. Previous works focused predominantly on
oracles that do not leverage stochastic information on the quality of the
prediction, and deterministic algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Monitoring the calibration of probability forecasts with an application
  to concept drift detection involving image classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25573v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25573v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher T. Franck, Anne R. Driscoll, Zoe Szajnfarber, William H. Woodall
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning approaches for image classification have led to impressive
advances in that field. For example, convolutional neural networks are able to
achieve remarkable image classification accuracy across a wide range of
applications in industry, defense, and other areas. While these machine
learning models boast impressive accuracy, a related concern is how to assess
and maintain calibration in the predictions these models make. A classification
model is said to be well calibrated if its predicted probabilities correspond
with the rates events actually occur. While there are many available methods to
assess machine learning calibration and recalibrate faulty predictions, less
effort has been spent on developing approaches that continually monitor
predictive models for potential loss of calibration as time passes. We propose
a cumulative sum-based approach with dynamic limits that enable detection of
miscalibration in both traditional process monitoring and concept drift
applications. This enables early detection of operational context changes that
impact image classification performance in the field. The proposed chart can be
used broadly in any situation where the user needs to monitor probability
predictions over time for potential lapses in calibration. Importantly, our
method operates on probability predictions and event outcomes and does not
require under-the-hood access to the machine learning model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Perturbation Bounds for Low-Rank Inverse Approximations under Noise <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25571v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25571v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Phuc Tran, Nisheeth K. Vishnoi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-rank pseudoinverses are widely used to approximate matrix inverses in
scalable machine learning, optimization, and scientific computing. However,
real-world matrices are often observed with noise, arising from sampling,
sketching, and quantization. The spectral-norm robustness of low-rank inverse
approximations remains poorly understood. We systematically study the
spectral-norm error $\| (\tilde{A}^{-1})_p - A_p^{-1} \|$ for an $n\times n$
symmetric matrix $A$, where $A_p^{-1}$ denotes the best rank-\(p\)
approximation of $A^{-1}$, and $\tilde{A} = A + E$ is a noisy observation.
Under mild assumptions on the noise, we derive sharp non-asymptotic
perturbation bounds that reveal how the error scales with the eigengap,
spectral decay, and noise alignment with low-curvature directions of $A$. Our
analysis introduces a novel application of contour integral techniques to the
\emph{non-entire} function $f(z) = 1/z$, yielding bounds that improve over
naive adaptations of classical full-inverse bounds by up to a factor of
$\sqrt{n}$. Empirically, our bounds closely track the true perturbation error
across a variety of real-world and synthetic matrices, while estimates based on
classical results tend to significantly overpredict. These findings offer
practical, spectrum-aware guarantees for low-rank inverse approximations in
noisy computational environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Framework for Bounding Deterministic Risk with PAC-Bayes: Applications
  to Majority Votes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25569v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25569v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Leblanc, Pascal Germain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  PAC-Bayes is a popular and efficient framework for obtaining generalization
guarantees in situations involving uncountable hypothesis spaces.
Unfortunately, in its classical formulation, it only provides guarantees on the
expected risk of a randomly sampled hypothesis. This requires stochastic
predictions at test time, making PAC-Bayes unusable in many practical
situations where a single deterministic hypothesis must be deployed. We propose
a unified framework to extract guarantees holding for a single hypothesis from
stochastic PAC-Bayesian guarantees. We present a general oracle bound and
derive from it a numerical bound and a specialization to majority vote. We
empirically show that our approach consistently outperforms popular baselines
(by up to a factor of 2) when it comes to generalization bounds on
deterministic classifiers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PitchFlower: A flow-based neural audio codec with pitch controllability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25566v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25566v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Diego Torres, Axel Roebel, Nicolas Obin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present PitchFlower, a flow-based neural audio codec with explicit pitch
controllability. Our approach enforces disentanglement through a simple
perturbation: during training, F0 contours are flattened and randomly shifted,
while the true F0 is provided as conditioning. A vector-quantization bottleneck
prevents pitch recovery, and a flow-based decoder generates high quality audio.
Experiments show that PitchFlower achieves more accurate pitch control than
WORLD at much higher audio quality, and outperforms SiFiGAN in controllability
while maintaining comparable quality. Beyond pitch, this framework provides a
simple and extensible path toward disentangling other speech attributes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging an Atmospheric Foundational Model for Subregional Sea Surface
  Temperature Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25563v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25563v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Víctor Medina, Giovanny A. Cuervo-Londoño, Javier Sánchez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The accurate prediction of oceanographic variables is crucial for
understanding climate change, managing marine resources, and optimizing
maritime activities. Traditional ocean forecasting relies on numerical models;
however, these approaches face limitations in terms of computational cost and
scalability. In this study, we adapt Aurora, a foundational deep learning model
originally designed for atmospheric forecasting, to predict sea surface
temperature (SST) in the Canary Upwelling System. By fine-tuning this model
with high-resolution oceanographic reanalysis data, we demonstrate its ability
to capture complex spatiotemporal patterns while reducing computational
demands. Our methodology involves a staged fine-tuning process, incorporating
latitude-weighted error metrics and optimizing hyperparameters for efficient
learning. The experimental results show that the model achieves a low RMSE of
0.119K, maintaining high anomaly correlation coefficients (ACC $\approx
0.997$). The model successfully reproduces large-scale SST structures but faces
challenges in capturing finer details in coastal regions. This work contributes
to the field of data-driven ocean forecasting by demonstrating the feasibility
of using deep learning models pre-trained in different domains for oceanic
applications. Future improvements include integrating additional oceanographic
variables, increasing spatial resolution, and exploring physics-informed neural
networks to enhance interpretability and understanding. These advancements can
improve climate modeling and ocean prediction accuracy, supporting
decision-making in environmental and economic sectors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hybrid Quantum-Classical Recurrent Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25557v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25557v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenduan Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a hybrid quantum-classical recurrent neural network (QRNN)
architecture in which the entire recurrent core is realized as a parametrized
quantum circuit (PQC) controlled by a classical feedforward network. The hidden
state is the quantum state of an $n$-qubit PQC, residing in an exponentially
large Hilbert space $\mathbb{C}^{2^n}$. The PQC is unitary by construction,
making the hidden-state evolution norm-preserving without external constraints.
At each timestep, mid-circuit readouts are combined with the input embedding
and processed by the feedforward network, which provides explicit classical
nonlinearity. The outputs parametrize the PQC, which updates the hidden state
via unitary dynamics. The QRNN is compact and physically consistent, and it
unifies (i) unitary recurrence as a high-capacity memory, (ii) partial
observation via mid-circuit measurements, and (iii) nonlinear classical control
for input-conditioned parametrization. We evaluate the model in simulation with
up to 14 qubits on sentiment analysis, MNIST, permuted MNIST, copying memory,
and language modeling, adopting projective measurements as a limiting case to
obtain mid-circuit readouts while maintaining a coherent recurrent quantum
memory. We further devise a soft attention mechanism over the mid-circuit
readouts in a sequence-to-sequence model and show its effectiveness for machine
translation. To our knowledge, this is the first model (RNN or otherwise)
grounded in quantum operations to achieve competitive performance against
strong classical baselines across a broad class of sequence-learning tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust variable selection for spatial point processes observed with
  noise 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25550v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25550v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominik Sturm, Ivo F. Sbalzarini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a method for variable selection in the intensity function of
spatial point processes that combines sparsity-promoting estimation with
noise-robust model selection. As high-resolution spatial data becomes
increasingly available through remote sensing and automated image analysis,
identifying spatial covariates that influence the localization of events is
crucial to understand the underlying mechanism. However, results from automated
acquisition techniques are often noisy, for example due to measurement
uncertainties or detection errors, which leads to spurious displacements and
missed events. We study the impact of such noise on sparse point-process
estimation across different models, including Poisson and Thomas processes. To
improve noise robustness, we propose to use stability selection based on
point-process subsampling and to incorporate a non-convex best-subset penalty
to enhance model-selection performance. In extensive simulations, we
demonstrate that such an approach reliably recovers true covariates under
diverse noise scenarios and improves both selection accuracy and stability. We
then apply the proposed method to a forestry data set, analyzing the
distribution of trees in relation to elevation and soil nutrients in a tropical
rain forest. This shows the practical utility of the method, which provides a
systematic framework for robust variable selection in spatial point-process
models under noise, without requiring additional knowledge of the process.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Error Bounds and Optimal Schedules for Masked Diffusions with Factorized
  Approximations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25544v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25544v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hugo Lavenant, Giacomo Zanella
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently proposed generative models for discrete data, such as Masked
Diffusion Models (MDMs), exploit conditional independence approximations to
reduce the computational cost of popular Auto-Regressive Models (ARMs), at the
price of some bias in the sampling distribution. We study the resulting
computation-vs-accuracy trade-off, providing general error bounds (in relative
entropy) that depend only on the average number of tokens generated per
iteration and are independent of the data dimensionality (i.e. sequence
length), thus supporting the empirical success of MDMs. We then investigate the
gain obtained by using non-constant schedule sizes (i.e. varying the number of
unmasked tokens during the generation process) and identify the optimal
schedule as a function of a so-called information profile of the data
distribution, thus allowing for a principled optimization of schedule sizes. We
define methods directly as sampling algorithms and do not use classical
derivations as time-reversed diffusion processes, leading us to simple and
transparent proofs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Transformer</span>s Provably Learn Directed Acyclic Graphs via Kernel-Guided
  Mutual Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25542v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25542v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Cheng, Yu Huang, Zhe Xiong, Yingbin Liang, Vincent Y. F. Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Uncovering hidden graph structures underlying real-world data is a critical
challenge with broad applications across scientific domains. Recently,
transformer-based models leveraging the attention mechanism have demonstrated
strong empirical success in capturing complex dependencies within graphs.
However, the theoretical understanding of their training dynamics has been
limited to tree-like graphs, where each node depends on a single parent.
Extending provable guarantees to more general directed acyclic graphs (DAGs) --
which involve multiple parents per node -- remains challenging, primarily due
to the difficulty in designing training objectives that enable different
attention heads to separately learn multiple different parent relationships.
  In this work, we address this problem by introducing a novel
information-theoretic metric: the kernel-guided mutual information (KG-MI),
based on the $f$-divergence. Our objective combines KG-MI with a multi-head
attention framework, where each head is associated with a distinct marginal
transition kernel to model diverse parent-child dependencies effectively. We
prove that, given sequences generated by a $K$-parent DAG, training a
single-layer, multi-head transformer via gradient ascent converges to the
global optimum in polynomial time. Furthermore, we characterize the attention
score patterns at convergence. In addition, when particularizing the
$f$-divergence to the KL divergence, the learned attention scores accurately
reflect the ground-truth adjacency matrix, thereby provably recovering the
underlying graph structure. Experimental results validate our theoretical
findings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Using latent representations to link disjoint longitudinal data for
  mixed-effects regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25531v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25531v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Clemens Schächter, Maren Hackenberg, Michelle Pfaffenlehner, Félix B. Tambe-Ndonfack, Thorsten Schmidt, Astrid Pechmann, Janbernd Kirschner, Jan Hasenauser, Harald Binder
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many rare diseases offer limited established treatment options, leading
patients to switch therapies when new medications emerge. To analyze the impact
of such treatment switches within the low sample size limitations of rare
disease trials, it is important to use all available data sources. This,
however, is complicated when usage of measurement instruments change during the
observation period, for example when instruments are adapted to specific age
ranges. The resulting disjoint longitudinal data trajectories, complicate the
application of traditional modeling approaches like mixed-effects regression.
We tackle this by mapping observations of each instrument to a aligned
low-dimensional temporal trajectory, enabling longitudinal modeling across
instruments. Specifically, we employ a set of variational autoencoder
architectures to embed item values into a shared latent space for each time
point. Temporal disease dynamics and treatment switch effects are then captured
through a mixed-effects regression model applied to latent representations. To
enable statistical inference, we present a novel statistical testing approach
that accounts for the joint parameter estimation of mixed-effects regression
and variational autoencoders. The methodology is applied to quantify the impact
of treatment switches for patients with spinal muscular atrophy. Here, our
approach aligns motor performance items from different measurement instruments
for mixed-effects regression and maps estimated effects back to the observed
item level to quantify the treatment switch effect. Our approach allows for
model selection as well as for assessing effects of treatment switching. The
results highlight the potential of modeling in joint latent representations for
addressing small data challenges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 3 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Convergence of off-policy TD(0) with linear function approximation for
  reversible Markov chains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25514v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25514v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maik Overmars, Jasper Goseling, Richard Boucherie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the convergence of off-policy TD(0) with linear function
approximation when used to approximate the expected discounted reward in a
Markov chain. It is well known that the combination of off-policy learning and
function approximation can lead to divergence of the algorithm. Existing
results for this setting modify the algorithm, for instance by reweighing the
updates using importance sampling. This establishes convergence at the expense
of additional complexity. In contrast, our approach is to analyse the standard
algorithm, but to restrict our attention to the class of reversible Markov
chains. We demonstrate convergence under this mild reversibility condition on
the structure of the chain, which in many applications can be assumed using
domain knowledge. In particular, we establish a convergence guarantee under an
upper bound on the discount factor in terms of the difference between the
on-policy and off-policy process. This improves upon known results in the
literature that state that convergence holds for a sufficiently small discount
factor by establishing an explicit bound. Convergence is with probability one
and achieves projected Bellman error equal to zero. To obtain these results, we
adapt the stochastic approximation framework that was used by Tsitsiklis and
Van Roy [1997 for the on-policy case, to the off-policy case. We illustrate our
results using different types of reversible Markov chains, such as
one-dimensional random walks and random walks on a weighted graph.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FaCT: Faithful Concept Traces for Explaining Neural Network Decisions <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25512v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25512v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amin Parchami-Araghi, Sukrut Rao, Jonas Fischer, Bernt Schiele
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep networks have shown remarkable performance across a wide range of tasks,
yet getting a global concept-level understanding of how they function remains a
key challenge. Many post-hoc concept-based approaches have been introduced to
understand their workings, yet they are not always faithful to the model.
Further, they make restrictive assumptions on the concepts a model learns, such
as class-specificity, small spatial extent, or alignment to human expectations.
In this work, we put emphasis on the faithfulness of such concept-based
explanations and propose a new model with model-inherent mechanistic
concept-explanations. Our concepts are shared across classes and, from any
layer, their contribution to the logit and their input-visualization can be
faithfully traced. We also leverage foundation models to propose a new
concept-consistency metric, C$^2$-Score, that can be used to evaluate
concept-based methods. We show that, compared to prior work, our concepts are
quantitatively more consistent and users find our concepts to be more
interpretable, all while retaining competitive ImageNet performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2025; Code is available at
  https://github.com/m-parchami/FaCT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Support Vector Machine-Based Burnout Risk Prediction with an Interactive
  Interface for Organizational Use 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25509v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25509v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bruno W. G. Teodosio, Mário J. O. T. Lira, Pedro H. M. Araújo, Lucas R. C. Farias
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Burnout is a psychological syndrome marked by emotional exhaustion,
depersonalization, and reduced personal accomplishment, with a significant
impact on individual well-being and organizational performance. This study
proposes a machine learning approach to predict burnout risk using the
HackerEarth Employee Burnout Challenge dataset. Three supervised algorithms
were evaluated: nearest neighbors (KNN), random forest, and support vector
machine (SVM), with model performance evaluated through 30-fold
cross-validation using the determination coefficient (R2). Among the models
tested, SVM achieved the highest predictive performance (R2 = 0.84) and was
statistically superior to KNN and Random Forest based on paired $t$-tests. To
ensure practical applicability, an interactive interface was developed using
Streamlit, allowing non-technical users to input data and receive burnout risk
predictions. The results highlight the potential of machine learning to support
early detection of burnout and promote data-driven mental health strategies in
organizational settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, including figures and references. Streamlit app available
  at: https://employee-burnout-svm.streamlit.app/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TempoPFN: Synthetic <span class="highlight-title">Pre-train</span>ing of Linear RNNs for Zero-shot Time
  Series Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25502v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25502v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vladyslav Moroshan, Julien Siems, Arber Zela, Timur Carstensen, Frank Hutter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models for zero-shot time series forecasting face challenges in
efficient long-horizon prediction and reproducibility, with existing
synthetic-only approaches underperforming on challenging benchmarks. This paper
presents TempoPFN, a univariate time series foundation model based on linear
Recurrent Neural Networks (RNNs) pre-trained exclusively on synthetic data. The
model uses a GatedDeltaProduct architecture with state-weaving for fully
parallelizable training across sequence lengths, eliminating the need for
windowing or summarization techniques while maintaining robust temporal
state-tracking. Our comprehensive synthetic data pipeline unifies diverse
generators, including stochastic differential equations, Gaussian processes,
and audio synthesis, with novel augmentations. In zero-shot evaluations on the
Gift-Eval benchmark, TempoPFN achieves top-tier competitive performance,
outperforming all existing synthetic-only approaches and surpassing the vast
majority of models trained on real-world data, while being more efficient than
existing baselines by leveraging fully parallelizable training and inference.
We open-source our complete data generation pipeline and training code,
providing a reproducible foundation for future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 18 figures, 13 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Right for the Right Reasons: Avoiding Reasoning Shortcuts via
  Prototypical Neurosymbolic AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25497v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25497v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Andolfi, Eleonora Giunchiglia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neurosymbolic AI is growing in popularity thanks to its ability to combine
neural perception and symbolic reasoning in end-to-end trainable models.
However, recent findings reveal these are prone to shortcut reasoning, i.e., to
learning unindented concepts--or neural predicates--which exploit spurious
correlations to satisfy the symbolic constraints. In this paper, we address
reasoning shortcuts at their root cause and we introduce prototypical
neurosymbolic architectures. These models are able to satisfy the symbolic
constraints (be right) because they have learnt the correct basic concepts (for
the right reasons) and not because of spurious correlations, even in extremely
low data regimes. Leveraging the theory of prototypical learning, we
demonstrate that we can effectively avoid reasoning shortcuts by training the
models to satisfy the background knowledge while taking into account the
similarity of the input with respect to the handful of labelled datapoints. We
extensively validate our approach on the recently proposed rsbench benchmark
suite in a variety of settings and tasks with very scarce supervision: we show
significant improvements in learning the right concepts both in synthetic tasks
(MNIST-EvenOdd and Kand-Logic) and real-world, high-stake ones (BDD-OIA). Our
findings pave the way to prototype grounding as an effective,
annotation-efficient strategy for safe and reliable neurosymbolic learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gradient-Weight Alignment as a Train-Time Proxy for Generalization in
  Classification Tasks <span class="chip">NeurIPS
  2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25480v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25480v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian A. Hölzl, Daniel Rueckert, Georgios Kaissis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robust validation metrics remain essential in contemporary deep learning, not
only to detect overfitting and poor generalization, but also to monitor
training dynamics. In the supervised classification setting, we investigate
whether interactions between training data and model weights can yield such a
metric that both tracks generalization during training and attributes
performance to individual training samples. We introduce Gradient-Weight
Alignment (GWA), quantifying the coherence between per-sample gradients and
model weights. We show that effective learning corresponds to coherent
alignment, while misalignment indicates deteriorating generalization. GWA is
efficiently computable during training and reflects both sample-specific
contributions and dataset-wide learning dynamics. Extensive experiments show
that GWA accurately predicts optimal early stopping, enables principled model
comparisons, and identifies influential training samples, providing a
validation-set-free approach for model analysis directly from the training
data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39th Conference on Neural Information Processing Systems (NeurIPS
  2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An In-Depth Analysis of Cyber Attacks in Secured Platforms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25470v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25470v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Parick Ozoh, John K Omoniyi, Bukola Ibitoye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is an increase in global malware threats. To address this, an
encryption-type ransomware has been introduced on the Android operating system.
The challenges associated with malicious threats in phone use have become a
pressing issue in mobile communication, disrupting user experiences and posing
significant privacy threats. This study surveys commonly used machine learning
techniques for detecting malicious threats in phones and examines their
performance. The majority of past research focuses on customer feedback and
reviews, with concerns that people might create false reviews to promote or
devalue products and services for personal gain. Hence, the development of
techniques for detecting malicious threats using machine learning has been a
key focus. This paper presents a comprehensive comparative study of current
research on the issue of malicious threats and methods for tackling these
challenges. Nevertheless, a huge amount of information is required by these
methods, presenting a challenge for developing robust, specialized automated
anti-malware systems. This research describes the Android Applications dataset,
and the accuracy of the techniques is measured using the accuracy levels of the
metrics employed in this study.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scalable Utility-Aware Multiclass Calibration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25458v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25458v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahmoud Hegazy, Michael I. Jordan, Aymeric Dieuleveut
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring that classifiers are well-calibrated, i.e., their predictions align
with observed frequencies, is a minimal and fundamental requirement for
classifiers to be viewed as trustworthy. Existing methods for assessing
multiclass calibration often focus on specific aspects associated with
prediction (e.g., top-class confidence, class-wise calibration) or utilize
computationally challenging variational formulations. In this work, we study
scalable \emph{evaluation} of multiclass calibration. To this end, we propose
utility calibration, a general framework that measures the calibration error
relative to a specific utility function that encapsulates the goals or decision
criteria relevant to the end user. We demonstrate how this framework can unify
and re-interpret several existing calibration metrics, particularly allowing
for more robust versions of the top-class and class-wise calibration metrics,
and, going beyond such binarized approaches, toward assessing calibration for
richer classes of downstream utilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Agentic AI: A Comprehensive <span class="highlight-title">Survey</span> of Architectures, Applications, and
  Future Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25445v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25445v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamad Abou Ali, Fadi Dornaika
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Agentic AI represents a transformative shift in artificial intelligence, but
its rapid advancement has led to a fragmented understanding, often conflating
modern neural systems with outdated symbolic models -- a practice known as
conceptual retrofitting. This survey cuts through this confusion by introducing
a novel dual-paradigm framework that categorizes agentic systems into two
distinct lineages: the Symbolic/Classical (relying on algorithmic planning and
persistent state) and the Neural/Generative (leveraging stochastic generation
and prompt-driven orchestration). Through a systematic PRISMA-based review of
90 studies (2018--2025), we provide a comprehensive analysis structured around
this framework across three dimensions: (1) the theoretical foundations and
architectural principles defining each paradigm; (2) domain-specific
implementations in healthcare, finance, and robotics, demonstrating how
application constraints dictate paradigm selection; and (3) paradigm-specific
ethical and governance challenges, revealing divergent risks and mitigation
strategies. Our analysis reveals that the choice of paradigm is strategic:
symbolic systems dominate safety-critical domains (e.g., healthcare), while
neural systems prevail in adaptive, data-rich environments (e.g., finance).
Furthermore, we identify critical research gaps, including a significant
deficit in governance models for symbolic systems and a pressing need for
hybrid neuro-symbolic architectures. The findings culminate in a strategic
roadmap arguing that the future of Agentic AI lies not in the dominance of one
paradigm, but in their intentional integration to create systems that are both
adaptable and reliable. This work provides the essential conceptual toolkit to
guide future research, development, and policy toward robust and trustworthy
hybrid intelligent systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">GPT</span>Opt: Towards Efficient LLM-Based Black-Box Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25404v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25404v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jamison Meindl, Yunsheng Tian, Tony Cui, Veronika Thost, Zhang-Wei Hong, Jie Chen, Wojciech Matusik, Mina Konaković Luković
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Global optimization of expensive, derivative-free black-box functions demands
extreme sample efficiency. Classical methods such as Bayesian Optimization (BO)
can be effective, but they often require careful parameter tuning to each
application domain. At the same time, Large Language Models (LLMs) have shown
broad capabilities, yet state-of-the-art models remain limited in solving
continuous black-box optimization tasks. We introduce GPTOpt, an LLM-based
optimization method that equips LLMs with continuous black-box optimization
capabilities. By fine-tuning large language models on extensive synthetic
datasets derived from diverse BO parameterizations, GPTOpt leverages LLM
pre-training to generalize across optimization tasks. On a variety of black-box
optimization benchmarks, GPTOpt surpasses traditional optimizers, highlighting
the capacity of LLMs for advanced numerical reasoning and introducing a
flexible framework for global optimization without parameter tuning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Deep Learning Framework for Multi-Operator Learning: Architectures and
  Approximation Theory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25379v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25379v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adrien Weihs, Jingmin Sun, Zecheng Zhang, Hayden Schaeffer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While many problems in machine learning focus on learning mappings between
finite-dimensional spaces, scientific applications require approximating
mappings between function spaces, i.e., operators. We study the problem of
learning collections of operators and provide both theoretical and empirical
advances. We distinguish between two regimes: (i) multiple operator learning,
where a single network represents a continuum of operators parameterized by a
parametric function, and (ii) learning several distinct single operators, where
each operator is learned independently. For the multiple operator case, we
introduce two new architectures, $\mathrm{MNO}$ and $\mathrm{MONet}$, and
establish universal approximation results in three settings: continuous,
integrable, or Lipschitz operators. For the latter, we further derive explicit
scaling laws that quantify how the network size must grow to achieve a target
approximation accuracy. For learning several single operators, we develop a
framework for balancing architectural complexity across subnetworks and show
how approximation order determines computational efficiency. Empirical
experiments on parametric PDE benchmarks confirm the strong expressive power
and efficiency of the proposed architectures. Overall, this work establishes a
unified theoretical and practical foundation for scalable neural operator
learning across multiple operators.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Prompt</span> Estimation from Prototypes for Federated <span class="highlight-title">Prompt</span> Tuning of Vision
  <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25372v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25372v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        M Yashwanth, Sharannya Ghosh, Aditay Tripathi, Anirban Chakraborty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Prompt Tuning (VPT) of pre-trained Vision Transformers (ViTs) has
proven highly effective as a parameter-efficient fine-tuning technique for
adapting large models to downstream tasks with limited data. Its parameter
efficiency makes it particularly suitable for Federated Learning (FL), where
both communication and computation budgets are often constrained. However,
global prompt tuning struggles to generalize across heterogeneous clients,
while personalized tuning overfits to local data and lacks generalization. We
propose PEP-FedPT (Prompt Estimation from Prototypes for Federated Prompt
Tuning), a unified framework designed to achieve both generalization and
personalization in federated prompt tuning of ViTs. Within this framework, we
introduce the novel Class-Contextualized Mixed Prompt (CCMP) - based on
class-specific prompts maintained alongside a globally shared prompt. For each
input, CCMP adaptively combines class-specific prompts using weights derived
from global class prototypes and client class priors. This approach enables
per-sample prompt personalization without storing client-dependent trainable
parameters. The prompts are collaboratively optimized via traditional federated
averaging technique on the same. Comprehensive evaluations on CIFAR-100,
TinyImageNet, DomainNet, and iNaturalist datasets demonstrate that PEP-FedPT
consistently surpasses the state-of-the-art baselines under diverse data
heterogeneity scenarios, establishing a strong foundation for efficient and
generalizable federated prompt tuning of Vision Transformers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Position: Biology is the Challenge Physics-Informed ML Needs to Evolve 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25368v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25368v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julien Martinelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Physics-Informed Machine Learning (PIML) has successfully integrated
mechanistic understanding into machine learning, particularly in domains
governed by well-known physical laws. This success has motivated efforts to
apply PIML to biology, a field rich in dynamical systems but shaped by
different constraints. Biological modeling, however, presents unique
challenges: multi-faceted and uncertain prior knowledge, heterogeneous and
noisy data, partial observability, and complex, high-dimensional networks. In
this position paper, we argue that these challenges should not be seen as
obstacles to PIML, but as catalysts for its evolution. We propose
Biology-Informed Machine Learning (BIML): a principled extension of PIML that
retains its structural grounding while adapting to the practical realities of
biology. Rather than replacing PIML, BIML retools its methods to operate under
softer, probabilistic forms of prior knowledge. We outline four foundational
pillars as a roadmap for this transition: uncertainty quantification,
contextualization, constrained latent structure inference, and scalability.
Foundation Models and Large Language Models will be key enablers, bridging
human expertise with computational modeling. We conclude with concrete
recommendations to build the BIML ecosystem and channel PIML-inspired
innovation toward challenges of high scientific and societal relevance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Convexity-dependent Two-Phase Training Algorithm for Deep Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25366v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25366v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomas Hrycej, Bernhard Bermeitinger, Massimo Pavone, Götz-Henrik Wiegand, Siegfried Handschuh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The key task of machine learning is to minimize the loss function that
measures the model fit to the training data. The numerical methods to do this
efficiently depend on the properties of the loss function. The most decisive
among these properties is the convexity or non-convexity of the loss function.
The fact that the loss function can have, and frequently has, non-convex
regions has led to a widespread commitment to non-convex methods such as Adam.
However, a local minimum implies that, in some environment around it, the
function is convex. In this environment, second-order minimizing methods such
as the Conjugate Gradient (CG) give a guaranteed superlinear convergence. We
propose a novel framework grounded in the hypothesis that loss functions in
real-world tasks swap from initial non-convexity to convexity towards the
optimum. This is a property we leverage to design an innovative two-phase
optimization algorithm. The presented algorithm detects the swap point by
observing the gradient norm dependence on the loss. In these regions,
non-convex (Adam) and convex (CG) algorithms are used, respectively. Computing
experiments confirm the hypothesis that this simple convexity structure is
frequent enough to be practically exploited to substantially improve
convergence and accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Appeared on KDIR IC3K Conference 2025 (Best Paper Award)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parameter Averaging in Link Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25361v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25361v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rupesh Sapkota, Caglar Demir, Arnab Sharma, Axel-Cyrille Ngonga Ngomo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensemble methods are widely employed to improve generalization in machine
learning. This has also prompted the adoption of ensemble learning for the
knowledge graph embedding (KGE) models in performing link prediction. Typical
approaches to this end train multiple models as part of the ensemble, and the
diverse predictions are then averaged. However, this approach has some
significant drawbacks. For instance, the computational overhead of training
multiple models increases latency and memory overhead. In contrast, model
merging approaches offer a promising alternative that does not require training
multiple models. In this work, we introduce model merging, specifically
weighted averaging, in KGE models. Herein, a running average of model
parameters from a training epoch onward is maintained and used for predictions.
To address this, we additionally propose an approach that selectively updates
the running average of the ensemble model parameters only when the
generalization performance improves on a validation dataset. We evaluate these
two different weighted averaging approaches on link prediction tasks, comparing
the state-of-the-art benchmark ensemble approach. Additionally, we evaluate the
weighted averaging approach considering literal-augmented KGE models and
multi-hop query answering tasks as well. The results demonstrate that the
proposed weighted averaging approach consistently improves performance across
diverse evaluation settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analysis of Semi-Supervised Learning on Hypergraphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25354v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25354v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adrien Weihs, Andrea Bertozzi, Matthew Thorpe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hypergraphs provide a natural framework for modeling higher-order
interactions, yet their theoretical underpinnings in semi-supervised learning
remain limited. We provide an asymptotic consistency analysis of variational
learning on random geometric hypergraphs, precisely characterizing the
conditions ensuring the well-posedness of hypergraph learning as well as
showing convergence to a weighted $p$-Laplacian equation. Motivated by this, we
propose Higher-Order Hypergraph Learning (HOHL), which regularizes via powers
of Laplacians from skeleton graphs for multiscale smoothness. HOHL converges to
a higher-order Sobolev seminorm. Empirically, it performs strongly on standard
baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 3D Optimization for AI Inference Scaling: Balancing Accuracy, Cost, and
  Latency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.18905v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.18905v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minseok Jung, Abhas Ricky, Muhammad Rameez Chatni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI inference scaling is often tuned through 1D heuristics (a fixed reasoning
passes) or 2D bivariate trade-offs (e.g., performance vs. compute), which fail
to consider cost and latency constraints. We introduce a 3D optimization
framework that jointly calibrates accuracy, cost, and latency within a unified
decision space, enabling constraints-aware inference scaling. Using Monte Carlo
simulations across three representative scenarios and nine simulated large
language models, we evaluate four optimization methods to address the 3D
multi-objective optimization (MOO) problem. Framing inference scaling in MOO
shapes a feasible space that 1D and 2D optimizations fail to capture, enabling
environmentadaptive selection of the inference scaling k. Results show that
knee-point optimization achieves the best balance, while accuracy-maximization
remains favorable when precision is prioritized. The framework establishes a
theoretical foundation for deployment-aware inference scaling across diverse
operational contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SpecCLIP: Aligning and Translating Spectroscopic Measurements for Stars 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.01939v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.01939v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaosheng Zhao, Yang Huang, Guirong Xue, Xiao Kong, Jifeng Liu, Xiaoyu Tang, Timothy C. Beers, Yuan-Sen Ting, A-Li Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, large language models (LLMs) have transformed natural
language understanding through vast datasets and large-scale parameterization.
Inspired by this success, we present SpecCLIP, a foundation model framework
that extends LLM-inspired methodologies to stellar spectral analysis. Stellar
spectra, akin to structured language, encode rich physical and chemical
information about stars. By training foundation models on large-scale spectral
datasets, our goal is to learn robust and informative embeddings that support
diverse downstream applications. As a proof of concept, SpecCLIP involves
pre-training on two spectral types--LAMOST low-resolution and Gaia XP--followed
by contrastive alignment using the CLIP (Contrastive Language-Image
Pre-training) framework, adapted to associate spectra from different
instruments. This alignment is complemented by auxiliary decoders that preserve
spectrum-specific information and enable translation (prediction) between
spectral types, with the former achieved by maximizing mutual information
between embeddings and input spectra. The result is a cross-spectrum framework
enabling intrinsic calibration and flexible applications across instruments. We
demonstrate that fine-tuning these models on moderate-sized labeled datasets
improves adaptability to tasks such as stellar-parameter estimation and
chemical-abundance determination. SpecCLIP also enhances the accuracy and
precision of parameter estimates benchmarked against external survey data.
Additionally, its similarity search and cross-spectrum prediction capabilities
offer potential for anomaly detection. Our results suggest that contrastively
trained foundation models enriched with spectrum-aware decoders can advance
precision stellar spectroscopy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 8 figures, 5 tables. Minor update: added corrected
  acknowledgments and corrected a misstated hyperparameter value (noted in
  footnote) for reproducibility. Submitted to AAS Journals. Comments welcome</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Curiosity-driven RL for symbolic equation solving <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.17022v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.17022v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin P. O'Keeffe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore if RL can be useful for symbolic mathematics. Previous work showed
contrastive learning can solve linear equations in one variable. We show
model-free PPO \cite{schulman2017proximal} augmented with curiosity-based
exploration and graph-based actions can solve nonlinear equations such as those
involving radicals, exponentials, and trig functions. Our work suggests
curiosity-based exploration may be useful for general symbolic reasoning tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the NeurIPS 2025 MATH-AI Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Price equation reveals a universal force-metric-bias law of
  algorithmic learning and natural selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18549v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18549v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Steven A. Frank
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diverse learning algorithms, optimization methods, and natural selection
share a common mathematical structure, despite their apparent differences. Here
I show that a simple notational partitioning of change by the Price equation
reveals a universal force-metric-bias (FMB) law: $\Delta\mathbf{\theta} =
\mathbf{M}\,\mathbf{f} + \mathbf{b} + \mathbf{\xi}$. The force $\mathbf{f}$
drives improvement in parameters, $\Delta\mathbf{\theta}$, in proportion to the
slope of performance with respect to the parameters. The metric $\mathbf{M}$
rescales movement by inverse curvature. The bias $\mathbf{b}$ adds momentum or
changes in the frame of reference. The noise $\mathbf{\xi}$ enables
exploration. This framework unifies natural selection, Bayesian updating,
Newton's method, stochastic gradient descent, stochastic Langevin dynamics,
Adam optimization, and most other algorithms as special cases of the same
underlying process. The Price equation also reveals why Fisher information,
Kullback-Leibler divergence, and d'Alembert's principle arise naturally in
learning dynamics. By exposing this common structure, the FMB law provides a
principled foundation for understanding, comparing, and designing learning
algorithms across disciplines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Version 2: fixed definition of force in abstract; Version 3: added
  citations and some minor editing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ASGO: Adaptive Structured Gradient Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20762v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20762v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kang An, Yuxing Liu, Rui Pan, Yi Ren, Shiqian Ma, Donald Goldfarb, Tong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training deep neural networks is a structured optimization problem, because
the parameters are naturally represented by matrices and tensors rather than by
vectors. Under this structural representation, it has been widely observed that
gradients are low-rank and Hessians are approximately block diagonal. These
structured properties are crucial for designing efficient optimization
algorithms, but are not utilized by many current popular optimizers like Adam.
In this paper, we present a novel optimization algorithm ASGO that capitalizes
on these properties by employing a preconditioner that is adaptively updated
using structured gradients. By a fine-grained theoretical analysis, ASGO is
proven to achieve superior convergence rates compared to existing structured
gradient methods. Based on this convergence theory, we further demonstrate that
ASGO can benefit from low-rank gradients and block diagonal Hessians. We also
discuss practical modifications of ASGO and empirically verify ASGO's
effectiveness on language model tasks. Code is available at
https://github.com/infinity-stars/ASGO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamical Decoupling of Generalization and Overfitting in Large
  Two-Layer Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.21269v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.21269v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Montanari, Pierfrancesco Urbani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the inductive bias and generalization properties of large
overparametrized machine learning models requires to characterize the dynamics
of the training algorithm. We study the learning dynamics of large two-layer
neural networks via dynamical mean field theory, a well established technique
of non-equilibrium statistical physics. We show that, for large network width
$m$, and large number of samples per input dimension $n/d$, the training
dynamics exhibits a separation of timescales which implies: $(i)$~The emergence
of a slow time scale associated with the growth in Gaussian/Rademacher
complexity of the network; $(ii)$~Inductive bias towards small complexity if
the initialization has small enough complexity; $(iii)$~A dynamical decoupling
between feature learning and overfitting regimes; $(iv)$~A non-monotone
behavior of the test error, associated `feature unlearning' regime at large
times.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>88 pages; 63 pdf figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SMMILE: An Expert-Driven Benchmark for Multimodal Medical In-Context
  Learning <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21355v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21355v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Melanie Rieff, Maya Varma, Ossian Rabow, Subathra Adithan, Julie Kim, Ken Chang, Hannah Lee, Nidhi Rohatgi, Christian Bluethgen, Mohamed S. Muneer, Jean-Benoit Delbrouck, Michael Moor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal in-context learning (ICL) remains underexplored despite
significant potential for domains such as medicine. Clinicians routinely
encounter diverse, specialized tasks requiring adaptation from limited
examples, such as drawing insights from a few relevant prior cases or
considering a constrained set of differential diagnoses. While multimodal large
language models (MLLMs) have shown advances in medical visual question
answering (VQA), their ability to learn multimodal tasks from context is
largely unknown. We introduce SMMILE, the first expert-driven multimodal ICL
benchmark for medical tasks. Eleven medical experts curated problems, each
including a multimodal query and multimodal in-context examples as task
demonstrations. SMMILE encompasses 111 problems (517 question-image-answer
triplets) covering 6 medical specialties and 13 imaging modalities. We further
introduce SMMILE++, an augmented variant with 1038 permuted problems. A
comprehensive evaluation of 15 MLLMs demonstrates that most models exhibit
moderate to poor multimodal ICL ability in medical tasks. In open-ended
evaluations, ICL contributes only an 8% average improvement over zero-shot on
SMMILE and 9.4% on SMMILE++. We observe a susceptibility for irrelevant
in-context examples: even a single noisy or irrelevant example can degrade
performance by up to 9.5%. Moreover, we observe that MLLMs are affected by a
recency bias, where placing the most relevant example last can lead to
substantial performance improvements of up to 71%. Our findings highlight
critical limitations and biases in current MLLMs when learning multimodal
medical tasks from context. SMMILE is available at
https://smmile-benchmark.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025 (Datasets & Benchmarks Track)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exact Sequence Interpolation with <span class="highlight-title">Transformer</span>s <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.02270v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.02270v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Albert Alcalde, Giovanni Fantuzzi, Enrique Zuazua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We prove that transformers can exactly interpolate datasets of finite input
sequences in $\mathbb{R}^d$, $d\geq 2$, with corresponding output sequences of
smaller or equal length. Specifically, given $N$ sequences of arbitrary but
finite lengths in $\mathbb{R}^d$ and output sequences of lengths $m^1, \dots,
m^N \in \mathbb{N}$, we construct a transformer with $\mathcal{O}(\sum_{j=1}^N
m^j)$ blocks and $\mathcal{O}(d \sum_{j=1}^N m^j)$ parameters that exactly
interpolates the dataset. Our construction provides complexity estimates that
are independent of the input sequence length, by alternating feed-forward and
self-attention layers and by capitalizing on the clustering effect inherent to
the latter. Our novel constructive method also uses low-rank parameter matrices
in the self-attention mechanism, a common feature of practical transformer
implementations. These results are first established in the hardmax
self-attention setting, where the geometric structure permits an explicit and
quantitative analysis, and are then extended to the softmax setting. Finally,
we demonstrate the applicability of our exact interpolation construction to
learning problems, in particular by providing convergence guarantees to a
global minimizer under regularized training strategies. Our analysis
contributes to the theoretical understanding of transformer models, offering an
explanation for their excellent performance in exact sequence-to-sequence
interpolation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 9 figures. Funded by the European Union (Horizon Europe
  MSCA project ModConFlex, grant number 101073558)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MP-FVM: Enhancing Finite Volume Method for Water Infiltration Modeling
  in Unsaturated Soils via Message-passing Encoder-decoder Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02806v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02806v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyuan Song, Zheyu Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The spatiotemporal water flow dynamics in unsaturated soils can generally be
modeled by the Richards equation. To overcome the computational challenges
associated with solving this highly nonlinear partial differential equation
(PDE), we present a novel solution algorithm, which we name as the MP-FVM
(Message Passing-Finite Volume Method), to holistically integrate adaptive
fixed-point iteration scheme, encoder-decoder neural network architecture,
Sobolev training, and message passing mechanism in a finite volume
discretization framework. We thoroughly discuss the need and benefits of
introducing these components to achieve synergistic improvements in accuracy
and stability of the solution. We also show that our MP-FVM algorithm can
accurately solve the mixed-form $n$-dimensional Richards equation with
guaranteed convergence under reasonable assumptions. Through several
illustrative examples, we demonstrate that our MP-FVM algorithm not only
achieves superior accuracy, but also better preserves the underlying physical
laws and mass conservation of the Richards equation compared to
state-of-the-art solution algorithms and the commercial HYDRUS solver.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages, 14 figures, Accepted by Computers and Geotechnics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Scaling Deep Neural Networks with Predictive Coding: Theory and
  Practice 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23323v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23323v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Innocenti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Backpropagation (BP) is the standard algorithm for training the deep neural
networks that power modern artificial intelligence including large language
models. However, BP is energy inefficient and unlikely to be implemented by the
brain. This thesis studies an alternative, potentially more efficient
brain-inspired algorithm called predictive coding (PC). Unlike BP, PC networks
(PCNs) perform inference by iterative equilibration of neuron activities before
learning or weight updates. Recent work has suggested that this iterative
inference procedure provides a range of benefits over BP, such as faster
training. However, these advantages have not been consistently observed, the
inference and learning dynamics of PCNs are still poorly understood, and deep
PCNs remain practically untrainable. Here, we make significant progress towards
scaling PCNs by taking a theoretical approach grounded in optimisation theory.
First, we show that the learning dynamics of PC can be understood as an
approximate trust-region method using second-order information, despite
explicitly using only first-order local updates. Second, going beyond this
approximation, we show that PC can in principle make use of arbitrarily
higher-order information, such that for feedforward networks the effective
landscape on which PC learns is far more benign and robust to vanishing
gradients than the (mean squared error) loss landscape. Third, motivated by a
study of the inference dynamics of PCNs, we propose a new parameterisation
called "$\mu$PC", which for the first time allows stable training of 100+ layer
networks with little tuning and competitive performance on simple tasks.
Overall, this thesis significantly advances our fundamental understanding of
the inference and learning dynamics of PCNs, while highlighting the need for
future research to focus on hardware co-design if PC is to compete with BP at
scale.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>PhD thesis</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Score-Aware Policy-Gradient and Performance Guarantees using Local
  Lyapunov Stability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.02804v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.02804v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Céline Comte, Matthieu Jonckheere, Jaron Sanders, Albert Senen-Cerda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a policy-gradient method for model-based
reinforcement learning (RL) that exploits a type of stationary distributions
commonly obtained from Markov decision processes (MDPs) in stochastic networks,
queueing systems, and statistical mechanics. Specifically, when the stationary
distribution of the MDP belongs to an exponential family that is parametrized
by policy parameters, we can improve existing policy gradient methods for
average-reward RL. Our key identification is a family of gradient estimators,
called score-aware gradient estimators (SAGEs), that enable policy gradient
estimation without relying on value-function estimation in the aforementioned
setting. We show that SAGE-based policy-gradient locally converges, and we
obtain its regret. This includes cases when the state space of the MDP is
countable and unstable policies can exist. Under appropriate assumptions such
as starting sufficiently close to a maximizer and the existence of a local
Lyapunov function, the policy under SAGE-based stochastic gradient ascent has
an overwhelming probability of converging to the associated optimal policy.
Furthermore, we conduct a numerical comparison between a SAGE-based
policy-gradient method and an actor-critic method on several examples inspired
from stochastic networks, queueing systems, and models derived from statistical
physics. Our results demonstrate that a SAGE-based method finds
close-to-optimal policies faster than an actor-critic method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scaling Up Liquid-Resistance Liquid-Capacitance Networks for Efficient
  Sequence Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.21717v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.21717v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mónika Farsang, Ramin Hasani, Daniela Rus, Radu Grosu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present LrcSSM, a $\textit{non-linear}$ recurrent model that processes
long sequences as fast as today's linear state-space layers. By forcing its
Jacobian matrix to be diagonal, the full sequence can be solved in parallel,
giving $\mathcal{O}(TD)$ time and memory and only $\mathcal{O}(\log T)$
sequential depth, for input-sequence length $T$ and a state dimension $D$.
Moreover, LrcSSM offers a formal gradient-stability guarantee that other
input-varying systems such as Liquid-S4 and Mamba do not provide. Importantly,
the diagonal Jacobian structure of our model results in no performance loss
compared to the original model with dense Jacobian, and the approach can be
generalized to other non-linear recurrent models, demonstrating broader
applicability. On a suite of long-range forecasting tasks, we demonstrate that
LrcSSM outperforms Transformers, LRU, S5, and Mamba.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring the In-Context Learning Capabilities of LLMs for Money
  Laundering Detection in Financial Graphs <span class="chip">ICDM 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.14785v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.14785v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erfan Pirmorad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The complexity and interconnectivity of entities involved in money laundering
demand investigative reasoning over graph-structured data. This paper explores
the use of large language models (LLMs) as reasoning engines over localized
subgraphs extracted from a financial knowledge graph. We propose a lightweight
pipeline that retrieves k-hop neighborhoods around entities of interest,
serializes them into structured text, and prompts an LLM via few-shot
in-context learning to assess suspiciousness and generate justifications. Using
synthetic anti-money laundering (AML) scenarios that reflect common laundering
behaviors, we show that LLMs can emulate analyst-style logic, highlight red
flags, and provide coherent explanations. While this study is exploratory, it
illustrates the potential of LLM-based graph reasoning in AML and lays
groundwork for explainable, language-driven financial crime analytics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AI4FCF-ICDM 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Sign Estimator: LLM Alignment in the Face of Choice Heterogeneity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23965v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23965v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Aouad, Aymane El Gadarri, Vivek F. Farias
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional LLM alignment methods are vulnerable to heterogeneity in human
preferences. Fitting a na\"ive probabilistic model to pairwise comparison data
(say over prompt-completion pairs) yields an inconsistent estimate of the
population-average utility -a canonical measure of social welfare. We propose a
new method, dubbed the sign estimator, that provides a simple, provably
consistent, and efficient estimator by replacing cross-entropy with binary
classification loss in the aggregation step. This simple modification recovers
consistent ordinal alignment under mild assumptions and achieves the first
polynomial finite-sample error bounds in this setting. In realistic simulations
of LLM alignment using digital twins, the sign estimator substantially reduces
preference distortion over a panel of simulated personas, cutting (angular)
estimation error by nearly 35% and decreasing disagreement with true population
preferences from 12% to 8% compared to standard RLHF. Our method also compares
favorably to panel data heuristics that explicitly model user heterogeneity and
require tracking individual-level preference data-all while maintaining the
implementation simplicity of existing LLM alignment pipelines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Handling Label Noise via Instance-Level Difficulty Modeling and Dynamic
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.00812v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.00812v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kuan Zhang, Chengliang Chai, Jingzhe Xu, Chi Zhang, Han Han, Ye Yuan, Guoren Wang, Lei Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies indicate that deep neural networks degrade in generalization
performance under noisy supervision. Existing methods focus on isolating clean
subsets or correcting noisy labels, facing limitations such as high
computational costs, heavy hyperparameter tuning process, and coarse-grained
optimization. To address these challenges, we propose a novel two-stage noisy
learning framework that enables instance-level optimization through a
dynamically weighted loss function, avoiding hyperparameter tuning. To obtain
stable and accurate information about noise modeling, we introduce a simple yet
effective metric, termed wrong event, which dynamically models the cleanliness
and difficulty of individual samples while maintaining computational costs. Our
framework first collects wrong event information and builds a strong base
model. Then we perform noise-robust training on the base model, using a
probabilistic model to handle the wrong event information of samples.
Experiments on five synthetic and real-world LNL benchmarks demonstrate our
method surpasses state-of-the-art methods in performance, achieves a nearly 75%
reduction in computational time and improves model scalability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decom-Renorm-Merge: Model Merging on the Right Space Improves
  Multitasking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.23117v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.23117v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuatyong Chaichana, Thanapat Trachu, Peerat Limkonchotiwat, Konpat Preechakul, Tirasan Khandhawit, Ekapol Chuangsuwanich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the era of large-scale training, model merging has evolved into a tool for
creating multitasking models efficiently. It enables the knowledge of models to
be fused, without the need for heavy computation as required in traditional
multitask learning. Existing merging methods often assume that entries at
identical positions in weight matrices serve the same function, enabling
straightforward entry-wise comparison and merging. However, this assumption
overlooks the complexity of finetuned neural networks, where neurons may
develop distinct feature compositions, making direct entry-wise merging
problematic. We present Decom-Renorm-Merge (DRM), a simple yet effective
approach that leverages Singular Value Decomposition to decompose and
coordinate weight matrices into an aligned joint space, where entry-wise
merging becomes possible. We showcase the effectiveness of DRM across various
settings ranging from smaller encoder-based such as ViT and DeBERTa,
encoder-decoder-based such as T5, and larger decoder-based such as Llama3.1-8B.
Our experimental results show that DRM outperforms several state-of-the-art
merging techniques across full finetuning and low-rank adaptation settings.
Moreover, our analysis reveals renormalization as the crucial component for
creating a robust and even joint space for merging, significantly contributing
to the method's performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and models are available at
  https://github.com/yophis/decom-renorm-merge</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NGGAN: Noise Generation GAN Based on the Practical Measurement <span class="highlight-title">Dataset</span>
  for Narrowband Powerline Communications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.01850v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.01850v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ying-Ren Chien, Po-Heng Chou, You-Jie Peng, Chun-Yuan Huang, Hen-Wai Tsao, Yu Tsao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To effectively process impulse noise for narrowband powerline communications
(NB-PLCs) transceivers, capturing comprehensive statistics of nonperiodic
asynchronous impulsive noise (APIN) is a critical task. However, existing
mathematical noise generative models only capture part of the characteristics
of noise. In this study, we propose a novel generative adversarial network
(GAN) called noise generation GAN (NGGAN) that learns the complicated
characteristics of practically measured noise samples for data synthesis. To
closely match the statistics of complicated noise over the NB-PLC systems, we
measured the NB-PLC noise via the analog coupling and bandpass filtering
circuits of a commercial NB-PLC modem to build a realistic dataset. To train
NGGAN, we adhere to the following principles: 1) we design the length of input
signals that the NGGAN model can fit to facilitate cyclostationary noise
generation; 2) the Wasserstein distance is used as a loss function to enhance
the similarity between the generated noise and training data; and 3) to measure
the similarity performances of GAN-based models based on the mathematical and
practically measured datasets, we conduct both quantitative and qualitative
analyses. The training datasets include: 1) a piecewise spectral
cyclostationary Gaussian model (PSCGM); 2) a frequency-shift (FRESH) filter;
and 3) practical measurements from NB-PLC systems. Simulation results
demonstrate that the generated noise samples from the proposed NGGAN are highly
close to the real noise samples. The principal component analysis (PCA) scatter
plots and Fr\'echet inception distance (FID) analysis have shown that NGGAN
outperforms other GAN-based models by generating noise samples with superior
fidelity and higher diversity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 15 figures, 11 tables, and published in IEEE Transactions
  on Instrumentation and Measurement, 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A method for the systematic generation of graph XAI benchmarks via
  Weisfeiler-Leman coloring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12437v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12437v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michele Fontanesi, Alessio Micheli, Marco Podda, Domenico Tortorella
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural networks have become the de facto model for learning from
structured data. However, the decision-making process of GNNs remains opaque to
the end user, which undermines their use in safety-critical applications.
Several explainable AI techniques for graphs have been developed to address
this major issue. Focusing on graph classification, these explainers identify
subgraph motifs that explain predictions. Therefore, a robust benchmarking of
graph explainers is required to ensure that the produced explanations are of
high quality, i.e., aligned with the GNN's decision process. However, current
graph-XAI benchmarks are limited to simplistic synthetic datasets or a few
real-world tasks curated by domain experts, hindering rigorous and reproducible
evaluation, and consequently stalling progress in the field. To overcome these
limitations, we propose a method to automate the construction of graph XAI
benchmarks from generic graph classification datasets. Our approach leverages
the Weisfeiler-Leman color refinement algorithm to efficiently perform
approximate subgraph matching and mine class-discriminating motifs, which serve
as proxy ground-truth class explanations. At the same time, we ensure that
these motifs can be learned by GNNs because their discriminating power aligns
with WL expressiveness. This work also introduces the OpenGraphXAI benchmark
suite, which consists of 15 ready-made graph-XAI datasets derived by applying
our method to real-world molecular classification datasets. The suite is
available to the public along with a codebase to generate over 2,000 additional
graph-XAI benchmarks. Finally, we present a use case that illustrates how the
suite can be used to assess the effectiveness of a selection of popular graph
explainers, demonstrating the critical role of a sufficiently large benchmark
collection for improving the significance of experimental results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust LLM Unlearning with MUDMAN: Meta-Unlearning with Disruption
  Masking And Normalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12484v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12484v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Filip Sondej, Yushi Yang, Mikołaj Kniejski, Marcel Windys
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models can retain dangerous knowledge and skills even after
extensive safety fine-tuning, posing both misuse and misalignment risks. Recent
studies show that even specialized unlearning methods can be easily reversed.
To address this, we systematically evaluate many existing and novel components
of unlearning methods and identify ones crucial for irreversible unlearning.
  We introduce Disruption Masking, a technique in which we only allow updating
weights, where the signs of the unlearning gradient and the retaining gradient
are the same. This ensures all updates are non-disruptive.
  Additionally, we identify the need for normalizing the unlearning gradients,
and also confirm the usefulness of meta-learning. We combine these insights
into MUDMAN (Meta-Unlearning with Disruption Masking and Normalization) and
validate its effectiveness at preventing the recovery of dangerous
capabilities. MUDMAN outperforms the prior TAR method by 40%, setting a new
state-of-the-art for robust unlearning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pearl: A Foundation Model for Placing Every Atom in the Right Location 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24670v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24670v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         Genesis Research Team, Alejandro Dobles, Nina Jovic, Kenneth Leidal, Pranav Murugan, David C. Williams, Drausin Wulsin, Nate Gruver, Christina X. Ji, Korrawat Pruegsanusak, Gianluca Scarpellini, Ansh Sharma, Wojciech Swiderski, Andrea Bootsma, Richard Strong Bowen, Charlotte Chen, Jamin Chen, Marc André Dämgen, Benjamin DiFrancesco, J. D. Fishman, Alla Ivanova, Zach Kagin, David Li-Bland, Zuli Liu, Igor Morozov, Jeffrey Ouyang-Zhang, Frank C. Pickard IV, Kushal S. Shah, Ben Shor, Gabriel Monteiro da Silva, Roy Tal, Maxx Tessmer, Carl Tilbury, Cyr Vetcher, Daniel Zeng, Maruan Al-Shedivat, Aleksandra Faust, Evan N. Feinberg, Michael V. LeVine, Matteus Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately predicting the three-dimensional structures of protein-ligand
complexes remains a fundamental challenge in computational drug discovery that
limits the pace and success of therapeutic design. Deep learning methods have
recently shown strong potential as structural prediction tools, achieving
promising accuracy across diverse biomolecular systems. However, their
performance and utility are constrained by scarce experimental data,
inefficient architectures, physically invalid poses, and the limited ability to
exploit auxiliary information available at inference. To address these issues,
we introduce Pearl (Placing Every Atom in the Right Location), a foundation
model for protein-ligand cofolding at scale. Pearl addresses these challenges
with three key innovations: (1) training recipes that include large-scale
synthetic data to overcome data scarcity; (2) architectures that incorporate an
SO(3)-equivariant diffusion module to inherently respect 3D rotational
symmetries, improving generalization and sample efficiency, and (3)
controllable inference, including a generalized multi-chain templating system
supporting both protein and non-polymeric components as well as dual
unconditional/conditional modes. Pearl establishes a new state-of-the-art
performance in protein-ligand cofolding. On the key metric of generating
accurate (RMSD < 2 \r{A}) and physically valid poses, Pearl surpasses AlphaFold
3 and other open source baselines on the public Runs N' Poses and PoseBusters
benchmarks, delivering 14.5% and 14.2% improvements, respectively, over the
next best model. In the pocket-conditional cofolding regime, Pearl delivers
$3.6\times$ improvement on a proprietary set of challenging, real-world drug
targets at the more rigorous RMSD < 1 \r{A} threshold. Finally, we demonstrate
that model performance correlates directly with synthetic dataset size used in
training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>technical report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Auto-Adaptive PINNs with Applications to Phase Transitions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23999v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23999v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Buck, Woojeong Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose an adaptive sampling method for the training of Physics Informed
Neural Networks (PINNs) which allows for sampling based on an arbitrary
problem-specific heuristic which may depend on the network and its gradients.
In particular we focus our analysis on the Allen-Cahn equations, attempting to
accurately resolve the characteristic interfacial regions using a PINN without
any post-hoc resampling. In experiments, we show the effectiveness of these
methods over residual-adaptive frameworks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Continuous Domain Generalization <span class="chip">NeurIPS25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13519v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13519v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zekun Cai, Yiheng Yao, Guangji Bai, Renhe Jiang, Xuan Song, Ryosuke Shibasaki, Liang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world data distributions often shift continuously across multiple latent
factors such as time, geography, and socioeconomic contexts. However, existing
domain generalization approaches typically treat domains as discrete or as
evolving along a single axis (e.g., time). This oversimplification fails to
capture the complex, multidimensional nature of real-world variation. This
paper introduces the task of Continuous Domain Generalization (CDG), which aims
to generalize predictive models to unseen domains defined by arbitrary
combinations of continuous variations. We present a principled framework
grounded in geometric and algebraic theories, showing that optimal model
parameters across domains lie on a low-dimensional manifold. To model this
structure, we propose a Neural Lie Transport Operator (NeuralLio), which
enables structure-preserving parameter transitions by enforcing geometric
continuity and algebraic consistency. To handle noisy or incomplete domain
variation descriptors, we introduce a gating mechanism to suppress irrelevant
dimensions and a local chart-based strategy for robust generalization.
Extensive experiments on synthetic and real-world datasets, including remote
sensing, scientific documents, and traffic forecasting, demonstrate that our
method significantly outperforms existing baselines in both generalization
accuracy and robustness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 9 figures. Accepted by NeurIPS25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SGFusion: Stochastic Geographic Gradient Fusion in Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23455v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23455v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khoa Nguyen, Khang Tran, NhatHai Phan, Cristian Borcea, Ruoming Jin, Issa Khalil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes Stochastic Geographic Gradient Fusion (SGFusion), a novel
training algorithm to leverage the geographic information of mobile users in
Federated Learning (FL). SGFusion maps the data collected by mobile devices
onto geographical zones and trains one FL model per zone, which adapts well to
the data and behaviors of users in that zone. SGFusion models the local
data-based correlation among geographical zones as a hierarchical random graph
(HRG) optimized by Markov Chain Monte Carlo sampling. At each training step,
every zone fuses its local gradient with gradients derived from a small set of
other zones sampled from the HRG. This approach enables knowledge fusion and
sharing among geographical zones in a probabilistic and stochastic gradient
fusion process with self-attention weights, such that "more similar" zones have
"higher probabilities" of sharing gradients with "larger attention weights."
SGFusion remarkably improves model utility without introducing undue
computational cost. Extensive theoretical and empirical results using a
heart-rate prediction dataset collected across 6 countries show that models
trained with SGFusion converge with upper-bounded expected errors and
significantly improve utility in all countries compared to existing approaches
without notable cost in system scalability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tracking the Median of Gradients with a Stochastic Proximal Point Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.12828v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.12828v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabian Schaipp, Guillaume Garrigos, Umut Simsekli, Robert Gower
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There are several applications of stochastic optimization where one can
benefit from a robust estimate of the gradient. For example, domains such as
distributed learning with corrupted nodes, the presence of large outliers in
the training data, learning under privacy constraints, or even heavy-tailed
noise due to the dynamics of the algorithm itself. Here we study SGD with
robust gradient estimators based on estimating the median.
  We first derive iterative methods based on the stochastic proximal point
method for computing the median gradient and generalizations thereof. Then we
propose an algorithm estimating the median gradient across iterations, and find
that several well known methods are particular cases of this framework. For
instance, we observe that different forms of clipping allow to compute online
estimators of the median of gradients, in contrast to (heavy-ball) momentum,
which corresponds to an online estimator of the mean. Finally, we provide a
theoretical framework for an algorithm computing the median gradient across
samples, and show that the resulting method can converge even under
heavy-tailed, state-dependent noise.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TuneNSearch: a hybrid transfer learning and local search approach for
  solving vehicle routing problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.12662v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.12662v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arthur Corrêa, Cristóvão Silva, Liming Xu, Alexandra Brintrup, Samuel Moniz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces TuneNSearch, a hybrid transfer learning and local
search approach for addressing diverse variants of the vehicle routing problem
(VRP). Our method uses reinforcement learning to generate high-quality
solutions, which are subsequently refined by an efficient local search
procedure. To ensure broad adaptability across VRP variants, TuneNSearch begins
with a pre-training phase on the multi-depot VRP (MDVRP), followed by a
fine-tuning phase to adapt it to other problem formulations. The learning phase
utilizes a Transformer-based architecture enhanced with edge-aware attention,
which integrates edge distances directly into the attention mechanism to better
capture spatial relationships inherent to routing problems. We show that the
pre-trained model generalizes effectively to single-depot variants, achieving
performance comparable to models trained specifically on single-depot
instances. Simultaneously, it maintains strong performance on multi-depot
variants, an ability that models pre-trained solely on single-depot problems
lack. For example, on 100-node instances of multi-depot variants, TuneNSearch
outperforms a model pre-trained on the CVRP by 44%. In contrast, on 100-node
instances of single-depot variants, TuneNSearch performs similar to the CVRP
model. To validate the effectiveness of our method, we conduct extensive
computational experiments on public benchmark and randomly generated instances.
Across multiple CVRPLIB datasets, TuneNSearch consistently achieves performance
deviations of less than 3% from the best-known solutions in the literature,
compared to 6-25% for other neural-based models, depending on problem
complexity. Overall, our approach demonstrates strong generalization to
different problem sizes, instance distributions, and VRP formulations, while
maintaining polynomial runtime complexity despite the integration of the local
search algorithm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lift What You Can: Green Online Learning with Heterogeneous Ensembles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.18962v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.18962v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kirsten Köbschall, Sebastian Buschjäger, Raphael Fischer, Lisa Hartung, Stefan Kramer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensemble methods for stream mining necessitate managing multiple models and
updating them as data distributions evolve. Considering the calls for more
sustainability, established methods are however not sufficiently considerate of
ensemble members' computational expenses and instead overly focus on predictive
capabilities. To address these challenges and enable green online learning, we
propose heterogeneous online ensembles (HEROS). For every training step, HEROS
chooses a subset of models from a pool of models initialized with diverse
hyperparameter choices under resource constraints to train. We introduce a
Markov decision process to theoretically capture the trade-offs between
predictive performance and sustainability constraints. Based on this framework,
we present different policies for choosing which models to train on incoming
data. Most notably, we propose the novel $\zeta$-policy, which focuses on
training near-optimal models at reduced costs. Using a stochastic model, we
theoretically prove that our $\zeta$-policy achieves near optimal performance
while using fewer resources compared to the best performing policy. In our
experiments across 11 benchmark datasets, we find empiric evidence that our
$\zeta$-policy is a strong contribution to the state-of-the-art, demonstrating
highly accurate performance, in some cases even outperforming competitors, and
simultaneously being much more resource-friendly.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reinforcement Learning Teachers of Test Time Scaling <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.08388v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.08388v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edoardo Cetin, Tianyu Zhao, Yujin Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training reasoning language models (LMs) with reinforcement learning (RL) for
one-hot correctness inherently relies on the LM being able to explore and solve
its task with some chance at initialization. Furthermore, a key use case of
reasoning LMs is to act as teachers for distilling new students and
cold-starting future RL iterations rather than being deployed themselves. From
these considerations, we introduce a new framework that avoids RL's exploration
challenge by training a new class of Reinforcement-Learned Teachers (RLTs)
focused on yielding the most effective downstream distillation. RLTs are
prompted with both the question and solution to each problem, and tasked to
simply "connect-the-dots" with detailed explanations tailored for their
students. We train RLTs with dense rewards obtained by feeding each explanation
to the student and testing its understanding of the problem's solution. In
practice, the raw outputs of a 7B RLT provide higher final performance on
competition and graduate-level tasks than existing distillation and
cold-starting pipelines that collect and postprocess the reasoning traces of
orders of magnitude larger LMs. Furthermore, RLTs maintain their effectiveness
when training larger students and when applied zero-shot to out-of-distribution
tasks, unlocking new levels of efficiency and re-usability for the RL reasoning
framework. Code available at: https://github.com/SakanaAI/RLT
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Group Interventions on Deep Networks for Causal Discovery in Subsystems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23906v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23906v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wasim Ahmad, Joachim Denzler, Maha Shadaydeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal discovery uncovers complex relationships between variables, enhancing
predictions, decision-making, and insights into real-world systems, especially
in nonlinear multivariate time series. However, most existing methods primarily
focus on pairwise cause-effect relationships, overlooking interactions among
groups of variables, i.e., subsystems and their collective causal influence. In
this study, we introduce gCDMI, a novel multi-group causal discovery method
that leverages group-level interventions on trained deep neural networks and
employs model invariance testing to infer causal relationships. Our approach
involves three key steps. First, we use deep learning to jointly model the
structural relationships among groups of all time series. Second, we apply
group-wise interventions to the trained model. Finally, we conduct model
invariance testing to determine the presence of causal links among variable
groups. We evaluate our method on simulated datasets, demonstrating its
superior performance in identifying group-level causal relationships compared
to existing methods. Additionally, we validate our approach on real-world
datasets, including brain networks and climate ecosystems. Our results
highlight that applying group-level interventions to deep learning models,
combined with invariance testing, can effectively reveal complex causal
structures, offering valuable insights for domains such as neuroscience and
climate science.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE Access. We are working on the revised version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multimodal Recurrent Ensembles for Predicting Brain Responses to
  Naturalistic Movies (Algonauts 2025) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17897v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17897v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Semih Eren, Deniz Kucukahmetler, Nico Scherf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately predicting distributed cortical responses to naturalistic stimuli
requires models that integrate visual, auditory and semantic information over
time. We present a hierarchical multimodal recurrent ensemble that maps
pretrained video, audio, and language embeddings to fMRI time series recorded
while four subjects watched almost 80 hours of movies provided by the Algonauts
2025 challenge. Modality-specific bidirectional RNNs encode temporal dynamics;
their hidden states are fused and passed to a second recurrent layer, and
lightweight subject-specific heads output responses for 1000 cortical parcels.
Training relies on a composite MSE-correlation loss and a curriculum that
gradually shifts emphasis from early sensory to late association regions.
Averaging 100 model variants further boosts robustness. The resulting system
ranked third on the competition leaderboard, achieving an overall Pearson r =
0.2094 and the highest single-parcel peak score (mean r = 0.63) among all
participants, with particularly strong gains for the most challenging subject
(Subject 5). The approach establishes a simple, extensible baseline for future
multimodal brain-encoding benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 2 figures, 1 table. Invited report, CCN 2025 Algonauts
  Project session (3rd-place team). Code:
  https://github.com/erensemih/Algonauts2025_ModalityRNN v3: Added equal
  contribution footnote to author list. Corrected reference list</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SATURN: SAT-based Reinforcement Learning to Unleash Language Model
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.16368v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.16368v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huanyu Liu, Jia Li, Hao Zhu, Kechi Zhang, Yihong Dong, Ge Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How to design reinforcement learning (RL) tasks that effectively unleash the
reasoning capability of large language models (LLMs) remains an open question.
Existing RL tasks (e.g., math, programming, and constructing reasoning tasks)
suffer from three key limitations: (1) Scalability. They rely heavily on human
annotation or expensive LLM synthesis to generate sufficient training data. (2)
Verifiability. LLMs' outputs are hard to verify automatically and reliably. (3)
Controllable Difficulty. Most tasks lack fine-grained difficulty control,
making it hard to train LLMs to develop reasoning ability from easy to hard.
  To address these limitations, we propose Saturn, a SAT-based RL framework
that uses Boolean Satisfiability (SAT) problems to train and evaluate LLMs
reasoning. Saturn enables scalable task construction, rule-based verification,
and precise difficulty control. Saturn designs a curriculum learning pipeline
that continuously improves LLMs' reasoning capability by constructing SAT tasks
of increasing difficulty and training LLMs from easy to hard. To ensure stable
training, we design a principled mechanism to control difficulty transitions.
  We introduce Saturn-2.6k, a dataset of 2,660 SAT problems with varying
difficulty. It supports the evaluation of how LLM reasoning changes with
problem difficulty. We apply Saturn to DeepSeek-R1-Distill-Qwen and obtain
Saturn-1.5B and Saturn-7B. We achieve several notable results: (1) On SAT
problems, Saturn-1.5B and Saturn-7B achieve average pass@3 improvements of
+14.0 and +28.1, respectively. (2) On math and programming tasks, Saturn-1.5B
and Saturn-7B improve average scores by +4.9 and +1.8 on benchmarks (e.g.,
AIME, LiveCodeBench). (3) Compared to the state-of-the-art (SOTA) approach in
constructing RL tasks, Saturn achieves further improvements of +8.8%. We
release the source code, data, and models to support future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Brain-inspired Computational Intelligence via Predictive Coding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.07870v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.07870v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tommaso Salvatori, Ankur Mali, Christopher L. Buckley, Thomas Lukasiewicz, Rajesh P. N. Rao, Karl Friston, Alexander Ororbia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence (AI) is rapidly becoming one of the key technologies
of this century. The majority of results in AI thus far have been achieved
using deep neural networks trained with a learning algorithm called error
backpropagation, always considered biologically implausible. To this end,
recent works have studied learning algorithms for deep neural networks inspired
by the neurosciences. One such theory, called predictive coding (PC), has shown
promising properties that make it potentially valuable for the machine learning
community: it can model information processing in different areas of the brain,
can be used in control and robotics, has a solid mathematical foundation in
variational inference, and performs its computations asynchronously. Inspired
by such properties, works that propose novel PC-like algorithms are starting to
be present in multiple sub-fields of machine learning and AI at large. Here, we
survey such efforts by first providing a broad overview of the history of PC to
provide common ground for the understanding of the recent developments, then by
describing current efforts and results, and concluding with a large discussion
of possible implications and ways forward.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 Pages, 9 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HyperMARL: Adaptive Hypernetworks for Multi-Agent RL <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04233v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04233v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kale-ab Abebe Tessera, Arrasy Rahman, Amos Storkey, Stefano V. Albrecht
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adaptive cooperation in multi-agent reinforcement learning (MARL) requires
policies to express homogeneous, specialised, or mixed behaviours, yet
achieving this adaptivity remains a critical challenge. While parameter sharing
(PS) is standard for efficient learning, it notoriously suppresses the
behavioural diversity required for specialisation. This failure is largely due
to cross-agent gradient interference, a problem we find is surprisingly
exacerbated by the common practice of coupling agent IDs with observations.
Existing remedies typically add complexity through altered objectives, manual
preset diversity levels, or sequential updates -- raising a fundamental
question: can shared policies adapt without these intricacies? We propose a
solution built on a key insight: an agent-conditioned hypernetwork can generate
agent-specific parameters and decouple observation- and agent-conditioned
gradients, directly countering the interference from coupling agent IDs with
observations. Our resulting method, HyperMARL, avoids the complexities of prior
work and empirically reduces policy gradient variance. Across diverse MARL
benchmarks (22 scenarios, up to 30 agents), HyperMARL achieves performance
competitive with six key baselines while preserving behavioural diversity
comparable to non-parameter sharing methods, establishing it as a versatile and
principled approach for adaptive MARL. The code is publicly available at
https://github.com/KaleabTessera/HyperMARL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at the 39th Conference on Neural Information Processing
  Systems (NeurIPS 2025). A preliminary version of this work was presented at
  the CoCoMARL workshop, RLC 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reliable Evaluation and Benchmarks for Statement Autoformalization <span class="chip">EMNLP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.07222v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.07222v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Auguste Poiroux, Gail Weiss, Viktor Kunčak, Antoine Bosselut
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating statement autoformalization, translating natural language
mathematics into formal languages like Lean 4, remains a significant challenge,
with few metrics, datasets, and standards to robustly measure progress. In this
work, we present a comprehensive approach combining improved metrics, robust
benchmarks, and systematic evaluation, to fill this gap. First, we introduce
BEq+, an automated metric that correlates strongly with human judgment, along
with ProofNetVerif, a new dataset for assessing the quality of evaluation
metrics, containing 3,752 annotated examples. Second, we develop two new
autoformalization benchmarks: ProofNet#, a corrected version of ProofNet, and
RLM25, with 619 new pairs of research-level mathematics from six formalization
projects. Through systematic experimentation across these benchmarks, we find
that current techniques can achieve up to 45.1% accuracy on undergraduate
mathematics but struggle with research-level content without proper context.
Our work establishes a reliable foundation for evaluating and advancing
autoformalization systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2025. New benchmarks released, see
  https://github.com/augustepoiroux/RLMEval ,
  https://huggingface.co/datasets/PAug/ProofNetSharp , and
  https://huggingface.co/datasets/PAug/ProofNetVerif . For code, see
  https://github.com/augustepoiroux/LeanInteract</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OmegAMP: Targeted AMP Discovery through Biologically Informed Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.17247v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.17247v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Diogo Soares, Leon Hetzel, Paulina Szymczak, Marcelo Der Torossian Torres, Johanna Sommer, Cesar de la Fuente-Nunez, Fabian Theis, Stephan Günnemann, Ewa Szczurek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning-based antimicrobial peptide (AMP) discovery faces critical
challenges such as limited controllability, lack of representations that
efficiently model antimicrobial properties, and low experimental hit rates. To
address these challenges, we introduce OmegAMP, a framework designed for
reliable AMP generation with increased controllability. Its diffusion-based
generative model leverages a novel conditioning mechanism to achieve
fine-grained control over desired physicochemical properties and to direct
generation towards specific activity profiles, including species-specific
effectiveness. This is further enhanced by a biologically informed encoding
space that significantly improves overall generative performance. Complementing
these generative capabilities, OmegAMP leverages a novel synthetic data
augmentation strategy to train classifiers for AMP filtering, drastically
reducing false positive rates and thereby increasing the likelihood of
experimental success. Our in silico experiments demonstrate that OmegAMP
delivers state-of-the-art performance across key stages of the AMP discovery
pipeline, enabling us to achieve an unprecedented success rate in wet lab
experiments. We tested 25 candidate peptides, 24 of them (96%) demonstrated
antimicrobial activity, proving effective even against multi-drug resistant
strains. Our findings underscore OmegAMP's potential to significantly advance
computational frameworks in the fight against antimicrobial resistance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Transformer</span>s from Compressed Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23665v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23665v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan C. Leon Alcazar, Mattia Soldan, Mohammad Saatialsoruji, Alejandro Pardo, Hani Itani, Juan Camilo Perez, Bernard Ghanem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compressed file formats are the corner stone of efficient data storage and
transmission, yet their potential for representation learning remains largely
underexplored. We introduce TEMPEST (TransformErs froM comPressed
rEpreSenTations), a method that exploits the inherent byte-stream structure of
compressed files to design an effective tokenization and encoding strategy. By
leveraging this compact encoding, a standard transformer can directly learn
semantic representations from compressed data streams, bypassing the need for
raw byte-level processing or full media decoding. Our proposal substantially
reduces the number of tokens required for semantic classification, thereby
lowering both computational complexity and memory usage. Through extensive
experiments across diverse datasets, coding schemes, and modalities, we show
that TEMPEST achieves accuracy competitive wit the state-of-the-art while
delivering efficiency gains in memory and compute.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differential Privacy as a Perk: Federated Learning over Multiple-Access
  Fading Channels with a Multi-Antenna Base Station 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23463v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23463v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Liang, Haifeng Wen, Kaishun Wu, Hong Xing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) is a distributed learning paradigm that preserves
privacy by eliminating the need to exchange raw data during training. In its
prototypical edge instantiation with underlying wireless transmissions enabled
by analog over-the-air computing (AirComp), referred to as \emph{over-the-air
FL (AirFL)}, the inherent channel noise plays a unique role of \emph{frenemy}
in the sense that it degrades training due to noisy global aggregation while
providing a natural source of randomness for privacy-preserving mechanisms,
formally quantified by \emph{differential privacy (DP)}. It remains,
nevertheless, challenging to effectively harness such channel impairments, as
prior arts, under assumptions of either simple channel models or restricted
types of loss functions, mostly considering (local) DP enhancement with a
single-round or non-convergent bound on privacy loss. In this paper, we study
AirFL over multiple-access fading channels with a multi-antenna base station
(BS) subject to user-level DP requirements. Despite a recent study, which
claimed in similar settings that artificial noise (AN) must be injected to
ensure DP in general, we demonstrate, on the contrary, that DP can be gained as
a \emph{perk} even \emph{without} employing any AN. Specifically, we derive a
novel bound on DP that converges under general bounded-domain assumptions on
model parameters, along with a convergence bound with general smooth and
non-convex loss functions. Next, we optimize over receive beamforming and power
allocations to characterize the optimal convergence-privacy trade-offs, which
also reveal explicit conditions in which DP is achievable without compromising
training. Finally, our theoretical findings are validated by extensive
numerical results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 5 figures, submitted for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Robustness of AlphaZero Algorithms to Test-Time Environment
  Changes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04317v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04317v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isidoro Tamassia, Wendelin Böhmer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The AlphaZero framework provides a standard way of combining Monte Carlo
planning with prior knowledge provided by a previously trained policy-value
neural network. AlphaZero usually assumes that the environment on which the
neural network was trained will not change at test time, which constrains its
applicability. In this paper, we analyze the problem of deploying AlphaZero
agents in potentially changed test environments and demonstrate how the
combination of simple modifications to the standard framework can significantly
boost performance, even in settings with a low planning budget available. The
code is publicly available on GitHub.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at the 37th Benelux Conference on Artificial Intelligence
  and the 34th Belgian Dutch Conference on Machine Learning (BNAIC/BeNeLearn
  2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring End-to-end Differentiable Neural Charged Particle Tracking --
  A Loss Landscape Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.13420v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.13420v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobias Kortus, Ralf Keidel, Nicolas R. Gauger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Measurement and analysis of high energetic particles for scientific, medical
or industrial applications is a complex procedure, requiring the design of
sophisticated detector and data processing systems. The development of adaptive
and differentiable software pipelines using a combination of conventional and
machine learning algorithms is therefore getting ever more important to
optimize and operate the system efficiently while maintaining end-to-end (E2E)
differentiability. We propose for the application of charged particle tracking
an E2E differentiable decision-focused learning scheme using graph neural
networks with combinatorial components solving a linear assignment problem for
each detector layer. We demonstrate empirically that including differentiable
variations of discrete assignment operations allows for efficient network
optimization, working better or on par with approaches that lack E2E
differentiability. In additional studies, we dive deeper into the optimization
process and provide further insights from a loss landscape perspective. We
demonstrate that while both methods converge into similar performing, globally
well-connected regions, they suffer under substantial predictive instability
across initialization and optimization methods, which can have unpredictable
consequences on the performance of downstream tasks such as image
reconstruction. We also point out a dependency between the interpolation factor
of the gradient estimator and the prediction stability of the model, suggesting
the choice of sufficiently small values. Given the strong global connectivity
of learned solutions and the excellent training performance, we argue that E2E
differentiability provides, besides the general availability of gradient
information, an important tool for robust particle tracking to mitigate
prediction instabilities by favoring solutions that perform well on downstream
tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Transactions on Machine Learning Research (TMLR), 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Taxonomy and Trends in Reinforcement Learning for Robotics and Control
  Systems: A Structured <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.21758v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.21758v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kumater Ter, Ore-Ofe Ajayi, Daniel Udekwe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) has become a foundational approach for enabling
intelligent robotic behavior in dynamic and uncertain environments. This work
presents an in-depth review of RL principles, advanced deep reinforcement
learning (DRL) algorithms, and their integration into robotic and control
systems. Beginning with the formalism of Markov Decision Processes (MDPs), the
study outlines essential elements of the agent-environment interaction and
explores core algorithmic strategies including actor-critic methods,
value-based learning, and policy gradients. Emphasis is placed on modern DRL
techniques such as DDPG, TD3, PPO, and SAC, which have shown promise in solving
high-dimensional, continuous control tasks. A structured taxonomy is introduced
to categorize RL applications across domains such as locomotion, manipulation,
multi-agent coordination, and human-robot interaction, along with training
methodologies and deployment readiness levels. The review synthesizes recent
research efforts, highlighting technical trends, design patterns, and the
growing maturity of RL in real-world robotics. Overall, this work aims to
bridge theoretical advances with practical implementations, providing a
consolidated perspective on the evolving role of RL in autonomous robotic
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TabArena: A Living Benchmark for Machine Learning on Tabular Data <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.16791v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.16791v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nick Erickson, Lennart Purucker, Andrej Tschalzev, David Holzmüller, Prateek Mutalik Desai, David Salinas, Frank Hutter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the growing popularity of deep learning and foundation models for
tabular data, the need for standardized and reliable benchmarks is higher than
ever. However, current benchmarks are static. Their design is not updated even
if flaws are discovered, model versions are updated, or new models are
released. To address this, we introduce TabArena, the first continuously
maintained living tabular benchmarking system. To launch TabArena, we manually
curate a representative collection of datasets and well-implemented models,
conduct a large-scale benchmarking study to initialize a public leaderboard,
and assemble a team of experienced maintainers. Our results highlight the
influence of validation method and ensembling of hyperparameter configurations
to benchmark models at their full potential. While gradient-boosted trees are
still strong contenders on practical tabular datasets, we observe that deep
learning methods have caught up under larger time budgets with ensembling. At
the same time, foundation models excel on smaller datasets. Finally, we show
that ensembles across models advance the state-of-the-art in tabular machine
learning. We observe that some deep learning models are overrepresented in
cross-model ensembles due to validation set overfitting, and we encourage model
developers to address this issue. We launch TabArena with a public leaderboard,
reproducible code, and maintenance protocols to create a living benchmark
available at https://tabarena.ai.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted (spotlight) at NeurIPS 2025 Datasets and Benchmarks Track.
  v3: NeurIPS camera-ready version. v2: fixed author list. 51 pages. Code
  available at https://tabarena.ai/code; examples at
  https://tabarena.ai/code-examples; dataset curation at
  https://tabarena.ai/data-tabular-ml-iid-study and
  https://tabarena.ai/dataset-curation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GnnXemplar: Exemplars to Explanations -- Natural Language Rules for
  Global GNN Interpretability <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.18376v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.18376v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Burouj Armgaan, Eshan Jain, Harsh Pandey, Mahesh Chandran, Sayan Ranu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) are widely used for node classification, yet
their opaque decision-making limits trust and adoption. While local
explanations offer insights into individual predictions, global explanation
methods, those that characterize an entire class, remain underdeveloped.
Existing global explainers rely on motif discovery in small graphs, an approach
that breaks down in large, real-world settings where subgraph repetition is
rare, node attributes are high-dimensional, and predictions arise from complex
structure-attribute interactions. We propose GnnXemplar, a novel global
explainer inspired from Exemplar Theory from cognitive science. GnnXemplar
identifies representative nodes in the GNN embedding space, exemplars, and
explains predictions using natural language rules derived from their
neighborhoods. Exemplar selection is framed as a coverage maximization problem
over reverse k-nearest neighbors, for which we provide an efficient greedy
approximation. To derive interpretable rules, we employ a self-refining prompt
strategy using large language models (LLMs). Experiments across diverse
benchmarks show that GnnXemplar significantly outperforms existing methods in
fidelity, scalability, and human interpretability, as validated by a user study
with 60 participants.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages, 20 figures, NeurIPS 2025 (Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Probabilistic Kernel Function for Fast Angle Testing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.20274v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.20274v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kejing Lu, Chuan Xiao, Yoshiharu Ishikawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we study the angle testing problem in the context of
similarity search in high-dimensional Euclidean spaces and propose two
projection-based probabilistic kernel functions, one designed for angle
comparison and the other for angle thresholding. Unlike existing approaches
that rely on random projection vectors drawn from Gaussian distributions, our
approach leverages reference angles and employs a deterministic structure for
the projection vectors. Notably, our kernel functions do not require asymptotic
assumptions, such as the number of projection vectors tending to infinity, and
can be both theoretically and experimentally shown to outperform
Gaussian-distribution-based kernel functions. We apply the proposed kernel
function to Approximate Nearest Neighbor Search (ANNS) and demonstrate that our
approach achieves a 2.5X ~ 3X higher query-per-second (QPS) throughput compared
to the widely-used graph-based search algorithm HNSW.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OS-Harm: A Benchmark for Measuring Safety of Computer Use Agents <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14866v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14866v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Kuntz, Agatha Duzan, Hao Zhao, Francesco Croce, Zico Kolter, Nicolas Flammarion, Maksym Andriushchenko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computer use agents are LLM-based agents that can directly interact with a
graphical user interface, by processing screenshots or accessibility trees.
While these systems are gaining popularity, their safety has been largely
overlooked, despite the fact that evaluating and understanding their potential
for harmful behavior is essential for widespread adoption. To address this gap,
we introduce OS-Harm, a new benchmark for measuring safety of computer use
agents. OS-Harm is built on top of the OSWorld environment and aims to test
models across three categories of harm: deliberate user misuse, prompt
injection attacks, and model misbehavior. To cover these cases, we create 150
tasks that span several types of safety violations (harassment, copyright
infringement, disinformation, data exfiltration, etc.) and require the agent to
interact with a variety of OS applications (email client, code editor, browser,
etc.). Moreover, we propose an automated judge to evaluate both accuracy and
safety of agents that achieves high agreement with human annotations (0.76 and
0.79 F1 score). We evaluate computer use agents based on a range of frontier
models - such as o4-mini, Claude 3.7 Sonnet, Gemini 2.5 Pro - and provide
insights into their safety. In particular, all models tend to directly comply
with many deliberate misuse queries, are relatively vulnerable to static prompt
injections, and occasionally perform unsafe actions. The OS-Harm benchmark is
available at https://github.com/tml-epfl/os-harm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025 Datasets & Benchmarks Track (Spotlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SNN-Based Online Learning of Concepts and Action Laws in an Open World 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.12308v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.12308v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christel Grimaud, Dominique Longin, Andreas Herzig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the architecture of a fully autonomous, bio-inspired cognitive
agent built around a spiking neural network (SNN) implementing the agent's
semantic memory. This agent explores its universe and learns concepts of
objects/situations and of its own actions in a one-shot manner. While
object/situation concepts are unary, action concepts are triples made up of an
initial situation, a motor activity, and an outcome. They embody the agent's
knowledge of its universe's action laws. Both kinds of concepts have different
degrees of generality. To make decisions the agent queries its semantic memory
for the expected outcomes of envisaged actions and chooses the action to take
on the basis of these predictions. Our experiments show that the agent handles
new situations by appealing to previously learned general concepts and rapidly
modifies its concepts to adapt to environment changes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning-Augmented Online Bipartite Fractional Matching <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.19252v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.19252v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Davin Choo, Billy Jin, Yongho Shin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online bipartite matching is a fundamental problem in online optimization,
extensively studied both in its integral and fractional forms due to its
theoretical significance and practical applications, such as online advertising
and resource allocation. Motivated by recent progress in learning-augmented
algorithms, we study online bipartite fractional matching when the algorithm is
given advice in the form of a suggested matching in each iteration. We develop
algorithms for both the vertex-weighted and unweighted variants that provably
dominate the naive "coin flip" strategy of randomly choosing between the
advice-following and advice-free algorithms. Moreover, our algorithm for the
vertex-weighted setting extends to the AdWords problem under the small bids
assumption, yielding a significant improvement over the seminal work of
Mahdian, Nazerzadeh, and Saberi (EC 2007, TALG 2012). Complementing our
positive results, we establish a hardness bound on the robustness-consistency
tradeoff that is attainable by any algorithm. We empirically validate our
algorithms through experiments on synthetic and real-world data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in NeurIPS 2025. Full version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differential Mamba <span class="chip">AACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.06204v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.06204v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nadav Schneider, Itamar Zimerman, Eliya Nachmani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequence models like Transformers and RNNs often overallocate attention to
irrelevant context, leading to noisy intermediate representations. This
degrades LLM capabilities by promoting hallucinations, weakening long-range and
retrieval abilities, and reducing robustness. Recent work has shown that
differential design can mitigate this issue in Transformers, improving their
effectiveness across various applications. In this paper, we explore whether
these techniques, originally developed for Transformers, can be applied to
Mamba, a recent architecture based on selective state-space layers that
achieves Transformer-level performance with greater efficiency. We show that a
naive adaptation of differential design to Mamba is insufficient and requires
careful architectural modifications. To address this, we introduce a novel
differential mechanism for Mamba, empirically validated on language modeling
benchmarks, demonstrating improved retrieval capabilities and superior
performance over vanilla Mamba. Finally, we conduct extensive ablation studies
and empirical analyses to justify our design choices and provide evidence that
our approach effectively mitigates the overallocation problem in Mamba-based
models. Our code is publicly available: https://github.com/NadavSc/Diff-Mamba
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AACL 2025. We provide the code at
  https://github.com/NadavSc/Diff-Mamba</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Redistributing Rewards Across Time and Agents for Multi-Agent
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.04864v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.04864v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditya Kapoor, Kale-ab Tessera, Mayank Baranwal, Harshad Khadilkar, Jan Peters, Stefano Albrecht, Mingfei Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Credit assignmen, disentangling each agent's contribution to a shared reward,
is a critical challenge in cooperative multi-agent reinforcement learning
(MARL). To be effective, credit assignment methods must preserve the
environment's optimal policy. Some recent approaches attempt this by enforcing
return equivalence, where the sum of distributed rewards must equal the team
reward. However, their guarantees are conditional on a learned model's
regression accuracy, making them unreliable in practice. We introduce
Temporal-Agent Reward Redistribution (TAR$^2$), an approach that decouples
credit modeling from this constraint. A neural network learns unnormalized
contribution scores, while a separate, deterministic normalization step
enforces return equivalence by construction. We demonstrate that this method is
equivalent to a valid Potential-Based Reward Shaping (PBRS), which guarantees
the optimal policy is preserved regardless of model accuracy. Empirically, on
challenging SMACLite and Google Research Football (GRF) benchmarks, TAR$^2$
accelerates learning and achieves higher final performance than strong
baselines. These results establish our method as an effective solution for the
agent-temporal credit assignment problem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 4 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">7</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ALDEN: Reinforcement Learning for Active Navigation and Evidence
  Gathering in Long Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25668v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25668v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyu Yang, Terry Ruas, Yijun Tian, Jan Philip Wahle, Daniel Kurzawe, Bela Gipp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models (VLMs) excel at interpreting text-rich images but
struggle with long, visually complex documents that demand analysis and
integration of information spread across multiple pages. Existing approaches
typically rely on fixed reasoning templates or rigid pipelines, which force
VLMs into a passive role and hinder both efficiency and generalization. We
present Active Long-DocumEnt Navigation (ALDEN), a multi-turn reinforcement
learning framework that fine-tunes VLMs as interactive agents capable of
actively navigating long, visually rich documents. ALDEN introduces a novel
fetch action that directly accesses the page by index, complementing the
classic search action and better exploiting document structure. For dense
process supervision and efficient training, we propose a rule-based cross-level
reward that provides both turn- and token-level signals. To address the
empirically observed training instability caused by numerous visual tokens from
long documents, we further propose a visual-semantic anchoring mechanism that
applies a dual-path KL-divergence constraint to stabilize visual and textual
representations separately during training. Trained on a corpus constructed
from three open-source datasets, ALDEN achieves state-of-the-art performance on
five long-document benchmarks. Overall, ALDEN marks a step beyond passive
document reading toward agents that autonomously navigate and reason across
long, visually rich documents, offering a robust path to more accurate and
efficient long-document understanding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PureKV: Plug-and-Play KV Cache Optimization with Spatial-Temporal Sparse
  Attention for Vision-Language Large Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25600v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25600v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhonghua Jiang, Kunxi Li, Yiyun Zhou, Sihao Liu, Zhaode Wang, Chengfei lv, Shengyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language Large Models (VLLMs) face significant efficiency challenges
when processing high-resolution inputs. The quadratic complexity in attention
and autoregressive generation, as well as the constantly growing key value (KV)
cache size, severely hinder the prefilling and decoding stages. Recent efforts
have attempted to compress KV cache by identifying and pruning KV cache of less
important tokens, but these methods typically rely on attention scores to
estimate token importance, making them incompatible with efficient attention
mechanisms such as FlashAttention and Sparse Attention, which do not explicitly
compute attention matrices. Moreover, existing methods overlook how sparse
attention, while accelerating the prefilling stage, alters the information
structure of the KV cache, thereby compromising the effectiveness of downstream
KV cache compression strategies. To address this issue, we propose PureKV, a
plug-and-play framework for joint optimization of sparse attention and KV cache
compression. We first introduce a KV cache compression strategy that is fully
compatible with efficient attention accelerators. Our method utilizes lower
layer attention scores to estimate the importance of high layers' KV cache,
enabling active pruning without compromising accuracy. In addition, we have
designed a Spatial-Temporal Sparse Attention (ST-SpAttn) module specifically
tailored for video KV cache compression algorithms. This module combines
spatial and temporal attention sparsity to improve the compression efficiency
of KV cache optimization algorithms by purifying spatial noise and temporal
redundancy in KV cache. At the same time, ST-SpAttn also accelerated the
prefilling stage of VLLMs. Extensive experiments on VLLMs (VideoLLaMA2,
Qwen2.5-VL) have shown that PureKV achieves 5.0 times KV cache compression and
3.16 times prefill acceleration, with negligible quality degradation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Energy consumption assessment of a Virtual Reality Remote Rendering
  application over 5G networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25357v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25357v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roberto Viola, Mikel Irazola, José Ramón Juárez, Minh Nguyen, Alexander Zoubarev, Alexander Futasz, Louay Bassbouss, Amr A. AbdelNabi, Javier Fernández Hidalgo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the energy implications of remote rendering for
Virtual Reality (VR) applications within a real 5G testbed. Remote rendering
enables lightweight devices to access high-performance graphical content by
offloading computationally intensive tasks to Cloud-native Network Functions
(CNFs) running on remote servers. However, this approach raises concerns
regarding energy consumption across the various network components involved,
including the remote computing node, the 5G Core, the Radio Access Network
(RAN), and the User Equipment (UE). This work proposes and evaluates two
complementary energy monitoring solutions, one hardware-based and one
software-based, to measure energy consumption at different system levels. A VR
remote renderer, deployed as CNF and leveraging the Media over QUIC (MoQ)
protocol, is used as test case for assessing its energy footprint under
different multimedia and network configurations. The results provide critical
insights into the trade-off between energy consumption and performance of a
real-world VR application running in a 5G environment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hallucination Localization in Video Captioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25225v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25225v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shota Nakada, Kazuhiro Saito, Yuchi Ishikawa, Hokuto Munakata, Tatsuya Komatsu, Masayoshi Kondo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel task, hallucination localization in video captioning,
which aims to identify hallucinations in video captions at the span level (i.e.
individual words or phrases). This allows for a more detailed analysis of
hallucinations compared to existing sentence-level hallucination detection
task. To establish a benchmark for hallucination localization, we construct
HLVC-Dataset, a carefully curated dataset created by manually annotating 1,167
video-caption pairs from VideoLLM-generated captions. We further implement a
VideoLLM-based baseline method and conduct quantitative and qualitative
evaluations to benchmark current performance on hallucination localization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Performance Evaluation of Multimedia Traffic in Cloud Storage Services
  over Wi-Fi and LTE Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25079v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25079v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Albert Espinal, V. Sanchez Padilla, Yesenia Cevallos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The performance of Dropbox, Google Drive, and OneDrive cloud storage services
was evaluated under Wi-Fi and LTE network conditions during multimedia file
uploads. Traffic was captured using Wireshark, and key metrics (including
delay, jitter, bandwidth, and packet loss) were analyzed. Google Drive
maintained the most consistent performance across both types of networks,
showing low latency and reduced jitter. Dropbox showed efficient bandwidth
utilization, but experienced a longer delay over LTE, attributed to a greater
number of intermediate hops. OneDrive presented variable behavior, with
elevated packet rates and increased sensitivity to fluctuations in the mobile
network. A bimodal distribution of packet sizes was observed and modeled using
a dual Poisson function. In general, Wi-Fi connections provided greater
stability for multimedia transfers, while LTE performance varied depending on
platform-specific implementations. The results contribute to a better
understanding of traffic behavior in cloud-based storage applications and
suggest further analysis with larger datasets and heterogeneous access
networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2025 20th Iberian Conference on Information Systems and Technologies
  (CISTI), Lecture Notes in Networks and Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Artificial Neural Networks Trained on Noisy Speech Exhibit the McGurk
  Effect 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05715v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05715v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Grasse, Matthew S. Tata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans are able to fuse information from both auditory and visual modalities
to help with understanding speech. This is demonstrated through a phenomenon
known as the McGurk Effect, during which a listener is presented with
incongruent auditory and visual speech that fuse together into the percept of
illusory intermediate phonemes. Building on a recent framework that proposes
how to address developmental 'why' questions using artificial neural networks,
we evaluated a set of recent artificial neural networks trained on audiovisual
speech by testing them with audiovisually incongruent words designed to elicit
the McGurk effect. We show that networks trained entirely on congruent
audiovisual speech nevertheless exhibit the McGurk percept. We further
investigated 'why' by comparing networks trained on clean speech to those
trained on noisy speech, and discovered that training with noisy speech led to
a pronounced increase in both visual responses and McGurk responses across all
models. Furthermore, we observed that systematically increasing the level of
auditory noise during ANN training also increased the amount of audiovisual
integration up to a point, but at extreme noise levels, this integration failed
to develop. These results suggest that excessive noise exposure during critical
periods of audiovisual learning may negatively influence the development of
audiovisual speech integration. This work also demonstrates that the McGurk
effect reliably emerges untrained from the behaviour of both supervised and
unsupervised networks, even networks trained only on congruent speech. This
supports the notion that artificial neural networks might be useful models for
certain aspects of perception and cognition.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ XY-Cut++: Advanced Layout Ordering via Hierarchical Mask Mechanism on a
  Novel Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.10258v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.10258v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuai Liu, Youmeng Li, Jizeng Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Document Reading Order Recovery is a fundamental task in document image
understanding, playing a pivotal role in enhancing Retrieval-Augmented
Generation (RAG) and serving as a critical preprocessing step for large
language models (LLMs). Existing methods often struggle with complex
layouts(e.g., multi-column newspapers), high-overhead interactions between
cross-modal elements (visual regions and textual semantics), and a lack of
robust evaluation benchmarks. We introduce XY-Cut++, an advanced layout
ordering method that integrates pre-mask processing, multi-granularity
segmentation, and cross-modal matching to address these challenges. Our method
significantly enhances layout ordering accuracy compared to traditional XY-Cut
techniques. Specifically, XY-Cut++ achieves state-of-the-art performance (98.8
BLEU overall) while maintaining simplicity and efficiency. It outperforms
existing baselines by up to 24\% and demonstrates consistent accuracy across
simple and complex layouts on the newly introduced DocBench-100 dataset. This
advancement establishes a reliable foundation for document structure recovery,
setting a new standard for layout ordering tasks and facilitating more
effective RAG and LLM preprocessing.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-10-28T00:00:00Z">2025-10-28</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">54</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MetricX-25 and GemSpanEval: Google Translate Submissions to the WMT25
  Evaluation Shared Task 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24707v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24707v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juraj Juraska, Tobias Domhan, Mara Finkelstein, Tetsuji Nakagawa, Geza Kovacs, Daniel Deutsch, Pidong Wang, Markus Freitag
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present our submissions to the unified WMT25 Translation
Evaluation Shared Task. For the Quality Score Prediction subtask, we create a
new generation of MetricX with improvements in the input format and the
training protocol, while for the Error Span Detection subtask we develop a new
model, GemSpanEval, trained to predict error spans along with their severities
and categories. Both systems are based on the state-of-the-art multilingual
open-weights model Gemma 3, fine-tuned on publicly available WMT data. We
demonstrate that MetricX-25, adapting Gemma 3 to an encoder-only architecture
with a regression head on top, can be trained to effectively predict both MQM
and ESA quality scores, and significantly outperforms its predecessor. Our
decoder-only GemSpanEval model, on the other hand, we show to be competitive in
error span detection with xCOMET, a strong encoder-only sequence-tagging
baseline. With error span detection formulated as a generative task, we
instruct the model to also output the context for each predicted error span,
thus ensuring that error spans are identified unambiguously.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to WMT25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ComboBench: Can LLMs Manipulate Physical Devices to Play Virtual Reality
  Games? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24706v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24706v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuqing Li, Jiayi Yan, Chenyu Niu, Jen-tse Huang, Yun Peng, Wenxuan Wang, Yepang Liu, Michael R. Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Virtual Reality (VR) games require players to translate high-level semantic
actions into precise device manipulations using controllers and head-mounted
displays (HMDs). While humans intuitively perform this translation based on
common sense and embodied understanding, whether Large Language Models (LLMs)
can effectively replicate this ability remains underexplored. This paper
introduces a benchmark, ComboBench, evaluating LLMs' capability to translate
semantic actions into VR device manipulation sequences across 262 scenarios
from four popular VR games: Half-Life: Alyx, Into the Radius, Moss: Book II,
and Vivecraft. We evaluate seven LLMs, including GPT-3.5, GPT-4, GPT-4o,
Gemini-1.5-Pro, LLaMA-3-8B, Mixtral-8x7B, and GLM-4-Flash, compared against
annotated ground truth and human performance. Our results reveal that while
top-performing models like Gemini-1.5-Pro demonstrate strong task decomposition
capabilities, they still struggle with procedural reasoning and spatial
understanding compared to humans. Performance varies significantly across
games, suggesting sensitivity to interaction complexity. Few-shot examples
substantially improve performance, indicating potential for targeted
enhancement of LLMs' VR manipulation capabilities. We release all materials at
https://sites.google.com/view/combobench.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Agent Data Protocol: Unifying <span class="highlight-title">Dataset</span>s for Diverse, Effective
  Fine-tuning of LLM Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24702v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24702v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yueqi Song, Ketan Ramaneti, Zaid Sheikh, Ziru Chen, Boyu Gou, Tianbao Xie, Yiheng Xu, Danyang Zhang, Apurva Gandhi, Fan Yang, Joseph Liu, Tianyue Ou, Zhihao Yuan, Frank Xu, Shuyan Zhou, Xingyao Wang, Xiang Yue, Tao Yu, Huan Sun, Yu Su, Graham Neubig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Public research results on large-scale supervised finetuning of AI agents
remain relatively rare, since the collection of agent training data presents
unique challenges. In this work, we argue that the bottleneck is not a lack of
underlying data sources, but that a large variety of data is fragmented across
heterogeneous formats, tools, and interfaces. To this end, we introduce the
agent data protocol (ADP), a light-weight representation language that serves
as an "interlingua" between agent datasets in diverse formats and unified agent
training pipelines downstream. The design of ADP is expressive enough to
capture a large variety of tasks, including API/tool use, browsing, coding,
software engineering, and general agentic workflows, while remaining simple to
parse and train on without engineering at a per-dataset level. In experiments,
we unified a broad collection of 13 existing agent training datasets into ADP
format, and converted the standardized ADP data into training-ready formats for
multiple agent frameworks. We performed SFT on these data, and demonstrated an
average performance gain of ~20% over corresponding base models, and delivers
state-of-the-art or near-SOTA performance on standard coding, browsing, tool
use, and research benchmarks, without domain-specific tuning. All code and data
are released publicly, in the hope that ADP could help lower the barrier to
standardized, scalable, and reproducible agent training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tongyi DeepResearch Technical Report 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24701v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24701v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         Tongyi DeepResearch Team, Baixuan Li, Bo Zhang, Dingchu Zhang, Fei Huang, Guangyu Li, Guoxin Chen, Huifeng Yin, Jialong Wu, Jingren Zhou, Kuan Li, Liangcai Su, Litu Ou, Liwen Zhang, Pengjun Xie, Rui Ye, Wenbiao Yin, Xinmiao Yu, Xinyu Wang, Xixi Wu, Xuanzhong Chen, Yida Zhao, Zhen Zhang, Zhengwei Tao, Zhongwang Zhang, Zile Qiao, Chenxi Wang, Donglei Yu, Gang Fu, Haiyang Shen, Jiayin Yang, Jun Lin, Junkai Zhang, Kui Zeng, Li Yang, Hailong Yin, Maojia Song, Ming Yan, Peng Xia, Qian Xiao, Rui Min, Ruixue Ding, Runnan Fang, Shaowei Chen, Shen Huang, Shihang Wang, Shihao Cai, Weizhou Shen, Xiaobin Wang, Xin Guan, Xinyu Geng, Yingcheng Shi, Yuning Wu, Zhuo Chen, Zijian Li, Yong Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Tongyi DeepResearch, an agentic large language model, which is
specifically designed for long-horizon, deep information-seeking research
tasks. To incentivize autonomous deep research agency, Tongyi DeepResearch is
developed through an end-to-end training framework that combines agentic
mid-training and agentic post-training, enabling scalable reasoning and
information seeking across complex tasks. We design a highly scalable data
synthesis pipeline that is fully automatic, without relying on costly human
annotation, and empowers all training stages. By constructing customized
environments for each stage, our system enables stable and consistent
interactions throughout. Tongyi DeepResearch, featuring 30.5 billion total
parameters, with only 3.3 billion activated per token, achieves
state-of-the-art performance across a range of agentic deep research
benchmarks, including Humanity's Last Exam, BrowseComp, BrowseComp-ZH,
WebWalkerQA, xbench-DeepSearch, FRAMES and xbench-DeepSearch-2510. We
open-source the model, framework, and complete solutions to empower the
community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://tongyi-agent.github.io/blog</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ParallelMuse: Agentic Parallel Thinking for Deep Information Seeking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24698v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24698v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baixuan Li, Dingchu Zhang, Jialong Wu, Wenbiao Yin, Zhengwei Tao, Yida Zhao, Liwen Zhang, Haiyang Shen, Runnan Fang, Pengjun Xie, Jingren Zhou, Yong Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parallel thinking expands exploration breadth, complementing the deep
exploration of information-seeking (IS) agents to further enhance
problem-solving capability. However, conventional parallel thinking faces two
key challenges in this setting: inefficiency from repeatedly rolling out from
scratch, and difficulty in integrating long-horizon reasoning trajectories
during answer generation, as limited context capacity prevents full
consideration of the reasoning process. To address these issues, we propose
ParallelMuse, a two-stage paradigm designed for deep IS agents. The first
stage, Functionality-Specified Partial Rollout, partitions generated sequences
into functional regions and performs uncertainty-guided path reuse and
branching to enhance exploration efficiency. The second stage, Compressed
Reasoning Aggregation, exploits reasoning redundancy to losslessly compress
information relevant to answer derivation and synthesize a coherent final
answer. Experiments across multiple open-source agents and benchmarks
demonstrate up to 62% performance improvement with a 10--30% reduction in
exploratory token consumption.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AgentFold: Long-Horizon Web Agents with Proactive Context Management 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24699v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24699v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Ye, Zhongwang Zhang, Kuan Li, Huifeng Yin, Zhengwei Tao, Yida Zhao, Liangcai Su, Liwen Zhang, Zile Qiao, Xinyu Wang, Pengjun Xie, Fei Huang, Siheng Chen, Jingren Zhou, Yong Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLM-based web agents show immense promise for information seeking, yet their
effectiveness on long-horizon tasks is hindered by a fundamental trade-off in
context management. Prevailing ReAct-based agents suffer from context
saturation as they accumulate noisy, raw histories, while methods that fixedly
summarize the full history at each step risk the irreversible loss of critical
details. Addressing these, we introduce AgentFold, a novel agent paradigm
centered on proactive context management, inspired by the human cognitive
process of retrospective consolidation. AgentFold treats its context as a
dynamic cognitive workspace to be actively sculpted, rather than a passive log
to be filled. At each step, it learns to execute a `folding' operation, which
manages its historical trajectory at multiple scales: it can perform granular
condensations to preserve vital, fine-grained details, or deep consolidations
to abstract away entire multi-step sub-tasks. The results on prominent
benchmarks are striking: with simple supervised fine-tuning (without continual
pre-training or RL), our AgentFold-30B-A3B agent achieves 36.2% on BrowseComp
and 47.3% on BrowseComp-ZH. Notably, this performance not only surpasses or
matches open-source models of a dramatically larger scale, such as the
DeepSeek-V3.1-671B-A37B, but also surpasses leading proprietary agents like
OpenAI's o4-mini.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WebLeaper: Empowering Efficiency and Efficacy in WebAgent via Enabling
  Info-Rich Seeking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24697v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24697v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengwei Tao, Haiyang Shen, Baixuan Li, Wenbiao Yin, Jialong Wu, Kuan Li, Zhongwang Zhang, Huifeng Yin, Rui Ye, Liwen Zhang, Xinyu Wang, Pengjun Xie, Jingren Zhou, Yong Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Model (LLM)-based agents have emerged as a transformative
approach for open-ended problem solving, with information seeking (IS) being a
core capability that enables autonomous reasoning and decision-making. While
prior research has largely focused on improving retrieval depth, we observe
that current IS agents often suffer from low search efficiency, which in turn
constrains overall performance. A key factor underlying this inefficiency is
the sparsity of target entities in training tasks, which limits opportunities
for agents to learn and generalize efficient search behaviors. To address these
challenges, we propose WebLeaper, a framework for constructing high-coverage IS
tasks and generating efficient solution trajectories. We formulate IS as a
tree-structured reasoning problem, enabling a substantially larger set of
target entities to be embedded within a constrained context. Leveraging curated
Wikipedia tables, we propose three variants for synthesizing IS tasks, Basic,
Union, and Reverse-Union, to systematically increase both IS efficiency and
efficacy. Finally, we curate training trajectories by retaining only those that
are simultaneously accurate and efficient, ensuring that the model is optimized
for both correctness and search performance. Extensive experiments on both
basic and comprehensive settings, conducted on five IS benchmarks, BrowserComp,
GAIA, xbench-DeepSearch, WideSearch, and Seal-0, demonstrate that our method
consistently achieves improvements in both effectiveness and efficiency over
strong baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AgentFrontier: Expanding the Capability Frontier of LLM Agents with
  ZPD-Guided Data Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuanzhong Chen, Zile Qiao, Guoxin Chen, Liangcai Su, Zhen Zhang, Xinyu Wang, Pengjun Xie, Fei Huang, Jingren Zhou, Yong Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training large language model agents on tasks at the frontier of their
capabilities is key to unlocking advanced reasoning. We introduce a data
synthesis approach inspired by the educational theory of the Zone of Proximal
Development (ZPD), which defines this frontier as tasks an LLM cannot solve
alone but can master with guidance. To operationalize this, we present the
AgentFrontier Engine, an automated pipeline that synthesizes high-quality,
multidisciplinary data situated precisely within the LLM's ZPD. This engine
supports both continued pre-training with knowledge-intensive data and targeted
post-training on complex reasoning tasks. From the same framework, we derive
the ZPD Exam, a dynamic and automated benchmark designed to evaluate agent
capabilities on these frontier tasks. We train AgentFrontier-30B-A3B model on
our synthesized data, which achieves state-of-the-art results on demanding
benchmarks like Humanity's Last Exam, even surpassing some leading proprietary
agents. Our work demonstrates that a ZPD-guided approach to data synthesis
offers a scalable and effective path toward building more capable LLM agents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Repurposing Synthetic Data for Fine-grained Search Agent Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24694v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24694v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yida Zhao, Kuan Li, Xixi Wu, Liwen Zhang, Dingchu Zhang, Baixuan Li, Maojia Song, Zhuo Chen, Chenxi Wang, Xinyu Wang, Kewei Tu, Pengjun Xie, Jingren Zhou, Yong Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLM-based search agents are increasingly trained on entity-centric synthetic
data to solve complex, knowledge-intensive tasks. However, prevailing training
methods like Group Relative Policy Optimization (GRPO) discard this rich entity
information, relying instead on sparse, outcome-based rewards. This critical
limitation renders them unable to distinguish informative "near-miss"
samples-those with substantially correct reasoning but a flawed final
answer-from complete failures, thus discarding valuable learning signals. We
address this by leveraging the very entities discarded during training. Our
empirical analysis reveals a strong positive correlation between the number of
ground-truth entities identified during an agent's reasoning process and final
answer accuracy. Building on this insight, we introduce Entity-aware Group
Relative Policy Optimization (E-GRPO), a novel framework that formulates a
dense entity-aware reward function. E-GRPO assigns partial rewards to incorrect
samples proportional to their entity match rate, enabling the model to
effectively learn from these "near-misses". Experiments on diverse
question-answering (QA) and deep research benchmarks show that E-GRPO
consistently and significantly outperforms the GRPO baseline. Furthermore, our
analysis reveals that E-GRPO not only achieves superior accuracy but also
induces more efficient reasoning policies that require fewer tool calls,
demonstrating a more effective and sample-efficient approach to aligning search
agents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ StorageXTuner: An LLM Agent-Driven Automatic Tuning Framework for
  Heterogeneous Storage Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25017v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25017v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Lin, Zhenyu Zhang, Viraj Thakkar, Zhenjie Sun, Mai Zheng, Zhichao Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatically configuring storage systems is hard: parameter spaces are large
and conditions vary across workloads, deployments, and versions. Heuristic and
ML tuners are often system specific, require manual glue, and degrade under
changes. Recent LLM-based approaches help but usually treat tuning as a
single-shot, system-specific task, which limits cross-system reuse, constrains
exploration, and weakens validation. We present StorageXTuner, an LLM
agent-driven auto-tuning framework for heterogeneous storage engines.
StorageXTuner separates concerns across four agents - Executor (sandboxed
benchmarking), Extractor (performance digest), Searcher (insight-guided
configuration exploration), and Reflector (insight generation and management).
The design couples an insight-driven tree search with layered memory that
promotes empirically validated insights and employs lightweight checkers to
guard against unsafe actions. We implement a prototype and evaluate it on
RocksDB, LevelDB, CacheLib, and MySQL InnoDB with YCSB, MixGraph, and TPC-H/C.
Relative to out-of-the-box settings and to ELMo-Tune, StorageXTuner reaches up
to 575% and 111% higher throughput, reduces p99 latency by as much as 88% and
56%, and converges with fewer trials.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ArXiv version; Affiliations: Arizona State University (Lin, Zhang,
  Thakkar, Sun, Cao) and Iowa State University (Zheng)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Emergence of Minimal Circuits for Indirect Object Identification in
  Attention-Only <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25013v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25013v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rabin Adhikari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mechanistic interpretability aims to reverse-engineer large language models
(LLMs) into human-understandable computational circuits. However, the
complexity of pretrained models often obscures the minimal mechanisms required
for specific reasoning tasks. In this work, we train small, attention-only
transformers from scratch on a symbolic version of the Indirect Object
Identification (IOI) task -- a benchmark for studying coreference -- like
reasoning in transformers. Surprisingly, a single-layer model with only two
attention heads achieves perfect IOI accuracy, despite lacking MLPs and
normalization layers. Through residual stream decomposition, spectral analysis,
and embedding interventions, we find that the two heads specialize into
additive and contrastive subcircuits that jointly implement IOI resolution.
Furthermore, we show that a two-layer, one-head model achieves similar
performance by composing information across layers through query-value
interactions. These results demonstrate that task-specific training induces
highly interpretable, minimal circuits, offering a controlled testbed for
probing the computational foundations of transformer reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ POWSM: A Phonetic Open Whisper-Style Speech Foundation Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24992v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24992v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chin-Jou Li, Kalvin Chang, Shikhar Bharadwaj, Eunjung Yeo, Kwanghee Choi, Jian Zhu, David Mortensen, Shinji Watanabe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in spoken language processing have led to substantial
progress in phonetic tasks such as automatic speech recognition (ASR), phone
recognition (PR), grapheme-to-phoneme conversion (G2P), and phoneme-to-grapheme
conversion (P2G). Despite their conceptual similarity, these tasks have largely
been studied in isolation, each relying on task-specific architectures and
datasets. In this paper, we introduce POWSM (Phonetic Open Whisper-style Speech
Model), the first unified framework capable of jointly performing multiple
phone-related tasks. POWSM enables seamless conversion between audio, text
(graphemes), and phones, opening up new possibilities for universal and
low-resource speech processing. Our model outperforms or matches specialized PR
models of similar size (Wav2Vec2Phoneme and ZIPA) while jointly supporting G2P,
P2G, and ASR. Our training data, code and models are released to foster open
science.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sequences of Logits Reveal the Low Rank Structure of Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24966v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24966v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Noah Golowich, Allen Liu, Abhishek Shetty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A major problem in the study of large language models is to understand their
inherent low-dimensional structure. We introduce an approach to study the
low-dimensional structure of language models at a model-agnostic level: as
sequential probabilistic models. We first empirically demonstrate that a wide
range of modern language models exhibit low-rank structure: in particular,
matrices built from the model's logits for varying sets of prompts and
responses have low approximate rank. We then show that this low-rank structure
can be leveraged for generation -- in particular, we can generate a response to
a target prompt using a linear combination of the model's outputs on unrelated,
or even nonsensical prompts.
  On the theoretical front, we observe that studying the approximate rank of
language models in the sense discussed above yields a simple universal
abstraction whose theoretical predictions parallel our experiments. We then
analyze the representation power of the abstraction and give provable learning
guarantees.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Model Behavioral Phases are Consistent Across Architecture,
  Training Data, and Scale <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24963v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24963v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        James A. Michaelov, Roger P. Levy, Benjamin K. Bergen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We show that across architecture (Transformer vs. Mamba vs. RWKV), training
dataset (OpenWebText vs. The Pile), and scale (14 million parameters to 12
billion parameters), autoregressive language models exhibit highly consistent
patterns of change in their behavior over the course of pretraining. Based on
our analysis of over 1,400 language model checkpoints on over 110,000 tokens of
English, we find that up to 98% of the variance in language model behavior at
the word level can be explained by three simple heuristics: the unigram
probability (frequency) of a given word, the $n$-gram probability of the word,
and the semantic similarity between the word and its context. Furthermore, we
see consistent behavioral phases in all language models, with their predicted
probabilities for words overfitting to those words' $n$-gram probabilities for
increasing $n$ over the course of training. Taken together, these results
suggest that learning in neural language models may follow a similar trajectory
irrespective of model details.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be presented at NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Finding Culture-Sensitive Neurons in Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24942v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24942v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiutian Zhao, Rochelle Choenni, Rohit Saxena, Ivan Titov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite their impressive performance, vision-language models (VLMs) still
struggle on culturally situated inputs. To understand how VLMs process
culturally grounded information, we study the presence of culture-sensitive
neurons, i.e. neurons whose activations show preferential sensitivity to inputs
associated with particular cultural contexts. We examine whether such neurons
are important for culturally diverse visual question answering and where they
are located. Using the CVQA benchmark, we identify neurons of culture
selectivity and perform causal tests by deactivating the neurons flagged by
different identification methods. Experiments on three VLMs across 25 cultural
groups demonstrate the existence of neurons whose ablation disproportionately
harms performance on questions about the corresponding cultures, while having
minimal effects on others. Moreover, we propose a new margin-based selector -
Contrastive Activation Selection (CAS), and show that it outperforms existing
probability- and entropy-based methods in identifying culture-sensitive
neurons. Finally, our layer-wise analyses reveals that such neurons tend to
cluster in certain decoder layers. Overall, our findings shed new light on the
internal organization of multimodal representations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SemCoT: Accelerating Chain-of-Thought Reasoning through
  Semantically-Aligned Implicit Tokens 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24940v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24940v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinhan He, Wendy Zheng, Yaochen Zhu, Zaiyi Zheng, Lin Su, Sriram Vasudevan, Qi Guo, Liangjie Hong, Jundong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The verbosity of Chain-of-Thought (CoT) reasoning hinders its mass deployment
in efficiency-critical applications. Recently, implicit CoT approaches have
emerged, which encode reasoning steps within LLM's hidden embeddings (termed
``implicit reasoning'') rather than explicit tokens. This approach accelerates
CoT by reducing the reasoning length and bypassing some LLM components.
However, existing implicit CoT methods face two significant challenges: (1)
they fail to preserve the semantic alignment between the implicit reasoning
(when transformed to natural language) and the ground-truth reasoning,
resulting in a significant CoT performance degradation, and (2) they focus on
reducing the length of the implicit reasoning; however, they neglect the
considerable time cost for an LLM to generate one individual implicit reasoning
token. To tackle these challenges, we propose a novel semantically-aligned
implicit CoT framework termed SemCoT. In particular, for the first challenge,
we design a contrastively trained sentence transformer that evaluates semantic
alignment between implicit and explicit reasoning, which is used to enforce
semantic preservation during implicit reasoning optimization. To address the
second challenge, we introduce an efficient implicit reasoning generator by
finetuning a lightweight language model using knowledge distillation. This
generator is guided by our sentence transformer to distill ground-truth
reasoning into semantically aligned implicit reasoning, while also optimizing
for accuracy. SemCoT is the first approach that enhances CoT efficiency by
jointly optimizing token-level generation speed and preserving semantic
alignment with ground-truth reasoning. Extensive experiments demonstrate the
superior performance of SemCoT compared to state-of-the-art methods in both
efficiency and effectiveness. Our code can be found at
https://github.com/YinhanHe123/SemCoT/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Disaggregation Reveals Hidden Training Dynamics: The Case of Agreement
  Attraction <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24934v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24934v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        James A. Michaelov, Catherine Arnett
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models generally produce grammatical text, but they are more likely
to make errors in certain contexts. Drawing on paradigms from
psycholinguistics, we carry out a fine-grained analysis of those errors in
different syntactic contexts. We demonstrate that by disaggregating over the
conditions of carefully constructed datasets and comparing model performance on
each over the course of training, it is possible to better understand the
intermediate stages of grammatical learning in language models. Specifically,
we identify distinct phases of training where language model behavior aligns
with specific heuristics such as word frequency and local context rather than
generalized grammatical rules. We argue that taking this approach to analyzing
language model behavior more generally can serve as a powerful tool for
understanding the intermediate learning phases, overall training dynamics, and
the specific generalizations learned by language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the First Workshop on Interpreting Cognition in Deep
  Learning Models (CogInterp @ NeurIPS 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RiddleBench: A New Generative Reasoning Benchmark for LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24932v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24932v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deepon Halder, Alan Saji, Thanmay Jayakumar, Ratish Puduppully, Anoop Kunchukuttan, Raj Dabre
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models have demonstrated strong performance on many
established reasoning benchmarks. However, these benchmarks primarily evaluate
structured skills like quantitative problem-solving, leaving a gap in assessing
flexible, multifaceted reasoning abilities that are central to human
intelligence. These abilities require integrating logical deduction with
spatial awareness and constraint satisfaction, which current evaluations do not
measure well. To address this, we introduce RiddleBench, a benchmark of 1,737
challenging puzzles in English designed to probe these core reasoning
capabilities. Evaluation of state-of-the-art models on RiddleBench shows
fundamental weaknesses. Even top proprietary models like Gemini 2.5 Pro, o3,
and Claude 4 Sonnet achieve accuracy just above 60% (60.30%, 63.37%, and
63.16%). Analysis further reveals deep failures, including hallucination
cascades (accepting flawed reasoning from other models) and poor
self-correction due to a strong self-confirmation bias. Their reasoning is also
fragile, with performance degrading significantly when constraints are
reordered or irrelevant information is introduced. RiddleBench functions as a
diagnostic tool for these issues and as a resource for guiding the development
of more robust and reliable language models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Idea2Plan: Exploring AI-Powered Research Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24891v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24891v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin Huang, Silviu Cucerzan, Sujay Kumar Jauhar, Ryen W. White
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated significant potential to
accelerate scientific discovery as valuable tools for analyzing data,
generating hypotheses, and supporting innovative approaches in various
scientific fields. In this work, we investigate how LLMs can handle the
transition from conceptual research ideas to well-structured research plans.
Effective research planning not only supports scientists in advancing their
research but also represents a crucial capability for the development of
autonomous research agents. Despite its importance, the field lacks a
systematic understanding of LLMs' research planning capability. To rigorously
measure this capability, we introduce the Idea2Plan task and Idea2Plan Bench, a
benchmark built from 200 ICML 2025 Spotlight and Oral papers released after
major LLM training cutoffs. Each benchmark instance includes a research idea
and a grading rubric capturing the key components of valid plans. We further
propose Idea2Plan JudgeEval, a complementary benchmark to assess the
reliability of LLM-based judges against expert annotations. Experimental
results show that GPT-5 and GPT-5-mini achieve the strongest performance on the
benchmark, though substantial headroom remains for future improvement. Our
study provides new insights into LLMs' capability for research planning and lay
the groundwork for future progress.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Seeing Through the MiRAGE: Evaluating Multimodal Retrieval Augmented
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24870v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24870v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Martin, William Walden, Reno Kriz, Dengjia Zhang, Kate Sanders, Eugene Yang, Chihsheng Jin, Benjamin Van Durme
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce MiRAGE, an evaluation framework for retrieval-augmented
generation (RAG) from multimodal sources. As audiovisual media becomes a
prevalent source of information online, it is essential for RAG systems to
integrate information from these sources into generation. However, existing
evaluations for RAG are text-centric, limiting their applicability to
multimodal, reasoning intensive settings because they don't verify information
against sources. MiRAGE is a claim-centric approach to multimodal RAG
evaluation, consisting of InfoF1, evaluating factuality and information
coverage, and CiteF1, measuring citation support and completeness. We show that
MiRAGE, when applied by humans, strongly aligns with extrinsic quality
judgments. We additionally introduce automatic variants of MiRAGE and three
prominent TextRAG metrics -- ACLE, ARGUE, and RAGAS -- demonstrating the
limitations of text-centric work and laying the groundwork for automatic
evaluation. We release open-source implementations and outline how to assess
multimodal RAG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/alexmartin1722/mirage</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do Large Language Models Grasp The Grammar? Evidence from
  Grammar-Book-Guided Probing in Luxembourgish 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24856v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24856v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lujun Li, Yewei Song, Lama Sleem, Yiqun Wang, Yangjie Xu, Cedric Lothritz, Niccolo Gentile, Radu State, Tegawende F. Bissyande, Jacques Klein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Grammar refers to the system of rules that governs the structural
organization and the semantic relations among linguistic units such as
sentences, phrases, and words within a given language. In natural language
processing, there remains a notable scarcity of grammar focused evaluation
protocols, a gap that is even more pronounced for low-resource languages.
Moreover, the extent to which large language models genuinely comprehend
grammatical structure, especially the mapping between syntactic structures and
meanings, remains under debate. To investigate this issue, we propose a Grammar
Book Guided evaluation pipeline intended to provide a systematic and
generalizable framework for grammar evaluation consisting of four key stages,
and in this work we take Luxembourgish as a case study. The results show a weak
positive correlation between translation performance and grammatical
understanding, indicating that strong translations do not necessarily imply
deep grammatical competence. Larger models perform well overall due to their
semantic strength but remain weak in morphology and syntax, struggling
particularly with Minimal Pair tasks, while strong reasoning ability offers a
promising way to enhance their grammatical understanding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D
  Intelligence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24693v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24693v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihan Liu, Zhikang Niu, Qiuyang Xiao, Zhisheng Zheng, Ruoqi Yuan, Yuhang Zang, Yuhang Cao, Xiaoyi Dong, Jianze Liang, Xie Chen, Leilei Sun, Dahua Lin, Jiaqi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite rapid progress in Multi-modal Large Language Models and Large
Audio-Language Models, existing audio benchmarks largely test semantics that
can be recovered from text captions, masking deficits in fine-grained
perceptual reasoning. We formalize audio 4D intelligence that is defined as
reasoning over sound dynamics in time and 3D space, and introduce STAR-Bench to
measure it. STAR-Bench combines a Foundational Acoustic Perception setting (six
attributes under absolute and relative regimes) with a Holistic Spatio-Temporal
Reasoning setting that includes segment reordering for continuous and discrete
processes and spatial tasks spanning static localization, multi-source
relations, and dynamic trajectories. Our data curation pipeline uses two
methods to ensure high-quality samples. For foundational tasks, we use
procedurally synthesized and physics-simulated audio. For holistic data, we
follow a four-stage process that includes human annotation and final selection
based on human performance. Unlike prior benchmarks where caption-only
answering reduces accuracy slightly, STAR-Bench induces far larger drops
(-31.5\% temporal, -35.2\% spatial), evidencing its focus on linguistically
hard-to-describe cues. Evaluating 19 models reveals substantial gaps compared
with humans and a capability hierarchy: closed-source models are bottlenecked
by fine-grained perception, while open-source models lag across perception,
knowledge, and reasoning. Our STAR-Bench provides critical insights and a clear
path forward for developing future models with a more robust understanding of
the physical world.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Homepage: https://internlm.github.io/StarBench/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SPICE: Self-Play In Corpus Environments Improves Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24684v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24684v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Liu, Chuanyang Jin, Seungone Kim, Weizhe Yuan, Wenting Zhao, Ilia Kulikov, Xian Li, Sainbayar Sukhbaatar, Jack Lanchantin, Jason Weston
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-improving systems require environmental interaction for continuous
adaptation. We introduce SPICE (Self-Play In Corpus Environments), a
reinforcement learning framework where a single model acts in two roles: a
Challenger that mines documents from a large corpus to generate diverse
reasoning tasks, and a Reasoner that solves them. Through adversarial dynamics,
the Challenger creates an automatic curriculum at the frontier of the
Reasoner's capability, while corpus grounding provides the rich,
near-inexhaustible external signal necessary for sustained improvement. Unlike
existing ungrounded self-play methods that offer more limited benefits, SPICE
achieves consistent gains across mathematical (+8.9%) and general reasoning
(+9.8%) benchmarks on multiple model families. Our analysis reveals how
document grounding is a key ingredient in SPICE to continuously generate its
own increasingly challenging goals and achieve them, enabling sustained
self-improvement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dissecting Role Cognition in Medical LLMs via Neuronal Ablation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24677v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24677v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xun Liang, Huayi Lai, Hanyu Wang, Wentao Zhang, Linfeng Zhang, Yanfang Chen, Feiyu Xiong, Zhiyu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have gained significant traction in medical
decision support systems, particularly in the
  context of medical question answering and role-playing simulations. A common
practice, Prompt-Based Role Playing (PBRP),
  instructs models to adopt different clinical roles (e.g., medical students,
residents, attending physicians) to simulate varied
  professional behaviors. However, the impact of such role prompts on model
reasoning capabilities remains unclear. This
  study introduces the RP-Neuron-Activated Evaluation Framework(RPNA) to
evaluate whether role prompts induce distinct,
  role-specific cognitive processes in LLMs or merely modify linguistic style.
We test this framework on three medical QA
  datasets, employing neuron ablation and representation analysis techniques to
assess changes in reasoning pathways. Our
  results demonstrate that role prompts do not significantly enhance the
medical reasoning abilities of LLMs. Instead, they
  primarily affect surface-level linguistic features, with no evidence of
distinct reasoning pathways or cognitive differentiation
  across clinical roles. Despite superficial stylistic changes, the core
decision-making mechanisms of LLMs remain uniform
  across roles, indicating that current PBRP methods fail to replicate the
cognitive complexity found in real-world medical
  practice. This highlights the limitations of role-playing in medical AI and
emphasizes the need for models that simulate genuine
  cognitive processes rather than linguistic imitation.We have released the
related code in the following repository:https:
  //github.com/IAAR-Shanghai/RolePlay_LLMDoctor
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InteractComp: Evaluating Search Agents With Ambiguous Queries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24668v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24668v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyi Deng, Lijun Huang, Yani Fan, Jiayi Zhang, Fashen Ren, Jinyi Bai, Fuzhen Yang, Dayi Miao, Zhaoyang Yu, Yifan Wu, Yanfei Zhang, Fengwei Teng, Yingjia Wan, Song Hu, Yude Li, Xin Jin, Conghao Hu, Haoyu Li, Qirui Fu, Tai Zhong, Xinyu Wang, Xiangru Tang, Nan Tang, Chenglin Wu, Yuyu Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language agents have demonstrated remarkable potential in web search and
information retrieval. However, these search agents assume user queries are
complete and unambiguous, an assumption that diverges from reality where users
begin with incomplete queries requiring clarification through interaction. Yet
most agents lack interactive mechanisms during the search process, and existing
benchmarks cannot assess this capability. To address this gap, we introduce
InteractComp, a benchmark designed to evaluate whether search agents can
recognize query ambiguity and actively interact to resolve it during search.
Following the principle of easy to verify, interact to disambiguate, we
construct 210 expert-curated questions across 9 domains through a
target-distractor methodology that creates genuine ambiguity resolvable only
through interaction. Evaluation of 17 models reveals striking failure: the best
model achieves only 13.73% accuracy despite 71.50% with complete context,
exposing systematic overconfidence rather than reasoning deficits. Forced
interaction produces dramatic gains, demonstrating latent capability current
strategies fail to engage. Longitudinal analysis shows interaction capabilities
stagnated over 15 months while search performance improved seven-fold,
revealing a critical blind spot. This stagnation, coupled with the immediate
feedback inherent to search tasks, makes InteractComp a valuable resource for
both evaluating and training interaction capabilities in search agents. The
code is available at https://github.com/FoundationAgents/InteractComp.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MQM Re-Annotation: A Technique for Collaborative Evaluation of Machine
  Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24664v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24664v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Parker Riley, Daniel Deutsch, Mara Finkelstein, Colten DiIanni, Juraj Juraska, Markus Freitag
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human evaluation of machine translation is in an arms race with translation
model quality: as our models get better, our evaluation methods need to be
improved to ensure that quality gains are not lost in evaluation noise. To this
end, we experiment with a two-stage version of the current state-of-the-art
translation evaluation paradigm (MQM), which we call MQM re-annotation. In this
setup, an MQM annotator reviews and edits a set of pre-existing MQM
annotations, that may have come from themselves, another human annotator, or an
automatic MQM annotation system. We demonstrate that rater behavior in
re-annotation aligns with our goals, and that re-annotation results in
higher-quality annotations, mostly due to finding errors that were missed
during the first pass.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evolving Diagnostic Agents in a Virtual Clinical Environment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24654v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24654v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengcheng Qiu, Chaoyi Wu, Junwei Liu, Qiaoyu Zheng, Yusheng Liao, Haowen Wang, Yun Yue, Qianrui Fan, Shuai Zhen, Jian Wang, Jinjie Gu, Yanfeng Wang, Ya Zhang, Weidi Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a framework for training large language models
(LLMs) as diagnostic agents with reinforcement learning, enabling them to
manage multi-turn diagnostic processes, adaptively select examinations, and
commit to final diagnoses. Unlike instruction-tuned models trained on static
case summaries, our method acquires diagnostic strategies through interactive
exploration and outcome-based feedback. Our contributions are fourfold: (i) We
present DiagGym, a diagnostics world model trained with electronic health
records that emits examination outcomes conditioned on patient history and
recommended examination, serving as a virtual clinical environment for
realistic diagnosis training and evaluation; (ii) We train DiagAgent via
end-to-end, multi-turn reinforcement learning to learn diagnostic policies that
optimize both information yield and diagnostic accuracy; (iii) We introduce
DiagBench, a diagnostic benchmark comprising 750 cases with physician-validated
examination recommendations and 99 cases annotated with 973 physician-written
rubrics on diagnosis process; (iv) we demonstrate superior performance across
diverse diagnostic settings. DiagAgent significantly outperforms 10
state-of-the-art LLMs, including DeepSeek-v3 and GPT-4o, as well as two
prompt-engineered agents. In single-turn settings, DiagAgent achieves 9.34%
higher diagnostic accuracy and 44.03% improvement in examination recommendation
hit ratio. In end-to-end settings, it delivers 15.12% increase in diagnostic
accuracy and 23.09% boost in examination recommendation F1 score. In
rubric-based evaluation, it surpasses the next-best model, Claude-sonnet-4, by
7.1% in weighted rubric score. These findings indicate that learning policies
in interactive clinical environments confers dynamic and clinically meaningful
diagnostic management abilities unattainable through passive training alone.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Retrieval for RAG via Reinforced Contrastive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24652v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24652v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Zhou, Lei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As retrieval-augmented generation (RAG) becomes increasingly widespread, the
role of information retrieval (IR) is shifting from retrieving information for
human users to retrieving contextual knowledge for artificial intelligence (AI)
systems, where relevance becomes difficult to define or annotate beforehand. To
address this challenge, we propose R3, a Retrieval framework optimized for RAG
through trialand-feedback Reinforced contrastive learning. Unlike prior
approaches that rely on annotated or synthetic data for supervised fine-tuning,
R3 enables the retriever to dynamically explore and optimize relevance within
the RAG environment. During training, the retrieved results interact with the
environment to produce contrastive signals that automatically guide the
retriever's self-improvement. Extensive experiments across diverse tasks
demonstrate that R3 improves RAG performance by 5.2% over the original
retriever and surpasses state-of-the-art retrievers by 4.9%, while achieving
comparable results to LLM-augmented retrieval and RAG systems built on
post-trained or instruction-tuned LLMs. It is both efficient and practical,
requiring only 4 GPUs and completing training within a single day.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantifying the Effects of Word Length, Frequency, and Predictability on
  Dyslexia 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24647v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24647v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hugo Rydel-Johnston, Alex Kafkas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We ask where, and under what conditions, dyslexic reading costs arise in a
large-scale naturalistic reading dataset. Using eye-tracking aligned to
word-level features (word length, frequency, and predictability), we model how
each feature influences dyslexic time costs. We find that all three features
robustly change reading times in both typical and dyslexic readers, and that
dyslexic readers show stronger sensitivities to each, especially
predictability. Counterfactual manipulations of these features substantially
narrow the dyslexic-control gap by about one third, with predictability showing
the strongest effect, followed by length and frequency. These patterns align
with dyslexia theories that posit heightened demands on linguistic working
memory and phonological encoding, and they motivate further work on lexical
complexity and parafoveal preview benefits to explain the remaining gap. In
short, we quantify when extra dyslexic costs arise, how large they are, and
offer actionable guidance for interventions and computational models for
dyslexics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ "Mm, Wat?" Detecting Other-initiated Repair Requests in Dialogue 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24628v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24628v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anh Ngo, Nicolas Rollet, Catherine Pelachaud, Chloe Clavel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Maintaining mutual understanding is a key component in human-human
conversation to avoid conversation breakdowns, in which repair, particularly
Other-Initiated Repair (OIR, when one speaker signals trouble and prompts the
other to resolve), plays a vital role. However, Conversational Agents (CAs)
still fail to recognize user repair initiation, leading to breakdowns or
disengagement. This work proposes a multimodal model to automatically detect
repair initiation in Dutch dialogues by integrating linguistic and prosodic
features grounded in Conversation Analysis. The results show that prosodic cues
complement linguistic features and significantly improve the results of
pretrained text and audio embeddings, offering insights into how different
features interact. Future directions include incorporating visual cues,
exploring multilingual and cross-context corpora to assess the robustness and
generalizability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Relative Scaling Laws for LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24626v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24626v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Held, David Hall, Percy Liang, Diyi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling laws describe how language models improve with additional data,
parameters, and compute. While widely used, they are typically measured on
aggregate test sets. Aggregate evaluations yield clean trends but average over
heterogeneous subpopulations, obscuring performance disparities. We introduce
relative scaling laws, which track how performance gaps between test
distributions evolve with scale rather than focusing solely on absolute error.
Using 255 decoder-only Transformers trained under matched-compute (IsoFLOP)
budgets from $10^{18}$--$10^{20}$ FLOPs on standard pretraining datasets, we
find diverse trajectories: academic domains on MMLU converge toward parity;
regional English dialects shift depending on population size; and clusters of
AI risk behaviours split, with capability- and influence-related risks
increasing during pretraining while adversarial risks do not. These results
show that although scaling improves overall performance, it is not a universal
equalizer. To support further study, we release all model checkpoints from this
work to enable practitioners to measure relative alongside traditional scaling
laws, in order to better prioritize robustness challenges in light of the
bitter lesson.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zero-Shot Cross-Lingual Transfer using Prefix-Based Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24619v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24619v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Snegha A, Sayambhu Sen, Piyush Singh Pasi, Abhishek Singhania, Preethi Jyothi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the release of new large language models (LLMs) like Llama and Mistral,
zero-shot cross-lingual transfer has become increasingly feasible due to their
multilingual pretraining and strong generalization capabilities. However,
adapting these decoder-only LLMs to new tasks across languages remains
challenging. While parameter-efficient fine-tuning (PeFT) techniques like
Low-Rank Adaptation (LoRA) are widely used, prefix-based techniques such as
soft prompt tuning, prefix tuning, and Llama Adapter are less explored,
especially for zero-shot transfer in decoder-only models. We present a
comprehensive study of three prefix-based methods for zero-shot cross-lingual
transfer from English to 35+ high- and low-resource languages. Our analysis
further explores transfer across linguistic families and scripts, as well as
the impact of scaling model sizes from 1B to 24B. With Llama 3.1 8B, prefix
methods outperform LoRA-baselines by up to 6% on the Belebele benchmark.
Similar improvements were observed with Mistral v0.3 7B as well. Despite using
only 1.23M learning parameters with prefix tuning, we achieve consistent
improvements across diverse benchmarks. These findings highlight the potential
of prefix-based techniques as an effective and scalable alternative to LoRA,
particularly in low-resource multilingual settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 Pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Long-Context Modeling with Dynamic Hierarchical Sparse Attention for
  On-Device LLMs <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24606v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24606v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siheng Xiong, Joe Zou, Faramarz Fekri, Yae Jee Cho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The quadratic cost of attention hinders the scalability of long-context LLMs,
especially in resource-constrained settings. Existing static sparse methods
such as sliding windows or global tokens utilizes the sparsity of attention to
reduce the cost of attention, but poorly adapts to the content-dependent
variations in attention due to their staticity. While previous work has
proposed several dynamic approaches to improve flexibility, they still depend
on predefined templates or heuristic mechanisms. Such strategies reduce
generality and prune tokens that remain contextually important, limiting their
accuracy across diverse tasks. To tackle these bottlenecks of existing methods
for long-context modeling, we introduce Dynamic Hierarchical Sparse Attention
(DHSA), a data-driven framework that dynamically predicts attention sparsity
online without retraining. Our proposed DHSA adaptively segments sequences into
variable-length chunks, then computes chunk representations by aggregating the
token embeddings within each chunk. To avoid the bias introduced by varying
chunk lengths, we apply length-normalized aggregation that scales the averaged
embeddings by the square root of the chunk size. Finally, DHSA upsamples the
chunk-level similarity scores to token level similarities to calculate
importance scores that determine which token-level interactions should be
preserved. Our experiments on Gemma2 with Needle-in-a-Haystack Test and
LongBench show that DHSA matches dense attention in accuracy, while reducing
prefill latency by 20-60% and peak memory usage by 35%. Compared to other
representative baselines such as block sparse attention, DHSA achieves
consistently higher accuracy (6-18% relative gains) with comparable or lower
cost, offering an efficient and adaptable solution for long-context on-device
LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2025 Workshop on Efficient Reasoning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusion LLM with Native Variable Generation Lengths: Let [EOS] Lead
  the Way 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24605v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24605v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yicun Yang, Cong Wang, Shaobo Wang, Zichen Wen, Biqing Qi, Hanlin Xu, Linfeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion-based large language models (dLLMs) have exhibited substantial
potential for parallel text generation, which may enable more efficient
generation compared to autoregressive models. However, current dLLMs suffer
from fixed generation lengths, which indicates the generation lengths of dLLMs
have to be determined before decoding as a hyper-parameter, leading to issues
in efficiency and flexibility. To solve these problems, in this work, we
propose to train a diffusion LLM with native variable generation lengths,
abbreviated as dLLM-Var. Concretely, we aim to train a model to accurately
predict the [EOS] token in the generated text, which makes a dLLM be able to
natively infer in a block diffusion manner, while still maintaining the ability
of global bi-directional (full) attention and high parallelism. Experiments on
standard benchmarks demonstrate that our method achieves a 30.1x speedup over
traditional dLLM inference paradigms and a 2.4x speedup relative to
autoregressive models such as Qwen and Llama. Our method achieves higher
accuracy and faster inference, elevating dLLMs beyond mere academic novelty and
supporting their practical use in real-world applications. Codes and models
have been released.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Retrieval-Augmented Generation-based Relation Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.13397v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.13397v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sefika Efeoglu, Adrian Paschke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Information Extraction (IE) is a transformative process that converts
unstructured text data into a structured format by employing entity and
relation extraction (RE) methodologies. The identification of the relation
between a pair of entities plays a crucial role within this framework. Despite
the existence of various techniques for relation extraction, their efficacy
heavily relies on access to labeled data and substantial computational
resources. In addressing these challenges, Large Language Models (LLMs) emerge
as promising solutions; however, they might return hallucinating responses due
to their own training data. To overcome these limitations, Retrieved-Augmented
Generation-based Relation Extraction (RAG4RE) in this work is proposed,
offering a pathway to enhance the performance of relation extraction tasks.
  This work evaluated the effectiveness of our RAG4RE approach utilizing
different LLMs. Through the utilization of established benchmarks, such as
TACRED, TACREV, Re-TACRED, and SemEval RE datasets, our aim is to
comprehensively evaluate the efficacy of our RAG4RE approach. In particularly,
we leverage prominent LLMs including Flan T5, Llama2, and Mistral in our
investigation. The results of our study demonstrate that our RAG4RE approach
surpasses performance of traditional RE approaches based solely on LLMs,
particularly evident in the TACRED dataset and its variations. Furthermore, our
approach exhibits remarkable performance compared to previous RE methodologies
across both TACRED and TACREV datasets, underscoring its efficacy and potential
for advancing RE tasks in natural language processing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>published at the Semantic Web journal. The last version is available:
  https://doi.org/10.1177/22104968251385519</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DBLPLink 2.0 -- An Entity Linker for the DBLP Scholarly Knowledge Graph 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.22811v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.22811v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Debayan Banerjee, Tilahun Abedissa Taffa, Ricardo Usbeck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work we present an entity linker for DBLP's 2025 version of RDF-based
Knowledge Graph. Compared to the 2022 version, DBLP now considers publication
venues as a new entity type called dblp:Stream. In the earlier version of
DBLPLink, we trained KG-embeddings and re-rankers on a dataset to produce
entity linkings. In contrast, in this work, we develop a zero-shot entity
linker using LLMs using a novel method, where we re-rank candidate entities
based on the log-probabilities of the "yes" token output at the penultimate
layer of the LLM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Creativity or Brute Force? Using Brainteasers as a Window into the
  Problem-Solving Abilities of Large Language Models <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10844v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10844v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simeng Han, Howard Dai, Stephen Xia, Grant Zhang, Chen Liu, Lichang Chen, Hoang Huy Nguyen, Hongyuan Mei, Jiayuan Mao, R. Thomas McCoy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accuracy remains a standard metric for evaluating AI systems, but it offers
limited insight into how models arrive at their solutions. In this work, we
introduce a benchmark based on brainteasers written in long narrative form to
probe more deeply into the types of reasoning strategies that models use.
Brainteasers are well-suited for this goal because they can be solved with
multiple approaches, such as a few-step solution that uses a creative insight
or a longer solution that uses more brute force. We investigate large language
models (LLMs) across multiple layers of reasoning, focusing not only on
correctness but also on the quality and creativity of their solutions. We
investigate many aspects of the reasoning process: (1) semantic parsing of the
brainteasers into precise mathematical competition style formats; (2)
generating solutions from these mathematical forms; (3) self-correcting
solutions based on gold solutions; (4) producing step-by-step sketches of
solutions; and (5) making use of hints. We find that LLMs are in many cases
able to find creative, insightful solutions to brainteasers, suggesting that
they capture some of the capacities needed to solve novel problems in creative
ways. Nonetheless, there also remain situations where they rely on brute force
despite the availability of more efficient, creative solutions, highlighting a
potential direction for improvement in the reasoning abilities of LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OpenFactCheck: Building, Benchmarking Customized Fact-Checking Systems
  and Evaluating the Factuality of Claims and LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.05583v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.05583v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxia Wang, Minghan Wang, Hasan Iqbal, Georgi Georgiev, Jiahui Geng, Preslav Nakov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increased use of large language models (LLMs) across a variety of
real-world applications calls for mechanisms to verify the factual accuracy of
their outputs. Difficulties lie in assessing the factuality of free-form
responses in open domains. Also, different papers use disparate evaluation
benchmarks and measurements, which renders them hard to compare and hampers
future progress. To mitigate these issues, we propose OpenFactCheck, a unified
framework for building customized automatic fact-checking systems, benchmarking
their accuracy, evaluating factuality of LLMs, and verifying claims in a
document. OpenFactCheck consists of three modules: (i) CUSTCHECKER allows users
to easily customize an automatic fact-checker and verify the factual
correctness of documents and claims, (ii) LLMEVAL, a unified evaluation
framework assesses LLM's factuality ability from various perspectives fairly,
and (iii) CHECKEREVAL is an extensible solution for gauging the reliability of
automatic fact-checkers' verification results using human-annotated datasets.
Data and code are publicly available at
https://github.com/yuxiaw/openfactcheck.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 8 tables, 11 figures, Published In Proceedings of the 31st
  International Conference on Computational Linguistics 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OpenFactCheck: A Unified Framework for Factuality Evaluation of LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.11832v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.11832v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hasan Iqbal, Yuxia Wang, Minghan Wang, Georgi Georgiev, Jiahui Geng, Iryna Gurevych, Preslav Nakov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increased use of large language models (LLMs) across a variety of
real-world applications calls for automatic tools to check the factual accuracy
of their outputs, as LLMs often hallucinate. This is difficult as it requires
assessing the factuality of free-form open-domain responses. While there has
been a lot of research on this topic, different papers use different evaluation
benchmarks and measures, which makes them hard to compare and hampers future
progress. To mitigate these issues, we developed OpenFactCheck, a unified
framework, with three modules: (i) RESPONSEEVAL, which allows users to easily
customize an automatic fact-checking system and to assess the factuality of all
claims in an input document using that system, (ii) LLMEVAL, which assesses the
overall factuality of an LLM, and (iii) CHECKEREVAL, a module to evaluate
automatic fact-checking systems. OpenFactCheck is open-sourced
(https://github.com/mbzuai-nlp/openfactcheck) and publicly released as a Python
library (https://pypi.org/project/openfactcheck/) and also as a web service
(http://app.openfactcheck.com). A video describing the system is available at
https://youtu.be/-i9VKL0HleI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 4 Figures, 3 Tables, Published In Proceedings of The 2024
  Conference on Empirical Methods in Natural Language Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quantifying Phonosemantic Iconicity Distributionally in 6 Languages <span class="chip">AACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.14040v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.14040v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        George Flint, Kaustubh Kislay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language is, as commonly theorized, largely arbitrary. Yet, systematic
relationships between phonetics and semantics have been observed in many
specific cases. To what degree could those systematic relationships manifest
themselves in large scale, quantitative investigations--both in previously
identified and unidentified phenomena? This work undertakes a distributional
approach to quantifying phonosemantic iconicity at scale across 6 diverse
languages (English, Spanish, Hindi, Finnish, Turkish, and Tamil). In each
language, we analyze the alignment of morphemes' phonetic and semantic
similarity spaces with a suite of statistical measures, and discover an array
of interpretable phonosemantic alignments not previously identified in the
literature, along with crosslinguistic patterns. We also analyze 5 previously
hypothesized phonosemantic alignments, finding support for some such alignments
and mixed results for others.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IJCNLP-AACL 2025 Main Conference Proceedings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UrduFactCheck: An Agentic Fact-Checking Framework for Urdu with Evidence
  Boosting and Benchmarking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.15063v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.15063v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sarfraz Ahmad, Hasan Iqbal, Momina Ahsan, Numaan Naeem, Muhammad Ahsan Riaz Khan, Arham Riaz, Muhammad Arslan Manzoor, Yuxia Wang, Preslav Nakov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid adoption of Large Language Models (LLMs) has raised important
concerns about the factual reliability of their outputs, particularly in
low-resource languages such as Urdu. Existing automated fact-checking systems
are predominantly developed for English, leaving a significant gap for the more
than 200 million Urdu speakers worldwide. In this work, we present
UrduFactBench and UrduFactQA, two novel hand-annotated benchmarks designed to
enable fact-checking and factual consistency evaluation in Urdu. While
UrduFactBench focuses on claim verification, UrduFactQA targets the factuality
of LLMs in question answering. These resources, the first of their kind for
Urdu, were developed through a multi-stage annotation process involving native
Urdu speakers. To complement these benchmarks, we introduce UrduFactCheck, a
modular fact-checking framework that incorporates both monolingual and
translation-based evidence retrieval strategies to mitigate the scarcity of
high-quality Urdu evidence. Leveraging these resources, we conduct an extensive
evaluation of twelve LLMs and demonstrate that translation-augmented pipelines
consistently enhance performance compared to monolingual ones. Our findings
reveal persistent challenges for open-source LLMs in Urdu and underscore the
importance of developing targeted resources. All code and data are publicly
available at https://github.com/mbzuai-nlp/UrduFactCheck.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 4 figures, 5 tables, 6 Listings, Published in Proceeding of
  The 2025 Conference on Empirical Methods in Natural Language Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ p-less Sampling: A Robust Hyperparameter-Free Approach for LLM Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.23234v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.23234v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runyan Tan, Shuang Wu, Phillip Howard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Obtaining high-quality outputs from Large Language Models (LLMs) often
depends upon the choice of a sampling-based decoding strategy to
probabilistically choose the next token at each generation step. While a
variety of such sampling methods have been proposed, their performance can be
sensitive to the selection of hyperparameters which may require different
settings depending upon the generation task and temperature configuration. In
this work, we introduce $p$-less sampling: an information-theoretic approach to
sampling which dynamically sets a truncation threshold at each decoding step
based on the entire token probability distribution. Unlike existing methods,
$p$-less sampling has no hyperparameters and consistently produces high-quality
outputs as temperature increases. We provide theoretical perspectives on
$p$-less sampling to ground our proposed method and conduct experiments to
empirically validate its effectiveness across a range of math, logical
reasoning, and creative writing tasks. Our results demonstrate how $p$-less
sampling consistently outperforms existing sampling approaches while exhibiting
much less degradation in text quality at higher temperature values. We further
show how $p$-less achieves greater inference-time efficiency than alternative
methods through lower average token sampling times and shorter generation
lengths, without sacrificing accuracy. Finally, we provide analyses to
highlight the benefits of $p$-less through qualitative examples, case studies,
and diversity assessments. The code is available at
https://github.com/ryttry/p-less .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CURATRON: Complete and Robust Preference Data for Rigorous Alignment of
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02745v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02745v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Son The Nguyen, Niranjan Uma Naresh, Theja Tulabandhula
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the challenges of aligning large language models (LLMs)
with human values via preference learning (PL), focusing on incomplete and
corrupted data in preference datasets. We propose a novel method for robustly
and completely recalibrating values within these datasets to enhance LLMs'
resilience against the issues. In particular, we devise a guaranteed polynomial
time ranking algorithm that robustifies several existing models, such as the
classic Bradley-Terry-Luce (BTL) (Bradley and Terry, 1952) model and certain
generalizations of it. To the best of our knowledge, our present work is the
first to propose an algorithm that provably recovers an $\epsilon$-optimal
ranking with high probability while allowing as large as $O(n)$ perturbed
pairwise comparison results per model response. Furthermore, we show robust
recovery results in the partially observed setting. Our experiments confirm
that our algorithms handle adversarial noise and unobserved comparisons well in
both general and LLM preference dataset settings. This work contributes to the
development and scaling of more reliable and ethically aligned AI models by
equipping the dataset curation pipeline with the ability to handle missing and
maliciously manipulated inputs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Chat<span class="highlight-title">GPT</span> Forecast Stock Price Movements? Return Predictability and
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.07619v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.07619v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alejandro Lopez-Lira, Yuehua Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We document the capability of large language models (LLMs) like ChatGPT to
predict stock market reactions from news headlines without direct financial
training. Using post-knowledge-cutoff headlines, GPT-4 captures initial market
responses, achieving approximately 90% portfolio-day hit rates for the
non-tradable initial reaction. GPT-4 scores also significantly predict the
subsequent drift, especially for small stocks and negative news. Forecasting
ability generally increases with model size, suggesting that financial
reasoning is an emerging capacity of complex LLMs. Strategy returns decline as
LLM adoption rises, consistent with improved price efficiency. To rationalize
these findings, we develop a theoretical model that incorporates LLM
technology, information-processing capacity constraints, underreaction, and
limits to arbitrage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Previously posted in SSRN
  https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4412788</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMD-Flagger: Leveraging Maximum Mean Discrepancy to Detect
  Hallucinations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.01367v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.01367v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kensuke Mitsuzawa, Damien Garreau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have become pervasive in our everyday life. Yet,
a fundamental obstacle prevents their use in many critical applications: their
propensity to generate fluent, human-quality content that is not grounded in
reality. The detection of such hallucinations is thus of the highest
importance. In this work, we propose a new method to flag hallucinated content:
MMD-Flagger. It relies on Maximum Mean Discrepancy (MMD), a non-parametric
distance between distributions. On a high-level perspective, MMD-Flagger tracks
the MMD between the output to inspect and counterparts generated with various
temperature parameters. We show empirically that inspecting the shape of this
trajectory is sufficient to detect most hallucinations. This novel method is
benchmarked on machine translation and summarization datasets, on which it
exhibits competitive performance relative to natural competitors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do predictability factors towards signing avatars hold across cultures? <span class="chip">ICASSP
  2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02103v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02103v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdelhadi Soudi, Manal El Hakkaoui, Kristof Van Laerhoven
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Avatar technology can offer accessibility possibilities and improve the
Deaf-and-Hard of Hearing sign language users access to communication, education
and services, such as the healthcare system. However, sign language users
acceptance of signing avatars as well as their attitudes towards them vary and
depend on many factors. Furthermore, research on avatar technology is mostly
done by researchers who are not Deaf. The study examines the extent to which
intrinsic or extrinsic factors contribute to predict the attitude towards
avatars across cultures. Intrinsic factors include the characteristics of the
avatar, such as appearance, movements and facial expressions. Extrinsic factors
include users technology experience, their hearing status, age and their sign
language fluency. This work attempts to answer questions such as, if lower
attitude ratings are related to poor technology experience with ASL users, for
example, is that also true for Moroccan Sign Language (MSL) users? For the
purposes of the study, we designed a questionnaire to understand MSL users
attitude towards avatars. Three groups of participants were surveyed: Deaf
(57), Hearing (20) and Hard-of-Hearing (3). The results of our study were then
compared with those reported in other relevant studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings of SLTAT 2023: Eighth International Workshop on Sign
  Language Translation and Avatar Technology, held in conjunction with ICASSP
  2023: IEEE International Conference on Acoustics, Speech, and Signal
  Processing, Rhodes, Greece, June 4-10, 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WXImpactBench: A Disruptive Weather Impact Understanding Benchmark for
  Evaluating Large Language Models <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.20249v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.20249v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongan Yu, Qingchen Hu, Xianda Du, Jiayin Wang, Fengran Mo, Renee Sieber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Climate change adaptation requires the understanding of disruptive weather
impacts on society, where large language models (LLMs) might be applicable.
However, their effectiveness is under-explored due to the difficulty of
high-quality corpus collection and the lack of available benchmarks. The
climate-related events stored in regional newspapers record how communities
adapted and recovered from disasters. However, the processing of the original
corpus is non-trivial. In this study, we first develop a disruptive weather
impact dataset with a four-stage well-crafted construction pipeline. Then, we
propose WXImpactBench, the first benchmark for evaluating the capacity of LLMs
on disruptive weather impacts. The benchmark involves two evaluation tasks,
multi-label classification and ranking-based question answering. Extensive
experiments on evaluating a set of LLMs provide first-hand analysis of the
challenges in developing disruptive weather impact understanding and climate
change adaptation systems. The constructed dataset and the code for the
evaluation framework are available to help society protect against
vulnerabilities from disasters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Non-Markovian Discrete Diffusion with Causal Language Models <span class="chip">NeurIPS
  2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09767v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09767v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangtian Zhang, Sizhuang He, Daniel Levine, Lawrence Zhao, David Zhang, Syed A Rizvi, Shiyang Zhang, Emanuele Zappala, Rex Ying, David van Dijk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Discrete diffusion models offer a flexible, controllable approach to
structured sequence generation, yet they still lag behind causal language
models in expressive power. A key limitation lies in their reliance on the
Markovian assumption, which restricts each step to condition only on the
current state, leading to potential uncorrectable error accumulation. In this
paper, we introduce CaDDi (Causal Discrete Diffusion Model), a discrete
diffusion model that conditions on the entire generative trajectory, thereby
lifting the Markov constraint and allowing the model to revisit and improve
past states. By unifying sequential (causal) and temporal (diffusion) reasoning
in a single non-Markovian transformer, CaDDi also treats standard causal
language models as a special case and permits the direct reuse of pretrained
LLM weights with no architectural changes. Empirically, CaDDi outperforms
state-of-the-art discrete diffusion baselines on natural-language benchmarks,
substantially narrowing the remaining gap to large autoregressive transformers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39th Conference on Neural Information Processing Systems (NeurIPS
  2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BugPilot: Complex Bug Generation for Efficient Learning of SWE Skills 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.19898v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.19898v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atharv Sonwane, Isadora White, Hyunji Lee, Matheus Pereira, Lucas Caccia, Minseon Kim, Zhengyan Shi, Chinmay Singh, Alessandro Sordoni, Marc-Alexandre Côté, Xingdi Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High quality bugs are key to training the next generation of language model
based software engineering (SWE) agents. We introduce a novel method for
synthetic generation of difficult and diverse bugs. Our method instructs SWE
Agents to introduce a feature into the codebase whereby they may
unintentionally break tests, resulting in bugs. Prior approaches often induce
an out-of-distribution effect by generating bugs intentionally (e.g. by
introducing local perturbation to existing code), which does not reflect
realistic development processes. We perform qualitative analysis to demonstrate
that our approach for generating bugs more closely reflects the patterns found
in human-authored edits. Through extensive experiments, we demonstrate that our
bugs provide more efficient training data for supervised fine-tuning,
outperforming other bug datasets by 2% with half the training data (1.2k vs. 3k
bugs). We train on our newly generated bugs in addition to existing bug
datasets to get FrogBoss a state-of-the-art 32B parameter model on SWE-bench
Verified with a pass@1 of 54.6% and FrogMini a state-of-the-art 14B model on
SWE-bench Verified with a pass@1 of 45.3% on SWE-bench Verified averaged over
three seeds.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InsurTech innovation using natural language processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.21112v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.21112v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Panyi Dong, Zhiyu Quan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid rise of InsurTech, traditional insurance companies are
increasingly exploring alternative data sources and advanced technologies to
sustain their competitive edge. This paper provides both a conceptual overview
and practical case studies of natural language processing (NLP) and its
emerging applications within insurance operations, focusing on transforming
raw, unstructured text into structured data suitable for actuarial analysis and
decision-making. Leveraging real-world alternative data provided by an
InsurTech industry partner that enriches traditional insurance data sources, we
apply various NLP techniques to demonstrate feature de-biasing, feature
compression, and industry classification in the commercial insurance context.
These enriched, text-derived insights not only add to and refine traditional
rating factors for commercial insurance pricing but also offer novel
perspectives for assessing underlying risk by introducing novel industry
classification techniques. Through these demonstrations, we show that NLP is
not merely a supplementary tool but a foundational element of modern,
data-driven insurance analytics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cite <span class="highlight-title">Pretrain</span>: Retrieval-Free Knowledge Attribution for Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.17585v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.17585v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yukun Huang, Sanxing Chen, Jian Pei, Manzil Zaheer, Bhuwan Dhingra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trustworthy language models should provide both correct and verifiable
answers. However, citations generated directly by standalone LLMs are often
unreliable. As a result, current systems insert citations by querying an
external retriever at inference time, introducing latency, infrastructure
dependence, and vulnerability to retrieval noise. We explore whether LLMs can
be made to reliably attribute to the documents seen during continual
pretraining without test-time retrieval, by revising the training process. To
study this, we construct CitePretrainBench, a benchmark that mixes real-world
corpora (Wikipedia, Common Crawl, arXiv) with novel documents and probes both
short-form (single-fact) and long-form (multi-fact) citation tasks. Our
approach follows a two-stage process: (1) continual pretraining to index
factual knowledge by binding it to persistent document identifiers; and (2)
instruction tuning to elicit citation behavior. We introduce Active Indexing
for the first stage, which creates generalizable, source-anchored bindings by
augmenting training with synthetic data that (i) restate each fact in diverse,
compositional forms and (ii) enforce bidirectional training (source-to-fact and
fact-to-source). This equips the model to both generate content from a cited
source and attribute its own answers, improving robustness to paraphrase and
composition. Experiments with Qwen-2.5-7B&3B show that Active Indexing
consistently outperforms a Passive Indexing baseline, which simply appends an
identifier to each document, achieving citation precision gains of up to 30.2%
across all tasks and models. Our ablation studies reveal that performance
continues to improve as we scale the amount of augmented data, showing a clear
upward trend even at 16x the original token count. Finally, we show that
internal citations complement external ones by making the model more robust to
retrieval noise.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Arena-Lite: Efficient and Reliable Large Language Model Evaluation via
  Tournament-Based Direct Comparisons 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01281v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01281v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seonil Son, Ju-Min Oh, Heegon Jin, Cheolhun Jang, Jeongbeom Jeong, Kuntae Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Large Language Models (LLMs) expand across domains, LLM judges have become
essential for systems evaluation. Current benchmarks typically compare system
outputs against baselines. This baseline-mediated approach, though convenient,
yields lower reliability than direct comparison between systems. We propose
Arena-Lite which integrates tournament structure on top of head-to-head
comparison. The application of a tournament structure and direct comparison
eliminates the need for baseline outputs, reduces the number of required
comparisons, and allows higher reliability in system rankings. We conducted two
experiments: (1) controlled stochastic modeling and (2) empirical validation
with a real LLM judge. Those experiments collectively demonstrate that
Arena-Lite consistently achieves higher reliability with fewer comparisons,
even with smaller datasets or weaker judges. We release an easy-to-use web
demonstration and code to foster adoption of Arena-Lite, streamlining model
selection across research and industry communities. Arena-Lite demo and code
are available on
\href{https://huggingface.co/spaces/NCSOFT/ArenaLite}{https://huggingface.co/spaces/NCSOFT/ArenaLite}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages for main body, 19 pages in total</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Says Who? Effective Zero-Shot Annotation of Focalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11390v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11390v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rebecca M. M. Hicke, Yuri Bizzoni, Pascale Feldkamp, Ross Deans Kristensen-McLachlan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Focalization describes the way in which access to narrative information is
restricted or controlled based on the knowledge available to knowledge of the
narrator. It is encoded via a wide range of lexico-grammatical features and is
subject to reader interpretation. Even trained annotators frequently disagree
on correct labels, suggesting this task is both qualitatively and
computationally challenging. In this work, we test how well five contemporary
large language model (LLM) families and two baselines perform when annotating
short literary excerpts for focalization. Despite the challenging nature of the
task, we find that LLMs show comparable performance to trained human
annotators, with GPT-4o achieving an average F1 of 84.79%. Further, we
demonstrate that the log probabilities output by GPT-family models frequently
reflect the difficulty of annotating particular excerpts. Finally, we provide a
case study analyzing sixteen Stephen King novels, demonstrating the usefulness
of this approach for computational literary studies and the insights gleaned
from examining focalization at scale.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CHR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TableTime: Reformulating Time Series Classification as Training-Free
  Table Understanding with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.15737v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.15737v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Wang, Mingyue Cheng, Qingyang Mao, Yitong Zhou, Daoyu Wang, Qi Liu, Feiyang Xu, Xin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated their effectiveness in
multivariate time series classification (MTSC). Effective adaptation of LLMs
for MTSC necessitates informative data representations. Existing LLM-based
methods directly encode embeddings for time series within the latent space of
LLMs from scratch to align with semantic space of LLMs. Despite their
effectiveness, we reveal that these methods conceal three inherent bottlenecks:
(1) they struggle to encode temporal and channel-specific information in a
lossless manner, both of which are critical components of multivariate time
series; (2) it is much difficult to align the learned representation space with
the semantic space of the LLMs; (3) they require task-specific retraining,
which is both computationally expensive and labor-intensive. To bridge these
gaps, we propose TableTime, which reformulates MTSC as a table understanding
task. Specifically, TableTime introduces the following strategies: (1) convert
multivariate time series into a tabular form, thus minimizing information loss
to the greatest extent; (2) represent tabular time series in text format to
achieve natural alignment with the semantic space of LLMs; (3) design a
reasoning framework that integrates contextual text information, neighborhood
assistance, multi-path inference and problem decomposition to enhance the
reasoning ability of LLMs and realize zero-shot classification. Extensive
experiments performed on 10 publicly representative datasets from UEA archive
verify the superiorities of the TableTime.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">171</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative View Stitching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24718v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24718v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chonghyuk Song, Michal Stary, Boyuan Chen, George Kopanas, Vincent Sitzmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autoregressive video diffusion models are capable of long rollouts that are
stable and consistent with history, but they are unable to guide the current
generation with conditioning from the future. In camera-guided video generation
with a predefined camera trajectory, this limitation leads to collisions with
the generated scene, after which autoregression quickly collapses. To address
this, we propose Generative View Stitching (GVS), which samples the entire
sequence in parallel such that the generated scene is faithful to every part of
the predefined camera trajectory. Our main contribution is a sampling algorithm
that extends prior work on diffusion stitching for robot planning to video
generation. While such stitching methods usually require a specially trained
model, GVS is compatible with any off-the-shelf video model trained with
Diffusion Forcing, a prevalent sequence diffusion framework that we show
already provides the affordances necessary for stitching. We then introduce
Omni Guidance, a technique that enhances the temporal consistency in stitching
by conditioning on both the past and future, and that enables our proposed
loop-closing mechanism for delivering long-range coherence. Overall, GVS
achieves camera-guided video generation that is stable, collision-free,
frame-to-frame consistent, and closes loops for a variety of predefined camera
paths, including Oscar Reutersv\"ard's Impossible Staircase. Results are best
viewed as videos at https://andrewsonga.github.io/gvs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://andrewsonga.github.io/gvs</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uniform Discrete Diffusion with Metric Path for Video Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24717v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24717v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoge Deng, Ting Pan, Fan Zhang, Yang Liu, Zhuoyan Luo, Yufeng Cui, Wenxuan Wang, Chunhua Shen, Shiguang Shan, Zhaoxiang Zhang, Xinlong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continuous-space video generation has advanced rapidly, while discrete
approaches lag behind due to error accumulation and long-context inconsistency.
In this work, we revisit discrete generative modeling and present Uniform
discRete diffuSion with metric pAth (URSA), a simple yet powerful framework
that bridges the gap with continuous approaches for the scalable video
generation. At its core, URSA formulates the video generation task as an
iterative global refinement of discrete spatiotemporal tokens. It integrates
two key designs: a Linearized Metric Path and a Resolution-dependent Timestep
Shifting mechanism. These designs enable URSA to scale efficiently to
high-resolution image synthesis and long-duration video generation, while
requiring significantly fewer inference steps. Additionally, we introduce an
asynchronous temporal fine-tuning strategy that unifies versatile tasks within
a single model, including interpolation and image-to-video generation.
Extensive experiments on challenging video and image generation benchmarks
demonstrate that URSA consistently outperforms existing discrete methods and
achieves performance comparable to state-of-the-art continuous diffusion
methods. Code and models are available at https://github.com/baaivision/URSA
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Routing Matters in MoE: Scaling Diffusion <span class="highlight-title">Transformer</span>s with Explicit
  Routing Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24711v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24711v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujie Wei, Shiwei Zhang, Hangjie Yuan, Yujin Han, Zhekai Chen, Jiayu Wang, Difan Zou, Xihui Liu, Yingya Zhang, Yu Liu, Hongming Shan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mixture-of-Experts (MoE) has emerged as a powerful paradigm for scaling model
capacity while preserving computational efficiency. Despite its notable success
in large language models (LLMs), existing attempts to apply MoE to Diffusion
Transformers (DiTs) have yielded limited gains. We attribute this gap to
fundamental differences between language and visual tokens. Language tokens are
semantically dense with pronounced inter-token variation, while visual tokens
exhibit spatial redundancy and functional heterogeneity, hindering expert
specialization in vision MoE. To this end, we present ProMoE, an MoE framework
featuring a two-step router with explicit routing guidance that promotes expert
specialization. Specifically, this guidance encourages the router to partition
image tokens into conditional and unconditional sets via conditional routing
according to their functional roles, and refine the assignments of conditional
image tokens through prototypical routing with learnable prototypes based on
semantic content. Moreover, the similarity-based expert allocation in latent
space enabled by prototypical routing offers a natural mechanism for
incorporating explicit semantic guidance, and we validate that such guidance is
crucial for vision MoE. Building on this, we propose a routing contrastive loss
that explicitly enhances the prototypical routing process, promoting
intra-expert coherence and inter-expert diversity. Extensive experiments on
ImageNet benchmark demonstrate that ProMoE surpasses state-of-the-art methods
under both Rectified Flow and DDPM training objectives. Code and models will be
made publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Does Object Binding Naturally Emerge in Large <span class="highlight-title">Pretrain</span>ed Vision
  <span class="highlight-title">Transformer</span>s? <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24709v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24709v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihao Li, Saeed Salehi, Lyle Ungar, Konrad P. Kording
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object binding, the brain's ability to bind the many features that
collectively represent an object into a coherent whole, is central to human
cognition. It groups low-level perceptual features into high-level object
representations, stores those objects efficiently and compositionally in
memory, and supports human reasoning about individual object instances. While
prior work often imposes object-centric attention (e.g., Slot Attention)
explicitly to probe these benefits, it remains unclear whether this ability
naturally emerges in pre-trained Vision Transformers (ViTs). Intuitively, they
could: recognizing which patches belong to the same object should be useful for
downstream prediction and thus guide attention. Motivated by the quadratic
nature of self-attention, we hypothesize that ViTs represent whether two
patches belong to the same object, a property we term IsSameObject. We decode
IsSameObject from patch embeddings across ViT layers using a similarity probe,
which reaches over 90% accuracy. Crucially, this object-binding capability
emerges reliably in self-supervised ViTs (DINO, MAE, CLIP), but markedly weaker
in ImageNet-supervised models, suggesting that binding is not a trivial
architectural artifact, but an ability acquired through specific pretraining
objectives. We further discover that IsSameObject is encoded in a
low-dimensional subspace on top of object features, and that this signal
actively guides attention. Ablating IsSameObject from model activations
degrades downstream performance and works against the learning objective,
implying that emergent object binding naturally serves the pretraining
objective. Our findings challenge the view that ViTs lack object binding and
highlight how symbolic knowledge of "which parts belong together" emerges
naturally in a connectionist system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a Spotlight at NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MIC-BEV: Multi-Infrastructure Camera Bird's-Eye-View <span class="highlight-title">Transformer</span> with
  Relation-Aware Fusion for 3D Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24688v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24688v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yun Zhang, Zhaoliang Zheng, Johnson Liu, Zhiyu Huang, Zewei Zhou, Zonglin Meng, Tianhui Cai, Jiaqi Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Infrastructure-based perception plays a crucial role in intelligent
transportation systems, offering global situational awareness and enabling
cooperative autonomy. However, existing camera-based detection models often
underperform in such scenarios due to challenges such as multi-view
infrastructure setup, diverse camera configurations, degraded visual inputs,
and various road layouts. We introduce MIC-BEV, a Transformer-based
bird's-eye-view (BEV) perception framework for infrastructure-based
multi-camera 3D object detection. MIC-BEV flexibly supports a variable number
of cameras with heterogeneous intrinsic and extrinsic parameters and
demonstrates strong robustness under sensor degradation. The proposed
graph-enhanced fusion module in MIC-BEV integrates multi-view image features
into the BEV space by exploiting geometric relationships between cameras and
BEV cells alongside latent visual cues. To support training and evaluation, we
introduce M2I, a synthetic dataset for infrastructure-based object detection,
featuring diverse camera configurations, road layouts, and environmental
conditions. Extensive experiments on both M2I and the real-world dataset
RoScenes demonstrate that MIC-BEV achieves state-of-the-art performance in 3D
object detection. It also remains robust under challenging conditions,
including extreme weather and sensor degradation. These results highlight the
potential of MIC-BEV for real-world deployment. The dataset and source code are
available at: https://github.com/HandsomeYun/MIC-BEV.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SAGE: Structure-Aware Generative Video Transitions between Diverse Clips 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24667v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24667v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mia Kan, Yilin Liu, Niloy Mitra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video transitions aim to synthesize intermediate frames between two clips,
but naive approaches such as linear blending introduce artifacts that limit
professional use or break temporal coherence. Traditional techniques
(cross-fades, morphing, frame interpolation) and recent generative inbetweening
methods can produce high-quality plausible intermediates, but they struggle
with bridging diverse clips involving large temporal gaps or significant
semantic differences, leaving a gap for content-aware and visually coherent
transitions. We address this challenge by drawing on artistic workflows,
distilling strategies such as aligning silhouettes and interpolating salient
features to preserve structure and perceptual continuity. Building on this, we
propose SAGE (Structure-Aware Generative vidEo transitions) as a zeroshot
approach that combines structural guidance, provided via line maps and motion
flow, with generative synthesis, enabling smooth, semantically consistent
transitions without fine-tuning. Extensive experiments and comparison with
current alternatives, namely [FILM, TVG, DiffMorpher, VACE, GI], demonstrate
that SAGE outperforms both classical and generative baselines on quantitative
metrics and user studies for producing transitions between diverse clips. Code
to be released on acceptance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Website: https://kan32501.github.io/sage.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Group Relative Attention Guidance for Image Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24657v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24657v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuanpu Zhang, Xuesong Niu, Ruidong Chen, Dan Song, Jianhao Zeng, Penghui Du, Haoxiang Cao, Kai Wu, An-an Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, image editing based on Diffusion-in-Transformer models has
undergone rapid development. However, existing editing methods often lack
effective control over the degree of editing, limiting their ability to achieve
more customized results. To address this limitation, we investigate the
MM-Attention mechanism within the DiT model and observe that the Query and Key
tokens share a bias vector that is only layer-dependent. We interpret this bias
as representing the model's inherent editing behavior, while the delta between
each token and its corresponding bias encodes the content-specific editing
signals. Based on this insight, we propose Group Relative Attention Guidance, a
simple yet effective method that reweights the delta values of different tokens
to modulate the focus of the model on the input image relative to the editing
instruction, enabling continuous and fine-grained control over editing
intensity without any tuning. Extensive experiments conducted on existing image
editing frameworks demonstrate that GRAG can be integrated with as few as four
lines of code, consistently enhancing editing quality. Moreover, compared to
the commonly used Classifier-Free Guidance, GRAG achieves smoother and more
precise control over the degree of editing. Our code will be released at
https://github.com/little-misfit/GRAG-Image-Editing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Eye-Tracking, Mouse Tracking, Stimulus Tracking,and Decision-Making
  <span class="highlight-title">Dataset</span>s in Digital Pathology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24653v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24653v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Veronica Thai, Rui Li, Meng Ling, Shuning Jiang, Jeremy Wolfe, Raghu Machiraju, Yan Hu, Zaibo Li, Anil Parwani, Jian Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interpretation of giga-pixel whole-slide images (WSIs) is an important but
difficult task for pathologists. Their diagnostic accuracy is estimated to
average around 70%. Adding a second pathologist does not substantially improve
decision consistency. The field lacks adequate behavioral data to explain
diagnostic errors and inconsistencies. To fill in this gap, we present
PathoGaze1.0, a comprehensive behavioral dataset capturing the dynamic visual
search and decision-making processes of the full diagnostic workflow during
cancer diagnosis. The dataset comprises 18.69 hours of eye-tracking, mouse
interaction, stimulus tracking, viewport navigation, and diagnostic decision
data (EMSVD) collected from 19 pathologists interpreting 397 WSIs. The data
collection process emphasizes ecological validity through an
application-grounded testbed, called PTAH. In total, we recorded 171,909
fixations, 263,320 saccades, and 1,867,362 mouse interaction events. In
addition, such data could also be used to improve the training of both
pathologists and AI systems that might support human experts. All experiments
were preregistered at https://osf.io/hj9a7, and the complete dataset along with
analysis code is available at https://go.osu.edu/pathogaze.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 9 figures, submitted to Nature Scientific Data</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Dual-Branch CNN for Robust Detection of AI-Generated Facial Forgeries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24640v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24640v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Zhang, Yuqi Song, Fei Zuo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of generative AI has enabled the creation of highly
realistic forged facial images, posing significant threats to AI security,
digital media integrity, and public trust. Face forgery techniques, ranging
from face swapping and attribute editing to powerful diffusion-based image
synthesis, are increasingly being used for malicious purposes such as
misinformation, identity fraud, and defamation. This growing challenge
underscores the urgent need for robust and generalizable face forgery detection
methods as a critical component of AI security infrastructure. In this work, we
propose a novel dual-branch convolutional neural network for face forgery
detection that leverages complementary cues from both spatial and frequency
domains. The RGB branch captures semantic information, while the frequency
branch focuses on high-frequency artifacts that are difficult for generative
models to suppress. A channel attention module is introduced to adaptively fuse
these heterogeneous features, highlighting the most informative channels for
forgery discrimination. To guide the network's learning process, we design a
unified loss function, FSC Loss, that combines focal loss, supervised
contrastive loss, and a frequency center margin loss to enhance class
separability and robustness. We evaluate our model on the DiFF benchmark, which
includes forged images generated from four representative methods:
text-to-image, image-to-image, face swap, and face edit. Our method achieves
strong performance across all categories and outperforms average human
accuracy. These results demonstrate the model's effectiveness and its potential
contribution to safeguarding AI ecosystems against visual forgery attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GroundLoc: Efficient Large-Scale Outdoor LiDAR-Only Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24623v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24623v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolai Steinke, Daniel Goehring
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this letter, we introduce GroundLoc, a LiDAR-only localization pipeline
designed to localize a mobile robot in large-scale outdoor environments using
prior maps. GroundLoc employs a Bird's-Eye View (BEV) image projection focusing
on the perceived ground area and utilizes the place recognition network R2D2,
or alternatively, the non-learning approach Scale-Invariant Feature Transform
(SIFT), to identify and select keypoints for BEV image map registration. Our
results demonstrate that GroundLoc outperforms state-of-the-art methods on the
SemanticKITTI and HeLiPR datasets across various sensors. In the multi-session
localization evaluation, GroundLoc reaches an Average Trajectory Error (ATE)
well below 50 cm on all Ouster OS2 128 sequences while meeting online runtime
requirements. The system supports various sensor models, as evidenced by
evaluations conducted with Velodyne HDL-64E, Ouster OS2 128, Aeva Aeries II,
and Livox Avia sensors. The prior maps are stored as 2D raster image maps,
which can be created from a single drive and require only 4 MB of storage per
square kilometer. The source code is available at
https://github.com/dcmlr/groundloc.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physics-Inspired Gaussian Kolmogorov-Arnold Networks for X-ray Scatter
  Correction in Cone-Beam CT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24579v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24579v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xu Jiang, Huiying Pan, Ligen Shi, Jianing Sun, Wenfeng Xu, Xing Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cone-beam CT (CBCT) employs a flat-panel detector to achieve
three-dimensional imaging with high spatial resolution. However, CBCT is
susceptible to scatter during data acquisition, which introduces CT value bias
and reduced tissue contrast in the reconstructed images, ultimately degrading
diagnostic accuracy. To address this issue, we propose a deep learning-based
scatter artifact correction method inspired by physical prior knowledge.
Leveraging the fact that the observed point scatter probability density
distribution exhibits rotational symmetry in the projection domain. The method
uses Gaussian Radial Basis Functions (RBF) to model the point scatter function
and embeds it into the Kolmogorov-Arnold Networks (KAN) layer, which provides
efficient nonlinear mapping capabilities for learning high-dimensional scatter
features. By incorporating the physical characteristics of the scattered photon
distribution together with the complex function mapping capacity of KAN, the
model improves its ability to accurately represent scatter. The effectiveness
of the method is validated through both synthetic and real-scan experiments.
Experimental results show that the model can effectively correct the scatter
artifacts in the reconstructed images and is superior to the current methods in
terms of quantitative metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OSWorld-MCP: Benchmarking MCP Tool Invocation In Computer-Use Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24563v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24563v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongrui Jia, Jitong Liao, Xi Zhang, Haiyang Xu, Tianbao Xie, Chaoya Jiang, Ming Yan, Si Liu, Wei Ye, Fei Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With advances in decision-making and reasoning capabilities, multimodal
agents show strong potential in computer application scenarios. Past
evaluations have mainly assessed GUI interaction skills, while tool invocation
abilities, such as those enabled by the Model Context Protocol (MCP), have been
largely overlooked. Comparing agents with integrated tool invocation to those
evaluated only on GUI interaction is inherently unfair. We present OSWorld-MCP,
the first comprehensive and fair benchmark for assessing computer-use agents'
tool invocation, GUI operation, and decision-making abilities in a real-world
environment. We design a novel automated code-generation pipeline to create
tools and combine them with a curated selection from existing tools. Rigorous
manual validation yields 158 high-quality tools (covering 7 common
applications), each verified for correct functionality, practical
applicability, and versatility. Extensive evaluations of state-of-the-art
multimodal agents on OSWorld-MCP show that MCP tools generally improve task
success rates (e.g., from 8.3% to 20.4% for OpenAI o3 at 15 steps, from 40.1%
to 43.3% for Claude 4 Sonnet at 50 steps), underscoring the importance of
assessing tool invocation capabilities. However, even the strongest models have
relatively low tool invocation rates, Only 36.3%, indicating room for
improvement and highlighting the benchmark's challenge. By explicitly measuring
MCP tool usage skills, OSWorld-MCP deepens understanding of multimodal agents
and sets a new standard for evaluating performance in complex, tool-assisted
environments. Our code, environment, and data are publicly available at
https://osworld-mcp.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal
  Reasoning in MLLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24514v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24514v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huanyu Zhang, Wenshan Wu, Chengzu Li, Ning Shang, Yan Xia, Yangyu Huang, Yifan Zhang, Li Dong, Zhang Zhang, Liang Wang, Tieniu Tan, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Multimodal Large Language Models (MLLMs) excel at visual understanding,
they often struggle in complex scenarios that require visual planning and
imagination. Inspired by how humans use sketching as a form of visual thinking
to develop and communicate ideas, we introduce Latent Sketchpad, a framework
that equips MLLMs with an internal visual scratchpad. The internal visual
representations of MLLMs have traditionally been confined to perceptual
understanding. We repurpose them to support generative visual thought without
compromising reasoning ability. Building on frontier MLLMs, our approach
integrates visual generation directly into their native autoregressive
reasoning process. It allows the model to interleave textual reasoning with the
generation of visual latents. These latents guide the internal thought process
and can be translated into sketch images for interpretability. To realize this,
we introduce two components: a Context-Aware Vision Head autoregressively
produces visual representations, and a pretrained Sketch Decoder renders these
into human-interpretable images. We evaluate the framework on our new dataset
MazePlanning. Experiments across various MLLMs show that Latent Sketchpad
delivers comparable or even superior reasoning performance to their backbone.
It further generalizes across distinct frontier MLLMs, including Gemma3 and
Qwen2.5-VL. By extending model's textual reasoning to visual thinking, our
framework opens new opportunities for richer human-computer interaction and
broader applications. More details and resources are available on our project
page: https://latent-sketchpad.github.io/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Local Performance vs. Out-of-Distribution Generalization: An Empirical
  Analysis of Personalized Federated Learning in Heterogeneous Data
  Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24503v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24503v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mortesa Hussaini, Jan Theiß, Anthony Stein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the context of Federated Learning with heterogeneous data environments,
local models tend to converge to their own local model optima during local
training steps, deviating from the overall data distributions. Aggregation of
these local updates, e.g., with FedAvg, often does not align with the global
model optimum (client drift), resulting in an update that is suboptimal for
most clients. Personalized Federated Learning approaches address this challenge
by exclusively focusing on the average local performances of clients' models on
their own data distribution. Generalization to out-of-distribution samples,
which is a substantial benefit of FedAvg and represents a significant component
of robustness, appears to be inadequately incorporated into the assessment and
evaluation processes. This study involves a thorough evaluation of Federated
Learning approaches, encompassing both their local performance and their
generalization capabilities. Therefore, we examine different stages within a
single communication round to enable a more nuanced understanding of the
considered metrics. Furthermore, we propose and incorporate a modified approach
of FedAvg, designated as Federated Learning with Individualized Updates (FLIU),
extending the algorithm by a straightforward individualization step with an
adaptive personalization factor. We evaluate and compare the approaches
empirically using MNIST and CIFAR-10 under various distributional conditions,
including benchmark IID and pathological non-IID, as well as additional novel
test environments with Dirichlet distribution specifically developed to stress
the algorithms on complex data heterogeneity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast and accurate neural reflectance transformation imaging through
  knowledge distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24486v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24486v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tinsae G. Dulecha, Leonardo Righetto, Ruggero Pintus, Enrico Gobbetti, Andrea Giachetti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reflectance Transformation Imaging (RTI) is very popular for its ability to
visually analyze surfaces by enhancing surface details through interactive
relighting, starting from only a few tens of photographs taken with a fixed
camera and variable illumination. Traditional methods like Polynomial Texture
Maps (PTM) and Hemispherical Harmonics (HSH) are compact and fast, but struggle
to accurately capture complex reflectance fields using few per-pixel
coefficients and fixed bases, leading to artifacts, especially in highly
reflective or shadowed areas. The NeuralRTI approach, which exploits a neural
autoencoder to learn a compact function that better approximates the local
reflectance as a function of light directions, has been shown to produce
superior quality at comparable storage cost. However, as it performs
interactive relighting with custom decoder networks with many parameters, the
rendering step is computationally expensive and not feasible at full resolution
for large images on limited hardware. Earlier attempts to reduce costs by
directly training smaller networks have failed to produce valid results. For
this reason, we propose to reduce its computational cost through a novel
solution based on Knowledge Distillation (DisK-NeuralRTI). ...
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decoupled MeanFlow: Turning Flow Models into Flow Maps for Accelerated
  Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24474v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24474v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyungmin Lee, Sihyun Yu, Jinwoo Shin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Denoising generative models, such as diffusion and flow-based models, produce
high-quality samples but require many denoising steps due to discretization
error. Flow maps, which estimate the average velocity between timesteps,
mitigate this error and enable faster sampling. However, their training
typically demands architectural changes that limit compatibility with
pretrained flow models. We introduce Decoupled MeanFlow, a simple decoding
strategy that converts flow models into flow map models without architectural
modifications. Our method conditions the final blocks of diffusion transformers
on the subsequent timestep, allowing pretrained flow models to be directly
repurposed as flow maps. Combined with enhanced training techniques, this
design enables high-quality generation in as few as 1 to 4 steps. Notably, we
find that training flow models and subsequently converting them is more
efficient and effective than training flow maps from scratch. On ImageNet
256x256 and 512x512, our models attain 1-step FID of 2.16 and 2.12,
respectively, surpassing prior art by a large margin. Furthermore, we achieve
FID of 1.51 and 1.68 when increasing the steps to 4, which nearly matches the
performance of flow models while delivering over 100x faster inference.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Kineo: Calibration-Free Metric Motion Capture From Sparse RGB Cameras 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24464v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24464v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charles Javerliat, Pierre Raimbaud, Guillaume Lavoué
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Markerless multiview motion capture is often constrained by the need for
precise camera calibration, limiting accessibility for non-experts and
in-the-wild captures. Existing calibration-free approaches mitigate this
requirement but suffer from high computational cost and reduced reconstruction
accuracy.
  We present Kineo, a fully automatic, calibration-free pipeline for markerless
motion capture from videos captured by unsynchronized, uncalibrated,
consumer-grade RGB cameras. Kineo leverages 2D keypoints from off-the-shelf
detectors to simultaneously calibrate cameras, including Brown-Conrady
distortion coefficients, and reconstruct 3D keypoints and dense scene point
maps at metric scale. A confidence-driven spatio-temporal keypoint sampling
strategy, combined with graph-based global optimization, ensures robust
calibration at a fixed computational cost independent of sequence length. We
further introduce a pairwise reprojection consensus score to quantify 3D
reconstruction reliability for downstream tasks.
  Evaluations on EgoHumans and Human3.6M demonstrate substantial improvements
over prior calibration-free methods. Compared to previous state-of-the-art
approaches, Kineo reduces camera translation error by approximately 83-85%,
camera angular error by 86-92%, and world mean-per-joint error (W-MPJPE) by
83-91%.
  Kineo is also efficient in real-world scenarios, processing multi-view
sequences faster than their duration in specific configuration (e.g., 36min to
process 1h20min of footage). The full pipeline and evaluation code are openly
released to promote reproducibility and practical adoption at
https://liris-xr.github.io/kineo/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Critical Study towards the Detection of Parkinsons Disease using ML
  Technologies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24456v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24456v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vivek Chetia, Abdul Taher Khan, Rahish Gogoi, David Kapsian Khual, Purnendu Bikash, Sajal Saha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proposed solution is Deep Learning Technique that will be able classify
three types of tea leaves diseases from which two diseases are caused by the
pests and one due to pathogens (infectious organisms) and environmental
conditions and also show the area damaged by a disease in leaves. Namely Red
Rust, Helopeltis and Red spider mite respectively. In this paper we have
evaluated two models namely SSD MobileNet V2 and Faster R-CNN ResNet50 V1 for
the object detection. The SSD MobileNet V2 gave precision of 0.209 for IOU
range of 0.50:0.95 with recall of 0.02 on IOU 0.50:0.95 and final mAP of 20.9%.
While Faster R-CNN ResNet50 V1 has precision of 0.252 on IOU range of 0.50:0.95
and recall of 0.044 on IOU of 0.50:0.95 with a mAP of 25%, which is better than
SSD. Also used Mask R-CNN for Object Instance Segmentation where we have
implemented our custom method to calculate the damaged diseased portion of
leaves. Keywords: Tea Leaf Disease, Deep Learning, Red Rust, Helopeltis and Red
Spider Mite, SSD MobileNet V2, Faster R-CNN ResNet50 V1 and Mask RCNN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Visual Intelligence: Insights from Video <span class="highlight-title">Pretrain</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24448v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24448v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pablo Acuaviva, Aram Davtyan, Mariam Hassan, Sebastian Stapf, Ahmad Rahimi, Alexandre Alahi, Paolo Favaro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated that large-scale pretraining
enables systems to adapt rapidly to new problems with little supervision in the
language domain. This success, however, has not translated as effectively to
the visual domain, where models, including LLMs, continue to struggle with
compositional understanding, sample efficiency, and general-purpose
problem-solving. We investigate Video Diffusion Models (VDMs) as a promising
direction for bridging this gap. Pretraining on spatiotemporal data endows
these models with strong inductive biases for structure and dynamics, which we
hypothesize can support broad task adaptability. To test this, we design a
controlled evaluation in which both a pretrained LLM and a pretrained VDM are
equipped with lightweight adapters and presented with tasks in their natural
modalities. Across benchmarks including ARC-AGI, ConceptARC, visual games,
route planning, and cellular automata, VDMs demonstrate higher data efficiency
than their language counterparts. Taken together, our results indicate that
video pretraining offers inductive biases that support progress toward visual
foundation models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updated version from preprint arXiv:2506.07280 (Gen2Gen) focused on
  visual intelligence. This work can be considered as v2</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SPARTA: Evaluating Reasoning Segmentation Robustness through Black-Box
  Adversarial Paraphrasing in Text Autoencoder Latent Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24446v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24446v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Viktoriia Zinkovich, Anton Antonov, Andrei Spiridonov, Denis Shepelev, Andrey Moskalenko, Daria Pugacheva, Elena Tutubalina, Andrey Kuznetsov, Vlad Shakhuro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) have shown impressive capabilities
in vision-language tasks such as reasoning segmentation, where models generate
segmentation masks based on textual queries. While prior work has primarily
focused on perturbing image inputs, semantically equivalent textual
paraphrases-crucial in real-world applications where users express the same
intent in varied ways-remain underexplored. To address this gap, we introduce a
novel adversarial paraphrasing task: generating grammatically correct
paraphrases that preserve the original query meaning while degrading
segmentation performance. To evaluate the quality of adversarial paraphrases,
we develop a comprehensive automatic evaluation protocol validated with human
studies. Furthermore, we introduce SPARTA-a black-box, sentence-level
optimization method that operates in the low-dimensional semantic latent space
of a text autoencoder, guided by reinforcement learning. SPARTA achieves
significantly higher success rates, outperforming prior methods by up to 2x on
both the ReasonSeg and LLMSeg-40k datasets. We use SPARTA and competitive
baselines to assess the robustness of advanced reasoning segmentation models.
We reveal that they remain vulnerable to adversarial paraphrasing-even under
strict semantic and grammatical constraints. All code and data will be released
publicly upon acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deeply-Conditioned Image Compression via Self-Generated Priors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24437v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24437v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhineng Zhao, Zhihai He, Zikun Zhou, Siwei Ma, Yaowei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learned image compression (LIC) has shown great promise for achieving high
rate-distortion performance. However, current LIC methods are often limited in
their capability to model the complex correlation structures inherent in
natural images, particularly the entanglement of invariant global structures
with transient local textures within a single monolithic representation. This
limitation precipitates severe geometric deformation at low bitrates. To
address this, we introduce a framework predicated on functional decomposition,
which we term Deeply-Conditioned Image Compression via self-generated priors
(DCIC-sgp). Our central idea is to first encode a potent, self-generated prior
to encapsulate the image's structural backbone. This prior is subsequently
utilized not as mere side-information, but to holistically modulate the entire
compression pipeline. This deep conditioning, most critically of the analysis
transform, liberates it to dedicate its representational capacity to the
residual, high-entropy details. This hierarchical, dependency-driven approach
achieves an effective disentanglement of information streams. Our extensive
experiments validate this assertion; visual analysis demonstrates that our
method substantially mitigates the geometric deformation artifacts that plague
conventional codecs at low bitrates. Quantitatively, our framework establishes
highly competitive performance, achieving significant BD-rate reductions of
14.4%, 15.7%, and 15.1% against the VVC test model VTM-12.1 on the Kodak, CLIC,
and Tecnick datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ XAI Evaluation Framework for Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24414v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24414v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reem Hammoud, Abdul karim Gizzini, Ali J. Ghandour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring transparency and trust in artificial intelligence (AI) models is
essential, particularly as they are increasingly applied in safety-critical and
high-stakes domains. Explainable AI (XAI) has emerged as a promising approach
to address this challenge, yet the rigorous evaluation of XAI methods remains
crucial for optimizing the trade-offs between model complexity, predictive
performance, and interpretability. While extensive progress has been achieved
in evaluating XAI techniques for classification tasks, evaluation strategies
tailored to semantic segmentation remain relatively underexplored. This work
introduces a comprehensive and systematic evaluation framework specifically
designed for assessing XAI in semantic segmentation, explicitly accounting for
both spatial and contextual task complexities. The framework employs
pixel-level evaluation strategies and carefully designed metrics to provide
fine-grained interpretability insights. Simulation results using recently
adapted class activation mapping (CAM)-based XAI schemes demonstrate the
efficiency, robustness, and reliability of the proposed methodology. These
findings contribute to advancing transparent, trustworthy, and accountable
semantic segmentation models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 50 Years of Water Body Monitoring: The Case of Qaraaoun Reservoir,
  Lebanon 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24413v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24413v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Ahmad Faour, Nabil Amacha, Ali J. Ghandour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The sustainable management of the Qaraaoun Reservoir, the largest surface
water body in Lebanon located in the Bekaa Plain, depends on reliable
monitoring of its storage volume despite frequent sensor malfunctions and
limited maintenance capacity. This study introduces a sensor-free approach that
integrates open-source satellite imagery, advanced water-extent segmentation,
and machine learning to estimate the reservoir surface area and volume in near
real time. Sentinel-2 and Landsat images are processed, where surface water is
delineated using a newly proposed water segmentation index. A machine learning
model based on Support Vector Regression (SVR) is trained on a curated dataset
that includes water surface area, water level, and water volume calculations
using a reservoir bathymetry survey. The model is then able to estimate
reservoir volume relying solely on surface area extracted from satellite
imagery, without the need for ground measurements. Water segmentation using the
proposed index aligns with ground truth for more than 95 percent of the
shoreline. Hyperparameter tuning with GridSearchCV yields an optimized SVR
performance with error under 1.5 percent of full reservoir capacity and
coefficients of determination exceeding 0.98. These results demonstrate the
robustness and cost-effectiveness of the method, offering a practical solution
for continuous, sensor-independent monitoring of reservoir storage. The
proposed methodology can be replicated for other water bodies, and the
resulting 50 years of time-series data is valuable for research on climate
change and environmental patterns.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OS-Sentinel: Towards Safety-Enhanced Mobile GUI Agents via Hybrid
  Validation in Realistic Workflows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24411v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24411v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiushi Sun, Mukai Li, Zhoumianze Liu, Zhihui Xie, Fangzhi Xu, Zhangyue Yin, Kanzhi Cheng, Zehao Li, Zichen Ding, Qi Liu, Zhiyong Wu, Zhuosheng Zhang, Ben Kao, Lingpeng Kong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computer-using agents powered by Vision-Language Models (VLMs) have
demonstrated human-like capabilities in operating digital environments like
mobile platforms. While these agents hold great promise for advancing digital
automation, their potential for unsafe operations, such as system compromise
and privacy leakage, is raising significant concerns. Detecting these safety
concerns across the vast and complex operational space of mobile environments
presents a formidable challenge that remains critically underexplored. To
establish a foundation for mobile agent safety research, we introduce
MobileRisk-Live, a dynamic sandbox environment accompanied by a safety
detection benchmark comprising realistic trajectories with fine-grained
annotations. Built upon this, we propose OS-Sentinel, a novel hybrid safety
detection framework that synergistically combines a Formal Verifier for
detecting explicit system-level violations with a VLM-based Contextual Judge
for assessing contextual risks and agent actions. Experiments show that
OS-Sentinel achieves 10%-30% improvements over existing approaches across
multiple metrics. Further analysis provides critical insights that foster the
development of safer and more reliable autonomous mobile agents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Hybrid Approach for Visual Multi-Object Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24410v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24410v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Toan Van Nguyen, Rasmus G. K. Christiansen, Dirk Kraft, Leon Bodenhagen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a visual multi-object tracking method that jointly
employs stochastic and deterministic mechanisms to ensure identifier
consistency for unknown and time-varying target numbers under nonlinear
dynamics. A stochastic particle filter addresses nonlinear dynamics and
non-Gaussian noise, with support from particle swarm optimization (PSO) to
guide particles toward state distribution modes and mitigate divergence through
proposed fitness measures incorporating motion consistency, appearance
similarity, and social-interaction cues with neighboring targets. Deterministic
association further enforces identifier consistency via a proposed cost matrix
incorporating spatial consistency between particles and current detections,
detection confidences, and track penalties. Subsequently, a novel scheme is
proposed for the smooth updating of target states while preserving their
identities, particularly for weak tracks during interactions with other targets
and prolonged occlusions. Moreover, velocity regression over past states
provides trend-seed velocities, enhancing particle sampling and state updates.
The proposed tracker is designed to operate flexibly for both pre-recorded
videos and camera live streams, where future frames are unavailable.
Experimental results confirm superior performance compared to state-of-the-art
trackers. The source-code reference implementations of both the proposed method
and compared-trackers are provided on GitHub:
https://github.com/SDU-VelKoTek/GenTrack2
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GenTrack: A New Generation of Multi-Object Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24399v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24399v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Toan Van Nguyen, Rasmus G. K. Christiansen, Dirk Kraft, Leon Bodenhagen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel multi-object tracking (MOT) method, dubbed
GenTrack, whose main contributions include: a hybrid tracking approach
employing both stochastic and deterministic manners to robustly handle unknown
and time-varying numbers of targets, particularly in maintaining target
identity (ID) consistency and managing nonlinear dynamics, leveraging particle
swarm optimization (PSO) with some proposed fitness measures to guide
stochastic particles toward their target distribution modes, enabling effective
tracking even with weak and noisy object detectors, integration of social
interactions among targets to enhance PSO-guided particles as well as improve
continuous updates of both strong (matched) and weak (unmatched) tracks,
thereby reducing ID switches and track loss, especially during occlusions, a
GenTrack-based redefined visual MOT baseline incorporating a comprehensive
state and observation model based on space consistency, appearance, detection
confidence, track penalties, and social scores for systematic and efficient
target updates, and the first-ever publicly available source-code reference
implementation with minimal dependencies, featuring three variants, including
GenTrack Basic, PSO, and PSO-Social, facilitating flexible reimplementation.
Experimental results have shown that GenTrack provides superior performance on
standard benchmarks and real-world scenarios compared to state-of-the-art
trackers, with integrated implementations of baselines for fair comparison.
Potential directions for future work are also discussed. The source-code
reference implementations of both the proposed method and compared-trackers are
provided on GitHub: https://github.com/SDU-VelKoTek/GenTrack
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Detection of Post-Stroke Brain Abnormalities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24398v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24398v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youwan Mahé, Elise Bannier, Stéphanie Leplaideur, Elisa Fromont, Francesca Galassi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Post-stroke MRI not only delineates focal lesions but also reveals secondary
structural changes, such as atrophy and ventricular enlargement. These
abnormalities, increasingly recognised as imaging biomarkers of recovery and
outcome, remain poorly captured by supervised segmentation methods. We evaluate
REFLECT, a flow-based generative model, for unsupervised detection of both
focal and non-lesional abnormalities in post-stroke patients. Using dual-expert
central-slice annotations on ATLAS data, performance was assessed at the object
level with Free-Response ROC analysis for anomaly maps. Two models were trained
on lesion-free slices from stroke patients (ATLAS) and on healthy controls
(IXI) to test the effect of training data. On ATLAS test subjects, the
IXI-trained model achieved higher lesion segmentation (Dice = 0.37 vs 0.27) and
improved sensitivity to non-lesional abnormalities (FROC = 0.62 vs 0.43).
Training on fully healthy anatomy improves the modelling of normal variability,
enabling broader and more reliable detection of structural abnormalities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ When are radiology reports useful for training medical image
  classifiers? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24385v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24385v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Herman Bergström, Zhongqi Yue, Fredrik D. Johansson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical images used to train machine learning models are often accompanied by
radiology reports containing rich expert annotations. However, relying on these
reports as inputs for clinical prediction requires the timely manual work of a
trained radiologist. This raises a natural question: when can radiology reports
be leveraged during training to improve image-only classification? Prior works
are limited to evaluating pre-trained image representations by fine-tuning them
to predict diagnostic labels, often extracted from reports, ignoring tasks with
labels that are weakly associated with the text. To address this gap, we
conduct a systematic study of how radiology reports can be used during both
pre-training and fine-tuning, across diagnostic and prognostic tasks (e.g.,
12-month readmission), and under varying training set sizes. Our findings
reveal that: (1) Leveraging reports during pre-training is beneficial for
downstream classification tasks where the label is well-represented in the
text; however, pre-training through explicit image-text alignment can be
detrimental in settings where it's not; (2) Fine-tuning with reports can lead
to significant improvements and even have a larger impact than the pre-training
method in certain settings. These results provide actionable insights into when
and how to leverage privileged text data to train medical image classifiers
while highlighting gaps in current research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Luminance-Aware Multi-Scale Network for Polarization Image Fusion with
  a Multi-Scene <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24379v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24379v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuangfan Huang, Xiaosong Li, Gao Wang, Tao Ye, Haishu Tan, Huafeng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Polarization image fusion combines S0 and DOLP images to reveal surface
roughness and material properties through complementary texture features, which
has important applications in camouflage recognition, tissue pathology
analysis, surface defect detection and other fields. To intergrate
coL-Splementary information from different polarized images in complex
luminance environment, we propose a luminance-aware multi-scale network (MLSN).
In the encoder stage, we propose a multi-scale spatial weight matrix through a
brightness-branch , which dynamically weighted inject the luminance into the
feature maps, solving the problem of inherent contrast difference in polarized
images. The global-local feature fusion mechanism is designed at the bottleneck
layer to perform windowed self-attention computation, to balance the global
context and local details through residual linking in the feature dimension
restructuring stage. In the decoder stage, to further improve the adaptability
to complex lighting, we propose a Brightness-Enhancement module, establishing
the mapping relationship between luminance distribution and texture features,
realizing the nonlinear luminance correction of the fusion result. We also
present MSP, an 1000 pairs of polarized images that covers 17 types of indoor
and outdoor complex lighting scenes. MSP provides four-direction polarization
raw maps, solving the scarcity of high-quality datasets in polarization image
fusion. Extensive experiment on MSP, PIF and GAND datasets verify that the
proposed MLSN outperms the state-of-the-art methods in subjective and objective
evaluations, and the MS-SSIM and SD metircs are higher than the average values
of other methods by 8.57%, 60.64%, 10.26%, 63.53%, 22.21%, and 54.31%,
respectively. The source code and dataset is avalable at
https://github.com/1hzf/MLS-UNet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stroke Lesion Segmentation in Clinical Workflows: A Modular,
  Lightweight, and Deployment-Ready Tool 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24378v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24378v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yann Kerverdo, Florent Leray, Youwan Mahé, Stéphanie Leplaideur, Francesca Galassi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning frameworks such as nnU-Net achieve state-of-the-art performance
in brain lesion segmentation but remain difficult to deploy clinically due to
heavy dependencies and monolithic design. We introduce \textit{StrokeSeg}, a
modular and lightweight framework that translates research-grade stroke lesion
segmentation models into deployable applications. Preprocessing, inference, and
postprocessing are decoupled: preprocessing relies on the Anima toolbox with
BIDS-compliant outputs, and inference uses ONNX Runtime with \texttt{Float16}
quantisation, reducing model size by about 50\%. \textit{StrokeSeg} provides
both graphical and command-line interfaces and is distributed as Python scripts
and as a standalone Windows executable. On a held-out set of 300 sub-acute and
chronic stroke subjects, segmentation performance was equivalent to the
original PyTorch pipeline (Dice difference $<10^{-3}$), demonstrating that
high-performing research pipelines can be transformed into portable, clinically
usable tools.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decoupling What to Count and Where to See for Referring Expression
  Counting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24374v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24374v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuda Zou, Zijian Zhang, Yongchao Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Referring Expression Counting (REC) extends class-level object counting to
the fine-grained subclass-level, aiming to enumerate objects matching a textual
expression that specifies both the class and distinguishing attribute. A
fundamental challenge, however, has been overlooked: annotation points are
typically placed on class-representative locations (e.g., heads), forcing
models to focus on class-level features while neglecting attribute information
from other visual regions (e.g., legs for "walking"). To address this, we
propose W2-Net, a novel framework that explicitly decouples the problem into
"what to count" and "where to see" via a dual-query mechanism. Specifically,
alongside the standard what-to-count (w2c) queries that localize the object, we
introduce dedicated where-to-see (w2s) queries. The w2s queries are guided to
seek and extract features from attribute-specific visual regions, enabling
precise subclass discrimination. Furthermore, we introduce Subclass Separable
Matching (SSM), a novel matching strategy that incorporates a repulsive force
to enhance inter-subclass separability during label assignment. W2-Net
significantly outperforms the state-of-the-art on the REC-8K dataset, reducing
counting error by 22.5% (validation) and 18.0% (test), and improving
localization F1 by 7% and 8%, respectively. Code will be available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Knowledge Transferring with Switching Dual-Student Framework
  for Semi-Supervised Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24366v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24366v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thanh-Huy Nguyen, Hoang-Thien Nguyen, Ba-Thinh Lam, Vi Vu, Bach X. Nguyen, Jianhua Xing, Tianyang Wang, Xingjian Li, Min Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Teacher-student frameworks have emerged as a leading approach in
semi-supervised medical image segmentation, demonstrating strong performance
across various tasks. However, the learning effects are still limited by the
strong correlation and unreliable knowledge transfer process between teacher
and student networks. To overcome this limitation, we introduce a novel
switching Dual-Student architecture that strategically selects the most
reliable student at each iteration to enhance dual-student collaboration and
prevent error reinforcement. We also introduce a strategy of Loss-Aware
Exponential Moving Average to dynamically ensure that the teacher absorbs
meaningful information from students, improving the quality of pseudo-labels.
Our plug-and-play framework is extensively evaluated on 3D medical image
segmentation datasets, where it outperforms state-of-the-art semi-supervised
methods, demonstrating its effectiveness in improving segmentation accuracy
under limited supervision.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper is under review at Pattern Recognition Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NVSim: Novel View Synthesis Simulator for Large Scale Indoor Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24335v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24335v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyu Jeong, Eunsung Kim, Sehun Park, Andrew Jaeyong Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present NVSim, a framework that automatically constructs large-scale,
navigable indoor simulators from only common image sequences, overcoming the
cost and scalability limitations of traditional 3D scanning. Our approach
adapts 3D Gaussian Splatting to address visual artifacts on sparsely observed
floors a common issue in robotic traversal data. We introduce Floor-Aware
Gaussian Splatting to ensure a clean, navigable ground plane, and a novel
mesh-free traversability checking algorithm that constructs a topological graph
by directly analyzing rendered views. We demonstrate our system's ability to
generate valid, large-scale navigation graphs from real-world data. A video
demonstration is avilable at https://youtu.be/tTiIQt6nXC8
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sound Source Localization for Spatial Mapping of Surgical Actions in
  Dynamic Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24332v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24332v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonas Hein, Lazaros Vlachopoulos, Maurits Geert Laurent Olthof, Bastian Sigrist, Philipp Fürnstahl, Matthias Seibold
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Purpose: Surgical scene understanding is key to advancing computer-aided and
intelligent surgical systems. Current approaches predominantly rely on visual
data or end-to-end learning, which limits fine-grained contextual modeling.
This work aims to enhance surgical scene representations by integrating 3D
acoustic information, enabling temporally and spatially aware multimodal
understanding of surgical environments.
  Methods: We propose a novel framework for generating 4D audio-visual
representations of surgical scenes by projecting acoustic localization
information from a phased microphone array onto dynamic point clouds from an
RGB-D camera. A transformer-based acoustic event detection module identifies
relevant temporal segments containing tool-tissue interactions which are
spatially localized in the audio-visual scene representation. The system was
experimentally evaluated in a realistic operating room setup during simulated
surgical procedures performed by experts.
  Results: The proposed method successfully localizes surgical acoustic events
in 3D space and associates them with visual scene elements. Experimental
evaluation demonstrates accurate spatial sound localization and robust fusion
of multimodal data, providing a comprehensive, dynamic representation of
surgical activity.
  Conclusion: This work introduces the first approach for spatial sound
localization in dynamic surgical scenes, marking a significant advancement
toward multimodal surgical scene representations. By integrating acoustic and
visual data, the proposed framework enables richer contextual understanding and
provides a foundation for future intelligent and autonomous surgical systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What do vision-language models see in the context? Investigating
  multimodal in-context learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24331v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24331v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriel O. dos Santos, Esther Colombini, Sandra Avila
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context learning (ICL) enables Large Language Models (LLMs) to learn tasks
from demonstration examples without parameter updates. Although it has been
extensively studied in LLMs, its effectiveness in Vision-Language Models (VLMs)
remains underexplored. In this work, we present a systematic study of ICL in
VLMs, evaluating seven models spanning four architectures on three image
captioning benchmarks. We analyze how prompt design, architectural choices, and
training strategies influence multimodal ICL. To our knowledge, we are the
first to analyze how attention patterns in VLMs vary with an increasing number
of in-context demonstrations. Our results reveal that training on imag-text
interleaved data enhances ICL performance but does not imply effective
integration of visual and textual information from demonstration examples. In
contrast, instruction tuning improves instruction-following but can reduce
reliance on in-context demonstrations, suggesting a trade-off between
instruction alignment and in-context adaptation. Attention analyses further
show that current VLMs primarily focus on textual cues and fail to leverage
visual information, suggesting a limited capacity for multimodal integration.
These findings highlight key limitations in the ICL abilities of current VLMs
and provide insights for enhancing their ability to learn from multimodal
in-context examples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Few-Shot Remote Sensing Image Scene Classification with CLIP and <span class="highlight-title">Prompt</span>
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24321v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24321v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ivica Dimitrovski, Vlatko Spasev, Ivan Kitanovski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Remote sensing applications increasingly rely on deep learning for scene
classification. However, their performance is often constrained by the scarcity
of labeled data and the high cost of annotation across diverse geographic and
sensor domains. While recent vision-language models like CLIP have shown
promise by learning transferable representations at scale by aligning visual
and textual modalities, their direct application to remote sensing remains
suboptimal due to significant domain gaps and the need for task-specific
semantic adaptation. To address this critical challenge, we systematically
explore prompt learning as a lightweight and efficient adaptation strategy for
few-shot remote sensing image scene classification. We evaluate several
representative methods, including Context Optimization, Conditional Context
Optimization, Multi-modal Prompt Learning, and Prompting with Self-Regulating
Constraints. These approaches reflect complementary design philosophies: from
static context optimization to conditional prompts for enhanced generalization,
multi-modal prompts for joint vision-language adaptation, and semantically
regularized prompts for stable learning without forgetting. We benchmark these
prompt-learning methods against two standard baselines: zero-shot CLIP with
hand-crafted prompts and a linear probe trained on frozen CLIP features.
Through extensive experiments on multiple benchmark remote sensing datasets,
including cross-dataset generalization tests, we demonstrate that prompt
learning consistently outperforms both baselines in few-shot scenarios.
Notably, Prompting with Self-Regulating Constraints achieves the most robust
cross-domain performance. Our findings underscore prompt learning as a scalable
and efficient solution for bridging the domain gap in satellite and aerial
imagery, providing a strong foundation for future research in this field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ViPER: Empowering the Self-Evolution of Visual Perception Abilities in
  Vision-Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24285v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24285v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juntian Zhang, Song Jin, Chuanqi Cheng, Yuhan Liu, Yankai Lin, Xun Zhang, Yufei Zhang, Fei Jiang, Guojun Yin, Wei Lin, Rui Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The limited capacity for fine-grained visual perception presents a critical
bottleneck for Vision-Language Models (VLMs) in real-world applications.
Addressing this is challenging due to the scarcity of high-quality data and the
limitations of existing methods: supervised fine-tuning (SFT) often compromises
general capabilities, while reinforcement fine-tuning (RFT) prioritizes textual
reasoning over visual perception. To bridge this gap, we propose a novel
two-stage task that structures visual perception learning as a coarse-to-fine
progressive process. Based on this task formulation, we develop ViPER, a
self-bootstrapping framework specifically designed to enable iterative
evolution through self-critiquing and self-prediction. By synergistically
integrating image-level and instance-level reconstruction with a two-stage
reinforcement learning strategy, ViPER establishes a closed-loop training
paradigm, where internally synthesized data directly fuel the enhancement of
perceptual ability. Applied to the Qwen2.5-VL family, ViPER produces the
Qwen-Viper series. With an average gain of 1.7% on seven comprehensive
benchmarks spanning various tasks and up to 6.0% on fine-grained perception,
Qwen-Viper consistently demonstrates superior performance across different
vision-language scenarios while maintaining generalizability. Beyond enabling
self-improvement in perceptual capabilities, ViPER provides concrete evidence
for the reciprocal relationship between generation and understanding, a
breakthrough to developing more autonomous and capable VLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Training-free Source Attribution of AI-generated Images via Resynthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24278v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24278v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pietro Bongini, Valentina Molinari, Andrea Costanzo, Benedetta Tondi, Mauro Barni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthetic image source attribution is a challenging task, especially in data
scarcity conditions requiring few-shot or zero-shot classification
capabilities. We present a new training-free one-shot attribution method based
on image resynthesis. A prompt describing the image under analysis is
generated, then it is used to resynthesize the image with all the candidate
sources. The image is attributed to the model which produced the resynthesis
closest to the original image in a proper feature space. We also introduce a
new dataset for synthetic image attribution consisting of face images from
commercial and open-source text-to-image generators. The dataset provides a
challenging attribution framework, useful for developing new attribution models
and testing their capabilities on different generative architectures. The
dataset structure allows to test approaches based on resynthesis and to compare
them to few-shot methods. Results from state-of-the-art few-shot approaches and
other baselines show that the proposed resynthesis method outperforms existing
techniques when only a few samples are available for training or fine-tuning.
The experiments also demonstrate that the new dataset is a challenging one and
represents a valuable benchmark for developing and evaluating future few-shot
and zero-shot methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 4 figures, 1 table, accepted at "The 17th IEEE
  INTERNATIONAL WORKSHOP ON INFORMATION FORENSICS AND SECURITY (WIFS2025)",
  Perth, Australia</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DynaRend: Learning 3D Dynamics via Masked Future Rendering for Robotic
  Manipulation <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24261v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24261v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyi Tian, Le Wang, Sanping Zhou, Sen Wang, Jiayi Li, Gang Hua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning generalizable robotic manipulation policies remains a key challenge
due to the scarcity of diverse real-world training data. While recent
approaches have attempted to mitigate this through self-supervised
representation learning, most either rely on 2D vision pretraining paradigms
such as masked image modeling, which primarily focus on static semantics or
scene geometry, or utilize large-scale video prediction models that emphasize
2D dynamics, thus failing to jointly learn the geometry, semantics, and
dynamics required for effective manipulation. In this paper, we present
DynaRend, a representation learning framework that learns 3D-aware and
dynamics-informed triplane features via masked reconstruction and future
prediction using differentiable volumetric rendering. By pretraining on
multi-view RGB-D video data, DynaRend jointly captures spatial geometry, future
dynamics, and task semantics in a unified triplane representation. The learned
representations can be effectively transferred to downstream robotic
manipulation tasks via action value map prediction. We evaluate DynaRend on two
challenging benchmarks, RLBench and Colosseum, as well as in real-world robotic
experiments, demonstrating substantial improvements in policy success rate,
generalization to environmental perturbations, and real-world applicability
across diverse manipulation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UtilGen: Utility-Centric Generative Data Augmentation with Dual-Level
  Task Adaptation <span class="chip">NeurIPS
  2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24262v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24262v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiyu Guo, Shuo Yang, Yiming Huang, Yancheng Long, Xiaobo Xia, Xiu Su, Bo Zhao, Zeke Xie, Liqiang Nie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data augmentation using generative models has emerged as a powerful paradigm
for enhancing performance in computer vision tasks. However, most existing
augmentation approaches primarily focus on optimizing intrinsic data attributes
-- such as fidelity and diversity -- to generate visually high-quality
synthetic data, while often neglecting task-specific requirements. Yet, it is
essential for data generators to account for the needs of downstream tasks, as
training data requirements can vary significantly across different tasks and
network architectures. To address these limitations, we propose UtilGen, a
novel utility-centric data augmentation framework that adaptively optimizes the
data generation process to produce task-specific, high-utility training data
via downstream task feedback. Specifically, we first introduce a weight
allocation network to evaluate the task-specific utility of each synthetic
sample. Guided by these evaluations, UtilGen iteratively refines the data
generation process using a dual-level optimization strategy to maximize the
synthetic data utility: (1) model-level optimization tailors the generative
model to the downstream task, and (2) instance-level optimization adjusts
generation policies -- such as prompt embeddings and initial noise -- at each
generation round. Extensive experiments on eight benchmark datasets of varying
complexity and granularity demonstrate that UtilGen consistently achieves
superior performance, with an average accuracy improvement of 3.87% over
previous SOTA. Further analysis of data influence and distribution reveals that
UtilGen produces more impactful and task-relevant synthetic data, validating
the effectiveness of the paradigm shift from visual characteristics-centric to
task utility-centric data augmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39th Conference on Neural Information Processing Systems (NeurIPS
  2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeshadowMamba: Deshadowing as 1D Sequential Similarity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24260v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24260v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaotong Yang, Yi Chen, Yanying Li, Shengfeng He, Yangyang Xu, Junyu Dong, Jian Yang, Yong Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent deep models for image shadow removal often rely on attention-based
architectures to capture long-range dependencies. However, their fixed
attention patterns tend to mix illumination cues from irrelevant regions,
leading to distorted structures and inconsistent colors. In this work, we
revisit shadow removal from a sequence modeling perspective and explore the use
of Mamba, a selective state space model that propagates global context through
directional state transitions. These transitions yield an efficient global
receptive field while preserving positional continuity. Despite its potential,
directly applying Mamba to image data is suboptimal, since it lacks awareness
of shadow-non-shadow semantics and remains susceptible to color interference
from nearby regions. To address these limitations, we propose CrossGate, a
directional modulation mechanism that injects shadow-aware similarity into
Mamba's input gate, allowing selective integration of relevant context along
transition axes. To further ensure appearance fidelity, we introduce ColorShift
regularization, a contrastive learning objective driven by global color
statistics. By synthesizing structured informative negatives, it guides the
model to suppress color contamination and achieve robust color restoration.
Together, these components adapt sequence modeling to the structural integrity
and chromatic consistency required for shadow removal. Extensive experiments on
public benchmarks demonstrate that DeshadowMamba achieves state-of-the-art
visual quality and strong quantitative performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Delving into Cascaded Instability: A Lipschitz Continuity View on Image
  Restoration and Object Detection Synergy <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24232v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24232v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qing Zhao, Weijian Deng, Pengxu Wei, ZiYi Dong, Hannan Lu, Xiangyang Ji, Liang Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To improve detection robustness in adverse conditions (e.g., haze and low
light), image restoration is commonly applied as a pre-processing step to
enhance image quality for the detector. However, the functional mismatch
between restoration and detection networks can introduce instability and hinder
effective integration -- an issue that remains underexplored. We revisit this
limitation through the lens of Lipschitz continuity, analyzing the functional
differences between restoration and detection networks in both the input space
and the parameter space. Our analysis shows that restoration networks perform
smooth, continuous transformations, while object detectors operate with
discontinuous decision boundaries, making them highly sensitive to minor
perturbations. This mismatch introduces instability in traditional cascade
frameworks, where even imperceptible noise from restoration is amplified during
detection, disrupting gradient flow and hindering optimization. To address
this, we propose Lipschitz-regularized object detection (LROD), a simple yet
effective framework that integrates image restoration directly into the
detector's feature learning, harmonizing the Lipschitz continuity of both tasks
during training. We implement this framework as Lipschitz-regularized YOLO
(LR-YOLO), extending seamlessly to existing YOLO detectors. Extensive
experiments on haze and low-light benchmarks demonstrate that LR-YOLO
consistently improves detection stability, optimization smoothness, and overall
accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking Microsaccade Recognition with Event Cameras: A Novel
  <span class="highlight-title">Dataset</span> and Evaluation <span class="chip">BMVC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24231v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24231v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Waseem Shariff, Timothy Hanley, Maciej Stec, Hossein Javidnia, Peter Corcoran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Microsaccades are small, involuntary eye movements vital for visual
perception and neural processing. Traditional microsaccade studies typically
use eye trackers or frame-based analysis, which, while precise, are costly and
limited in scalability and temporal resolution. Event-based sensing offers a
high-speed, low-latency alternative by capturing fine-grained spatiotemporal
changes efficiently. This work introduces a pioneering event-based microsaccade
dataset to support research on small eye movement dynamics in cognitive
computing. Using Blender, we render high-fidelity eye movement scenarios and
simulate microsaccades with angular displacements from 0.5 to 2.0 degrees,
divided into seven distinct classes. These are converted to event streams using
v2e, preserving the natural temporal dynamics of microsaccades, with durations
ranging from 0.25 ms to 2.25 ms. We evaluate the dataset using Spiking-VGG11,
Spiking-VGG13, and Spiking-VGG16, and propose Spiking-VGG16Flow, an
optical-flow-enhanced variant implemented in SpikingJelly. The models achieve
around 90 percent average accuracy, successfully classifying microsaccades by
angular displacement, independent of event count or duration. These results
demonstrate the potential of spiking neural networks for fine motion
recognition and establish a benchmark for event-based vision research. The
dataset, code, and trained models will be publicly available at
https://waseemshariff126.github.io/microsaccades/ .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in British Machine Vision Conference (BMVC) 2025, Main
  Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SCOPE: Saliency-Coverage Oriented Token Pruning for Efficient Multimodel
  LLMs <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24214v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24214v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinhong Deng, Wen Li, Joey Tianyi Zhou, Yang He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) typically process a large number of
visual tokens, leading to considerable computational overhead, even though many
of these tokens are redundant. Existing visual token pruning methods primarily
focus on selecting the most salient tokens based on attention scores, resulting
in the semantic incompleteness of the selected tokens. In this paper, we
propose a novel visual token pruning strategy, called
\textbf{S}aliency-\textbf{C}overage \textbf{O}riented token \textbf{P}runing
for \textbf{E}fficient MLLMs (SCOPE), to jointly model both the saliency and
coverage of the selected visual tokens to better preserve semantic
completeness. Specifically, we introduce a set-coverage for a given set of
selected tokens, computed based on the token relationships. We then define a
token-coverage gain for each unselected token, quantifying how much additional
coverage would be obtained by including it. By integrating the saliency score
into the token-coverage gain, we propose our SCOPE score and iteratively select
the token with the highest SCOPE score. We conduct extensive experiments on
multiple vision-language understanding benchmarks using the LLaVA-1.5 and
LLaVA-Next models. Experimental results demonstrate that our method
consistently outperforms prior approaches. Our code is available at
\href{https://github.com/kinredon/SCOPE}{https://github.com/kinredon/SCOPE}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Inference Intervention: Identity-Decoupled Diffusion for Face
  Anonymization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24213v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24213v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoxin Yang, Yihong Lin, Jingdan Kang, Xuemiao Xu, Yue Li, Cheng Xu, Shengfeng He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Face anonymization aims to conceal identity information while preserving
non-identity attributes. Mainstream diffusion models rely on inference-time
interventions such as negative guidance or energy-based optimization, which are
applied post-training to suppress identity features. These interventions often
introduce distribution shifts and entangle identity with non-identity
attributes, degrading visual fidelity and data utility. To address this, we
propose \textbf{ID\textsuperscript{2}Face}, a training-centric anonymization
framework that removes the need for inference-time optimization. The rationale
of our method is to learn a structured latent space where identity and
non-identity information are explicitly disentangled, enabling direct and
controllable anonymization at inference. To this end, we design a conditional
diffusion model with an identity-masked learning scheme. An Identity-Decoupled
Latent Recomposer uses an Identity Variational Autoencoder to model identity
features, while non-identity attributes are extracted from same-identity pairs
and aligned through bidirectional latent alignment. An Identity-Guided Latent
Harmonizer then fuses these representations via soft-gating conditioned on
noisy feature prediction. The model is trained with a recomposition-based
reconstruction loss to enforce disentanglement. At inference, anonymization is
achieved by sampling a random identity vector from the learned identity space.
To further suppress identity leakage, we introduce an Orthogonal Identity
Mapping strategy that enforces orthogonality between sampled and source
identity vectors. Experiments demonstrate that ID\textsuperscript{2}Face
outperforms existing methods in visual quality, identity suppression, and
utility preservation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MC-SJD : Maximal Coupling Speculative Jacobi Decoding for Autoregressive
  Visual Generation Acceleration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24211v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24211v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junhyuk So, Hyunho Kook, Chaeyeon Jang, Eunhyeok Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While autoregressive (AR) modeling has recently emerged as a new paradigm in
visual generation, its practical adoption is severely constrained by the slow
inference speed of per-token generation, which often requires thousands of
steps to produce a single sample. To address this challenge, we propose MC-SJD,
a training-free, lossless parallel decoding framework designed to accelerate AR
visual generation by extending the recently introduced Speculative Jacobi
Decoding (SJD). Although SJD shows strong potential for accelerating AR
generation, we demonstrate that token instability across iterations
significantly reduces the acceptance rate, a limitation that primarily arises
from the independent sampling process used during draft token generation. To
overcome this, we introduce MC-SJD, an information-theoretic approach based on
coupling, which substantially accelerates standard SJD by maximizing the
probability of sampling identical draft tokens across consecutive iterations,
all while preserving its lossless property. Remarkably, this method requires
only a single-line modification to the existing algorithm, yet achieves
substantial performance gains, delivering up to a ~4.2x acceleration in image
generation and ~13.3x acceleration in video generation compared to standard AR
decoding, without any degradation in output quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CLFSeg: A Fuzzy-Logic based Solution for Boundary Clarity and
  Uncertainty Reduction in Medical Image Segmentation <span class="chip">BMVC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24202v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24202v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anshul Kaushal, Kunal Jangid, Vinod K. Kurmi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate polyp and cardiac segmentation for early detection and treatment is
essential for the diagnosis and treatment planning of cancer-like diseases.
Traditional convolutional neural network (CNN) based models have represented
limited generalizability, robustness, and inability to handle uncertainty,
which affects the segmentation performance. To solve these problems, this paper
introduces CLFSeg, an encoder-decoder based framework that aggregates the
Fuzzy-Convolutional (FC) module leveraging convolutional layers and fuzzy
logic. This module enhances the segmentation performance by identifying local
and global features while minimizing the uncertainty, noise, and ambiguity in
boundary regions, ensuring computing efficiency. In order to handle class
imbalance problem while focusing on the areas of interest with tiny and
boundary regions, binary cross-entropy (BCE) with dice loss is incorporated.
Our proposed model exhibits exceptional performance on four publicly available
datasets, including CVC-ColonDB, CVC-ClinicDB, EtisLaribPolypDB, and ACDC.
Extensive experiments and visual studies show CLFSeg surpasses the existing
SOTA performance and focuses on relevant regions of interest in anatomical
structures. The proposed CLFSeg improves performance while ensuring computing
efficiency, which makes it a potential solution for real-world medical
diagnostic scenarios. Project page is available at
https://visdomlab.github.io/CLFSeg/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The 36th British Machine Vision Conference (BMVC) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vanish into Thin Air: Cross-<span class="highlight-title">prompt</span> Universal Adversarial Attacks for
  SAM2 <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24195v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24195v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqi Zhou, Yifan Hu, Yufei Song, Zijing Li, Shengshan Hu, Leo Yu Zhang, Dezhong Yao, Long Zheng, Hai Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies reveal the vulnerability of the image segmentation foundation
model SAM to adversarial examples. Its successor, SAM2, has attracted
significant attention due to its strong generalization capability in video
segmentation. However, its robustness remains unexplored, and it is unclear
whether existing attacks on SAM can be directly transferred to SAM2. In this
paper, we first analyze the performance gap of existing attacks between SAM and
SAM2 and highlight two key challenges arising from their architectural
differences: directional guidance from the prompt and semantic entanglement
across consecutive frames. To address these issues, we propose UAP-SAM2, the
first cross-prompt universal adversarial attack against SAM2 driven by dual
semantic deviation. For cross-prompt transferability, we begin by designing a
target-scanning strategy that divides each frame into k regions, each randomly
assigned a prompt, to reduce prompt dependency during optimization. For
effectiveness, we design a dual semantic deviation framework that optimizes a
UAP by distorting the semantics within the current frame and disrupting the
semantic consistency across consecutive frames. Extensive experiments on six
datasets across two segmentation tasks demonstrate the effectiveness of the
proposed method for SAM2. The comparative results show that UAP-SAM2
significantly outperforms state-of-the-art (SOTA) attacks by a large margin.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Vision-Language Models for Autonomous Driving through
  Task-Specific <span class="highlight-title">Prompt</span>ing and Spatial Reasoning <span class="chip">IROS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24152v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24152v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aodi Wu, Xubo Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This technical report presents our solution for the RoboSense Challenge at
IROS 2025, which evaluates Vision-Language Models (VLMs) on autonomous driving
scene understanding across perception, prediction, planning, and corruption
detection tasks. We propose a systematic framework built on four core
components. First, a Mixture-of-Prompts router classifies questions and
dispatches them to task-specific expert prompts, eliminating interference
across diverse question types. Second, task-specific prompts embed explicit
coordinate systems, spatial reasoning rules, role-playing,
Chain-of-Thought/Tree-of-Thought reasoning, and few-shot examples tailored to
each task. Third, a visual assembly module composes multi-view images with
object crops, magenta markers, and adaptive historical frames based on question
requirements. Fourth, we configure model inference parameters (temperature,
top-p, message roles) per task to optimize output quality. Implemented on
Qwen2.5-VL-72B, our approach achieves 70.87% average accuracy on Phase-1 (clean
data) and 72.85% on Phase-2 (corrupted data), demonstrating that structured
prompting and spatial grounding substantially enhance VLM performance on
safety-critical autonomous driving tasks. Code and prompt are available at
https://github.com/wuaodi/UCAS-CSU-phase2.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>RoboSense Challenge with IROS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MSRANetV2: An Explainable Deep Learning Architecture for Multi-class
  Classification of Colorectal Histopathological Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24136v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24136v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ovi Sarkar, Md Shafiuzzaman, Md. Faysal Ahamed, Golam Mahmud, Muhammad E. H. Chowdhury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Colorectal cancer (CRC) is a leading worldwide cause of cancer-related
mortality, and the role of prompt precise detection is of paramount interest in
improving patient outcomes. Conventional diagnostic methods such as colonoscopy
and histological examination routinely exhibit subjectivity, are extremely
time-consuming, and are susceptible to variation. Through the development of
digital pathology, deep learning algorithms have become a powerful approach in
enhancing diagnostic precision and efficiency. In our work, we proposed a
convolutional neural network architecture named MSRANetV2, specially optimized
for the classification of colorectal tissue images. The model employs a
ResNet50V2 backbone, extended with residual attention mechanisms and
squeeze-and-excitation (SE) blocks, to extract deep semantic and fine-grained
spatial features. With channel alignment and upsampling operations, MSRANetV2
effectively fuses multi-scale representations, thereby enhancing the robustness
of the classification. We evaluated our model on a five-fold stratified
cross-validation strategy on two publicly available datasets: CRC-VAL-HE-7K and
NCT-CRC-HE-100K. The proposed model achieved remarkable average Precision,
recall, F1-score, AUC, and test accuracy were 0.9884 plus-minus 0.0151, 0.9900
plus-minus 0.0151, 0.9900 plus-minus 0.0145, 0.9999 plus-minus 0.00006, and
0.9905 plus-minus 0.0025 on the 7K dataset. On the 100K dataset, they were
0.9904 plus-minus 0.0091, 0.9900 plus-minus 0.0071, 0.9900 plus-minus 0.0071,
0.9997 plus-minus 0.00016, and 0.9902 plus-minus 0.0006. Additionally, Grad-CAM
visualizations were incorporated to enhance model interpretability by
highlighting tissue areas that are medically relevant. These findings validate
that MSRANetV2 is a reliable, interpretable, and high-performing architectural
model for classifying CRC tissues.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VC4VG: Optimizing Video Captions for Text-to-Video Generation <span class="chip">EMNLP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24134v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24134v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Du, Zhuoran Lin, Kaiqiang Song, Biao Wang, Zhicheng Zheng, Tiezheng Ge, Bo Zheng, Qin Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in text-to-video (T2V) generation highlight the critical role
of high-quality video-text pairs in training models capable of producing
coherent and instruction-aligned videos. However, strategies for optimizing
video captions specifically for T2V training remain underexplored. In this
paper, we introduce VC4VG (Video Captioning for Video Generation), a
comprehensive caption optimization framework tailored to the needs of T2V
models.We begin by analyzing caption content from a T2V perspective,
decomposing the essential elements required for video reconstruction into
multiple dimensions, and proposing a principled caption design methodology. To
support evaluation, we construct VC4VG-Bench, a new benchmark featuring
fine-grained, multi-dimensional, and necessity-graded metrics aligned with
T2V-specific requirements.Extensive T2V fine-tuning experiments demonstrate a
strong correlation between improved caption quality and video generation
performance, validating the effectiveness of our approach. We release all
benchmark tools and code at https://github.com/qyr0403/VC4VG to support further
research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Compositional Image Synthesis with Inference-Time Scaling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24133v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24133v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minsuk Ji, Sanghyeok Lee, Namhyuk Ahn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite their impressive realism, modern text-to-image models still struggle
with compositionality, often failing to render accurate object counts,
attributes, and spatial relations. To address this challenge, we present a
training-free framework that combines an object-centric approach with
self-refinement to improve layout faithfulness while preserving aesthetic
quality. Specifically, we leverage large language models (LLMs) to synthesize
explicit layouts from input prompts, and we inject these layouts into the image
generation process, where a object-centric vision-language model (VLM) judge
reranks multiple candidates to select the most prompt-aligned outcome
iteratively. By unifying explicit layout-grounding with self-refine-based
inference-time scaling, our framework achieves stronger scene alignment with
prompts compared to recent text-to-image models. The code are available at
https://github.com/gcl-inha/ReFocus.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>projcet page: https://github.com/gcl-inha/ReFocus</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ETC: training-free diffusion models acceleration with Error-aware Trend
  Consistency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24129v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24129v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiajian Xie, Hubery Yin, Chen Li, Zhou Zhao, Shengyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have achieved remarkable generative quality but remain
bottlenecked by costly iterative sampling. Recent training-free methods
accelerate diffusion process by reusing model outputs. However, these methods
ignore denoising trends and lack error control for model-specific tolerance,
leading to trajectory deviations under multi-step reuse and exacerbating
inconsistencies in the generated results. To address these issues, we introduce
Error-aware Trend Consistency (ETC), a framework that (1) introduces a
consistent trend predictor that leverages the smooth continuity of diffusion
trajectories, projecting historical denoising patterns into stable future
directions and progressively distributing them across multiple approximation
steps to achieve acceleration without deviating; (2) proposes a model-specific
error tolerance search mechanism that derives corrective thresholds by
identifying transition points from volatile semantic planning to stable quality
refinement. Experiments show that ETC achieves a 2.65x acceleration over FLUX
with negligible (-0.074 SSIM score) degradation of consistency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DogMo: A Large-Scale Multi-View RGB-D <span class="highlight-title">Dataset</span> for 4D Canine Motion
  Recovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24117v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24117v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zan Wang, Siyu Chen, Luya Mo, Xinfeng Gao, Yuxin Shen, Lebin Ding, Wei Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present DogMo, a large-scale multi-view RGB-D video dataset capturing
diverse canine movements for the task of motion recovery from images. DogMo
comprises 1.2k motion sequences collected from 10 unique dogs, offering rich
variation in both motion and breed. It addresses key limitations of existing
dog motion datasets, including the lack of multi-view and real 3D data, as well
as limited scale and diversity. Leveraging DogMo, we establish four motion
recovery benchmark settings that support systematic evaluation across monocular
and multi-view, RGB and RGB-D inputs. To facilitate accurate motion recovery,
we further introduce a three-stage, instance-specific optimization pipeline
that fits the SMAL model to the motion sequences. Our method progressively
refines body shape and pose through coarse alignment, dense correspondence
supervision, and temporal regularization. Our dataset and method provide a
principled foundation for advancing research in dog motion recovery and open up
new directions at the intersection of computer vision, computer graphics, and
animal behavior modeling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UHKD: A Unified Framework for Heterogeneous Knowledge Distillation via
  Frequency-Domain Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24116v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24116v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fengming Yu, Haiwei Pan, Kejia Zhang, Jian Guan, Haiying Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge distillation (KD) is an effective model compression technique that
transfers knowledge from a high-performance teacher to a lightweight student,
reducing cost while maintaining accuracy. In visual applications, where
large-scale image models are widely used, KD enables efficient deployment.
However, architectural diversity introduces semantic discrepancies that hinder
the use of intermediate representations. Most existing KD methods are designed
for homogeneous models and degrade in heterogeneous scenarios, especially when
intermediate features are involved. Prior studies mainly focus on the logits
space, making limited use of the semantic information in intermediate layers.
To address this limitation, Unified Heterogeneous Knowledge Distillation (UHKD)
is proposed as a framework that leverages intermediate features in the
frequency domain for cross-architecture transfer. Fourier transform is applied
to capture global feature information, alleviating representational
discrepancies between heterogeneous teacher-student pairs. A Feature
Transformation Module (FTM) produces compact frequency-domain representations
of teacher features, while a learnable Feature Alignment Module (FAM) projects
student features and aligns them via multi-level matching. Training is guided
by a joint objective combining mean squared error on intermediate features with
Kullback-Leibler divergence on logits. Experiments on CIFAR-100 and ImageNet-1K
demonstrate gains of 5.59% and 0.83% over the latest method, highlighting UHKD
as an effective approach for unifying heterogeneous representations and
enabling efficient utilization of visual knowledge
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ZTRS: Zero-Imitation End-to-end Autonomous Driving with Trajectory
  Scoring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24108v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24108v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenxin Li, Wenhao Yao, Zi Wang, Xinglong Sun, Jingde Chen, Nadine Chang, Maying Shen, Jingyu Song, Zuxuan Wu, Shiyi Lan, Jose M. Alvarez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end autonomous driving maps raw sensor inputs directly into
ego-vehicle trajectories to avoid cascading errors from perception modules and
to leverage rich semantic cues. Existing frameworks largely rely on Imitation
Learning (IL), which can be limited by sub-optimal expert demonstrations and
covariate shift during deployment. On the other hand, Reinforcement Learning
(RL) has recently shown potential in scaling up with simulations, but is
typically confined to low-dimensional symbolic inputs (e.g. 3D objects and
maps), falling short of full end-to-end learning from raw sensor data. We
introduce ZTRS (Zero-Imitation End-to-End Autonomous Driving with Trajectory
Scoring), a framework that combines the strengths of both worlds: sensor inputs
without losing information and RL training for robust planning. To the best of
our knowledge, ZTRS is the first framework that eliminates IL entirely by only
learning from rewards while operating directly on high-dimensional sensor data.
ZTRS utilizes offline reinforcement learning with our proposed Exhaustive
Policy Optimization (EPO), a variant of policy gradient tailored for enumerable
actions and rewards. ZTRS demonstrates strong performance across three
benchmarks: Navtest (generic real-world open-loop planning), Navhard (open-loop
planning in challenging real-world and synthetic scenarios), and HUGSIM
(simulated closed-loop driving). Specifically, ZTRS achieves the
state-of-the-art result on Navhard and outperforms IL-based baselines on
HUGSIM. Code will be available at https://github.com/woxihuanjiangguo/ZTRS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing <span class="highlight-title">Pre-train</span>ed Representation Classifiability can Boost its
  Interpretability <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24105v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24105v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shufan Shen, Zhaobo Qi, Junshu Sun, Qingming Huang, Qi Tian, Shuhui Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The visual representation of a pre-trained model prioritizes the
classifiability on downstream tasks, while the widespread applications for
pre-trained visual models have posed new requirements for representation
interpretability. However, it remains unclear whether the pre-trained
representations can achieve high interpretability and classifiability
simultaneously. To answer this question, we quantify the representation
interpretability by leveraging its correlation with the ratio of interpretable
semantics within the representations. Given the pre-trained representations,
only the interpretable semantics can be captured by interpretations, whereas
the uninterpretable part leads to information loss. Based on this fact, we
propose the Inherent Interpretability Score (IIS) that evaluates the
information loss, measures the ratio of interpretable semantics, and quantifies
the representation interpretability. In the evaluation of the representation
interpretability with different classifiability, we surprisingly discover that
the interpretability and classifiability are positively correlated, i.e.,
representations with higher classifiability provide more interpretable
semantics that can be captured in the interpretations. This observation further
supports two benefits to the pre-trained representations. First, the
classifiability of representations can be further improved by fine-tuning with
interpretability maximization. Second, with the classifiability improvement for
the representations, we obtain predictions based on their interpretations with
less accuracy degradation. The discovered positive correlation and
corresponding applications show that practitioners can unify the improvements
in interpretability and classifiability for pre-trained vision models. Codes
are available at https://github.com/ssfgunner/IIS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025 (Spotlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OmniText: A Training-Free Generalist for Controllable Text-Image
  Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24093v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24093v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Agus Gunawan, Samuel Teodoro, Yun Chen, Soo Ye Kim, Jihyong Oh, Munchurl Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in diffusion-based text synthesis have demonstrated
significant performance in inserting and editing text within images via
inpainting. However, despite the potential of text inpainting methods, three
key limitations hinder their applicability to broader Text Image Manipulation
(TIM) tasks: (i) the inability to remove text, (ii) the lack of control over
the style of rendered text, and (iii) a tendency to generate duplicated
letters. To address these challenges, we propose OmniText, a training-free
generalist capable of performing a wide range of TIM tasks. Specifically, we
investigate two key properties of cross- and self-attention mechanisms to
enable text removal and to provide control over both text styles and content.
Our findings reveal that text removal can be achieved by applying
self-attention inversion, which mitigates the model's tendency to focus on
surrounding text, thus reducing text hallucinations. Additionally, we
redistribute cross-attention, as increasing the probability of certain text
tokens reduces text hallucination. For controllable inpainting, we introduce
novel loss functions in a latent optimization framework: a cross-attention
content loss to improve text rendering accuracy and a self-attention style loss
to facilitate style customization. Furthermore, we present OmniText-Bench, a
benchmark dataset for evaluating diverse TIM tasks. It includes input images,
target text with masks, and style references, covering diverse applications
such as text removal, rescaling, repositioning, and insertion and editing with
various styles. Our OmniText framework is the first generalist method capable
of performing diverse TIM tasks. It achieves state-of-the-art performance
across multiple tasks and metrics compared to other text inpainting methods and
is comparable with specialist methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contributed equally to this work. The last two
  authors are co-corresponding authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Objects: Contextual Synthetic Data Generation for Fine-Grained
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24078v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24078v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Yang, Xindi Wu, Zhiwei Deng, Esin Tureci, Olga Russakovsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image (T2I) models are increasingly used for synthetic dataset
generation, but generating effective synthetic training data for classification
remains challenging. Fine-tuning a T2I model with a few real examples can help
improve the quality of synthetic training data; however, it may also cause
overfitting and reduce diversity in the generated samples. We propose a
fine-tuning strategy BOB (BeyondOBjects) to mitigate these concerns for
fine-grained classification. Given a small set of real examples, we first
extract class-agnostic attributes such as scene background and object pose. We
then explicitly condition on these attributes during fine-tuning of the T2I
model and marginalize them out during generation. This design mitigates
overfitting, preserves the T2I model's generative prior, reduces estimation
errors, and further minimizes unintended inter-class associations. Extensive
experiments across multiple T2I models, backbones, and datasets show that our
method achieves state-of-the-art performance in low-shot fine-grained
classification when augmented with synthetic data. Concretely, BOB outperforms
DataDream by 7.4% on the Aircraft dataset (from 50.0% to 57.4% when fine-tuning
a CLIP classifier with five real images augmented with 100 synthetic images).
In three of the four benchmarks, fine-tuning downstream models with 5 real
images augmented with BOB achieves better performance than fine-tuning with 10
real images. Collectively, BOB outperforms prior art in 18 of 24 experimental
settings, with 2+% accuracy improvements in 14 of these settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing CLIP Robustness via Cross-Modality Alignment <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24038v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24038v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingyu Zhu, Beier Zhu, Shuo Wang, Kesen Zhao, Hanwang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models (VLMs) such as CLIP demonstrate strong generalization
in zero-shot classification but remain highly vulnerable to adversarial
perturbations. Existing methods primarily focus on adversarial fine-tuning or
prompt optimization; they often overlook the gaps in CLIP's encoded features,
which is shown as the text and image features lie far apart from each other.
This misalignment is significantly amplified under adversarial perturbations,
leading to severe degradation in classification performance. To address this
problem, we propose Cross-modality Alignment, dubbed COLA, an optimal
transport-based framework that explicitly addresses adversarial misalignment by
restoring both global image-text alignment and local structural consistency in
the feature space. (1) COLA first projects adversarial image embeddings onto a
subspace spanned by class text features, effectively filtering out non-semantic
distortions while preserving discriminative information. (2) It then models
images and texts as discrete distributions over multiple augmented views and
refines their alignment via OT, with the subspace projection seamlessly
integrated into the cost computation. This design ensures stable cross-modal
alignment even under adversarial conditions. COLA is training-free and
compatible with existing fine-tuned models. Extensive evaluations across 14
zero-shot classification benchmarks demonstrate the effectiveness of COLA,
especially with an average improvement of 6.7% on ImageNet and its variants
under PGD adversarial attacks, while maintaining high accuracy on clean
samples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025 Spotlight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Kernelized Sparse Fine-Tuning with Bi-level Parameter Competition for
  Vision Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24037v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24037v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shufan Shen, Junshu Sun, Shuhui Wang, Qingming Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parameter-efficient fine-tuning (PEFT) aims to adapt pre-trained vision
models to downstream tasks. Among PEFT paradigms, sparse tuning achieves
remarkable performance by adjusting only the weights most relevant to
downstream tasks, rather than densely tuning the entire weight matrix. Current
methods follow a two-stage paradigm. First, it locates task-relevant weights by
gradient information, which overlooks the parameter adjustments during
fine-tuning and limits the performance. Second, it updates only the located
weights by applying a sparse mask to the gradient of the weight matrix, which
results in high memory usage due to the storage of all weight matrices in the
optimizer. In this paper, we propose a one-stage method named SNELLA to
overcome the above limitations. For memory usage, SNELLA selectively updates
the weight matrix by adding it to another sparse matrix that is merged by two
low-rank learnable matrices. We extend the low-rank decomposition by
introducing nonlinear kernel functions, thereby increasing the rank of the
resulting merged matrix to prevent the interdependency among weight updates,
enabling better adaptation to downstream tasks. For locating task-relevant
weights, we propose an adaptive bi-level sparsity allocation mechanism that
encourages weights to compete across and inside layers based on their
importance scores in an end-to-end manner. Extensive experiments are conducted
on classification, segmentation, and generation tasks using different
pre-trained vision models. The results show that SNELLA achieves SOTA
performance with low memory usage. Notably, SNELLA obtains 1.8% (91.9% v.s.
90.1%) higher Top-1 accuracy on the FGVC benchmark compared to SPT-LoRA.
Compared to previous methods, SNELLA achieves a memory reduction of 31.1%-39.9%
across models with parameter scales from 86M to 632M. Our source codes are
available at https://github.com/ssfgunner/SNELL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ResNet: Enabling Deep Convolutional Neural Networks through Residual
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24036v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24036v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingyu Liu, Kun Ming Goh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional Neural Networks (CNNs) has revolutionized computer vision, but
training very deep networks has been challenging due to the vanishing gradient
problem. This paper explores Residual Networks (ResNet), introduced by He et
al. (2015), which overcomes this limitation by using skip connections. ResNet
enables the training of networks with hundreds of layers by allowing gradients
to flow directly through shortcut connections that bypass intermediate layers.
In our implementation on the CIFAR-10 dataset, ResNet-18 achieves 89.9%
accuracy compared to 84.1% for a traditional deep CNN of similar depth, while
also converging faster and training more stably.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>3 pages, 5 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Auto<span class="highlight-title">Prompt</span>: Automated Red-Teaming of Text-to-Image Models via LLM-Driven
  Adversarial <span class="highlight-title">Prompt</span>s <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24034v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24034v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yufan Liu, Wanqian Zhang, Huashan Chen, Lin Wang, Xiaojun Jia, Zheng Lin, Weiping Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite rapid advancements in text-to-image (T2I) models, their safety
mechanisms are vulnerable to adversarial prompts, which maliciously generate
unsafe images. Current red-teaming methods for proactively assessing such
vulnerabilities usually require white-box access to T2I models, and rely on
inefficient per-prompt optimization, as well as inevitably generate
semantically meaningless prompts easily blocked by filters. In this paper, we
propose APT (AutoPrompT), a black-box framework that leverages large language
models (LLMs) to automatically generate human-readable adversarial suffixes for
benign prompts. We first introduce an alternating optimization-finetuning
pipeline between adversarial suffix optimization and fine-tuning the LLM
utilizing the optimized suffix. Furthermore, we integrates a dual-evasion
strategy in optimization phase, enabling the bypass of both perplexity-based
filter and blacklist word filter: (1) we constrain the LLM generating
human-readable prompts through an auxiliary LLM perplexity scoring, which
starkly contrasts with prior token-level gibberish, and (2) we also introduce
banned-token penalties to suppress the explicit generation of banned-tokens in
blacklist. Extensive experiments demonstrate the excellent red-teaming
performance of our human-readable, filter-resistant adversarial prompts, as
well as superior zero-shot transferability which enables instant adaptation to
unseen prompts and exposes critical vulnerabilities even in commercial APIs
(e.g., Leonardo.Ai.).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Listening without Looking: Modality Bias in Audio-Visual Captioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24024v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24024v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchi Ishikawa, Toranosuke Manabe, Tatsuya Komatsu, Yoshimitsu Aoki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-visual captioning aims to generate holistic scene descriptions by
jointly modeling sound and vision. While recent methods have improved
performance through sophisticated modality fusion, it remains unclear to what
extent the two modalities are complementary in current audio-visual captioning
models and how robust these models are when one modality is degraded. We
address these questions by conducting systematic modality robustness tests on
LAVCap, a state-of-the-art audio-visual captioning model, in which we
selectively suppress or corrupt the audio or visual streams to quantify
sensitivity and complementarity. The analysis reveals a pronounced bias toward
the audio stream in LAVCap. To evaluate how balanced audio-visual captioning
models are in their use of both modalities, we augment AudioCaps with textual
annotations that jointly describe the audio and visual streams, yielding the
AudioVisualCaps dataset. In our experiments, we report LAVCap baseline results
on AudioVisualCaps. We also evaluate the model under modality robustness tests
on AudioVisualCaps and the results indicate that LAVCap trained on
AudioVisualCaps exhibits less modality bias than when trained on AudioCaps.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mars-Bench: A Benchmark for Evaluating Foundation Models for Mars
  Science Tasks <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24010v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24010v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mirali Purohit, Bimal Gajera, Vatsal Malaviya, Irish Mehta, Kunal Kasodekar, Jacob Adler, Steven Lu, Umaa Rebbapragada, Hannah Kerner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models have enabled rapid progress across many specialized domains
by leveraging large-scale pre-training on unlabeled data, demonstrating strong
generalization to a variety of downstream tasks. While such models have gained
significant attention in fields like Earth Observation, their application to
Mars science remains limited. A key enabler of progress in other domains has
been the availability of standardized benchmarks that support systematic
evaluation. In contrast, Mars science lacks such benchmarks and standardized
evaluation frameworks, which have limited progress toward developing foundation
models for Martian tasks. To address this gap, we introduce Mars-Bench, the
first benchmark designed to systematically evaluate models across a broad range
of Mars-related tasks using both orbital and surface imagery. Mars-Bench
comprises 20 datasets spanning classification, segmentation, and object
detection, focused on key geologic features such as craters, cones, boulders,
and frost. We provide standardized, ready-to-use datasets and baseline
evaluations using models pre-trained on natural images, Earth satellite data,
and state-of-the-art vision-language models. Results from all analyses suggest
that Mars-specific foundation models may offer advantages over general-domain
counterparts, motivating further exploration of domain-adapted pre-training.
Mars-Bench aims to establish a standardized foundation for developing and
comparing machine learning models for Mars science. Our data, models, and code
are available at: https://mars-bench.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards the Automatic Segmentation, Modeling and Meshing of the Aortic
  Vessel Tree from Multicenter Acquisitions: An <span class="highlight-title">Overview</span> of the SEG.A. 2023
  Segmentation of the Aorta Challenge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24009v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24009v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Jin, Antonio Pepe, Gian Marco Melito, Yuxuan Chen, Yunsu Byeon, Hyeseong Kim, Kyungwon Kim, Doohyun Park, Euijoon Choi, Dosik Hwang, Andriy Myronenko, Dong Yang, Yufan He, Daguang Xu, Ayman El-Ghotni, Mohamed Nabil, Hossam El-Kady, Ahmed Ayyad, Amr Nasr, Marek Wodzinski, Henning Müller, Hyeongyu Kim, Yejee Shin, Abbas Khan, Muhammad Asad, Alexander Zolotarev, Caroline Roney, Anthony Mathur, Martin Benning, Gregory Slabaugh, Theodoros Panagiotis Vagenas, Konstantinos Georgas, George K. Matsopoulos, Jihan Zhang, Zhen Zhang, Liqin Huang, Christian Mayer, Heinrich Mächler, Jan Egger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The automated analysis of the aortic vessel tree (AVT) from computed
tomography angiography (CTA) holds immense clinical potential, but its
development has been impeded by a lack of shared, high-quality data. We
launched the SEG.A. challenge to catalyze progress in this field by introducing
a large, publicly available, multi-institutional dataset for AVT segmentation.
The challenge benchmarked automated algorithms on a hidden test set, with
subsequent optional tasks in surface meshing for computational simulations. Our
findings reveal a clear convergence on deep learning methodologies, with 3D
U-Net architectures dominating the top submissions. A key result was that an
ensemble of the highest-ranking algorithms significantly outperformed
individual models, highlighting the benefits of model fusion. Performance was
strongly linked to algorithmic design, particularly the use of customized
post-processing steps, and the characteristics of the training data. This
initiative not only establishes a new performance benchmark but also provides a
lasting resource to drive future innovation toward robust, clinically
translatable tools.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AdvBlur: Adversarial Blur for Robust Diabetic Retinopathy Classification
  and Cross-Domain Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24000v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24000v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heethanjan Kanagalingam, Thenukan Pathmanathan, Mokeeshan Vathanakumar, Tharmakulasingam Mukunthan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diabetic retinopathy (DR) is a leading cause of vision loss worldwide, yet
early and accurate detection can significantly improve treatment outcomes.
While numerous Deep learning (DL) models have been developed to predict DR from
fundus images, many face challenges in maintaining robustness due to
distributional variations caused by differences in acquisition devices,
demographic disparities, and imaging conditions. This paper addresses this
critical limitation by proposing a novel DR classification approach, a method
called AdvBlur. Our method integrates adversarial blurred images into the
dataset and employs a dual-loss function framework to address domain
generalization. This approach effectively mitigates the impact of unseen
distributional variations, as evidenced by comprehensive evaluations across
multiple datasets. Additionally, we conduct extensive experiments to explore
the effects of factors such as camera type, low-quality images, and dataset
size. Furthermore, we perform ablation studies on blurred images and the loss
function to ensure the validity of our choices. The experimental results
demonstrate the effectiveness of our proposed method, achieving competitive
performance compared to state-of-the-art domain generalization DR models on
unseen external datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TeleEgo: Benchmarking Egocentric AI Assistants in the Wild 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23981v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23981v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaqi Yan, Ruilong Ren, Jingren Liu, Shuning Xu, Ling Wang, Yiheng Wang, Yun Wang, Long Zhang, Xiangyu Chen, Changzhi Sun, Jixiang Luo, Dell Zhang, Hao Sun, Chi Zhang, Xuelong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Egocentric AI assistants in real-world settings must process multi-modal
inputs (video, audio, text), respond in real time, and retain evolving
long-term memory. However, existing benchmarks typically evaluate these
abilities in isolation, lack realistic streaming scenarios, or support only
short-term tasks. We introduce \textbf{TeleEgo}, a long-duration, streaming,
omni-modal benchmark for evaluating egocentric AI assistants in realistic daily
contexts. The dataset features over 14 hours per participant of synchronized
egocentric video, audio, and text across four domains: work \& study, lifestyle
\& routines, social activities, and outings \& culture. All data is aligned on
a unified global timeline and includes high-quality visual narrations and
speech transcripts, curated through human refinement.TeleEgo defines 12
diagnostic subtasks across three core capabilities: Memory (recalling past
events), Understanding (interpreting the current moment), and Cross-Memory
Reasoning (linking distant events). It contains 3,291 human-verified QA items
spanning multiple question formats (single-choice, binary, multi-choice, and
open-ended), evaluated strictly in a streaming setting. We propose two key
metrics -- Real-Time Accuracy and Memory Persistence Time -- to jointly assess
correctness, temporal responsiveness, and long-term retention. TeleEgo provides
a realistic and comprehensive evaluation to advance the development of
practical AI assistants.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Cost-and-Quality Controllable Arbitrary-scale Super-resolution
  with Fourier Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23978v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23978v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kazutoshi Akita, Norimichi Ukita
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cost-and-Quality (CQ) controllability in arbitrary-scale super-resolution is
crucial. Existing methods predict Fourier components one by one using a
recurrent neural network. However, this approach leads to performance
degradation and inefficiency due to independent prediction. This paper proposes
predicting multiple components jointly to improve both quality and efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synergistic Neural Forecasting of Air Pollution with Stochastic Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23977v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23977v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yohan Abeysinghe, Muhammad Akhtar Munir, Sanoojan Baliah, Ron Sarafian, Fahad Shahbaz Khan, Yinon Rudich, Salman Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Air pollution remains a leading global health and environmental risk,
particularly in regions vulnerable to episodic air pollution spikes due to
wildfires, urban haze and dust storms. Accurate forecasting of particulate
matter (PM) concentrations is essential to enable timely public health warnings
and interventions, yet existing models often underestimate rare but hazardous
pollution events. Here, we present SynCast, a high-resolution neural
forecasting model that integrates meteorological and air composition data to
improve predictions of both average and extreme pollution levels. Built on a
regionally adapted transformer backbone and enhanced with a diffusion-based
stochastic refinement module, SynCast captures the nonlinear dynamics driving
PM spikes more accurately than existing approaches. Leveraging on harmonized
ERA5 and CAMS datasets, our model shows substantial gains in forecasting
fidelity across multiple PM variables (PM$_1$, PM$_{2.5}$, PM$_{10}$),
especially under extreme conditions. We demonstrate that conventional loss
functions underrepresent distributional tails (rare pollution events) and show
that SynCast, guided by domain-aware objectives and extreme value theory,
significantly enhances performance in highly impacted regions without
compromising global accuracy. This approach provides a scalable foundation for
next-generation air quality early warning systems and supports climate-health
risk mitigation in vulnerable regions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reasoning Visual Language Model for Chest X-Ray Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23968v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23968v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andriy Myronenko, Dong Yang, Baris Turkbey, Mariam Aboian, Sena Azamat, Esra Akcicek, Hongxu Yin, Pavlo Molchanov, Marc Edgar, Yufan He, Pengfei Guo, Yucheng Tang, Daguang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models (VLMs) have shown strong promise for medical image
analysis, but most remain opaque, offering predictions without the transparent,
stepwise reasoning clinicians rely on. We present a framework that brings
chain-of-thought (CoT) reasoning to chest X-ray interpretation. Inspired by
reasoning-first training paradigms, our approach is designed to learn how
experts reason, not just what they conclude, by aligning intermediate steps
with observable image evidence and radiology workflow. Beyond accuracy, the
explicit reasoning traces support clinical auditability: they reveal why a
conclusion was reached, which alternatives were considered, and where
uncertainty remains, enabling quality assurance, error analysis, and safer
human-AI collaboration.
  Our model couples high-fidelity visual encoding with a two-stage training
recipe: a reasoning-style supervised fine-tuning (SFT) followed by
reinforcement learning (RL) that uses verifiable rewards over a list of X-ray
abnormalities. The model outputs reasoning that mirrors radiologists systematic
thought process, uncertainty, and differential diagnosis. In
out-of-distribution evaluation, the approach achieves competitive multi-label
classification while improving interpretability. In a reader study with expert
radiologists, full reasoning traces increased confidence, supported error
auditing, and reduced time to finalize reports. We release code and the model
NV-Reason-CXR-3B to support community progress toward trustworthy, explainable
AI in chest radiography and other medical imaging tasks where reasoning quality
is as critical as prediction quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NV-Reason-CXR-3B</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SafeVision: Efficient Image Guardrail with Robust Policy Adherence and
  Explainability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23960v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23960v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peiyang Xu, Minzhou Pan, Zhaorun Chen, Shuang Yang, Chaowei Xiao, Bo Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid proliferation of digital media, the need for efficient and
transparent safeguards against unsafe content is more critical than ever.
Traditional image guardrail models, constrained by predefined categories, often
misclassify content due to their pure feature-based learning without semantic
reasoning. Moreover, these models struggle to adapt to emerging threats,
requiring costly retraining for new threats. To address these limitations, we
introduce SafeVision, a novel image guardrail that integrates human-like
reasoning to enhance adaptability and transparency. Our approach incorporates
an effective data collection and generation framework, a policy-following
training pipeline, and a customized loss function. We also propose a diverse QA
generation and training strategy to enhance learning effectiveness. SafeVision
dynamically aligns with evolving safety policies at inference time, eliminating
the need for retraining while ensuring precise risk assessments and
explanations. Recognizing the limitations of existing unsafe image benchmarks,
which either lack granularity or cover limited risks, we introduce VisionHarm,
a high-quality dataset comprising two subsets: VisionHarm Third-party
(VisionHarm-T) and VisionHarm Comprehensive(VisionHarm-C), spanning diverse
harmful categories. Through extensive experiments, we show that SafeVision
achieves state-of-the-art performance on different benchmarks. SafeVision
outperforms GPT-4o by 8.6% on VisionHarm-T and by 15.5% on VisionHarm-C, while
being over 16x faster. SafeVision sets a comprehensive, policy-following, and
explainable image guardrail with dynamic adaptation to emerging threats.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>42 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural USD: An object-centric framework for iterative editing and
  control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23956v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23956v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alejandro Escontrela, Shrinu Kushagra, Sjoerd van Steenkiste, Yulia Rubanova, Aleksander Holynski, Kelsey Allen, Kevin Murphy, Thomas Kipf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Amazing progress has been made in controllable generative modeling,
especially over the last few years. However, some challenges remain. One of
them is precise and iterative object editing. In many of the current methods,
trying to edit the generated image (for example, changing the color of a
particular object in the scene or changing the background while keeping other
elements unchanged) by changing the conditioning signals often leads to
unintended global changes in the scene. In this work, we take the first steps
to address the above challenges. Taking inspiration from the Universal Scene
Descriptor (USD) standard developed in the computer graphics community, we
introduce the "Neural Universal Scene Descriptor" or Neural USD. In this
framework, we represent scenes and objects in a structured, hierarchical
manner. This accommodates diverse signals, minimizes model-specific
constraints, and enables per-object control over appearance, geometry, and
pose. We further apply a fine-tuning approach which ensures that the above
control signals are disentangled from one another. We evaluate several design
considerations for our framework, demonstrating how Neural USD enables
iterative and incremental workflows. More information at:
https://escontrela.me/neural_usd .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 16 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient License Plate Recognition via Pseudo-Labeled Supervision with
  Grounding DINO and YOLOv8 <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25032v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25032v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zahra Ebrahimi Vargoorani, Amir Mohammad Ghoreyshi, Ching Yee Suen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing a highly accurate automatic license plate recognition system
(ALPR) is challenging due to environmental factors such as lighting, rain, and
dust. Additional difficulties include high vehicle speeds, varying camera
angles, and low-quality or low-resolution images. ALPR is vital in traffic
control, parking, vehicle tracking, toll collection, and law enforcement
applications. This paper proposes a deep learning strategy using YOLOv8 for
license plate detection and recognition tasks. This method seeks to enhance the
performance of the model using datasets from Ontario, Quebec, California, and
New York State. It achieved an impressive recall rate of 94% on the dataset
from the Center for Pattern Recognition and Machine Intelligence (CENPARMI) and
91% on the UFPR-ALPR dataset. In addition, our method follows a semi-supervised
learning framework, combining a small set of manually labeled data with
pseudo-labels generated by Grounding DINO to train our detection model.
Grounding DINO, a powerful vision-language model, automatically annotates many
images with bounding boxes for license plates, thereby minimizing the reliance
on labor-intensive manual labeling. By integrating human-verified and
model-generated annotations, we can scale our dataset efficiently while
maintaining label quality, which significantly enhances the training process
and overall model performance. Furthermore, it reports character error rates
for both datasets, providing additional insight into system performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 8 figures. Presented at 2025 IEEE International Workshop on
  Machine Learning for Signal Processing (MLSP), August 31 - September 3, 2025,
  Istanbul, Turkey</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Resi-VidTok: An Efficient and Decomposed Progressive Tokenization
  Framework for Ultra-Low-Rate and Lightweight Video Transmission 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25002v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25002v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyu Liu, Yi Ma, Rahim Tafazolli, Zhi Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-time transmission of video over wireless networks remains highly
challenging, even with advanced deep models, particularly under severe channel
conditions such as limited bandwidth and weak connectivity. In this paper, we
propose Resi-VidTok, a Resilient Tokenization-Enabled framework designed for
ultra-low-rate and lightweight video transmission that delivers strong
robustness while preserving perceptual and semantic fidelity on commodity
digital hardware. By reorganizing spatio--temporal content into a discrete,
importance-ordered token stream composed of key tokens and refinement tokens,
Resi-VidTok enables progressive encoding, prefix-decodable reconstruction, and
graceful quality degradation under constrained channels. A key contribution is
a resilient 1D tokenization pipeline for video that integrates differential
temporal token coding, explicitly supporting reliable recovery from incomplete
token sets using a single shared framewise decoder--without auxiliary temporal
extractors or heavy generative models. Furthermore, stride-controlled frame
sparsification combined with a lightweight decoder-side interpolator reduces
transmission load while maintaining motion continuity. Finally, a
channel-adaptive source--channel coding and modulation scheme dynamically
allocates rate and protection according to token importance and channel
condition, yielding stable quality across adverse SNRs. Evaluation results
indicate robust visual and semantic consistency at channel bandwidth ratios
(CBR) as low as 0.0004 and real-time reconstruction at over 30 fps,
demonstrating the practicality of Resi-VidTok for energy-efficient,
latency-sensitive, and reliability-critical wireless applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FT-ARM: Fine-Tuned Agentic Reflection Multimodal Language Model for
  Pressure Ulcer Severity Classification with Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24980v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24980v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reza Saadati Fard, Emmanuel Agu, Palawat Busaranuvong, Deepak Kumar, Shefalika Gautam, Bengisu Tulu, Diane Strong, Lorraine Loretz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pressure ulcers (PUs) are a serious and prevalent healthcare concern.
Accurate classification of PU severity (Stages I-IV) is essential for proper
treatment but remains challenging due to subtle visual distinctions and
subjective interpretation, leading to variability among clinicians. Prior
AI-based approaches using Convolutional Neural Networks (CNNs) and Vision
Transformers (ViTs) achieved promising accuracy but offered limited
interpretability. We present FT-ARM (Fine-Tuned Agentic Reflection Multimodal
model), a fine-tuned multimodal large language model (MLLM) with an agentic
self-reflection mechanism for pressure ulcer severity classification. Inspired
by clinician-style diagnostic reassessment, FT-ARM iteratively refines its
predictions by reasoning over visual features and encoded clinical knowledge
from text, enhancing both accuracy and consistency. On the publicly available
Pressure Injury Image Dataset (PIID), FT-ARM, fine-tuned from LLaMA 3.2 90B,
achieved 85% accuracy in classifying PU stages I-IV, surpassing prior CNN-based
models by +4%. Unlike earlier CNN/ViT studies that relied solely on offline
evaluations, FT-ARM is designed and tested for live inference, reflecting
real-time deployment conditions. Furthermore, it produces clinically grounded
natural-language explanations, improving interpretability and trust. By
integrating fine-tuning and reflective reasoning across multimodal inputs,
FT-ARM advances the reliability, transparency, and clinical applicability of
automated wound assessment systems, addressing the critical need for consistent
and explainable PU staging to support improved patient care.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SCOUT: A Lightweight Framework for Scenario Coverage Assessment in
  Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24949v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24949v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anil Yildiz, Sarah M. Thornton, Carl Hildebrandt, Sreeja Roy-Singh, Mykel J. Kochenderfer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Assessing scenario coverage is crucial for evaluating the robustness of
autonomous agents, yet existing methods rely on expensive human annotations or
computationally intensive Large Vision-Language Models (LVLMs). These
approaches are impractical for large-scale deployment due to cost and
efficiency constraints. To address these shortcomings, we propose SCOUT
(Scenario Coverage Oversight and Understanding Tool), a lightweight surrogate
model designed to predict scenario coverage labels directly from an agent's
latent sensor representations. SCOUT is trained through a distillation process,
learning to approximate LVLM-generated coverage labels while eliminating the
need for continuous LVLM inference or human annotation. By leveraging
precomputed perception features, SCOUT avoids redundant computations and
enables fast, scalable scenario coverage estimation. We evaluate our method
across a large dataset of real-life autonomous navigation scenarios,
demonstrating that it maintains high accuracy while significantly reducing
computational cost. Our results show that SCOUT provides an effective and
practical alternative for large-scale coverage analysis. While its performance
depends on the quality of LVLM-generated training labels, SCOUT represents a
major step toward efficient scenario coverage oversight in autonomous systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IBIS: A Powerful Hybrid Architecture for Human Activity Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24936v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24936v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alison M. Fernandes, Hermes I. Del Monego, Bruno S. Chang, Anelise Munaretto, Hélder M. Fontes, Rui L. Campos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing interest in Wi-Fi sensing stems from its potential to capture
environmental data in a low-cost, non-intrusive way, making it ideal for
applications like healthcare, space occupancy analysis, and gesture-based IoT
control. However, a major limitation in this field is the common problem of
overfitting, where models perform well on training data but fail to generalize
to new data. To overcome this, we introduce a novel hybrid architecture that
integrates Inception-BiLSTM with a Support Vector Machine (SVM), which we refer
to as IBIS. Our IBIS approach is uniquely engineered to improve model
generalization and create more robust classification boundaries. By applying
this method to Doppler-derived data, we achieve a movement recognition accuracy
of nearly 99%. Comprehensive performance metrics and confusion matrices confirm
the significant effectiveness of our proposed solution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages. 8 figures. Wireless Days Conference, December 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modality-Aware SAM: Sharpness-Aware-Minimization Driven Gradient
  Modulation for Harmonized Multimodal Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24919v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24919v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hossein R. Nowdeh, Jie Ji, Xiaolong Ma, Fatemeh Afghah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In multimodal learning, dominant modalities often overshadow others, limiting
generalization. We propose Modality-Aware Sharpness-Aware Minimization (M-SAM),
a model-agnostic framework that applies to many modalities and supports early
and late fusion scenarios. In every iteration, M-SAM in three steps optimizes
learning. \textbf{First, it identifies the dominant modality} based on
modalities' contribution in the accuracy using Shapley. \textbf{Second, it
decomposes the loss landscape}, or in another language, it modulates the loss
to prioritize the robustness of the model in favor of the dominant modality,
and \textbf{third, M-SAM updates the weights} by backpropagation of modulated
gradients. This ensures robust learning for the dominant modality while
enhancing contributions from others, allowing the model to explore and exploit
complementary features that strengthen overall performance. Extensive
experiments on four diverse datasets show that M-SAM outperforms the latest
state-of-the-art optimization and gradient manipulation methods and
significantly balances and improves multimodal learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding Multi-View <span class="highlight-title">Transformer</span>s <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24907v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24907v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michal Stary, Julien Gaubil, Ayush Tewari, Vincent Sitzmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-view transformers such as DUSt3R are revolutionizing 3D vision by
solving 3D tasks in a feed-forward manner. However, contrary to previous
optimization-based pipelines, the inner mechanisms of multi-view transformers
are unclear. Their black-box nature makes further improvements beyond data
scaling challenging and complicates usage in safety- and reliability-critical
applications. Here, we present an approach for probing and visualizing 3D
representations from the residual connections of the multi-view transformers'
layers. In this manner, we investigate a variant of the DUSt3R model, shedding
light on the development of its latent state across blocks, the role of the
individual layers, and suggest how it differs from methods with stronger
inductive biases of explicit global pose. Finally, we show that the
investigated variant of DUSt3R estimates correspondences that are refined with
reconstructed geometry. The code used for the analysis is available at
https://github.com/JulienGaubil/und3rstand .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at the ICCV 2025 E2E3D Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VividCam: Learning Unconventional Camera Motions from Virtual Synthetic
  Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24904v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24904v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiucheng Wu, Handong Zhao, Zhixin Shu, Jing Shi, Yang Zhang, Shiyu Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although recent text-to-video generative models are getting more capable of
following external camera controls, imposed by either text descriptions or
camera trajectories, they still struggle to generalize to unconventional camera
motions, which is crucial in creating truly original and artistic videos. The
challenge lies in the difficulty of finding sufficient training videos with the
intended uncommon camera motions. To address this challenge, we propose
VividCam, a training paradigm that enables diffusion models to learn complex
camera motions from synthetic videos, releasing the reliance on collecting
realistic training videos. VividCam incorporates multiple disentanglement
strategies that isolates camera motion learning from synthetic appearance
artifacts, ensuring more robust motion representation and mitigating domain
shift. We demonstrate that our design synthesizes a wide range of precisely
controlled and complex camera motions using surprisingly simple synthetic data.
Notably, this synthetic data often consists of basic geometries within a
low-poly 3D scene and can be efficiently rendered by engines like Unity. Our
video results can be found in https://wuqiuche.github.io/VividCamDemoPage/ .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pixels to Signals: A Real-Time Framework for Traffic Demand Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24902v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24902v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        H Mhatre, M Vyas, A Mittal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traffic congestion is becoming a challenge in the rapidly growing urban
cities, resulting in increasing delays and inefficiencies within urban
transportation systems. To address this issue a comprehensive methodology is
designed to optimize traffic flow and minimize delays. The framework is
structured with three primary components: (a) vehicle detection, (b) traffic
prediction, and (c) traffic signal optimization. This paper presents the first
component, vehicle detection. The methodology involves analyzing multiple
sequential frames from a camera feed to compute the background, i.e. the
underlying roadway, by averaging pixel values over time. The computed
background is then utilized to extract the foreground, where the Density-Based
Spatial Clustering of Applications with Noise (DBSCAN) algorithm is applied to
detect vehicles. With its computational efficiency and minimal infrastructure
modification requirements, the proposed methodology offers a practical and
scalable solution for real-world deployment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Proper Body Landmark Subset Enables More Accurate and 5X Faster
  Recognition of Isolated Signs in LIBRAS 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24887v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24887v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniele L. V. dos Santos, Thiago B. Pereira, Carlos Eduardo G. R. Alves, Richard J. M. G. Tello, Francisco de A. Boldt, Thiago M. Paixão
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the feasibility of using lightweight body landmark
detection for the recognition of isolated signs in Brazilian Sign Language
(LIBRAS). Although the skeleton-based approach by Alves et al. (2024) enabled
substantial improvements in recognition performance, the use of OpenPose for
landmark extraction hindered time performance. In a preliminary investigation,
we observed that simply replacing OpenPose with the lightweight MediaPipe,
while improving processing speed, significantly reduced accuracy. To overcome
this limitation, we explored landmark subset selection strategies aimed at
optimizing recognition performance. Experimental results showed that a proper
landmark subset achieves comparable or superior performance to state-of-the-art
methods while reducing processing time by more than 5X compared to Alves et al.
(2024). As an additional contribution, we demonstrated that spline-based
imputation effectively mitigates missing landmark issues, leading to
substantial accuracy gains. These findings highlight that careful landmark
selection, combined with simple imputation techniques, enables efficient and
accurate isolated sign recognition, paving the way for scalable Sign Language
Recognition systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Int. Conf. on Computer Vision Theory and Applications
  (VISAPP 2026)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FruitProm: Probabilistic Maturity Estimation and Detection of Fruits and
  Vegetables 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24885v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24885v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sidharth Rai, Rahul Harsha Cheppally, Benjamin Vail, Keziban Yalçın Dokumacı, Ajay Sharda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Maturity estimation of fruits and vegetables is a critical task for
agricultural automation, directly impacting yield prediction and robotic
harvesting. Current deep learning approaches predominantly treat maturity as a
discrete classification problem (e.g., unripe, ripe, overripe). This rigid
formulation, however, fundamentally conflicts with the continuous nature of the
biological ripening process, leading to information loss and ambiguous class
boundaries. In this paper, we challenge this paradigm by reframing maturity
estimation as a continuous, probabilistic learning task. We propose a novel
architectural modification to the state-of-the-art, real-time object detector,
RT-DETRv2, by introducing a dedicated probabilistic head. This head enables the
model to predict a continuous distribution over the maturity spectrum for each
detected object, simultaneously learning the mean maturity state and its
associated uncertainty. This uncertainty measure is crucial for downstream
decision-making in robotics, providing a confidence score for tasks like
selective harvesting. Our model not only provides a far richer and more
biologically plausible representation of plant maturity but also maintains
exceptional detection performance, achieving a mean Average Precision (mAP) of
85.6\% on a challenging, large-scale fruit dataset. We demonstrate through
extensive experiments that our probabilistic approach offers more granular and
accurate maturity assessments than its classification-based counterparts,
paving the way for more intelligent, uncertainty-aware automated systems in
modern agriculture
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Sidharth Rai, Rahul Harsha Cheppally contributed equally to this work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Seeing Through the MiRAGE: Evaluating Multimodal Retrieval Augmented
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24870v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24870v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Martin, William Walden, Reno Kriz, Dengjia Zhang, Kate Sanders, Eugene Yang, Chihsheng Jin, Benjamin Van Durme
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce MiRAGE, an evaluation framework for retrieval-augmented
generation (RAG) from multimodal sources. As audiovisual media becomes a
prevalent source of information online, it is essential for RAG systems to
integrate information from these sources into generation. However, existing
evaluations for RAG are text-centric, limiting their applicability to
multimodal, reasoning intensive settings because they don't verify information
against sources. MiRAGE is a claim-centric approach to multimodal RAG
evaluation, consisting of InfoF1, evaluating factuality and information
coverage, and CiteF1, measuring citation support and completeness. We show that
MiRAGE, when applied by humans, strongly aligns with extrinsic quality
judgments. We additionally introduce automatic variants of MiRAGE and three
prominent TextRAG metrics -- ACLE, ARGUE, and RAGAS -- demonstrating the
limitations of text-centric work and laying the groundwork for automatic
evaluation. We release open-source implementations and outline how to assess
multimodal RAG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/alexmartin1722/mirage</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Generation Phases of Flow Matching: a Denoising Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24830v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24830v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anne Gagneux, Ségolène Martin, Rémi Gribonval, Mathurin Massias
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Flow matching has achieved remarkable success, yet the factors influencing
the quality of its generation process remain poorly understood. In this work,
we adopt a denoising perspective and design a framework to empirically probe
the generation process. Laying down the formal connections between flow
matching models and denoisers, we provide a common ground to compare their
performances on generation and denoising. This enables the design of principled
and controlled perturbations to influence sample generation: noise and drift.
This leads to new insights on the distinct dynamical phases of the generative
process, enabling us to precisely characterize at which stage of the generative
process denoisers succeed or fail and why this matters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MCIHN: A Hybrid Network Model Based on Multi-path Cross-modal
  Interaction for Multimodal Emotion Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24827v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24827v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyang Zhang, Zhou Yang, Ke Sun, Yucai Pang, Guoliang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal emotion recognition is crucial for future human-computer
interaction. However, accurate emotion recognition still faces significant
challenges due to differences between different modalities and the difficulty
of characterizing unimodal emotional information. To solve these problems, a
hybrid network model based on multipath cross-modal interaction (MCIHN) is
proposed. First, adversarial autoencoders (AAE) are constructed separately for
each modality. The AAE learns discriminative emotion features and reconstructs
the features through a decoder to obtain more discriminative information about
the emotion classes. Then, the latent codes from the AAE of different
modalities are fed into a predefined Cross-modal Gate Mechanism model (CGMM) to
reduce the discrepancy between modalities, establish the emotional relationship
between interacting modalities, and generate the interaction features between
different modalities. Multimodal fusion using the Feature Fusion module (FFM)
for better emotion recognition. Experiments were conducted on publicly
available SIMS and MOSI datasets, demonstrating that MCIHN achieves superior
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper will be published in the MMAsia2025 conference proceedings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal
  Perception and Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24821v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24821v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Inclusion AI,  :, Bowen Ma, Cheng Zou, Canxiang Yan, Chunxiang Jin, Chunjie Shen, Dandan Zheng, Fudong Wang, Furong Xu, GuangMing Yao, Jun Zhou, Jingdong Chen, Jianing Li, Jianxin Sun, Jiajia Liu, Jianjiang Zhu, Jianping Jiang, Jun Peng, Kaixiang Ji, Kaimeng Ren, Libin Wang, Lixiang Ru, Longhua Tan, Lan Wang, Mochen Bai, Ning Gao, Qingpei Guo, Qinglong Zhang, Qiang Xu, Rui Liu, Ruijie Xiong, Ruobing Zheng, Sirui Gao, Tianqi Li, Tinghao Liu, Weilong Chai, Xinyu Xiao, Xiaomei Wang, Xiaolong Wang, Xiao Lu, Xiaoyu Li, Xingning Dong, Xuzheng Yu, Yi Yuan, Yuting Gao, Yuting Xiao, Yunxiao Sun, Yipeng Chen, Yifan Mao, Yifei Wu, Yongjie Lyu, Ziping Ma, Zhiqiang Fang, Zhihao Qiu, Ziyuan Huang, Zizheng Yang, Zhengyu He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Ming-Flash-Omni, an upgraded version of Ming-Omni, built upon a
sparser Mixture-of-Experts (MoE) variant of Ling-Flash-2.0 with 100 billion
total parameters, of which only 6.1 billion are active per token. This
architecture enables highly efficient scaling (dramatically improving
computational efficiency while significantly expanding model capacity) and
empowers stronger unified multimodal intelligence across vision, speech, and
language, representing a key step toward Artificial General Intelligence (AGI).
Compared to its predecessor, the upgraded version exhibits substantial
improvements across multimodal understanding and generation. We significantly
advance speech recognition capabilities, achieving state-of-the-art performance
in contextual ASR and highly competitive results in dialect-aware ASR. In image
generation, Ming-Flash-Omni introduces high-fidelity text rendering and
demonstrates marked gains in scene consistency and identity preservation during
image editing. Furthermore, Ming-Flash-Omni introduces generative segmentation,
a capability that not only achieves strong standalone segmentation performance
but also enhances spatial control in image generation and improves editing
consistency. Notably, Ming-Flash-Omni achieves state-of-the-art results in
text-to-image generation and generative segmentation, and sets new records on
all 12 contextual ASR benchmarks, all within a single unified architecture.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SafeEditor: Unified MLLM for Efficient Post-hoc T2I Safety Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24820v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24820v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruiyang Zhang, Jiahao Luo, Xiaoru Feng, Qiufan Pang, Yaodong Yang, Juntao Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid advancement of text-to-image (T2I) models, ensuring their
safety has become increasingly critical. Existing safety approaches can be
categorized into training-time and inference-time methods. While inference-time
methods are widely adopted due to their cost-effectiveness, they often suffer
from limitations such as over-refusal and imbalance between safety and utility.
To address these challenges, we propose a multi-round safety editing framework
that functions as a model-agnostic, plug-and-play module, enabling efficient
safety alignment for any text-to-image model. Central to this framework is
MR-SafeEdit, a multi-round image-text interleaved dataset specifically
constructed for safety editing in text-to-image generation. We introduce a
post-hoc safety editing paradigm that mirrors the human cognitive process of
identifying and refining unsafe content. To instantiate this paradigm, we
develop SafeEditor, a unified MLLM capable of multi-round safety editing on
generated images. Experimental results show that SafeEditor surpasses prior
safety approaches by reducing over-refusal while achieving a more favorable
safety-utility balance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ADMN: A Layer-Wise Adaptive Multimodal Network for Dynamic Input Noise
  and Compute Resources 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07862v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07862v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jason Wu, Yuyang Yuan, Kang Yang, Lance Kaplan, Mani Srivastava
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal deep learning systems are deployed in dynamic scenarios due to the
robustness afforded by multiple sensing modalities. Nevertheless, they struggle
with varying compute resource availability (due to multi-tenancy, device
heterogeneity, etc.) and fluctuating quality of inputs (from sensor feed
corruption, environmental noise, etc.). Statically provisioned multimodal
systems cannot adapt when compute resources change over time, while existing
dynamic networks struggle with strict compute budgets. Additionally, both
systems often neglect the impact of variations in modality quality.
Consequently, modalities suffering substantial corruption may needlessly
consume resources better allocated towards other modalities. We propose ADMN, a
layer-wise Adaptive Depth Multimodal Network capable of tackling both
challenges: it adjusts the total number of active layers across all modalities
to meet strict compute resource constraints and continually reallocates layers
across input modalities according to their modality quality. Our evaluations
showcase ADMN can match the accuracy of state-of-the-art networks while
reducing up to 75% of their floating-point operations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Neurips 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMPerspective: Do MLLMs Understand Perspective? A Comprehensive
  Benchmark for Perspective Perception, Reasoning, and Robustness <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.20426v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.20426v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yolo Yunlong Tang, Pinxin Liu, Zhangyun Tan, Mingqian Feng, Rui Mao, Chao Huang, Jing Bi, Yunzhong Xiao, Susan Liang, Hang Hua, Ali Vosoughi, Luchuan Song, Zeliang Zhang, Chenliang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding perspective is fundamental to human visual perception, yet the
extent to which multimodal large language models (MLLMs) internalize
perspective geometry remains unclear. We introduce MMPerspective, the first
benchmark specifically designed to systematically evaluate MLLMs' understanding
of perspective through 10 carefully crafted tasks across three complementary
dimensions: Perspective Perception, Reasoning, and Robustness. Our benchmark
comprises 2,711 real-world and synthetic image instances with 5,083
question-answer pairs that probe key capabilities, such as vanishing point
perception and counting, perspective type reasoning, line relationship
understanding in 3D space, invariance to perspective-preserving
transformations, etc. Through a comprehensive evaluation of 43 state-of-the-art
MLLMs, we uncover significant limitations: while models demonstrate competence
on surface-level perceptual tasks, they struggle with compositional reasoning
and maintaining spatial consistency under perturbations. Our analysis further
reveals intriguing patterns between model architecture, scale, and perspective
capabilities, highlighting both robustness bottlenecks and the benefits of
chain-of-thought prompting. MMPerspective establishes a valuable testbed for
diagnosing and advancing spatial understanding in vision-language systems.
Resources available at: https://yunlong10.github.io/MMPerspective/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2025 DB Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VADTree: Explainable Training-Free Video Anomaly Detection via
  Hierarchical Granularity-Aware Tree <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.22693v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.22693v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenlong Li, Yifei Xu, Yuan Rao, Zhenhua Wang, Shuiguang Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video anomaly detection (VAD) focuses on identifying anomalies in videos.
Supervised methods demand substantial in-domain training data and fail to
deliver clear explanations for anomalies. In contrast, training-free methods
leverage the knowledge reserves and language interactivity of large pre-trained
models to detect anomalies. However, the current fixed-length temporal window
sampling approaches struggle to accurately capture anomalies with varying
temporal spans. Therefore, we propose VADTree that utilizes a Hierarchical
Granularityaware Tree (HGTree) structure for flexible sampling in VAD. VADTree
leverages the knowledge embedded in a pre-trained Generic Event Boundary
Detection (GEBD) model to characterize potential anomaly event boundaries.
Specifically, VADTree decomposes the video into generic event nodes based on
boundary confidence, and performs adaptive coarse-fine hierarchical structuring
and redundancy removal to construct the HGTree. Then, the multi-dimensional
priors are injected into the visual language models (VLMs) to enhance the
node-wise anomaly perception, and anomaly reasoning for generic event nodes is
achieved via large language models (LLMs). Finally, an inter-cluster node
correlation method is used to integrate the multi-granularity anomaly scores.
Extensive experiments on three challenging datasets demonstrate that VADTree
achieves state-of-the-art performance in training-free settings while
drastically reducing the number of sampled video segments. The code will be
available at https://github.com/wenlongli10/VADTree.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025 poster</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DWaste: Greener AI for Waste Sorting using Mobile and Edge Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.18513v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.18513v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suman Kunwar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of convenience packaging has led to generation of enormous waste,
making efficient waste sorting crucial for sustainable waste management. To
address this, we developed DWaste, a computer vision-powered platform designed
for real-time waste sorting on resource-constrained smartphones and edge
devices, including offline functionality. We benchmarked various image
classification models (EfficientNetV2S/M, ResNet50/101, MobileNet) and object
detection (YOLOv8n, YOLOv11n) including our purposed YOLOv8n-CBAM model using
our annotated dataset designed for recycling. We found a clear trade-off
between accuracy and resource consumption: the best classifier,
EfficientNetV2S, achieved high accuracy(~ 96%) but suffered from high latency
(~ 0.22s) and elevated carbon emissions. In contrast, lightweight object
detection models delivered strong performance (up to 80% mAP) with ultra-fast
inference (~ 0.03s) and significantly smaller model sizes (< 7MB ), making them
ideal for real-time, low-power use. Model quantization further maximized
efficiency, substantially reducing model size and VRAM usage by up to 75%. Our
work demonstrates the successful implementation of "Greener AI" models to
support real-time, sustainable waste sorting on edge devices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RETTA: Retrieval-Enhanced Test-Time Adaptation for Zero-Shot Video
  Captioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.07046v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.07046v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunchuan Ma, Laiyun Qing, Guorong Li, Yuankai Qi, Amin Beheshti, Quan Z. Sheng, Qingming Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the significant progress of fully-supervised video captioning,
zero-shot methods remain much less explored. In this paper, we propose a novel
zero-shot video captioning framework named Retrieval-Enhanced Test-Time
Adaptation (RETTA), which takes advantage of existing pretrained large-scale
vision and language models to directly generate captions with test-time
adaptation. Specifically, we bridge video and text using four key models: a
general video-text retrieval model XCLIP, a general image-text matching model
CLIP, a text alignment model AnglE, and a text generation model GPT-2, due to
their source-code availability. The main challenge is how to enable the text
generation model to be sufficiently aware of the content in a given video so as
to generate corresponding captions. To address this problem, we propose using
learnable tokens as a communication medium among these four frozen models
GPT-2, XCLIP, CLIP, and AnglE. Different from the conventional way that trains
these tokens with training data, we propose to learn these tokens with soft
targets of the inference data under several carefully crafted loss functions,
which enable the tokens to absorb video information catered for GPT-2. This
procedure can be efficiently done in just a few iterations (we use 16
iterations in the experiments) and does not require ground truth data.
Extensive experimental results on three widely used datasets, MSR-VTT, MSVD,
and VATEX, show absolute 5.1%-32.4% improvements in terms of the main metric
CIDEr compared to several state-of-the-art zero-shot video captioning methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Pattern Recognition</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Frequency-Aware Vision <span class="highlight-title">Transformer</span>s for High-Fidelity Super-Resolution
  of Earth System Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12427v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12427v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ehsan Zeraatkar, Salah A Faroughi, Jelena Tešić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Super-resolution (SR) is crucial for enhancing the spatial fidelity of Earth
System Model (ESM) outputs, allowing fine-scale structures vital to climate
science to be recovered from coarse simulations. However, traditional deep
super-resolution methods, including convolutional and transformer-based models,
tend to exhibit spectral bias, reconstructing low-frequency content more
readily than valuable high-frequency details. In this work, we introduce two
frequency-aware frameworks: the Vision Transformer-Tuned Sinusoidal Implicit
Representation (ViSIR), combining Vision Transformers and sinusoidal
activations to mitigate spectral bias, and the Vision Transformer Fourier
Representation Network (ViFOR), which integrates explicit Fourier-based
filtering for independent low- and high-frequency learning. Evaluated on the
E3SM-HR Earth system dataset across surface temperature, shortwave, and
longwave fluxes, these models outperform leading CNN, GAN, and vanilla
transformer baselines, with ViFOR demonstrating up to 2.6~dB improvements in
PSNR and significantly higher SSIM. Detailed ablation and scaling studies
highlight the benefit of full-field training, the impact of frequency
hyperparameters, and the potential for generalization. The results establish
ViFOR as a state-of-the-art, scalable solution for climate data downscaling.
Future extensions will address temporal super-resolution, multimodal climate
variables, automated parameter selection, and integration of physical
conservation constraints to broaden scientific applicability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Polygonal network disorder and the turning distance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.06415v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.06415v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Dolce, Ryan Lavelle, Bernard Scott, Ashlyn Urbanski, Joseph Klobusicky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The turning distance is a well-studied metric for measuring the similarity
between two polygons. This metric is constructed by taking an $L^p$ distance
between step functions which track each shape's tangent angle of a path tracing
its boundary. In this study, we introduce \textit{turning disorders} for
polygonal planar networks, defined by averaging turning distances between
network faces and "ordered" shapes (regular polygons or circles). We derive
closed-form expressions of turning distances for special classes of regular
polygons, related to the divisibility of $m$ and $n$, and also between regular
polygons and circles. These formulas are used to show that the time for
computing the 2-turning distances reduces to $O((m+n) \log(m+n))$ when both
shapes are regular polygons, an improvement from $O(mn\log(mn))$ operations
needed to compute distances between general polygons of $n$ and $m$ sides. We
also apply these formulas to several examples of network microstructure with
varying disorder. For Archimedean lattices, a class of regular tilings, we can
express turning disorders with exact expressions. We also consider turning
disorders applied to two examples of stochastic processes on networks: spring
networks evolving under T1 moves and polygonal rupture processes. We find that
the two aspects of defining different turning disorders, the choice of ordered
shape and whether to apply area-weighting, can capture different notions of
network disorder.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Long-VITA: Scaling Large Multi-modal Models to 1 Million Tokens with
  Leading Short-Context Accuracy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.05177v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.05177v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunhang Shen, Chaoyou Fu, Shaoqi Dong, Xiong Wang, Yi-Fan Zhang, Peixian Chen, Mengdan Zhang, Haoyu Cao, Ke Li, Shaohui Lin, Xiawu Zheng, Yan Zhang, Yiyi Zhou, Ran He, Caifeng Shan, Rongrong Ji, Xing Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Long-VITA, a simple yet effective large multi-modal model for
long-context visual-language understanding tasks. It is adept at concurrently
processing and analyzing modalities of image, video, and text over 4K frames or
1M tokens while delivering advanced performances on short-context multi-modal
tasks. We propose an effective multi-modal training schema that starts with
large language models and proceeds through vision-language alignment, general
knowledge learning, and two sequential stages of long-sequence fine-tuning. We
further implement context-parallelism distributed inference and logits-masked
language modeling head to scale Long-VITA to infinitely long inputs of images
and texts during model inference. Regarding training data, Long-VITA is built
on a mix of 17M samples from public datasets only and demonstrates
state-of-the-art performance on various multi-modal benchmarks, compared
against recent cutting-edge models with internal data. Long-VITA is fully
open-source and reproducible.. By leveraging our inference designs, Long-VITA
models achieve a remarkable 2x prefill speedup and 4x context length extension
in a single node with 8 GPUs. We hope Long-VITA can serve as a competitive
baseline and offer valuable insights for the open-source community in advancing
long-context multi-modal understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/VITA-MLLM/Long-VITA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Real Unsupervised Anomaly Detection Via Confident Meta-Learning <span class="chip">ICCV2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02293v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02293v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Aqeel, Shakiba Sharifi, Marco Cristani, Francesco Setti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  So-called unsupervised anomaly detection is better described as
semi-supervised, as it assumes all training data are nominal. This assumption
simplifies training but requires manual data curation, introducing bias and
limiting adaptability. We propose Confident Meta-learning (CoMet), a novel
training strategy that enables deep anomaly detection models to learn from
uncurated datasets where nominal and anomalous samples coexist, eliminating the
need for explicit filtering. Our approach integrates Soft Confident Learning,
which assigns lower weights to low-confidence samples, and Meta-Learning, which
stabilizes training by regularizing updates based on training validation loss
covariance. This prevents overfitting and enhances robustness to noisy data.
CoMet is model-agnostic and can be applied to any anomaly detection method
trainable via gradient descent. Experiments on MVTec-AD, VIADUCT, and KSDD2
with two state-of-the-art models demonstrate the effectiveness of our approach,
consistently improving over the baseline methods, remaining insensitive to
anomalies in the training set, and setting a new state-of-the-art across all
datasets. Code is available at https://github.com/aqeeelmirza/CoMet
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE/CVF International Conference on Computer Vision
  (ICCV2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Superpowering Open-Vocabulary Object Detectors for X-ray Vision <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17071v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17071v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pablo Garcia-Fernandez, Lorenzo Vaquero, Mingxuan Liu, Feng Xue, Daniel Cores, Nicu Sebe, Manuel Mucientes, Elisa Ricci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-vocabulary object detection (OvOD) is set to revolutionize security
screening by enabling systems to recognize any item in X-ray scans. However,
developing effective OvOD models for X-ray imaging presents unique challenges
due to data scarcity and the modality gap that prevents direct adoption of
RGB-based solutions. To overcome these limitations, we propose RAXO, a
training-free framework that repurposes off-the-shelf RGB OvOD detectors for
robust X-ray detection. RAXO builds high-quality X-ray class descriptors using
a dual-source retrieval strategy. It gathers relevant RGB images from the web
and enriches them via a novel X-ray material transfer mechanism, eliminating
the need for labeled databases. These visual descriptors replace text-based
classification in OvOD, leveraging intra-modal feature distances for robust
detection. Extensive experiments demonstrate that RAXO consistently improves
OvOD performance, providing an average mAP increase of up to 17.0 points over
base detectors. To further support research in this emerging field, we also
introduce DET-COMPASS, a new benchmark featuring bounding box annotations for
over 300 object categories, enabling large-scale evaluation of OvOD in X-ray.
Code and dataset available at: https://github.com/PAGF188/RAXO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DRBD-Mamba for Robust and Efficient Brain Tumor Segmentation with
  Analytical Insights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.14383v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.14383v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danish Ali, Ajmal Mian, Naveed Akhtar, Ghulam Mubashar Hassan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate brain tumor segmentation is significant for clinical diagnosis and
treatment but remains challenging due to tumor heterogeneity. Mamba-based State
Space Models have demonstrated promising performance. However, despite their
computational efficiency over other neural architectures, they incur
considerable overhead for this task due to their sequential feature computation
across multiple spatial axes. Moreover, their robustness across diverse BraTS
data partitions remains largely unexplored, leaving a critical gap in reliable
evaluation. To address this, we first propose a dual-resolution bi-directional
Mamba (DRBD-Mamba), an efficient 3D segmentation model that captures
multi-scale long-range dependencies with minimal computational overhead. We
leverage a space-filling curve to preserve spatial locality during 3D-to-1D
feature mapping, thereby reducing reliance on computationally expensive
multi-axial feature scans. To enrich feature representation, we propose a gated
fusion module that adaptively integrates forward and reverse contexts, along
with a quantization block that improves robustness. We further propose five
systematic folds on BraTS2023 for rigorous evaluation of segmentation
techniques under diverse conditions and present analysis of common failure
scenarios. On the 20% test set used by recent methods, our model achieves Dice
improvements of 0.10% for whole tumor, 1.75% for tumor core, and 0.93% for
enhancing tumor. Evaluations on the proposed systematic folds demonstrate that
our model maintains competitive whole tumor accuracy while achieving clear
average Dice gains of 1.16% for tumor core and 1.68% for enhancing tumor over
existing state-of-the-art. Furthermore, our model achieves a 15x efficiency
improvement while maintaining high segmentation accuracy, highlighting its
robustness and computational advantage over existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PANDA: Towards Generalist Video Anomaly Detection via Agentic AI
  Engineer <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.26386v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.26386v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiwei Yang, Chen Gao, Mike Zheng Shou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video anomaly detection (VAD) is a critical yet challenging task due to the
complex and diverse nature of real-world scenarios. Previous methods typically
rely on domain-specific training data and manual adjustments when applying to
new scenarios and unseen anomaly types, suffering from high labor costs and
limited generalization. Therefore, we aim to achieve generalist VAD, \ie,
automatically handle any scene and any anomaly types without training data or
human involvement. In this work, we propose PANDA, an agentic AI engineer based
on MLLMs. Specifically, we achieve PANDA by comprehensively devising four key
capabilities: (1) self-adaptive scene-aware strategy planning, (2) goal-driven
heuristic reasoning, (3) tool-augmented self-reflection, and (4) self-improving
chain-of-memory. Concretely, we develop a self-adaptive scene-aware RAG
mechanism, enabling PANDA to retrieve anomaly-specific knowledge for anomaly
detection strategy planning. Next, we introduce a latent anomaly-guided
heuristic prompt strategy to enhance reasoning precision. Furthermore, PANDA
employs a progressive reflection mechanism alongside a suite of context-aware
tools to iteratively refine decision-making in complex scenarios. Finally, a
chain-of-memory mechanism enables PANDA to leverage historical experiences for
continual performance improvement. Extensive experiments demonstrate that PANDA
achieves state-of-the-art performance in multi-scenario, open-set, and complex
scenario settings without training and manual involvement, validating its
generalizable and robust anomaly detection capability. Code is released at
https://github.com/showlab/PANDA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mano Technical Report 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.17336v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.17336v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyu Fu, Anyang Su, Chenxu Zhao, Hanning Wang, Minghui Wu, Zhe Yu, Fei Hu, Mingjia Shi, Wei Dong, Jiayao Wang, Yuyang Chen, Ruiyang Yu, Siran Peng, Menglin Li, Nan Huang, Haitian Wei, Jiawei Yu, Yi Xin, Xilin Zhao, Kai Gu, Ping Jiang, Sifan Zhou, Shuo Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graphical user interfaces (GUIs) are the primary medium for human-computer
interaction, yet automating GUI interactions remains challenging due to the
complexity of visual elements, dynamic environments, and the need for
multi-step reasoning. Existing methods based on vision-language models (VLMs)
often suffer from limited resolution, domain mismatch, and insufficient
sequential decisionmaking capability. To address these issues, we propose Mano,
a robust GUI agent built upon a multi-modal foundation model pre-trained on
extensive web and computer system data. Our approach integrates a novel
simulated environment for high-fidelity data generation, a three-stage training
pipeline (supervised fine-tuning, offline reinforcement learning, and online
reinforcement learning), and a verification module for error recovery. Mano
demonstrates state-of-the-art performance on multiple GUI benchmarks, including
Mind2Web and OSWorld, achieving significant improvements in success rate and
operational accuracy. Our work provides new insights into the effective
integration of reinforcement learning with VLMs for practical GUI agent
deployment, highlighting the importance of domain-specific data, iterative
training, and holistic reward design.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OmniResponse: Online Multimodal Conversational Response Generation in
  Dyadic Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.21724v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.21724v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Luo, Jianghui Wang, Bing Li, Siyang Song, Bernard Ghanem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce Online Multimodal Conversational Response
Generation (OMCRG), a novel task designed to produce synchronized verbal and
non-verbal listener feedback online, based on the speaker's multimodal inputs.
OMCRG captures natural dyadic interactions and introduces new challenges in
aligning generated audio with listeners' facial responses. To tackle these
challenges, we incorporate text as an intermediate modality to connect audio
and facial responses. We propose OmniResponse, a Multimodal Large Language
Model (MLLM) that autoregressively generates accurate multimodal listener
responses. OmniResponse leverages a pretrained LLM enhanced with two core
components: Chrono-Text Markup, which precisely timestamps generated text
tokens, and TempoVoice, a controllable online text-to-speech (TTS) module that
outputs speech synchronized with facial responses. To advance OMCRG research,
we offer ResponseNet, a dataset of 696 detailed dyadic interactions featuring
synchronized split-screen videos, multichannel audio, transcripts, and
annotated facial behaviors. Comprehensive evaluations on ResponseNet
demonstrate that OmniResponse outperforms baseline models in terms of semantic
speech content, audio-visual synchronization, and generation quality. Our
dataset, code, and models are publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding
  in Vision-Language-Action Policies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.20072v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.20072v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhixuan Liang, Yizhuo Li, Tianshuo Yang, Chengyue Wu, Sitong Mao, Tian Nian, Liuao Pei, Shunbo Zhou, Xiaokang Yang, Jiangmiao Pang, Yao Mu, Ping Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language-Action (VLA) models adapt large vision-language backbones to
map images and instructions into robot actions. However, prevailing VLAs either
generate actions auto-regressively in a fixed left-to-right order or attach
separate MLP or diffusion heads outside the backbone, leading to fragmented
information pathways and specialized training requirements that hinder a
unified, scalable architecture. We present Discrete Diffusion VLA, a
unified-transformer policy that models discretized action chunks with discrete
diffusion. The design retains diffusion's progressive refinement paradigm while
remaining natively compatible with the discrete token interface of VLMs. Our
method achieves an adaptive decoding order that resolves easy action elements
before harder ones and uses secondary re-masking to revisit uncertain
predictions across refinement rounds, which improves consistency and enables
robust error correction. This unified decoder preserves pre-trained
vision-language priors, supports parallel decoding, breaks the autoregressive
bottleneck, and reduces the number of function evaluations. Discrete Diffusion
VLA achieves 96.3% avg. success rates on LIBERO, 71.2% visual matching on
SimplerEnv-Fractal and 54.2% overall on SimplerEnv-Bridge, improving over
autoregressive, MLP decoder and continuous diffusion baselines. These findings
indicate that discrete-diffusion VLA supports precise action modeling and
consistent training, laying groundwork for scaling VLA to larger models and
datasets. Our project page is https://github.com/Liang-ZX/DiscreteDiffusionVLA
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LongCat-Video Technical Report 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.22200v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.22200v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         Meituan LongCat Team, Xunliang Cai, Qilong Huang, Zhuoliang Kang, Hongyu Li, Shijun Liang, Liya Ma, Siyu Ren, Xiaoming Wei, Rixu Xie, Tong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video generation is a critical pathway toward world models, with efficient
long video inference as a key capability. Toward this end, we introduce
LongCat-Video, a foundational video generation model with 13.6B parameters,
delivering strong performance across multiple video generation tasks. It
particularly excels in efficient and high-quality long video generation,
representing our first step toward world models. Key features include: Unified
architecture for multiple tasks: Built on the Diffusion Transformer (DiT)
framework, LongCat-Video supports Text-to-Video, Image-to-Video, and
Video-Continuation tasks with a single model; Long video generation:
Pretraining on Video-Continuation tasks enables LongCat-Video to maintain high
quality and temporal coherence in the generation of minutes-long videos;
Efficient inference: LongCat-Video generates 720p, 30fps videos within minutes
by employing a coarse-to-fine generation strategy along both the temporal and
spatial axes. Block Sparse Attention further enhances efficiency, particularly
at high resolutions; Strong performance with multi-reward RLHF: Multi-reward
RLHF training enables LongCat-Video to achieve performance on par with the
latest closed-source and leading open-source models. Code and model weights are
publicly available to accelerate progress in the field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multispectral State-Space Feature Fusion: Bridging Shared and
  Cross-Parametric Interactions for Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.14643v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.14643v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jifeng Shen, Haibo Zhan, Shaohua Dong, Xin Zuo, Wankou Yang, Haibin Ling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern multispectral feature fusion for object detection faces two critical
limitations: (1) Excessive preference for local complementary features over
cross-modal shared semantics adversely affects generalization performance; and
(2) The trade-off between the receptive field size and computational complexity
present critical bottlenecks for scalable feature modeling. Addressing these
issues, a novel Multispectral State-Space Feature Fusion framework, dubbed
MS2Fusion, is proposed based on the state space model (SSM), achieving
efficient and effective fusion through a dual-path parametric interaction
mechanism. More specifically, the first cross-parameter interaction branch
inherits the advantage of cross-attention in mining complementary information
with cross-modal hidden state decoding in SSM. The second shared-parameter
branch explores cross-modal alignment with joint embedding to obtain
cross-modal similar semantic features and structures through parameter sharing
in SSM. Finally, these two paths are jointly optimized with SSM for fusing
multispectral features in a unified framework, allowing our MS2Fusion to enjoy
both functional complementarity and shared semantic space. In our extensive
experiments on mainstream benchmarks including FLIR, M3FD and LLVIP, our
MS2Fusion significantly outperforms other state-of-the-art multispectral object
detection methods, evidencing its superiority. Moreover, MS2Fusion is general
and applicable to other multispectral perception tasks. We show that, even
without specific design, MS2Fusion achieves state-of-the-art results on RGB-T
semantic segmentation and RGBT salient object detection, showing its
generality. The source code will be available at
https://github.com/61s61min/MS2Fusion.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted on 30/4/2025, Accepted by Information Fusion</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DArFace: Deformation Aware Robustness for Low Quality Face Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08423v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08423v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sadaf Gulshad, Abdullah Aldahlawi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facial recognition systems have achieved remarkable success by leveraging
deep neural networks, advanced loss functions, and large-scale datasets.
However, their performance often deteriorates in real-world scenarios involving
low-quality facial images. Such degradations, common in surveillance footage or
standoff imaging include low resolution, motion blur, and various distortions,
resulting in a substantial domain gap from the high-quality data typically used
during training. While existing approaches attempt to address robustness by
modifying network architectures or modeling global spatial transformations,
they frequently overlook local, non-rigid deformations that are inherently
present in real-world settings. In this work, we introduce \textbf{DArFace}, a
\textbf{D}eformation-\textbf{A}ware \textbf{r}obust \textbf{Face} recognition
framework that enhances robustness to such degradations without requiring
paired high- and low-quality training samples. Our method adversarially
integrates both global transformations (e.g., rotation, translation) and local
elastic deformations during training to simulate realistic low-quality
conditions. Moreover, we introduce a contrastive objective to enforce identity
consistency across different deformed views. Extensive evaluations on
low-quality benchmarks including TinyFace, IJB-B, and IJB-C demonstrate that
DArFace surpasses state-of-the-art methods, with significant gains attributed
to the inclusion of local deformation modeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MTFL: Multi-Timescale Feature Learning for Weakly-Supervised Anomaly
  Detection in Surveillance Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05900v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05900v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiling Zhang, Erkut Akdag, Egor Bondarev, Peter H. N. De With
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detection of anomaly events is relevant for public safety and requires a
combination of fine-grained motion information and contextual events at
variable time-scales. To this end, we propose a Multi-Timescale Feature
Learning (MTFL) method to enhance the representation of anomaly features.
Short, medium, and long temporal tubelets are employed to extract
spatio-temporal video features using a Video Swin Transformer. Experimental
results demonstrate that MTFL outperforms state-of-the-art methods on the
UCF-Crime dataset, achieving an anomaly detection performance 89.78% AUC.
Moreover, it performs complementary to SotA with 95.32% AUC on the ShanghaiTech
and 84.57% AP on the XD-Violence dataset. Furthermore, we generate an extended
dataset of the UCF-Crime for development and evaluation on a wider range of
anomalies, namely Video Anomaly Detection Dataset (VADD), involving 2,591
videos in 18 classes with extensive coverage of realistic anomalies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Detecting Latin in Historical Books with Large Language Models: A
  Multimodal Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.19585v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.19585v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Wu, Ke Shu, Jonas Fischer, Lidia Pivovarova, David Rosson, Eetu Mäkelä, Mikko Tolonen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel task of extracting Latin fragments from
mixed-language historical documents with varied layouts. We benchmark and
evaluate the performance of large foundation models against a multimodal
dataset of 724 annotated pages. The results demonstrate that reliable Latin
detection with contemporary models is achievable. Our study provides the first
comprehensive analysis of these models' capabilities and limits for this task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review. Both the dataset and code will be published</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Is Sora a World Simulator? A Comprehensive <span class="highlight-title">Survey</span> on General World
  Models and Beyond 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.03520v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.03520v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Zhu, Xiaofeng Wang, Wangbo Zhao, Chen Min, Bohan Li, Nianchen Deng, Min Dou, Yuqi Wang, Botian Shi, Kai Wang, Chi Zhang, Yang You, Zhaoxiang Zhang, Dawei Zhao, Liang Xiao, Jian Zhao, Jiwen Lu, Guan Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  General world models represent a crucial pathway toward achieving Artificial
General Intelligence (AGI), serving as the cornerstone for various applications
ranging from virtual environments to decision-making systems. Recently, the
emergence of the Sora model has attained significant attention due to its
remarkable simulation capabilities, which exhibits an incipient comprehension
of physical laws. In this survey, we embark on a comprehensive exploration of
the latest advancements in world models. Our analysis navigates through the
forefront of generative methodologies in video generation, where world models
stand as pivotal constructs facilitating the synthesis of highly realistic
visual content. Additionally, we scrutinize the burgeoning field of
autonomous-driving world models, meticulously delineating their indispensable
role in reshaping transportation and urban mobility. Furthermore, we delve into
the intricacies inherent in world models deployed within autonomous agents,
shedding light on their profound significance in enabling intelligent
interactions within dynamic environmental contexts. At last, we examine
challenges and limitations of world models, and discuss their potential future
directions. We hope this survey can serve as a foundational reference for the
research community and inspire continued innovation. This survey will be
regularly updated at:
https://github.com/GigaAI-research/General-World-Models-Survey.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This survey will be regularly updated at:
  https://github.com/GigaAI-research/General-World-Models-Survey</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.11842v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.11842v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuannan Liu, Zekun Li, Zheqi He, Peipei Li, Shuhan Xia, Xing Cui, Huaibo Huang, Xi Yang, Ran He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing deployment of Large Vision-Language Models (LVLMs) raises
safety concerns under potential malicious inputs. However, existing multimodal
safety evaluations primarily focus on model vulnerabilities exposed by static
image inputs, ignoring the temporal dynamics of video that may induce distinct
safety risks. To bridge this gap, we introduce Video-SafetyBench, the first
comprehensive benchmark designed to evaluate the safety of LVLMs under
video-text attacks. It comprises 2,264 video-text pairs spanning 48
fine-grained unsafe categories, each pairing a synthesized video with either a
harmful query, which contains explicit malice, or a benign query, which appears
harmless but triggers harmful behavior when interpreted alongside the video. To
generate semantically accurate videos for safety evaluation, we design a
controllable pipeline that decomposes video semantics into subject images (what
is shown) and motion text (how it moves), which jointly guide the synthesis of
query-relevant videos. To effectively evaluate uncertain or borderline harmful
outputs, we propose RJScore, a novel LLM-based metric that incorporates the
confidence of judge models and human-aligned decision threshold calibration.
Extensive experiments show that benign-query video composition achieves average
attack success rates of 67.2%, revealing consistent vulnerabilities to
video-induced attacks. We believe Video-SafetyBench will catalyze future
research into video-based safety evaluation and defense strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2025 Dataset and Benchmark Track, Project page:
  https://liuxuannan.github.io/Video-SafetyBench.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Advancing Compositional Awareness in CLIP with Efficient Fine-Tuning <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.24424v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.24424v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amit Peleg, Naman Deep Singh, Matthias Hein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models like CLIP have demonstrated remarkable zero-shot
capabilities in classification and retrieval. However, these models often
struggle with compositional reasoning - the ability to understand the
relationships between concepts. A recent benchmark, SugarCrepe++, reveals that
previous works on improving compositionality have mainly improved lexical
sensitivity but neglected semantic understanding. In addition, downstream
retrieval performance often deteriorates, although one would expect that
improving compositionality should enhance retrieval. In this work, we introduce
CLIC (Compositionally-aware Learning in CLIP), a fine-tuning method based on a
novel training technique combining multiple images and their associated
captions. CLIC improves compositionality across architectures as well as
differently pre-trained CLIP models, both in terms of lexical and semantic
understanding, and achieves consistent gains in retrieval performance. This
even applies to the recent CLIPS, which achieves SOTA retrieval performance.
Nevertheless, the short fine-tuning with CLIC leads to an improvement in
retrieval and to the best compositional CLIP model on SugarCrepe++. All our
models and code are available at https://clic-compositional-clip.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ImageNet-trained CNNs are not biased towards texture: Revisiting feature
  reliance through controlled suppression <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.20234v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.20234v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tom Burgert, Oliver Stoll, Paolo Rota, Begüm Demir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The hypothesis that Convolutional Neural Networks (CNNs) are inherently
texture-biased has shaped much of the discourse on feature use in deep
learning. We revisit this hypothesis by examining limitations in the
cue-conflict experiment by Geirhos et al. To address these limitations, we
propose a domain-agnostic framework that quantifies feature reliance through
systematic suppression of shape, texture, and color cues, avoiding the
confounds of forced-choice conflicts. By evaluating humans and neural networks
under controlled suppression conditions, we find that CNNs are not inherently
texture-biased but predominantly rely on local shape features. Nonetheless,
this reliance can be substantially mitigated through modern training strategies
or architectures (ConvNeXt, ViTs). We further extend the analysis across
computer vision, medical imaging, and remote sensing, revealing that reliance
patterns differ systematically: computer vision models prioritize shape,
medical imaging models emphasize color, and remote sensing models exhibit a
stronger reliance on texture. Code is available at
https://github.com/tomburgert/feature-reliance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2025 (oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VOLD: Reasoning Transfer from LLMs to Vision-Language Models via
  On-Policy Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23497v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23497v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Walid Bousselham, Hilde Kuehne, Cordelia Schmid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training vision-language models (VLMs) for complex reasoning remains a
challenging task, i.a. due to the scarcity of high-quality image-text reasoning
data. Conversely, text-based reasoning resources are abundant and scalable, but
it is still an open question how to leveraging them for VLM reasoning. To
address this problem, we propose VOLD, a framework to transfer reasoning
capabilities from text-only teacher models to VLM student models. To this end,
VOLD combines reinforcement learning via Group Relative Policy Optimization
(GRPO) with on-policy distillation, which allows the student reasoning traces
to be guided by the teacher model, resulting in a significant gain over using
GRPO alone. We further show that a cold-start alignment is essential for an
effective transfer during the online training phase in this scenario and that
without sufficient distributional alignment between teacher and student,
on-policy distillation fails to provide meaningful guidance. We evaluate VOLD
across diverse benchmarks including MMMU-Pro, MathVision, MathVista, and
LogicVista, showing that VOLD outperforms the baseline model significantly and
improves over the state of the art by a margin. Our ablation shows the
importance of a cold-start alignment via SFT for on-policy distillation with a
text-only teacher.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>www.walidbousselham.com/VOLD/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FRBNet: Revisiting Low-Light Vision through Frequency-Domain Radial
  Basis Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23444v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23444v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangtong Sun, Congyu Li, Ke Yang, Yuchen Pan, Hanwen Yu, Xichuan Zhang, Yiying Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-light vision remains a fundamental challenge in computer vision due to
severe illumination degradation, which significantly affects the performance of
downstream tasks such as detection and segmentation. While recent
state-of-the-art methods have improved performance through invariant feature
learning modules, they still fall short due to incomplete modeling of low-light
conditions. Therefore, we revisit low-light image formation and extend the
classical Lambertian model to better characterize low-light conditions. By
shifting our analysis to the frequency domain, we theoretically prove that the
frequency-domain channel ratio can be leveraged to extract
illumination-invariant features via a structured filtering process. We then
propose a novel and end-to-end trainable module named \textbf{F}requency-domain
\textbf{R}adial \textbf{B}asis \textbf{Net}work (\textbf{FRBNet}), which
integrates the frequency-domain channel ratio operation with a learnable
frequency domain filter for the overall illumination-invariant feature
enhancement. As a plug-and-play module, FRBNet can be integrated into existing
networks for low-light downstream tasks without modifying loss functions.
Extensive experiments across various downstream tasks demonstrate that FRBNet
achieves superior performance, including +2.2 mAP for dark object detection and
+2.9 mIoU for nighttime segmentation. Code is available at:
https://github.com/Sing-Forevet/FRBNet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Geo-Sign: Hyperbolic Contrastive Regularisation for Geometrically Aware
  Sign Language Translation <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.00129v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.00129v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edward Fish, Richard Bowden
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress in Sign Language Translation (SLT) has focussed primarily on
improving the representational capacity of large language models to incorporate
Sign Language features. This work explores an alternative direction: enhancing
the geometric properties of skeletal representations themselves. We propose
Geo-Sign, a method that leverages the properties of hyperbolic geometry to
model the hierarchical structure inherent in sign language kinematics. By
projecting skeletal features derived from Spatio-Temporal Graph Convolutional
Networks (ST-GCNs) into the Poincar\'e ball model, we aim to create more
discriminative embeddings, particularly for fine-grained motions like finger
articulations. We introduce a hyperbolic projection layer, a weighted Fr\'echet
mean aggregation scheme, and a geometric contrastive loss operating directly in
hyperbolic space. These components are integrated into an end-to-end
translation framework as a regularisation function, to enhance the
representations within the language model. This work demonstrates the potential
of hyperbolic geometry to improve skeletal representations for Sign Language
Translation, improving on SOTA RGB methods while preserving privacy and
improving computational efficiency. Code available here:
https://github.com/ed-fish/geo-sign.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Federated Learning with Partially Labeled Data: A Conditional
  Distillation Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.18833v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.18833v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pochuan Wang, Chen Shen, Masahiro Oda, Chiou-Shann Fuh, Kensaku Mori, Weichung Wang, Holger R. Roth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In medical imaging, developing generalized segmentation models that can
handle multiple organs and lesions is crucial. However, the scarcity of fully
annotated datasets and strict privacy regulations present significant barriers
to data sharing. Federated Learning (FL) allows decentralized model training,
but existing FL methods often struggle with partial labeling, leading to model
divergence and catastrophic forgetting. We propose ConDistFL, a novel FL
framework incorporating conditional distillation to address these challenges.
ConDistFL enables effective learning from partially labeled datasets,
significantly improving segmentation accuracy across distributed and
non-uniform datasets. In addition to its superior segmentation performance,
ConDistFL maintains computational and communication efficiency, ensuring its
scalability for real-world applications. Furthermore, ConDistFL demonstrates
remarkable generalizability, significantly outperforming existing FL methods in
out-of-federation tests, even adapting to unseen contrast phases (e.g.,
non-contrast CT images) in our experiments. Extensive evaluations on 3D CT and
2D chest X-ray datasets show that ConDistFL is an efficient, adaptable solution
for collaborative medical image segmentation in privacy-constrained settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This manuscript was submitted to IEEE JBHI and is currently under
  peer review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Through the Lens: Benchmarking Deepfake Detectors Against
  Moiré-Induced Distortions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23225v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23225v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Razaib Tariq, Minji Heo, Simon S. Woo, Shahroz Tariq
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deepfake detection remains a pressing challenge, particularly in real-world
settings where smartphone-captured media from digital screens often introduces
Moir\'e artifacts that can distort detection outcomes. This study
systematically evaluates state-of-the-art (SOTA) deepfake detectors on
Moir\'e-affected videos, an issue that has received little attention. We
collected a dataset of 12,832 videos, spanning 35.64 hours, from the Celeb-DF,
DFD, DFDC, UADFV, and FF++ datasets, capturing footage under diverse real-world
conditions, including varying screens, smartphones, lighting setups, and camera
angles. To further examine the influence of Moir\'e patterns on deepfake
detection, we conducted additional experiments using our DeepMoir\'eFake,
referred to as (DMF) dataset and two synthetic Moir\'e generation techniques.
Across 15 top-performing detectors, our results show that Moir\'e artifacts
degrade performance by as much as 25.4%, while synthetically generated Moir\'e
patterns lead to a 21.4% drop in accuracy. Surprisingly, demoir\'eing methods,
intended as a mitigation approach, instead worsened the problem, reducing
accuracy by up to 17.2%. These findings underscore the urgent need for
detection models that can robustly handle Moir\'e distortions alongside other
realworld challenges, such as compression, sharpening, and blurring. By
introducing the DMF dataset, we aim to drive future research toward closing the
gap between controlled experiments and practical deepfake detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>48 Pages, 29 Figures, 15 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Robustness of Vision-Language-Action Model against Multi-Modal
  Perturbations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.00037v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.00037v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianing Guo, Zhenhong Wu, Chang Tu, Yiyao Ma, Xiangqi Kong, Zhiqian Liu, Jiaming Ji, Shuning Zhang, Yuanpei Chen, Kai Chen, Qi Dou, Yaodong Yang, Xianglong Liu, Huijie Zhao, Weifeng Lv, Simin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Vision-Language-Action (VLA) models, robustness to real-world
perturbations is critical for deployment. Existing methods target simple visual
disturbances, overlooking the broader multi-modal perturbations that arise in
actions, instructions, environments, and observations. Here, we first evaluate
the robustness of mainstream VLAs under 17 perturbations across four
modalities. We find (1) actions as the most fragile modality, (2) Existing
visual-robust VLA do not gain robustness in other modality, and (3) pi0
demonstrates superior robustness with a diffusion-based action head. To build
multi-modal robust VLAs, we propose RobustVLA against perturbations in VLA
inputs and outputs. For output robustness, we perform offline robust
optimization against worst-case action noise that maximizes mismatch in flow
matching objective. This can be seen as adversarial training, label smoothing,
and outlier penalization. For input robustness, we enforce consistent actions
across input variations that preserve task semantics. To account for multiple
perturbations, we formulate robustness as a multi-armed bandit problem and
apply an upper confidence bound algorithm to automatically identify the most
harmful noise. Experiments on LIBERO demonstrate our RobustVLA delivers
absolute gains over baselines of 12.6% on the pi0 backbone and 10.4% on the
OpenVLA backbone across all 17 perturbations, achieving 50.6x faster inference
than existing visual-robust VLAs, and a 10.4% gain under mixed perturbations.
Our RobustVLA is particularly effective on real-world FR5 robot with limited
demonstrations, showing absolute gains by 65.6% under perturbations of four
modalities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CPathAgent: An Agent-based Foundation Model for Interpretable
  High-Resolution Pathology Image Analysis Mimicking Pathologists' Diagnostic
  Logic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.20510v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.20510v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Sun, Yixuan Si, Chenglu Zhu, Kai Zhang, Zhongyi Shui, Bowen Ding, Tao Lin, Lin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in computational pathology have led to the emergence of
numerous foundation models. These models typically rely on general-purpose
encoders with multi-instance learning for whole slide image (WSI)
classification or apply multimodal approaches to generate reports directly from
images. However, these models cannot emulate the diagnostic approach of
pathologists, who systematically examine slides at low magnification to obtain
an overview before progressively zooming in on suspicious regions to formulate
comprehensive diagnoses. Instead, existing models directly output final
diagnoses without revealing the underlying reasoning process. To address this
gap, we introduce CPathAgent, an innovative agent-based approach that mimics
pathologists' diagnostic workflow by autonomously navigating across WSI based
on observed visual features, thereby generating substantially more transparent
and interpretable diagnostic summaries. To achieve this, we develop a
multi-stage training strategy that unifies patch-level, region-level, and
WSI-level capabilities within a single model, which is essential for
replicating how pathologists understand and reason across diverse image scales.
Additionally, we construct PathMMU-HR2, the first expert-validated benchmark
for large region analysis. This represents a critical intermediate scale
between patches and whole slides, reflecting a key clinical reality where
pathologists typically examine several key large regions rather than entire
slides at once. Extensive experiments demonstrate that CPathAgent consistently
outperforms existing approaches across benchmarks at three different image
scales, validating the effectiveness of our agent-based diagnostic approach and
highlighting a promising direction for computational pathology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>52 pages, 34 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Is It Certainly a Deepfake? Reliability Analysis in Detection &
  Generation Ecosystem <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.17550v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.17550v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neslihan Kose, Anthony Rhodes, Umur Aybars Ciftci, Ilke Demir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As generative models are advancing in quality and quantity for creating
synthetic content, deepfakes begin to cause online mistrust. Deepfake detectors
are proposed to counter this effect, however, misuse of detectors claiming fake
content as real or vice versa further fuels this misinformation problem. We
present the first comprehensive uncertainty analysis of deepfake detectors,
systematically investigating how generative artifacts influence prediction
confidence. As reflected in detectors' responses, deepfake generators also
contribute to this uncertainty as their generative residues vary, so we cross
the uncertainty analysis of deepfake detectors and generators. Based on our
observations, the uncertainty manifold holds enough consistent information to
leverage uncertainty for deepfake source detection. Our approach leverages
Bayesian Neural Networks and Monte Carlo dropout to quantify both aleatoric and
epistemic uncertainties across diverse detector architectures. We evaluate
uncertainty on two datasets with nine generators, with four blind and two
biological detectors, compare different uncertainty methods, explore region-
and pixel-based uncertainty, and conduct ablation studies. We conduct and
analyze binary real/fake, multi-class real/fake, source detection, and
leave-one-out experiments between the generator/detector combinations to share
their generalization capability, model calibration, uncertainty, and robustness
against adversarial attacks. We further introduce uncertainty maps that
localize prediction confidence at the pixel level, revealing distinct patterns
correlated with generator-specific artifacts. Our analysis provides critical
insights for deploying reliable deepfake detection systems and establishes
uncertainty quantification as a fundamental requirement for trustworthy
synthetic media detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the ICCV 2025 workshop - STREAM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AnyCap Project: A Unified Framework, <span class="highlight-title">Dataset</span>, and Benchmark for
  Controllable Omni-modal Captioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.12841v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.12841v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Ren, Zhiqiang Lin, Yu Li, Gao Meng, Weiyun Wang, Junjie Wang, Zicheng Lin, Jifeng Dai, Yujiu Yang, Wenhai Wang, Ruihang Chu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Controllable captioning is essential for precise multimodal alignment and
instruction following, yet existing models often lack fine-grained control and
reliable evaluation protocols. To address this gap, we present the AnyCap
Project, an integrated solution spanning model, dataset, and evaluation. We
introduce AnyCapModel (ACM), a lightweight plug-and-play framework that
enhances the controllability of existing foundation models for omni-modal
captioning without retraining the base model. ACM reuses the original captions
from base models while incorporating user instructions and modality features to
generate improved captions. To remedy the data scarcity in controllable
multimodal captioning, we build AnyCapDataset (ACD), covering three modalities,
28 user-instruction types, and 300\,k high-quality data entries. We further
propose AnyCapEval, a new benchmark that provides more reliable evaluation
metrics for controllable captioning by decoupling content accuracy and
stylistic fidelity. ACM markedly improves caption quality across a diverse set
of base models on AnyCapEval. Notably, ACM-8B raises GPT-4o\'s content scores
by 45\% and style scores by 12\%, and it also achieves substantial gains on
widely used benchmarks such as MIA-Bench and VidCapBench.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Look and Tell: A <span class="highlight-title">Dataset</span> for Multimodal Grounding Across Egocentric and
  Exocentric Views <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.22672v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.22672v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna Deichler, Jonas Beskow
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Look and Tell, a multimodal dataset for studying referential
communication across egocentric and exocentric perspectives. Using Meta Project
Aria smart glasses and stationary cameras, we recorded synchronized gaze,
speech, and video as 25 participants instructed a partner to identify
ingredients in a kitchen. Combined with 3D scene reconstructions, this setup
provides a benchmark for evaluating how different spatial representations (2D
vs. 3D; ego vs. exo) affect multimodal grounding. The dataset contains 3.67
hours of recordings, including 2,707 richly annotated referential expressions,
and is designed to advance the development of embodied agents that can
understand and engage in situated dialogue.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures, 2 tables. Accepted to the NeurIPS 2025 Workshop
  on SPACE in Vision, Language, and Embodied AI (SpaVLE). Dataset:
  https://huggingface.co/datasets/annadeichler/KTH-ARIA-referential</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Generalized Label Shift Perspective for Cross-Domain Gaze Estimation <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13043v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13043v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao-Ran Yang, Xiaohui Chen, Chuan-Xian Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aiming to generalize the well-trained gaze estimation model to new target
domains, Cross-domain Gaze Estimation (CDGE) is developed for real-world
application scenarios. Existing CDGE methods typically extract the
domain-invariant features to mitigate domain shift in feature space, which is
proved insufficient by Generalized Label Shift (GLS) theory. In this paper, we
introduce a novel GLS perspective to CDGE and modelize the cross-domain problem
by label and conditional shift problem. A GLS correction framework is presented
and a feasible realization is proposed, in which a importance reweighting
strategy based on truncated Gaussian distribution is introduced to overcome the
continuity challenges in label shift correction. To embed the reweighted source
distribution to conditional invariant learning, we further derive a
probability-aware estimation of conditional operator discrepancy. Extensive
experiments on standard CDGE tasks with different backbone models validate the
superior generalization capability across domain and applicability on various
models of proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Acoustic Neural 3D Reconstruction Under Pose Drift <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.08930v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.08930v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianxiang Lin, Mohamad Qadri, Kevin Zhang, Adithya Pediredla, Christopher A. Metzler, Michael Kaess
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of optimizing neural implicit surfaces for 3D
reconstruction using acoustic images collected with drifting sensor poses. The
accuracy of current state-of-the-art 3D acoustic modeling algorithms is highly
dependent on accurate pose estimation; small errors in sensor pose can lead to
severe reconstruction artifacts. In this paper, we propose an algorithm that
jointly optimizes the neural scene representation and sonar poses. Our
algorithm does so by parameterizing the 6DoF poses as learnable parameters and
backpropagating gradients through the neural renderer and implicit
representation. We validated our algorithm on both real and simulated datasets.
It produces high-fidelity 3D reconstructions even under significant pose drift.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 8 figures. This paper is accepted by 2025 IEEE/RSJ
  International Conference on Intelligent Robots and Systems (IROS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stealthy Patch-Wise Backdoor Attack in 3D Point Cloud via Curvature
  Awareness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.09336v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.09336v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Feng, Dingxin Zhang, Runkai Zhao, Yong Xia, Heng Huang, Weidong Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Backdoor attacks pose a severe threat to deep neural networks (DNNs) by
implanting hidden backdoors that can be activated with predefined triggers to
manipulate model behaviors maliciously. Existing 3D point cloud backdoor
attacks primarily rely on sample-wise global modifications, which suffer from
low imperceptibility. Although optimization can improve stealthiness,
optimizing sample-wise triggers significantly increases computational cost. To
address these limitations, we propose the Stealthy Patch-Wise Backdoor Attack
(SPBA), the first patch-wise backdoor attack framework for 3D point clouds.
Specifically, SPBA decomposes point clouds into local patches and employs a
curvature-based imperceptibility score to guide trigger injection into visually
less sensitive patches. By optimizing a unified patch-wise trigger that
perturbs spectral features of selected patches, SPBA significantly enhances
optimization efficiency while maintaining high stealthiness. Extensive
experiments on ModelNet40 and ShapeNetPart further demonstrate that SPBA
surpasses prior state-of-the-art backdoor attacks in both attack effectiveness
and resistance to defense methods. The code is available at
https://github.com/HazardFY/SPBA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 6 figures, 11 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Normal and Abnormal Pathology Knowledge-Augmented Vision-Language Model
  for Anomaly Detection in Pathology Images <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.15256v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.15256v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinsol Song, Jiamu Wang, Anh Tien Nguyen, Keunho Byeon, Sangjeong Ahn, Sung Hak Lee, Jin Tae Kwak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anomaly detection in computational pathology aims to identify rare and scarce
anomalies where disease-related data are often limited or missing. Existing
anomaly detection methods, primarily designed for industrial settings, face
limitations in pathology due to computational constraints, diverse tissue
structures, and lack of interpretability. To address these challenges, we
propose Ano-NAViLa, a Normal and Abnormal pathology knowledge-augmented
Vision-Language model for Anomaly detection in pathology images. Ano-NAViLa is
built on a pre-trained vision-language model with a lightweight trainable MLP.
By incorporating both normal and abnormal pathology knowledge, Ano-NAViLa
enhances accuracy and robustness to variability in pathology images and
provides interpretability through image-text associations. Evaluated on two
lymph node datasets from different organs, Ano-NAViLa achieves the
state-of-the-art performance in anomaly detection and localization,
outperforming competing models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICCV 2025. Code is available at:
  https://github.com/QuIIL/ICCV2025_Ano-NAViLa</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Does CLIP perceive art the same way we do? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.05229v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.05229v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Asperti, Leonardo Dessì, Maria Chiara Tonetti, Nico Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  CLIP has emerged as a powerful multimodal model capable of connecting images
and text through joint embeddings, but to what extent does it 'see' the same
way humans do - especially when interpreting artworks? In this paper, we
investigate CLIP's ability to extract high-level semantic and stylistic
information from paintings, including both human-created and AI-generated
imagery. We evaluate its perception across multiple dimensions: content, scene
understanding, artistic style, historical period, and the presence of visual
deformations or artifacts. By designing targeted probing tasks and comparing
CLIP's responses to human annotations and expert benchmarks, we explore its
alignment with human perceptual and contextual understanding. Our findings
reveal both strengths and limitations in CLIP's visual representations,
particularly in relation to aesthetic cues and artistic intent. We further
discuss the implications of these insights for using CLIP as a guidance
mechanism during generative processes, such as style transfer or prompt-based
image synthesis. Our work highlights the need for deeper interpretability in
multimodal systems, especially when applied to creative domains where nuance
and subjectivity play a central role.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Caption-Driven Explainability: Probing CNNs for Bias via CLIP <span class="chip">ICIP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.22035v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.22035v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrick Koller, Amil V. Dravid, Guido M. Schuster, Aggelos K. Katsaggelos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robustness has become one of the most critical problems in machine learning
(ML). The science of interpreting ML models to understand their behavior and
improve their robustness is referred to as explainable artificial intelligence
(XAI). One of the state-of-the-art XAI methods for computer vision problems is
to generate saliency maps. A saliency map highlights the pixel space of an
image that excites the ML model the most. However, this property could be
misleading if spurious and salient features are present in overlapping pixel
spaces. In this paper, we propose a caption-based XAI method, which integrates
a standalone model to be explained into the contrastive language-image
pre-training (CLIP) model using a novel network surgery approach. The resulting
caption-based XAI model identifies the dominant concept that contributes the
most to the models prediction. This explanation minimizes the risk of the
standalone model falling for a covariate shift and contributes significantly
towards developing robust ML models. Our code is available at
<https://github.com/patch0816/caption-driven-xai>.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted and presented at the IEEE ICIP 2025 Satellite Workshop
  "Generative AI for World Simulations and Communications & Celebrating 40
  Years of Excellence in Education: Honoring Professor Aggelos Katsaggelos",
  Anchorage, Alaska, USA, September 14, 2025. Camera-ready preprint; the
  official IEEE Xplore publication will follow. Code is available at
  <https://github.com/patch0816/caption-driven-xai></span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CustomVideo: Customizing Text-to-Video Generation with Multiple Subjects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.09962v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.09962v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhao Wang, Aoxue Li, Lingting Zhu, Yong Guo, Qi Dou, Zhenguo Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Customized text-to-video generation aims to generate high-quality videos
guided by text prompts and subject references. Current approaches for
personalizing text-to-video generation suffer from tackling multiple subjects,
which is a more challenging and practical scenario. In this work, our aim is to
promote multi-subject guided text-to-video customization. We propose
CustomVideo, a novel framework that can generate identity-preserving videos
with the guidance of multiple subjects. To be specific, firstly, we encourage
the co-occurrence of multiple subjects via composing them in a single image.
Further, upon a basic text-to-video diffusion model, we design a simple yet
effective attention control strategy to disentangle different subjects in the
latent space of diffusion model. Moreover, to help the model focus on the
specific area of the object, we segment the object from given reference images
and provide a corresponding object mask for attention learning. Also, we
collect a multi-subject text-to-video generation dataset as a comprehensive
benchmark. Extensive qualitative, quantitative, and user study results
demonstrate the superiority of our method compared to previous state-of-the-art
approaches. The project page is https://kyfafyd.wang/projects/customvideo.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE TMM 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LiDAR Remote Sensing Meets Weak Supervision: Concepts, Methods, and
  Perspectives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18384v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18384v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Gao, Shaobo Xia, Pu Wang, Xiaohuan Xi, Sheng Nie, Cheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Light detection and ranging (LiDAR) remote sensing encompasses two major
directions: data interpretation and parameter inversion. However, both
directions rely heavily on costly and labor-intensive labeled data and field
measurements, which constrains their scalability and spatiotemporal
adaptability. Weakly Supervised Learning (WSL) provides a unified framework to
address these limitations. This paper departs from the traditional view that
treats interpretation and inversion as separate tasks and offers a systematic
review of recent advances in LiDAR remote sensing from a unified WSL
perspective. We cover typical WSL settings including incomplete
supervision(e.g., sparse point labels), inexact supervision (e.g., scene-level
tags), inaccurate supervision (e.g., noisy labels), and cross-domain
supervision (e.g., domain adaptation/generalization) and corresponding
techniques such as pseudo-labeling, consistency regularization, self-training,
and label refinement, which collectively enable robust learning from limited
and weak annotations.We further analyze LiDAR-specific challenges (e.g.,
irregular geometry, data sparsity, domain heterogeneity) that require tailored
weak supervision, and examine how sparse LiDAR observations can guide joint
learning with other remote-sensing data for continuous surface-parameter
retrieval. Finally, we highlight future directions where WSL acts as a bridge
between LiDAR and foundation models to leverage large-scale multimodal datasets
and reduce labeling costs, while also enabling broader WSL-driven advances in
generalization, open-world adaptation, and scalable LiDAR remote sensing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Real-Time Neural Video Compression with Unified Intra and Inter Coding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.14431v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.14431v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui Xiang, Yifan Bian, Li Li, Jingran Wu, Xianguo Zhang, Dong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural video compression (NVC) technologies have advanced rapidly in recent
years, yielding state-of-the-art schemes such as DCVC-RT that offer superior
compression efficiency to H.266/VVC and real-time encoding/decoding
capabilities. Nonetheless, existing NVC schemes have several limitations,
including inefficiency in dealing with disocclusion and new content, interframe
error propagation and accumulation, among others. To eliminate these
limitations, we borrow the idea from classic video coding schemes, which allow
intra coding within inter-coded frames. With the intra coding tool enabled,
disocclusion and new content are properly handled, and interframe error
propagation is naturally intercepted without the need for manual refresh
mechanisms. We present an NVC framework with unified intra and inter coding,
where every frame is processed by a single model that is trained to perform
intra/inter coding adaptively. Moreover, we propose a simultaneous two-frame
compression design to exploit interframe redundancy not only forwardly but also
backwardly. Experimental results show that our scheme outperforms DCVC-RT by an
average of 10.7\% BD-rate reduction, delivers more stable bitrate and quality
per frame, and retains real-time encoding/decoding performances. Code and
models will be released.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UMCFuse: A Unified Multiple Complex Scenes Infrared and Visible Image
  Fusion Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.02096v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.02096v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xilai Li, Xiaosong Li, Tianshu Tan, Huafeng Li, Tao Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Infrared and visible image fusion has emerged as a prominent research area in
computer vision. However, little attention has been paid to the fusion task in
complex scenes, leading to sub-optimal results under interference. To fill this
gap, we propose a unified framework for infrared and visible images fusion in
complex scenes, termed UMCFuse. Specifically, we classify the pixels of visible
images from the degree of scattering of light transmission, allowing us to
separate fine details from overall intensity. Maintaining a balance between
interference removal and detail preservation is essential for the
generalization capacity of the proposed method. Therefore, we propose an
adaptive denoising strategy for the fusion of detail layers. Meanwhile, we fuse
the energy features from different modalities by analyzing them from multiple
directions. Extensive fusion experiments on real and synthetic complex scenes
datasets cover adverse weather conditions, noise, blur, overexposure, fire, as
well as downstream tasks including semantic segmentation, object detection,
salient object detection, and depth estimation, consistently indicate the
superiority of the proposed method compared with the recent representative
methods. Our code is available at https://github.com/ixilai/UMCFuse.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in IEEE-TIP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GEMeX-RMCoT: An Enhanced Med-VQA <span class="highlight-title">Dataset</span> for Region-Aware Multimodal
  Chain-of-Thought Reasoning <span class="chip">ACM MM 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.17939v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.17939v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Liu, Xiangyu Zhao, Along He, Yidi Chen, Huazhu Fu, Xiao-Ming Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical visual question answering aims to support clinical decision-making by
enabling models to answer natural language questions based on medical images.
While recent advances in multi-modal learning have significantly improved
performance, current methods still suffer from limited answer reliability and
poor interpretability, impairing the ability of clinicians and patients to
understand and trust model outputs. To address these limitations, this work
first proposes a Region-Aware Multimodal Chain-of-Thought (RMCoT) dataset, in
which the process of producing an answer is preceded by a sequence of
intermediate reasoning steps that explicitly ground relevant visual regions of
the medical image, thereby providing fine-grained explainability. Furthermore,
we introduce a novel verifiable reward mechanism for reinforcement learning to
guide post-training, improving the alignment between the model's reasoning
process and its final answer. Remarkably, our method achieves comparable
performance using only one-eighth of the training data, demonstrating the
efficiency and effectiveness of the proposal. The dataset is available at
https://www.med-vqa.com/GEMeX/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACM MM 2025 (also known as GEMeX-ThinkVG)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Global urban visual perception varies across demographics and
  personalities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12758v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12758v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matias Quintana, Youlong Gu, Xiucheng Liang, Yujun Hou, Koichi Ito, Yihan Zhu, Mahmoud Abdelrahman, Filip Biljecki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding people's preferences is crucial for urban planning, yet current
approaches often combine responses from multi-cultural populations, obscuring
demographic differences and risking amplifying biases. We conducted a
largescale urban visual perception survey of streetscapes worldwide using
street view imagery, examining how demographics -- including gender, age,
income, education, race and ethnicity, and personality traits -- shape
perceptions among 1,000 participants with balanced demographics from five
countries and 45 nationalities. This dataset, Street Perception Evaluation
Considering Socioeconomics (SPECS), reveals demographic- and personality-based
differences across six traditional indicators -- safe, lively, wealthy,
beautiful, boring, depressing -- and four new ones -- live nearby, walk, cycle,
green. Location-based sentiments further shape these preferences. Machine
learning models trained on existing global datasets tend to overestimate
positive indicators and underestimate negative ones compared to human
responses, underscoring the need for local context. Our study aspires to
rectify the myopic treatment of street perception, which rarely considers
demographics or personality traits.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Task-Agnostic Fusion of Time Series and Imagery for Earth Observation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23118v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23118v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gianfranco Basile, Johannes Jakubik, Benedikt Blumenstiel, Thomas Brunschwiler, Juan Bernabe Moreno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a task-agnostic framework for multimodal fusion of time series and
single timestamp images, enabling cross-modal generation and robust downstream
performance. Our approach explores deterministic and learned strategies for
time series quantization and then leverages a masked correlation learning
objective, aligning discrete image and time series tokens in a unified
representation space. Instantiated in the Earth observation domain, the
pretrained model generates consistent global temperature profiles from
satellite imagery and is validated through counterfactual experiments. Across
downstream tasks, our task-agnostic pretraining outperforms task-specific
fusion by 6% in R^2 and 2% in RMSE on average, and exceeds baseline methods by
50\% in R$^2$ and 12\% in RMSE. Finally, we analyze gradient sensitivity across
modalities, providing insights into model robustness. Code, data, and weights
will be released under a permissive license.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GRASP: Geospatial pixel Reasoning viA Structured Policy learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.17102v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.17102v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengjie Jiang, Yunqi Zhou, Jiafeng Yan, Jing Li, Jiayang Li, Yue Zhou, Hongjie He, Jonathan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Geospatial pixel reasoning aims to generate segmentation masks in remote
sensing imagery directly from natural-language instructions. Most existing
approaches follow a paradigm that fine-tunes multimodal large language models
under supervision with dense pixel-level masks as ground truth. While effective
within the training data distribution, this design suffers from two main
drawbacks: (1) the high cost of large-scale dense mask annotation, and (2) the
limited generalization capability of supervised fine-tuning in out-of-domain
scenarios. To address these issues, we propose GRASP, a structured
policy-learning framework that integrates a multimodal large language model
with a pretrained segmentation model in a cascaded manner. To enhance
generalization, we introduce PRIME, a training paradigm that replaces
supervised fine-tuning with reinforcement learning to better align reasoning
and grounding behaviors with task objectives. To reduce annotation costs, we
design BoP-Rewards, which substitutes dense mask labels with bounding box and
positive points. It further verifies outputs through two complementary signals:
format, which constrains the reasoning and grounding structure to remain
syntactically parsable, and accuracy, which evaluates the quality of predicted
boxes and points. For evaluation, we train our method and all baselines on
EarthReason and GeoPixInstruct, constructing an in-domain benchmark by merging
their test sets. We further release GRASP-1k, a fully out-of-domain benchmark
with reasoning-intensive queries, reasoning traces, and fine-grained masks.
Experimental results demonstrate state-of-the-art (SOTA) in-domain performance
and up to 54\% improvement in out-of-domain scenarios, confirming that
reinforcement learning with cost-aware rewards provides a robust and scalable
paradigm for geospatial pixel reasoning. All code and datasets will be released
publicly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Switchable Token-Specific Codebook Quantization For Face Image
  Compression <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.22943v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.22943v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongbo Wang, Haonan Wang, Guodong Mu, Ruixin Zhang, Jiaqi Chen, Jingyun Zhang, Jun Wang, Yuan Xie, Zhizhong Zhang, Shouhong Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the ever-increasing volume of visual data, the efficient and lossless
transmission, along with its subsequent interpretation and understanding, has
become a critical bottleneck in modern information systems. The emerged
codebook-based solution utilize a globally shared codebook to quantize and
dequantize each token, controlling the bpp by adjusting the number of tokens or
the codebook size. However, for facial images, which are rich in attributes,
such global codebook strategies overlook both the category-specific
correlations within images and the semantic differences among tokens, resulting
in suboptimal performance, especially at low bpp. Motivated by these
observations, we propose a Switchable Token-Specific Codebook Quantization for
face image compression, which learns distinct codebook groups for different
image categories and assigns an independent codebook to each token. By
recording the codebook group to which each token belongs with a small number of
bits, our method can reduce the loss incurred when decreasing the size of each
codebook group. This enables a larger total number of codebooks under a lower
overall bpp, thereby enhancing the expressive capability and improving
reconstruction performance. Owing to its generalizable design, our method can
be integrated into any existing codebook-based representation learning approach
and has demonstrated its effectiveness on face recognition datasets, achieving
an average accuracy of 93.51% for reconstructed images at 0.05 bpp.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025 accepted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Long-RVOS: A Comprehensive Benchmark for Long-term Referring Video
  Object Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12702v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12702v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianming Liang, Haichao Jiang, Yuting Yang, Chaolei Tan, Shuai Li, Wei-Shi Zheng, Jian-Fang Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Referring video object segmentation (RVOS) aims to identify, track and
segment the objects in a video based on language descriptions, which has
received great attention in recent years. However, existing datasets remain
focus on short video clips within several seconds, with salient objects visible
in most frames. To advance the task towards more practical scenarios, we
introduce \textbf{Long-RVOS}, a large-scale benchmark for long-term referring
video object segmentation. Long-RVOS contains 2,000+ videos of an average
duration exceeding 60 seconds, covering a variety of objects that undergo
occlusion, disappearance-reappearance and shot changing. The objects are
manually annotated with three different types of descriptions to individually
evaluate the understanding of static attributes, motion patterns and
spatiotemporal relationships. Moreover, unlike previous benchmarks that rely
solely on the per-frame spatial evaluation, we introduce two new metrics to
assess the temporal and spatiotemporal consistency. We benchmark 6
state-of-the-art methods on Long-RVOS. The results show that current approaches
struggle severely with the long-video challenges. To address this, we further
propose ReferMo, a promising baseline method that integrates motion information
to expand the temporal receptive field, and employs a local-to-global
architecture to capture both short-term dynamics and long-term dependencies.
Despite simplicity, ReferMo achieves significant improvements over current
methods in long-term scenarios. We hope that Long-RVOS and our baseline can
drive future RVOS research towards tackling more realistic and long-form
videos.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: \url{https://isee-laboratory.github.io/Long-RVOS}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GS4: Generalizable Sparse Splatting Semantic SLAM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.06517v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.06517v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingqi Jiang, Chanho Kim, Chen Ziwen, Li Fuxin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional SLAM algorithms excel at camera tracking, but typically produce
incomplete and low-resolution maps that are not tightly integrated with
semantics prediction. Recent work integrates Gaussian Splatting (GS) into SLAM
to enable dense, photorealistic 3D mapping, yet existing GS-based SLAM methods
require per-scene optimization that is slow and consumes an excessive number of
Gaussians. We present GS4, the first generalizable GS-based semantic SLAM
system. Compared with prior approaches, GS4 runs 10x faster, uses 10x fewer
Gaussians, and achieves state-of-the-art performance across color, depth,
semantic mapping and camera tracking. From an RGB-D video stream, GS4
incrementally builds and updates a set of 3D Gaussians using a feed-forward
network. First, the Gaussian Prediction Model estimates a sparse set of
Gaussian parameters from input frame, which integrates both color and semantic
prediction with the same backbone. Then, the Gaussian Refinement Network merges
new Gaussians with the existing set while avoiding redundancy. Finally, we
propose to optimize GS for only 1-5 iterations that corrects drift and floaters
when significant pose changes are detected. Experiments on the real-world
ScanNet and ScanNet++ benchmarks demonstrate state-of-the-art semantic SLAM
performance, with strong generalization capability shown through zero-shot
transfer to the NYUv2 and TUM RGB-D datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bridging the gap to real-world language-grounded visual concept learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.21412v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.21412v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Whie Jung, Semin Kim, Junee Kim, Seunghoon Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human intelligence effortlessly interprets visual scenes along a rich
spectrum of semantic dimensions. However, existing approaches to
language-grounded visual concept learning are limited to a few predefined
primitive axes, such as color and shape, and are typically explored in
synthetic datasets. In this work, we propose a scalable framework that
adaptively identifies image-related concept axes and grounds visual concepts
along these axes in real-world scenes. Leveraging a pretrained vision-language
model and our universal prompting strategy, our framework identifies a diverse
image-related axes without any prior knowledge. Our universal concept encoder
adaptively binds visual features to the discovered axes without introducing
additional model parameters for each concept. To ground visual concepts along
the discovered axes, we optimize a compositional anchoring objective, which
ensures that each axis can be independently manipulated without affecting
others. We demonstrate the effectiveness of our framework on subsets of
ImageNet, CelebA-HQ, and AFHQ, showcasing superior editing capabilities across
diverse real-world concepts that are too varied to be manually predefined. Our
method also exhibits strong compositional generalization, outperforming
existing visual concept learning and text-based editing methods. The code is
available at https://github.com/whieya/Language-grounded-VCL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CalFuse: Multi-Modal Continual Learning via Feature Calibration and
  Parameter Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18672v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18672v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juncen Guo, Siao Liu, Xiaoguang Zhu, Lianlong Sun, Liangyu Teng, Jingyi Wu, Di Li, Linxiao Gong, Weiwei Jiang, Wei Zhou, Liang Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the proliferation of multi-modal data in large-scale visual recognition
systems, enabling models to continuously acquire knowledge from evolving data
streams while preserving prior information has become increasingly critical.
Class-Continual Learning (CCL) addresses this challenge by incrementally
incorporating new class knowledge without revisiting historical data, making it
essential for real-world big data applications. While traditional CCL methods
rely solely on visual features, recent advances in Vision-Language Models
(VLMs) such as CLIP demonstrate significant potential for CCL by leveraging
pre-trained multi-modal knowledge. However, existing approaches face challenges
in mitigating catastrophic forgetting while maintaining the cross-modal
generalization capabilities of VLMs. To address these limitations, we propose
CalFuse, a framework that synergizes feature Calibration with parameter Fusion
to enable effective multi-modal knowledge integration in continual learning
scenarios. CalFuse introduces a dynamic feature calibration mechanism that
adaptively balances original CLIP visual representations with task-specific
features, preserving the model's intrinsic cross-modal generalization while
adapting to new classes. Concurrently, a QR decomposition-based parameter
fusion strategy progressively integrates newly acquired knowledge with
historical task parameters, maintaining equilibrium between learning new class
representations and retaining prior knowledge across sequential tasks.
Extensive experiments on benchmark datasets validate the effectiveness of our
approach in large-scale multi-modal continual learning settings, demonstrating
superior performance over state-of-the-art methods in both average accuracy and
final task retention.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MsEdF: A Multi-stream Encoder-decoder Framework for Remote Sensing Image
  Captioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09282v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09282v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Swadhin Das, Raksha Sharma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Remote sensing images contain complex spatial patterns and semantic
structures, which makes the captioning model difficult to accurately describe.
Encoder-decoder architectures have become the widely used approach for RSIC by
translating visual content into descriptive text. However, many existing
methods rely on a single-stream architecture, which weakens the model to
accurately describe the image. Such single-stream architectures typically
struggle to extract diverse spatial features or capture complex semantic
relationships, limiting their effectiveness in scenes with high intraclass
similarity or contextual ambiguity. In this work, we propose a novel
Multi-stream Encoder-decoder Framework (MsEdF) which improves the performance
of RSIC by optimizing both the spatial representation and language generation
of encoder-decoder architecture. The encoder fuses information from two
complementary image encoders, thereby promoting feature diversity through the
integration of multiscale and structurally distinct cues. To improve the
capture of context-aware descriptions, we refine the input sequence's semantic
modeling on the decoder side using a stacked GRU architecture with an
element-wise aggregation scheme. Experiments on three benchmark RSIC datasets
show that MsEdF outperforms several baseline models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MDP3: A Training-free Approach for List-wise Frame Selection in
  Video-LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.02885v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.02885v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui Sun, Shiyin Lu, Huanyu Wang, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, Ming Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video large language models (Video-LLMs) have made significant progress in
understanding videos. However, processing multiple frames leads to lengthy
visual token sequences, presenting challenges such as the limited context
length cannot accommodate the entire video, and the inclusion of irrelevant
frames hinders visual perception. Hence, effective frame selection is crucial.
This paper emphasizes that frame selection should follow three key principles:
query relevance, list-wise diversity, and sequentiality. Existing methods, such
as uniform frame sampling and query-frame matching, do not capture all of these
principles. Thus, we propose Markov decision determinantal point process with
dynamic programming (MDP3) for frame selection, a training-free and
model-agnostic method that can be seamlessly integrated into existing
Video-LLMs. Our method first estimates frame similarities conditioned on the
query using a conditional Gaussian kernel within the reproducing kernel Hilbert
space~(RKHS). We then apply the determinantal point process~(DPP) to the
similarity matrix to capture both query relevance and list-wise diversity. To
incorporate sequentiality, we segment the video and apply DPP within each
segment, conditioned on the preceding segment selection, modeled as a Markov
decision process~(MDP) for allocating selection sizes across segments.
Theoretically, MDP3 provides a \((1 - 1/e)\)-approximate solution to the
NP-hard list-wise frame selection problem with pseudo-polynomial time
complexity, demonstrating its efficiency. Empirically, MDP3 significantly
outperforms existing methods, verifying its effectiveness and robustness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IGGT: Instance-Grounded Geometry <span class="highlight-title">Transformer</span> for Semantic 3D
  Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.22706v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.22706v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Li, Zhengyu Zou, Fangfu Liu, Xuanyang Zhang, Fangzhou Hong, Yukang Cao, Yushi Lan, Manyuan Zhang, Gang Yu, Dingwen Zhang, Ziwei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans naturally perceive the geometric structure and semantic content of a
3D world as intertwined dimensions, enabling coherent and accurate
understanding of complex scenes. However, most prior approaches prioritize
training large geometry models for low-level 3D reconstruction and treat
high-level spatial understanding in isolation, overlooking the crucial
interplay between these two fundamental aspects of 3D-scene analysis, thereby
limiting generalization and leading to poor performance in downstream 3D
understanding tasks. Recent attempts have mitigated this issue by simply
aligning 3D models with specific language models, thus restricting perception
to the aligned model's capacity and limiting adaptability to downstream tasks.
In this paper, we propose InstanceGrounded Geometry Transformer (IGGT), an
end-to-end large unified transformer to unify the knowledge for both spatial
reconstruction and instance-level contextual understanding. Specifically, we
design a 3D-Consistent Contrastive Learning strategy that guides IGGT to encode
a unified representation with geometric structures and instance-grounded
clustering through only 2D visual inputs. This representation supports
consistent lifting of 2D visual inputs into a coherent 3D scene with explicitly
distinct object instances. To facilitate this task, we further construct
InsScene-15K, a large-scale dataset with high-quality RGB images, poses, depth
maps, and 3D-consistent instance-level mask annotations with a novel data
curation pipeline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/lifuguan/IGGT_official</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VSA: Faster Video Diffusion with Trainable Sparse Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13389v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13389v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peiyuan Zhang, Yongqi Chen, Haofeng Huang, Will Lin, Zhengzhong Liu, Ion Stoica, Eric Xing, Hao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling video diffusion transformers (DiTs) is limited by their quadratic 3D
attention, even though most of the attention mass concentrates on a small
subset of positions. We turn this observation into VSA, a trainable,
hardware-efficient sparse attention that replaces full attention at \emph{both}
training and inference. In VSA, a lightweight coarse stage pools tokens into
tiles and identifies high-weight \emph{critical tokens}; a fine stage computes
token-level attention only inside those tiles subjecting to block computing
layout to ensure hard efficiency. This leads to a single differentiable kernel
that trains end-to-end, requires no post-hoc profiling, and sustains 85\% of
FlashAttention3 MFU. We perform a large sweep of ablation studies and
scaling-law experiments by pretraining DiTs from 60M to 1.4B parameters. VSA
reaches a Pareto point that cuts training FLOPS by 2.53$\times$ with no drop in
diffusion loss. Retrofitting the open-source Wan-2.1 model speeds up attention
time by 6$\times$ and lowers end-to-end generation time from 31s to 18s with
comparable quality. These results establish trainable sparse attention as a
practical alternative to full attention and a key enabler for further scaling
of video diffusion models. Code will be available at
https://github.com/hao-ai-lab/FastVideo.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Neurips 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Riemannian-Geometric Fingerprints of Generative Models <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.22802v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.22802v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hae Jin Song, Laurent Itti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent breakthroughs and rapid integration of generative models (GMs) have
sparked interest in the problem of model attribution and their fingerprints.
For instance, service providers need reliable methods of authenticating their
models to protect their IP, while users and law enforcement seek to verify the
source of generated content for accountability and trust. In addition, a
growing threat of model collapse is arising, as more model-generated data are
being fed back into sources (e.g., YouTube) that are often harvested for
training ("regurgitative training"), heightening the need to differentiate
synthetic from human data. Yet, a gap still exists in understanding generative
models' fingerprints, we believe, stemming from the lack of a formal framework
that can define, represent, and analyze the fingerprints in a principled way.
To address this gap, we take a geometric approach and propose a new definition
of artifact and fingerprint of GMs using Riemannian geometry, which allows us
to leverage the rich theory of differential geometry. Our new definition
generalizes previous work (Song et al., 2024) to non-Euclidean manifolds by
learning Riemannian metrics from data and replacing the Euclidean distances and
nearest-neighbor search with geodesic distances and kNN-based Riemannian center
of mass. We apply our theory to a new gradient-based algorithm for computing
the fingerprints in practice. Results show that it is more effective in
distinguishing a large array of GMs, spanning across 4 different datasets in 2
different resolutions (64 by 64, 256 by 256), 27 model architectures, and 2
modalities (Vision, Vision-Language). Using our proposed definition
significantly improves the performance on model attribution, as well as a
generalization to unseen datasets, model types, and modalities, suggesting its
practical efficacy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2025 Highlight paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InstanceAssemble: Layout-Aware Image Generation via Instance Assembling
  Attention <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.16691v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.16691v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiang Xiang, Shuang Sun, Binglei Li, Dejia Song, Huaxia Li, Nemo Chen, Xu Tang, Yao Hu, Junping Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have demonstrated remarkable capabilities in generating
high-quality images. Recent advancements in Layout-to-Image (L2I) generation
have leveraged positional conditions and textual descriptions to facilitate
precise and controllable image synthesis. Despite overall progress, current L2I
methods still exhibit suboptimal performance. Therefore, we propose
InstanceAssemble, a novel architecture that incorporates layout conditions via
instance-assembling attention, enabling position control with bounding boxes
(bbox) and multimodal content control including texts and additional visual
content. Our method achieves flexible adaption to existing DiT-based T2I models
through light-weighted LoRA modules. Additionally, we propose a Layout-to-Image
benchmark, Denselayout, a comprehensive benchmark for layout-to-image
generation, containing 5k images with 90k instances in total. We further
introduce Layout Grounding Score (LGS), an interpretable evaluation metric to
more precisely assess the accuracy of L2I generation. Experiments demonstrate
that our InstanceAssemble method achieves state-of-the-art performance under
complex layout conditions, while exhibiting strong compatibility with diverse
style LoRA modules. The code and pretrained models are publicly available at
https://github.com/FireRedTeam/InstanceAssemble.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MoPFormer: Motion-Primitive <span class="highlight-title">Transformer</span> for Wearable-Sensor Activity
  Recognition <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.20744v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.20744v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Zhang, Zhan Zhuang, Xuehao Wang, Xiaodong Yang, Yu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human Activity Recognition (HAR) with wearable sensors is challenged by
limited interpretability, which significantly impacts cross-dataset
generalization. To address this challenge, we propose Motion-Primitive
Transformer (MoPFormer), a novel self-supervised framework that enhances
interpretability by tokenizing inertial measurement unit signals into
semantically meaningful motion primitives and leverages a Transformer
architecture to learn rich temporal representations. MoPFormer comprises two
stages. The first stage is to partition multi-channel sensor streams into short
segments and quantize them into discrete ``motion primitive'' codewords, while
the second stage enriches those tokenized sequences through a context-aware
embedding module and then processes them with a Transformer encoder. The
proposed MoPFormer can be pre-trained using a masked motion-modeling objective
that reconstructs missing primitives, enabling it to develop robust
representations across diverse sensor configurations. Experiments on six HAR
benchmarks demonstrate that MoPFormer not only outperforms state-of-the-art
methods but also successfully generalizes across multiple datasets. More
importantly, the learned motion primitives significantly enhance both
interpretability and cross-dataset performance by capturing fundamental
movement patterns that remain consistent across similar activities, regardless
of dataset origin.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to See and Act: Task-Aware View Planning for Robotic
  Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.05186v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.05186v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongjie Bai, Zhouxia Wang, Yang Liu, Weixing Chen, Ziliang Chen, Mingtong Dai, Yongsen Zheng, Lingbo Liu, Guanbin Li, Liang Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent vision-language-action (VLA) models for multi-task robotic
manipulation commonly rely on static viewpoints and shared visual encoders,
which limit 3D perception and cause task interference, hindering robustness and
generalization. In this work, we propose Task-Aware View Planning (TAVP), a
framework designed to overcome these challenges by integrating active view
planning with task-specific representation learning. TAVP employs an efficient
exploration policy, accelerated by a novel pseudo-environment, to actively
acquire informative views. Furthermore, we introduce a Mixture-of-Experts (MoE)
visual encoder to disentangle features across different tasks, boosting both
representation fidelity and task generalization. By learning to see the world
in a task-aware way, TAVP generates more complete and discriminative visual
representations, demonstrating significantly enhanced action prediction across
a wide array of manipulation challenges. Extensive experiments on RLBench tasks
show that our proposed TAVP model achieves superior performance over
state-of-the-art fixed-view approaches. Visual results and code are provided
at: https://hcplab-sysu.github.io/TAVP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 8 figures, project page: https://hcplab-sysu.github.io/TAVP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unveiling Concept Attribution in Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.02542v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.02542v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quang H. Nguyen, Hoang Phan, Khoa D. Doan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have shown remarkable abilities in generating realistic and
high-quality images from text prompts. However, a trained model remains largely
black-box; little do we know about the roles of its components in exhibiting a
concept such as objects or styles. Recent works employ causal tracing to
localize knowledge-storing layers in generative models without showing how
other layers contribute to the target concept. In this work, we approach
diffusion models' interpretability problem from a more general perspective and
pose a question: \textit{``How do model components work jointly to demonstrate
knowledge?''}. To answer this question, we decompose diffusion models using
component attribution, systematically unveiling the importance of each
component (specifically the model parameter) in generating a concept. The
proposed framework, called \textbf{C}omponent \textbf{A}ttribution for
\textbf{D}iffusion Model (CAD), discovers the localization of concept-inducing
(positive) components, while interestingly uncovers another type of components
that contribute negatively to generating a concept, which is missing in the
previous knowledge localization work. Based on this holistic understanding of
diffusion models, we introduce two fast, inference-time model editing
algorithms, CAD-Erase and CAD-Amplify; in particular, CAD-Erase enables erasure
and CAD-Amplify allows amplification of a generated concept by ablating the
positive and negative components, respectively, while retaining knowledge of
other concepts. Extensive experimental results validate the significance of
both positive and negative components pinpointed by our framework,
demonstrating the potential of providing a complete view of interpreting
generative models. Our code is available
\href{https://github.com/mail-research/CAD-attribution4diffusion}{here}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TraceTrans: Translation and Spatial Tracing for Surgical Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.22379v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.22379v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiyu Luo, Haodong Li, Xinxing Cheng, He Zhao, Yang Hu, Xuan Song, Tianyang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image-to-image translation models have achieved notable success in converting
images across visual domains and are increasingly used for medical tasks such
as predicting post-operative outcomes and modeling disease progression.
However, most existing methods primarily aim to match the target distribution
and often neglect spatial correspondences between the source and translated
images. This limitation can lead to structural inconsistencies and
hallucinations, undermining the reliability and interpretability of the
predictions. These challenges are accentuated in clinical applications by the
stringent requirement for anatomical accuracy. In this work, we present
TraceTrans, a novel deformable image translation model designed for
post-operative prediction that generates images aligned with the target
distribution while explicitly revealing spatial correspondences with the
pre-operative input. The framework employs an encoder for feature extraction
and dual decoders for predicting spatial deformations and synthesizing the
translated image. The predicted deformation field imposes spatial constraints
on the generated output, ensuring anatomical consistency with the source.
Extensive experiments on medical cosmetology and brain MRI datasets demonstrate
that TraceTrans delivers accurate and interpretable post-operative predictions,
highlighting its potential for reliable clinical deployment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Objects to Anywhere: A Holistic Benchmark for Multi-level Visual
  Grounding in 3D Scenes <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04897v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04897v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianxu Wang, Zhuofan Zhang, Ziyu Zhu, Yue Fan, Jing Xiong, Pengxiang Li, Xiaojian Ma, Qing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D visual grounding has made notable progress in localizing objects within
complex 3D scenes. However, grounding referring expressions beyond objects in
3D scenes remains unexplored. In this paper, we introduce Anywhere3D-Bench, a
holistic 3D visual grounding benchmark consisting of 2,886 referring
expression-3D bounding box pairs spanning four different grounding levels:
human-activity areas, unoccupied space beyond objects, individual objects in
the scene, and fine-grained object parts. We assess a range of state-of-the-art
3D visual grounding methods alongside large language models (LLMs) and
multimodal LLMs (MLLMs) on Anywhere3D-Bench. Experimental results reveal that
space-level and part-level visual grounding pose the greatest challenges:
space-level tasks require a more comprehensive spatial reasoning ability, for
example, modeling distances and spatial relations within 3D space, while
part-level tasks demand fine-grained perception of object composition. Even the
best-performing models, Google Gemini-2.5-Pro and OpenAI o3, achieve just
around 30% accuracy on space-level tasks and around 40% on part-level tasks,
significantly lower than its performance on area-level and object-level tasks.
These findings underscore a critical gap in current models' capacity to
understand and reason about 3D scenes beyond object-level semantics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Update v3 of the NeurIPS 2025 Datasets and Benchmarks paper (v2),
  including additional evaluations of state-of-the-art multimodal large
  language models. Project page: https://anywhere-3d.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised Monocular Depth Estimation Based on Hierarchical
  Feature-Guided Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09782v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09782v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runze Liu, Dongchen Zhu, Guanghui Zhang, Yue Xu, Wenjun Shi, Xiaolin Zhang, Lei Wang, Jiamao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised monocular depth estimation has received widespread attention
because of its capability to train without ground truth. In real-world
scenarios, the images may be blurry or noisy due to the influence of weather
conditions and inherent limitations of the camera. Therefore, it is
particularly important to develop a robust depth estimation model. Benefiting
from the training strategies of generative networks, generative-based methods
often exhibit enhanced robustness. In light of this, we employ a
well-converging diffusion model among generative networks for unsupervised
monocular depth estimation. Additionally, we propose a hierarchical
feature-guided denoising module. This model significantly enriches the model's
capacity for learning and interpreting depth distribution by fully leveraging
image features to guide the denoising process. Furthermore, we explore the
implicit depth within reprojection and design an implicit depth consistency
loss. This loss function serves to enhance the performance of the model and
ensure the scale consistency of depth within a video sequence. We conduct
experiments on the KITTI, Make3D, and our self-collected SIMIT datasets. The
results indicate that our approach stands out among generative-based models,
while also showcasing remarkable robustness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Navigation with VLM framework: Towards Going to Any Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02787v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02787v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zecheng Yin, Chonghao Cheng, and Yao Guo, Zhen Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Navigating towards fully open language goals and exploring open scenes in an
intelligent way have always raised significant challenges. Recently, Vision
Language Models (VLMs) have demonstrated remarkable capabilities to reason with
both language and visual data. Although many works have focused on leveraging
VLMs for navigation in open scenes, they often require high computational cost,
rely on object-centric approaches, or depend on environmental priors in
detailed human instructions. We introduce Navigation with VLM (NavVLM), a
training-free framework that harnesses open-source VLMs to enable robots to
navigate effectively, even for human-friendly language goal such as abstract
places, actions, or specific objects in open scenes. NavVLM leverages the VLM
as its cognitive core to perceive environmental information and constantly
provides exploration guidance achieving intelligent navigation with only a neat
target rather than a detailed instruction with environment prior. We evaluated
and validated NavVLM in both simulation and real-world experiments. In
simulation, our framework achieves state-of-the-art performance in Success
weighted by Path Length (SPL) on object-specifc tasks in richly detailed
environments from Matterport 3D (MP3D), Habitat Matterport 3D (HM3D) and
Gibson. With navigation episode reported, NavVLM demonstrates the capabilities
to navigate towards any open-set languages. In real-world validation, we
validated our framework's effectiveness in real-world robot at indoor scene.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GaussianFusion: Gaussian-Based Multi-Sensor Fusion for End-to-End
  Autonomous Driving <span class="chip">NeurIPS2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.00034v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.00034v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuai Liu, Quanmin Liang, Zefeng Li, Boyang Li, Kai Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-sensor fusion is crucial for improving the performance and robustness
of end-to-end autonomous driving systems. Existing methods predominantly adopt
either attention-based flatten fusion or bird's eye view fusion through
geometric transformations. However, these approaches often suffer from limited
interpretability or dense computational overhead. In this paper, we introduce
GaussianFusion, a Gaussian-based multi-sensor fusion framework for end-to-end
autonomous driving. Our method employs intuitive and compact Gaussian
representations as intermediate carriers to aggregate information from diverse
sensors. Specifically, we initialize a set of 2D Gaussians uniformly across the
driving scene, where each Gaussian is parameterized by physical attributes and
equipped with explicit and implicit features. These Gaussians are progressively
refined by integrating multi-modal features. The explicit features capture rich
semantic and spatial information about the traffic scene, while the implicit
features provide complementary cues beneficial for trajectory planning. To
fully exploit rich spatial and semantic information in Gaussians, we design a
cascade planning head that iteratively refines trajectory predictions through
interactions with Gaussians. Extensive experiments on the NAVSIM and
Bench2Drive benchmarks demonstrate the effectiveness and robustness of the
proposed GaussianFusion framework. The source code will be released at
https://github.com/Say2L/GaussianFusion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS2025 (Spotlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PRISM-Bench: A Benchmark of Puzzle-Based Visual Tasks with CoT Error
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23594v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23594v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusu Qian, Cheng Wan, Chao Jia, Yinfei Yang, Qingyu Zhao, Zhe Gan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) have achieved remarkable progress on
vision-language tasks, yet their reasoning processes remain sometimes
unreliable. We introduce PRISM-Bench, a benchmark of puzzle-based visual
challenges designed to evaluate not only whether models can solve problems, but
how their reasoning unfolds. Unlike prior evaluations that measure only
final-answer accuracy, PRISM-Bench introduces a diagnostic task: given a visual
puzzle and a step-by-step chain-of-thought (CoT) containing exactly one error,
models must identify the first incorrect step. This setting enables
fine-grained assessment of logical consistency, error detection, and visual
reasoning. The puzzles in PRISM-Bench require multi-step symbolic, geometric,
and analogical reasoning, resisting shortcuts based on superficial pattern
matching. Evaluations across state-of-the-art MLLMs reveal a persistent gap
between fluent generation and faithful reasoning: models that produce plausible
CoTs often fail to locate simple logical faults. By disentangling answer
generation from reasoning verification, PRISM-Bench offers a sharper lens on
multimodal reasoning competence and underscores the need for diagnostic
evaluation protocols in the development of trustworthy MLLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Radar and Event Camera Fusion for Agile Robot Ego-Motion Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.18443v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.18443v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Lyu, Zhenghao Zou, Yanfeng Li, Xiaohu Guo, Chunhui Zhao, Quan Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Achieving reliable ego motion estimation for agile robots, e.g., aerobatic
aircraft, remains challenging because most robot sensors fail to respond timely
and clearly to highly dynamic robot motions, often resulting in measurement
blurring, distortion, and delays. In this paper, we propose an IMU-free and
feature-association-free framework to achieve aggressive ego-motion velocity
estimation of a robot platform in highly dynamic scenarios by combining two
types of exteroceptive sensors, an event camera and a millimeter wave radar,
First, we used instantaneous raw events and Doppler measurements to derive
rotational and translational velocities directly. Without a sophisticated
association process between measurement frames, the proposed method is more
robust in texture-less and structureless environments and is more
computationally efficient for edge computing devices. Then, in the back-end, we
propose a continuous-time state-space model to fuse the hybrid time-based and
event-based measurements to estimate the ego-motion velocity in a fixed-lagged
smoother fashion. In the end, we validate our velometer framework extensively
in self-collected experiment datasets. The results indicate that our IMU-free
and association-free ego motion estimation framework can achieve reliable and
efficient velocity output in challenging environments. The source code,
illustrative video and dataset are available at
https://github.com/ZzhYgwh/TwistEstimator.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2025.10.28 version v2 for TwistEstimator</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Boosting Omnidirectional Stereo Matching with a <span class="highlight-title">Pre-train</span>ed Depth
  Foundation Model <span class="chip">IROS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.23502v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.23502v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jannik Endres, Oliver Hahn, Charles Corbière, Simone Schaub-Meyer, Stefan Roth, Alexandre Alahi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Omnidirectional depth perception is essential for mobile robotics
applications that require scene understanding across a full 360{\deg} field of
view. Camera-based setups offer a cost-effective option by using stereo depth
estimation to generate dense, high-resolution depth maps without relying on
expensive active sensing. However, existing omnidirectional stereo matching
approaches achieve only limited depth accuracy across diverse environments,
depth ranges, and lighting conditions, due to the scarcity of real-world data.
We present DFI-OmniStereo, a novel omnidirectional stereo matching method that
leverages a large-scale pre-trained foundation model for relative monocular
depth estimation within an iterative optimization-based stereo matching
architecture. We introduce a dedicated two-stage training strategy to utilize
the relative monocular depth features for our omnidirectional stereo matching
before scale-invariant fine-tuning. DFI-OmniStereo achieves state-of-the-art
results on the real-world Helvipad dataset, reducing disparity MAE by
approximately 16% compared to the previous best omnidirectional stereo method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IROS 2025. Project page:
  https://vita-epfl.github.io/DFI-OmniStereo-website/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CAUSAL3D: A Comprehensive Benchmark for Causal Learning from Visual Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04852v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04852v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Disheng Liu, Yiran Qiao, Wuche Liu, Yiren Lu, Yunlai Zhou, Tuo Liang, Yu Yin, Jing Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  True intelligence hinges on the ability to uncover and leverage hidden causal
relations. Despite significant progress in AI and computer vision (CV), there
remains a lack of benchmarks for assessing models' abilities to infer latent
causality from complex visual data. In this paper, we introduce
\textsc{\textbf{Causal3D}}, a novel and comprehensive benchmark that integrates
structured data (tables) with corresponding visual representations (images) to
evaluate causal reasoning. Designed within a systematic framework, Causal3D
comprises 19 3D-scene datasets capturing diverse causal relations, views, and
backgrounds, enabling evaluations across scenes of varying complexity. We
assess multiple state-of-the-art methods, including classical causal discovery,
causal representation learning, and large/vision-language models (LLMs/VLMs).
Our experiments show that as causal structures grow more complex without prior
knowledge, performance declines significantly, highlighting the challenges even
advanced methods face in complex causal scenarios. Causal3D serves as a vital
resource for advancing causal reasoning in CV and fostering trustworthy AI in
critical domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GRAID: Enhancing Spatial Reasoning of VLMs Through High-Fidelity Data
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.22118v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.22118v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karim Elmaaroufi, Liheng Lai, Justin Svegliato, Yutong Bai, Sanjit A. Seshia, Matei Zaharia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Language Models (VLMs) achieve strong performance on many
vision-language tasks but often struggle with spatial
reasoning$\unicode{x2014}$a prerequisite for many applications. Empirically, we
find that a dataset produced by a current training data generation pipeline has
a 57.6% human validation rate. These rates stem from current limitations:
single-image 3D reconstruction introduces cascading modeling errors and
requires wide answer tolerances, while caption-based methods require
hyper-detailed annotations and suffer from generative hallucinations. We
present GRAID, built on the key insight that qualitative spatial relationships
can be reliably determined from 2D geometric primitives alone. By operating
exclusively on 2D bounding boxes from standard object detectors, GRAID avoids
both 3D reconstruction errors and generative hallucinations, resulting in
datasets that are of higher quality than existing tools that produce similar
datasets as validated by human evaluations. We apply our framework to the
BDD100k, NuImages, and Waymo datasets, generating over 8.5 million high-quality
VQA pairs creating questions spanning spatial relations, counting, ranking, and
size comparisons. We evaluate one of the datasets and find it achieves 91.16%
human-validated accuracy$\unicode{x2014}$compared to 57.6% on a dataset
generated by recent work. Critically, we demonstrate that when trained on GRAID
data, models learn spatial reasoning concepts that generalize: models
fine-tuned on 6 question types improve on over 10 held-out types, with accuracy
gains of 47.5% on BDD and 37.9% on NuImages for Llama 3.2B 11B, and when
trained on all questions types, achieve improvements on several existing
benchmarks such as BLINK. The GRAID framework, datasets, and additional
information can be found $\href{this https URL}{here}$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 3 figures, 3 tables, project page:
  https://ke7.github.io/graid/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MambaCAFU: Hybrid Multi-Scale and Multi-Attention Model with Mamba-Based
  Fusion for Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.03786v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.03786v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        T-Mai Bui, Fares Bougourzi, Fadi Dornaika, Vinh Truong Hoang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, deep learning has shown near-expert performance in
segmenting complex medical tissues and tumors. However, existing models are
often task-specific, with performance varying across modalities and anatomical
regions. Balancing model complexity and performance remains challenging,
particularly in clinical settings where both accuracy and efficiency are
critical. To address these issues, we propose a hybrid segmentation
architecture featuring a three-branch encoder that integrates CNNs,
Transformers, and a Mamba-based Attention Fusion (MAF) mechanism to capture
local, global, and long-range dependencies. A multi-scale attention-based CNN
decoder reconstructs fine-grained segmentation maps while preserving contextual
consistency. Additionally, a co-attention gate enhances feature selection by
emphasizing relevant spatial and semantic information across scales during both
encoding and decoding, improving feature interaction and cross-scale
communication. Extensive experiments on multiple benchmark datasets show that
our approach outperforms state-of-the-art methods in accuracy and
generalization, while maintaining comparable computational complexity. By
effectively balancing efficiency and effectiveness, our architecture offers a
practical and scalable solution for diverse medical imaging tasks. Source code
and trained models will be publicly released upon acceptance to support
reproducibility and further research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Physics Context Builders: A Modular Framework for Physical Reasoning in
  Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08619v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08619v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vahid Balazadeh, Mohammadmehdi Ataei, Hyunmin Cheong, Amir Hosein Khasahmadi, Rahul G. Krishnan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Physical reasoning remains a significant challenge for Vision-Language Models
(VLMs). This limitation arises from an inability to translate learned knowledge
into predictions about physical behavior. Although continual fine-tuning can
mitigate this issue, it is expensive for large models and impractical to
perform repeatedly for every task. This necessitates the creation of modular
and scalable ways to teach VLMs about physical reasoning. To that end, we
introduce Physics Context Builders (PCBs), a modular framework where
specialized smaller VLMs are fine-tuned to generate detailed physical scene
descriptions. These can be used as physical contexts to enhance the reasoning
capabilities of larger VLMs. PCBs enable the separation of visual perception
from reasoning, allowing us to analyze their relative contributions to physical
understanding. We perform experiments on CLEVRER and on Falling Tower, a
stability detection dataset with both simulated and real-world scenes, to
demonstrate that PCBs provide substantial performance improvements, increasing
average accuracy by up to 13.8% on complex physical reasoning tasks. Notably,
PCBs also show strong Sim2Real transfer, successfully generalizing from
simulated training data to real-world scenes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SignMouth: Leveraging Mouthing Cues for Sign Language Translation by
  Multimodal Contrastive Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.10266v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.10266v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenfang Wu, Tingting Yuan, Yupeng Li, Daling Wang, Xiaoming Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sign language translation (SLT) aims to translate natural language from sign
language videos, serving as a vital bridge for inclusive communication. While
recent advances leverage powerful visual backbones and large language models,
most approaches mainly focus on manual signals (hand gestures) and tend to
overlook non-manual cues like mouthing. In fact, mouthing conveys essential
linguistic information in sign languages and plays a crucial role in
disambiguating visually similar signs. In this paper, we propose SignClip, a
novel framework to improve the accuracy of sign language translation. It fuses
manual and non-manual cues, specifically spatial gesture and lip movement
features. Besides, SignClip introduces a hierarchical contrastive learning
framework with multi-level alignment objectives, ensuring semantic consistency
across sign-lip and visual-text modalities. Extensive experiments on two
benchmark datasets, PHOENIX14T and How2Sign, demonstrate the superiority of our
approach. For example, on PHOENIX14T, in the Gloss-free setting, SignClip
surpasses the previous state-of-the-art model SpaMo, improving BLEU-4 from
24.32 to 24.71, and ROUGE from 46.57 to 48.38.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Re-ttention: Ultra Sparse Visual Generation via Attention Statistical
  Reshape 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.22918v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.22918v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruichen Chen, Keith G. Mills, Liyao Jiang, Chao Gao, Di Niu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion Transformers (DiT) have become the de-facto model for generating
high-quality visual content like videos and images. A huge bottleneck is the
attention mechanism where complexity scales quadratically with resolution and
video length. One logical way to lessen this burden is sparse attention, where
only a subset of tokens or patches are included in the calculation. However,
existing techniques fail to preserve visual quality at extremely high sparsity
levels and might even incur non-negligible compute overheads. To address this
concern, we propose Re-ttention, which implements very high sparse attention
for visual generation models by leveraging the temporal redundancy of Diffusion
Models to overcome the probabilistic normalization shift within the attention
mechanism. Specifically, Re-ttention reshapes attention scores based on the
prior softmax distribution history in order to preserve the visual quality of
the full quadratic attention at very high sparsity levels. Experimental results
on T2V/T2I models such as CogVideoX and the PixArt DiTs demonstrate that
Re-ttention requires as few as 3.1% of the tokens during inference,
outperforming contemporary methods like FastDiTAttn, Sparse VideoGen and
MInference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>author comment: This version was previously removed by arXiv
  administrators as the submitter did not have the rights to agree to the
  license at the time of submission. The authors have now obtained the
  necessary permissions, and the paper is resubmitted accordingly</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hyperparameters in Continual Learning: A Reality Check 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09066v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09066v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sungmin Cha, Kyunghyun Cho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual learning (CL) aims to train a model on a sequence of tasks (i.e., a
CL scenario) while balancing the trade-off between plasticity (learning new
tasks) and stability (retaining prior knowledge). The dominantly adopted
conventional evaluation protocol for CL algorithms selects the best
hyperparameters (e.g., learning rate, mini-batch size, regularization
strengths, etc.) within a given scenario and then evaluates the algorithms
using these hyperparameters in the same scenario. However, this protocol has
significant shortcomings: it overestimates the CL capacity of algorithms and
relies on unrealistic hyperparameter tuning, which is not feasible for
real-world applications. From the fundamental principles of evaluation in
machine learning, we argue that the evaluation of CL algorithms should focus on
assessing the generalizability of their CL capacity to unseen scenarios. Based
on this, we propose the Generalizable Two-phase Evaluation Protocol (GTEP)
consisting of hyperparameter tuning and evaluation phases. Both phases share
the same scenario configuration (e.g., number of tasks) but are generated from
different datasets. Hyperparameters of CL algorithms are tuned in the first
phase and applied in the second phase to evaluate the algorithms. We apply this
protocol to class-incremental learning, both with and without pretrained
models. Across more than 8,000 experiments, our results show that most
state-of-the-art algorithms fail to replicate their reported performance,
highlighting that their CL capacity has been significantly overestimated in the
conventional evaluation protocol. Our implementation can be found in
https://github.com/csm9493/GTEP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>TMLR 2025 camera ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cyst-X: A Federated AI System Outperforms Clinical Guidelines to Detect
  Pancreatic Cancer Precursors and Reduce Unnecessary Surgery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.22017v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.22017v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyi Pan, Gorkem Durak, Elif Keles, Deniz Seyithanoglu, Zheyuan Zhang, Alpay Medetalibeyoglu, Halil Ertugrul Aktas, Andrea Mia Bejar, Ziliang Hong, Yavuz Taktak, Gulbiz Dagoglu Kartal, Mehmet Sukru Erturk, Timurhan Cebeci, Maria Jaramillo Gonzalez, Yury Velichko, Lili Zhao, Emil Agarunov, Federica Proietto Salanitri, Concetto Spampinato, Pallavi Tiwari, Ziyue Xu, Sachin Jambawalikar, Ivo G. Schoots, Marco J. Bruno, Chenchang Huang, Candice W. Bolan, Tamas Gonda, Frank H. Miller, Rajesh N. Keswani, Michael B. Wallace, Ulas Bagci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pancreatic cancer is projected to be the second-deadliest cancer by 2030,
making early detection critical. Intraductal papillary mucinous neoplasms
(IPMNs), key cancer precursors, present a clinical dilemma, as current
guidelines struggle to stratify malignancy risk, leading to unnecessary
surgeries or missed diagnoses. Here, we developed Cyst-X, an AI framework for
IPMN risk prediction trained on a unique, multi-center dataset of 1,461 MRI
scans from 764 patients. Cyst-X achieves significantly higher accuracy (AUC =
0.82) than both the established Kyoto guidelines (AUC = 0.75) and expert
radiologists, particularly in correct identification of high-risk lesions.
Clinically, this translates to a 20% increase in cancer detection sensitivity
(87.8% vs. 64.1%) for high-risk lesions. We demonstrate that this performance
is maintained in a federated learning setting, allowing for collaborative model
training without compromising patient privacy. To accelerate research in early
pancreatic cancer detection, we publicly release the Cyst-X dataset and models,
providing the first large-scale, multi-center MRI resource for pancreatic cyst
analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PSScreen V2: Partially Supervised Multiple Retinal Disease Screening 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.22589v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.22589v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boyi Zheng, Yalin Zheng, Hrvoje Bogunović, Qing Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we propose PSScreen V2, a partially supervised self-training
framework for multiple retinal disease screening. Unlike previous methods that
rely on fully labelled or single-domain datasets, PSScreen V2 is designed to
learn from multiple partially labelled datasets with different distributions,
addressing both label absence and domain shift challenges. To this end,
PSScreen V2 adopts a three-branch architecture with one teacher and two student
networks. The teacher branch generates pseudo labels from weakly augmented
images to address missing labels, while the two student branches introduce
novel feature augmentation strategies: Low-Frequency Dropout (LF-Dropout),
which enhances domain robustness by randomly discarding domain-related
low-frequency components, and Low-Frequency Uncertainty (LF-Uncert), which
estimates uncertain domain variability via adversarially learned Gaussian
perturbations of low-frequency statistics. Extensive experiments on multiple
in-domain and out-of-domain fundus datasets demonstrate that PSScreen V2
achieves state-of-the-art performance and superior domain generalization
ability. Furthermore, compatibility tests with diverse backbones, including the
vision foundation model DINOv2, as well as evaluations on chest X-ray datasets,
highlight the universality and adaptability of the proposed framework. The
codes are available at https://github.com/boyiZheng99/PSScreen_V2.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explicitly Modeling Subcortical Vision with a Neuro-Inspired Front-End
  Improves CNN Robustness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03089v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03089v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucas Piper, Arlindo L. Oliveira, Tiago Marques
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional neural networks (CNNs) trained on object recognition achieve
high task performance but continue to exhibit vulnerability under a range of
visual perturbations and out-of-domain images, when compared with biological
vision. Prior work has demonstrated that coupling a standard CNN with a
front-end (VOneBlock) that mimics the primate primary visual cortex (V1) can
improve overall model robustness. Expanding on this, we introduce Early Vision
Networks (EVNets), a new class of hybrid CNNs that combine the VOneBlock with a
novel SubcorticalBlock, whose architecture draws from computational models in
neuroscience and is parameterized to maximize alignment with subcortical
responses reported across multiple experimental studies. Without being
optimized to do so, the assembly of the SubcorticalBlock with the VOneBlock
improved V1 alignment across most standard V1 benchmarks, and better modeled
extra-classical receptive field phenomena. In addition, EVNets exhibit stronger
emergent shape bias and outperform the base CNN architecture by 9.3% on an
aggregate benchmark of robustness evaluations, including adversarial
perturbations, common corruptions, and domain shifts. Finally, we show that
EVNets can be further improved when paired with a state-of-the-art data
augmentation technique, surpassing the performance of the isolated data
augmentation approach by 6.2% on our robustness benchmark. This result reveals
complementary benefits between changes in architecture to better mimic biology
and training-based machine learning approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VLCE: A Knowledge-Enhanced Framework for Image Description in Disaster
  Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.21609v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.21609v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md. Mahfuzur Rahman, Kishor Datta Gupta, Marufa Kamal, Fahad Rahman, Sunzida Siddique, Ahmed Rafi Hasan, Mohd Ariful Haque, Roy George
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Immediate damage assessment is essential after natural catastrophes; yet,
conventional hand evaluation techniques are sluggish and perilous. Although
satellite and unmanned aerial vehicle (UAV) photos offer extensive perspectives
of impacted regions, current computer vision methodologies generally yield just
classification labels or segmentation masks, so constraining their capacity to
deliver a thorough situational comprehension. We introduce the Vision Language
Caption Enhancer (VLCE), a multimodal system designed to produce comprehensive,
contextually-informed explanations of disaster imagery. VLCE employs a
dual-architecture approach: a CNN-LSTM model with a ResNet50 backbone
pretrained on EuroSat satellite imagery for the xBD dataset, and a Vision
Transformer (ViT) model pretrained on UAV pictures for the RescueNet dataset.
Both systems utilize external semantic knowledge from ConceptNet and WordNet to
expand vocabulary coverage and improve description accuracy. We assess VLCE in
comparison to leading vision-language models (LLaVA and QwenVL) utilizing
CLIPScore for semantic alignment and InfoMetIC for caption informativeness.
Experimental findings indicate that VLCE markedly surpasses baseline models,
attaining a maximum of 95.33% on InfoMetIC while preserving competitive
semantic alignment. Our dual-architecture system demonstrates significant
potential for improving disaster damage assessment by automating the production
of actionable, information-dense descriptions from satellite and drone photos.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 40 figures, 3 algorithms</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large
  Multimodal Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.05034v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.05034v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yolo Yunlong Tang, Jing Bi, Pinxin Liu, Zhenyu Pan, Zhangyun Tan, Qianxiang Shen, Jiani Liu, Hang Hua, Junjia Guo, Yunzhong Xiao, Chao Huang, Zhiyuan Wang, Susan Liang, Xinyi Liu, Yizhi Song, Junhua Huang, Jia-Xing Zhong, Bozheng Li, Daiqing Qi, Ziyun Zeng, Ali Vosoughi, Luchuan Song, Zeliang Zhang, Daiki Shimada, Han Liu, Jiebo Luo, Chenliang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video understanding represents the most challenging frontier in computer
vision, requiring models to reason about complex spatiotemporal relationships,
long-term dependencies, and multimodal evidence. The recent emergence of
Video-Large Multimodal Models (Video-LMMs), which integrate visual encoders
with powerful decoder-based language models, has demonstrated remarkable
capabilities in video understanding tasks. However, the critical phase that
transforms these models from basic perception systems into sophisticated
reasoning engines, post-training, remains fragmented across the literature.
This survey provides the first comprehensive examination of post-training
methodologies for Video-LMMs, encompassing three fundamental pillars:
supervised fine-tuning (SFT) with chain-of-thought, reinforcement learning (RL)
from verifiable objectives, and test-time scaling (TTS) through enhanced
inference computation. We present a structured taxonomy that clarifies the
roles, interconnections, and video-specific adaptations of these techniques,
addressing unique challenges such as temporal localization, spatiotemporal
grounding, long video efficiency, and multimodal evidence integration. Through
systematic analysis of representative methods, we synthesize key design
principles, insights, and evaluation protocols while identifying critical open
challenges in reward design, scalability, and cost-performance optimization. We
further curate essential benchmarks, datasets, and metrics to facilitate
rigorous assessment of post-training effectiveness. This survey aims to provide
researchers and practitioners with a unified framework for advancing Video-LMM
capabilities. Additional resources and updates are maintained at:
https://github.com/yunlong10/Awesome-Video-LMM-Post-Training
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Version v1.1</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">16</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tongyi DeepResearch Technical Report 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24701v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24701v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         Tongyi DeepResearch Team, Baixuan Li, Bo Zhang, Dingchu Zhang, Fei Huang, Guangyu Li, Guoxin Chen, Huifeng Yin, Jialong Wu, Jingren Zhou, Kuan Li, Liangcai Su, Litu Ou, Liwen Zhang, Pengjun Xie, Rui Ye, Wenbiao Yin, Xinmiao Yu, Xinyu Wang, Xixi Wu, Xuanzhong Chen, Yida Zhao, Zhen Zhang, Zhengwei Tao, Zhongwang Zhang, Zile Qiao, Chenxi Wang, Donglei Yu, Gang Fu, Haiyang Shen, Jiayin Yang, Jun Lin, Junkai Zhang, Kui Zeng, Li Yang, Hailong Yin, Maojia Song, Ming Yan, Peng Xia, Qian Xiao, Rui Min, Ruixue Ding, Runnan Fang, Shaowei Chen, Shen Huang, Shihang Wang, Shihao Cai, Weizhou Shen, Xiaobin Wang, Xin Guan, Xinyu Geng, Yingcheng Shi, Yuning Wu, Zhuo Chen, Zijian Li, Yong Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Tongyi DeepResearch, an agentic large language model, which is
specifically designed for long-horizon, deep information-seeking research
tasks. To incentivize autonomous deep research agency, Tongyi DeepResearch is
developed through an end-to-end training framework that combines agentic
mid-training and agentic post-training, enabling scalable reasoning and
information seeking across complex tasks. We design a highly scalable data
synthesis pipeline that is fully automatic, without relying on costly human
annotation, and empowers all training stages. By constructing customized
environments for each stage, our system enables stable and consistent
interactions throughout. Tongyi DeepResearch, featuring 30.5 billion total
parameters, with only 3.3 billion activated per token, achieves
state-of-the-art performance across a range of agentic deep research
benchmarks, including Humanity's Last Exam, BrowseComp, BrowseComp-ZH,
WebWalkerQA, xbench-DeepSearch, FRAMES and xbench-DeepSearch-2510. We
open-source the model, framework, and complete solutions to empower the
community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://tongyi-agent.github.io/blog</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Retrieval for RAG via Reinforced Contrastive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24652v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24652v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Zhou, Lei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As retrieval-augmented generation (RAG) becomes increasingly widespread, the
role of information retrieval (IR) is shifting from retrieving information for
human users to retrieving contextual knowledge for artificial intelligence (AI)
systems, where relevance becomes difficult to define or annotate beforehand. To
address this challenge, we propose R3, a Retrieval framework optimized for RAG
through trialand-feedback Reinforced contrastive learning. Unlike prior
approaches that rely on annotated or synthetic data for supervised fine-tuning,
R3 enables the retriever to dynamically explore and optimize relevance within
the RAG environment. During training, the retrieved results interact with the
environment to produce contrastive signals that automatically guide the
retriever's self-improvement. Extensive experiments across diverse tasks
demonstrate that R3 improves RAG performance by 5.2% over the original
retriever and surpasses state-of-the-art retrievers by 4.9%, while achieving
comparable results to LLM-augmented retrieval and RAG systems built on
post-trained or instruction-tuned LLMs. It is both efficient and practical,
requiring only 4 GPUs and completing training within a single day.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Iterative Critique-Refine Framework for Enhancing LLM Personalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24469v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24469v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Durga Prasad Maram, Dhruvin Gandhi, Zonghai Yao, Gayathri Akkinapalli, Franck Dernoncourt, Yu Wang, Ryan A. Rossi, Nesreen K. Ahmed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personalized text generation requires models not only to produce coherent
text but also to align with a target user's style, tone, and topical focus.
Existing retrieval-augmented approaches such as LaMP and PGraphRAG enrich
profiles with user and neighbor histories, but they stop at generation and
often yield outputs that drift in tone, topic, or style. We present PerFine, a
unified, training-free critique-refine framework that enhances personalization
through iterative, profile-grounded feedback. In each iteration, an LLM
generator produces a draft conditioned on the retrieved profile, and a critic
LLM - also conditioned on the same profile - provides structured feedback on
tone, vocabulary, sentence structure, and topicality. The generator then
revises, while a novel knockout strategy retains the stronger draft across
iterations. We further study additional inference-time strategies such as
Best-of-N and Topic Extraction to balance quality and efficiency. Across Yelp,
Goodreads, and Amazon datasets, PerFine consistently improves personalization
over PGraphRAG, with GEval gains of +7-13%, steady improvements over 3-5
refinement iterations, and scalability with increasing critic size. These
results highlight that post-hoc, profile-aware feedback offers a powerful
paradigm for personalized LLM generation that is both training-free and
model-agnostic.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MiniOneRec: An Open-Source Framework for Scaling Generative
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24431v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24431v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyu Kong, Leheng Sheng, Junfei Tan, Yuxin Chen, Jiancan Wu, An Zhang, Xiang Wang, Xiangnan He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent success of large language models (LLMs) has renewed interest in
whether recommender systems can achieve similar scaling benefits. Conventional
recommenders, dominated by massive embedding tables, tend to plateau as
embedding dimensions grow. In contrast, the emerging generative paradigm
replaces embeddings with compact Semantic ID (SID) sequences produced by
autoregressive Transformers. Yet most industrial deployments remain
proprietary, leaving two fundamental questions open: (1) Do the expected
scaling laws hold on public benchmarks? (2) What is the minimal post-training
recipe that enables competitive performance?
  We present MiniOneRec, to the best of our knowledge, the first fully
open-source generative recommendation framework, which provides an end-to-end
workflow spanning SID construction, supervised fine-tuning, and
recommendation-oriented reinforcement learning. We generate SIDs via a Residual
Quantized VAE and post-train Qwen backbones ranging from 0.5B to 7B parameters
on the Amazon Review dataset. Our experiments reveal a consistent downward
trend in both training and evaluation losses with increasing model size,
validating the parameter efficiency of the generative approach. To further
enhance performance, we propose a lightweight yet effective post-training
pipeline that (1) enforces full-process SID alignment and (2) applies
reinforcement learning with constrained decoding and hybrid rewards. Together,
these techniques yield significant improvements in both ranking accuracy and
candidate diversity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Time and Place to Preference: LLM-Driven Geo-Temporal Context in
  Recommendations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24430v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24430v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yejin Kim, Shaghayegh Agah, Mayur Nankani, Neeraj Sharma, Feifei Peng, Maria Peifer, Sardar Hamidian, H Howie Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most recommender systems treat timestamps as numeric or cyclical values,
overlooking real-world context such as holidays, events, and seasonal patterns.
We propose a scalable framework that uses large language models (LLMs) to
generate geo-temporal embeddings from only a timestamp and coarse location,
capturing holidays, seasonal trends, and local/global events. We then introduce
a geo-temporal embedding informativeness test as a lightweight diagnostic,
demonstrating on MovieLens, LastFM, and a production dataset that these
embeddings provide predictive signal consistent with the outcomes of full model
integrations. Geo-temporal embeddings are incorporated into sequential models
through (1) direct feature fusion with metadata embeddings or (2) an auxiliary
loss that enforces semantic and geo-temporal alignment. Our findings highlight
the need for adaptive or hybrid recommendation strategies, and we release a
context-enriched MovieLens dataset to support future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Metadata-Driven Retrieval-Augmented Generation for Financial Question
  Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24402v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24402v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michail Dadopoulos, Anestis Ladas, Stratos Moschidis, Ioannis Negkakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) struggles on long, structured financial
filings where relevant evidence is sparse and cross-referenced. This paper
presents a systematic investigation of advanced metadata-driven
Retrieval-Augmented Generation (RAG) techniques, proposing and evaluating a
novel, multi-stage RAG architecture that leverages LLM-generated metadata. We
introduce a sophisticated indexing pipeline to create contextually rich
document chunks and benchmark a spectrum of enhancements, including
pre-retrieval filtering, post-retrieval reranking, and enriched embeddings,
benchmarked on the FinanceBench dataset. Our results reveal that while a
powerful reranker is essential for precision, the most significant performance
gains come from embedding chunk metadata directly with text ("contextual
chunks"). Our proposed optimal architecture combines LLM-driven pre-retrieval
optimizations with these contextual embeddings to achieve superior performance.
Additionally, we present a custom metadata reranker that offers a compelling,
cost-effective alternative to commercial solutions, highlighting a practical
trade-off between peak performance and operational efficiency. This study
provides a blueprint for building robust, metadata-aware RAG systems for
financial document analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint version submitted to the International Journal of Accounting
  Information Systems; currently under major revision. 20 pages, 1 figure, 1
  table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DUET: Dual Model Co-Training for Entire Space CTR Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24369v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24369v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yutian Xiao, Meng Yuan, Fuzhen Zhuang, Wei Chen, Shukuan Wang, Shanqi Liu, Chao Feng, Wenhui Yu, Xiang Li, Lantao Hu, Han Li, Zhao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The pre-ranking stage plays a pivotal role in large-scale recommender systems
but faces an intrinsic trade-off between model expressiveness and computational
efficiency. Owing to the massive candidate pool and strict latency constraints,
industry systems often rely on lightweight two-tower architectures, which are
computationally efficient yet limited in estimation capability. As a result,
they struggle to capture the complex synergistic and suppressive relationships
among candidate items, which are essential for producing contextually coherent
and diverse recommendation lists. Moreover, this simplicity further amplifies
the Sample Selection Bias (SSB) problem, as coarse-grained models trained on
biased exposure data must generalize to a much larger candidate space with
distinct distributions.
  To address these issues, we propose \textbf{DUET} (\textbf{DU}al Model
Co-Training for \textbf{E}ntire Space C\textbf{T}R Prediction), a set-wise
pre-ranking framework that achieves expressive modeling under tight
computational budgets. Instead of scoring items independently, DUET performs
set-level prediction over the entire candidate subset in a single forward pass,
enabling information-aware interactions among candidates while amortizing the
computational cost across the set. Moreover, a dual model co-training mechanism
extends supervision to unexposed items via mutual pseudo-label refinement,
effectively mitigating SSB. Validated through extensive offline experiments and
online A/B testing, DUET consistently outperforms state-of-the-art baselines
and achieves improvements across multiple core business metrics. At present,
DUET has been fully deployed in Kuaishou and Kuaishou Lite Apps, serving the
main traffic for hundreds of millions of users.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Resource-Efficient LLM Application for Structured Transformation of
  Unstructured Financial Contracts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23990v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23990v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maruf Ahmed Mridul, Oshani Seneviratne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The transformation of unstructured legal contracts into standardized,
machine-readable formats is essential for automating financial workflows. The
Common Domain Model (CDM) provides a standardized framework for this purpose,
but converting complex legal documents like Credit Support Annexes (CSAs) into
CDM representations remains a significant challenge. In this paper, we present
an extension of the CDMizer framework, a template-driven solution that ensures
syntactic correctness and adherence to the CDM schema during contract-to-CDM
conversion. We apply this extended framework to a real-world task, comparing
its performance with a benchmark developed by the International Swaps and
Derivatives Association (ISDA) for CSA clause extraction. Our results show that
CDMizer, when integrated with a significantly smaller, open-source Large
Language Model (LLM), achieves competitive performance in terms of accuracy and
efficiency against larger, proprietary models. This work underscores the
potential of resource-efficient solutions to automate legal contract
transformation, offering a cost-effective and scalable approach that can meet
the needs of financial institutions with constrained resources or strict data
privacy requirements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 1 figure, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Secure Retrieval-Augmented Generation against Poisoning Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25025v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25025v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zirui Cheng, Jikai Sun, Anjun Gao, Yueyang Quan, Zhuqing Liu, Xiaohua Hu, Minghong Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have transformed natural language processing
(NLP), enabling applications from content generation to decision support.
Retrieval-Augmented Generation (RAG) improves LLMs by incorporating external
knowledge but also introduces security risks, particularly from data poisoning,
where the attacker injects poisoned texts into the knowledge database to
manipulate system outputs. While various defenses have been proposed, they
often struggle against advanced attacks. To address this, we introduce RAGuard,
a detection framework designed to identify poisoned texts. RAGuard first
expands the retrieval scope to increase the proportion of clean texts, reducing
the likelihood of retrieving poisoned content. It then applies chunk-wise
perplexity filtering to detect abnormal variations and text similarity
filtering to flag highly similar texts. This non-parametric approach enhances
RAG security, and experiments on large-scale datasets demonstrate its
effectiveness in detecting and mitigating poisoning attacks, including strong
adaptive attacks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in IEEE BigData 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Seeing Through the MiRAGE: Evaluating Multimodal Retrieval Augmented
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24870v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24870v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Martin, William Walden, Reno Kriz, Dengjia Zhang, Kate Sanders, Eugene Yang, Chihsheng Jin, Benjamin Van Durme
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce MiRAGE, an evaluation framework for retrieval-augmented
generation (RAG) from multimodal sources. As audiovisual media becomes a
prevalent source of information online, it is essential for RAG systems to
integrate information from these sources into generation. However, existing
evaluations for RAG are text-centric, limiting their applicability to
multimodal, reasoning intensive settings because they don't verify information
against sources. MiRAGE is a claim-centric approach to multimodal RAG
evaluation, consisting of InfoF1, evaluating factuality and information
coverage, and CiteF1, measuring citation support and completeness. We show that
MiRAGE, when applied by humans, strongly aligns with extrinsic quality
judgments. We additionally introduce automatic variants of MiRAGE and three
prominent TextRAG metrics -- ACLE, ARGUE, and RAGAS -- demonstrating the
limitations of text-centric work and laying the groundwork for automatic
evaluation. We release open-source implementations and outline how to assess
multimodal RAG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/alexmartin1722/mirage</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Comparing Retrieval Strategies to Capture Interdisciplinary Scientific
  Research: A Bibliometric Evaluation of the Integration of Neuroscience and
  Computer Science 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03187v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03187v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Malena Mendez Isla, Agustin Mauro, Diego Kozlowski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interdisciplinary scientific research is increasingly important in knowledge
production, funding policies, and academic discussions on scholarly
communication. While many studies focus on interdisciplinary corpora defined a
priori -- usually through keyword-based searches within assumed
interdisciplinary domains -- few explore interdisciplinarity as an emergent
intersection between two distinct fields. Thus, methodological proposals for
building databases at the intersection of two fields of knowledge are scarce.
The goal of this article is to develop and compare different strategies for
defining an interdisciplinary corpus between two bodies of knowledge. As a case
study, we focus on the intersection between neuroscience and computer science.
To this end, we develop and compare four retrieval strategies, two of them
based on keywords and two based on citation and reference patterns. Our results
show that the reference-based strategy provides better retrieval, pseudorecall,
and F1. While we focus on comparing strategies for the study of the
intersection between the fields of neuroscience and computer science, this
methodological reflection is applicable to a wide range of interdisciplinary
domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CustomIR: Unsupervised Fine-Tuning of Dense Embeddings for Known
  Document Corpora 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.21729v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.21729v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathan Paull
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dense embedding models have become critical for modern information retrieval,
particularly in RAG pipelines, but their performance often degrades when
applied to specialized corpora outside their pre-training distribution. To
address thi we introduce CustomIR, a framework for unsupervised adaptation of
pre-trained language embedding models to domain-specific corpora using
synthetically generated query-document pairs. CustomIR leverages large language
models (LLMs) to create diverse queries grounded in a known target corpus,
paired with LLM-verified hard negatives, eliminating the need for costly human
annotation. Experiments on enterprise email and messaging datasets show that
CustomIR consistently improves retrieval effectiveness with small models
gaining up to 2.3 points in Recall@10. This performance increase allows these
small models to rival the performance of much larger alternatives, allowing for
cheaper RAG deployments. These results highlight that targeted synthetic
fine-tuning offers a scalable and cost-efficient strategy for increasing
domain-specific performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cross-Scenario Unified Modeling of User Interests at Billion Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.14788v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.14788v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manjie Xu, Cheng Chen, Xin Jia, Jingyi Zhou, Yongji Wu, Zejian Wang, Chi Zhang, Kai Zuo, Yibo Chen, Xu Tang, Yao Hu, Yixin Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  User interests on content platforms are inherently diverse, manifesting
through complex behavioral patterns across heterogeneous scenarios such as
search, feed browsing, and content discovery. Traditional recommendation
systems typically prioritize business metric optimization within isolated
specific scenarios, neglecting cross-scenario behavioral signals and struggling
to integrate advanced techniques like LLMs at billion-scale deployments, which
finally limits their ability to capture holistic user interests across platform
touchpoints. We propose RED-Rec, an LLM-enhanced hierarchical Recommender
Engine for Diversified scenarios, tailored for industry-level content
recommendation systems. RED-Rec unifies user interest representations across
multiple behavioral contexts by aggregating and synthesizing actions from
varied scenarios, resulting in comprehensive item and user modeling. At its
core, a two-tower LLM-powered framework enables nuanced, multifaceted
representations with deployment efficiency, and a scenario-aware dense mixing
and querying policy effectively fuses diverse behavioral signals to capture
cross-scenario user intent patterns and express fine-grained, context-specific
intents during serving. We validate RED-Rec through online A/B testing on
hundreds of millions of users in RedNote through online A/B testing, showing
substantial performance gains in both content recommendation and advertisement
targeting tasks. We further introduce a million-scale sequential recommendation
dataset, RED-MMU, for comprehensive offline training and evaluation. Our work
advances unified user modeling, unlocking deeper personalization and fostering
more meaningful user engagement in large-scale UGC platforms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/ariesssxu/RedSeqRec</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OneRec-V2 Technical Report 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.20900v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.20900v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guorui Zhou, Hengrui Hu, Hongtao Cheng, Huanjie Wang, Jiaxin Deng, Jinghao Zhang, Kuo Cai, Lejian Ren, Lu Ren, Liao Yu, Pengfei Zheng, Qiang Luo, Qianqian Wang, Qigen Hu, Rui Huang, Ruiming Tang, Shiyao Wang, Shujie Yang, Tao Wu, Wuchao Li, Xinchen Luo, Xingmei Wang, Yi Su, Yunfan Wu, Zexuan Cheng, Zhanyu Liu, Zixing Zhang, Bin Zhang, Boxuan Wang, Chaoyi Ma, Chengru Song, Chenhui Wang, Chenglong Chu, Di Wang, Dongxue Meng, Dunju Zang, Fan Yang, Fangyu Zhang, Feng Jiang, Fuxing Zhang, Gang Wang, Guowang Zhang, Han Li, Honghui Bao, Hongyang Cao, Jiaming Huang, Jiapeng Chen, Jiaqiang Liu, Jinghui Jia, Kun Gai, Lantao Hu, Liang Zeng, Qiang Wang, Qidong Zhou, Rongzhou Zhang, Shengzhe Wang, Shihui He, Shuang Yang, Siyang Mao, Sui Huang, Tiantian He, Tingting Gao, Wei Yuan, Xiao Liang, Xiaoxiao Xu, Xugang Liu, Yan Wang, Yang Zhou, Yi Wang, Yiwu Liu, Yue Song, Yufei Zhang, Yunfeng Zhao, Zhixin Ling, Ziming Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent breakthroughs in generative AI have transformed recommender systems
through end-to-end generation. OneRec reformulates recommendation as an
autoregressive generation task, achieving high Model FLOPs Utilization. While
OneRec-V1 has shown significant empirical success in real-world deployment, two
critical challenges hinder its scalability and performance: (1) inefficient
computational allocation where 97.66% of resources are consumed by sequence
encoding rather than generation, and (2) limitations in reinforcement learning
relying solely on reward models.
  To address these challenges, we propose OneRec-V2, featuring: (1) Lazy
Decoder-Only Architecture: Eliminates encoder bottlenecks, reducing total
computation by 94% and training resources by 90%, enabling successful scaling
to 8B parameters. (2) Preference Alignment with Real-World User Interactions:
Incorporates Duration-Aware Reward Shaping and Adaptive Ratio Clipping to
better align with user preferences using real-world feedback.
  Extensive A/B tests on Kuaishou demonstrate OneRec-V2's effectiveness,
improving App Stay Time by 0.467%/0.741% while balancing multi-objective
recommendations. This work advances generative recommendation scalability and
alignment with real-world feedback, representing a step forward in the
development of end-to-end recommender systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MemoryBench: A Benchmark for Memory and Continual Learning in LLM
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.17281v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.17281v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyao Ai, Yichen Tang, Changyue Wang, Jianming Long, Weihang Su, Yiqun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling up data, parameters, and test-time computation has been the
mainstream methods to improve LLM systems (LLMsys), but their upper bounds are
almost reached due to the gradual depletion of high-quality data and marginal
gains obtained from larger computational resource consumption. Inspired by the
abilities of human and traditional AI systems in learning from practice,
constructing memory and continual learning frameworks for LLMsys has become an
important and popular research direction in recent literature. Yet, existing
benchmarks for LLM memory often focus on evaluating the system on homogeneous
reading comprehension tasks with long-form inputs rather than testing their
abilities to learn from accumulated user feedback in service time. Therefore,
we propose a user feedback simulation framework and a comprehensive benchmark
covering multiple domains, languages, and types of tasks to evaluate the
continual learning abilities of LLMsys. Experiments show that the effectiveness
and efficiency of state-of-the-art baselines are far from satisfying, and we
hope this benchmark could pave the way for future studies on LLM memory and
optimization algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Your Dense Retriever is Secretly an Expeditious Reasoner 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.21727v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.21727v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichi Zhang, Jun Bai, Zhixin Cai, Shuhan Qin, Zhuofan Chen, Jinghua Guan, Wenge Rong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dense retrievers enhance retrieval by encoding queries and documents into
continuous vectors, but they often struggle with reasoning-intensive queries.
Although Large Language Models (LLMs) can reformulate queries to capture
complex reasoning, applying them universally incurs significant computational
cost. In this work, we propose Adaptive Query Reasoning (AdaQR), a hybrid query
rewriting framework. Within this framework, a Reasoner Router dynamically
directs each query to either fast dense reasoning or deep LLM reasoning. The
dense reasoning is achieved by the Dense Reasoner, which performs LLM-style
reasoning directly in the embedding space, enabling a controllable trade-off
between efficiency and accuracy. Experiments on large-scale retrieval
benchmarks BRIGHT show that AdaQR reduces reasoning cost by 28% while
preserving-or even improving-retrieval performance by 7%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">150</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative View Stitching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24718v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24718v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chonghyuk Song, Michal Stary, Boyuan Chen, George Kopanas, Vincent Sitzmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autoregressive video diffusion models are capable of long rollouts that are
stable and consistent with history, but they are unable to guide the current
generation with conditioning from the future. In camera-guided video generation
with a predefined camera trajectory, this limitation leads to collisions with
the generated scene, after which autoregression quickly collapses. To address
this, we propose Generative View Stitching (GVS), which samples the entire
sequence in parallel such that the generated scene is faithful to every part of
the predefined camera trajectory. Our main contribution is a sampling algorithm
that extends prior work on diffusion stitching for robot planning to video
generation. While such stitching methods usually require a specially trained
model, GVS is compatible with any off-the-shelf video model trained with
Diffusion Forcing, a prevalent sequence diffusion framework that we show
already provides the affordances necessary for stitching. We then introduce
Omni Guidance, a technique that enhances the temporal consistency in stitching
by conditioning on both the past and future, and that enables our proposed
loop-closing mechanism for delivering long-range coherence. Overall, GVS
achieves camera-guided video generation that is stable, collision-free,
frame-to-frame consistent, and closes loops for a variety of predefined camera
paths, including Oscar Reutersv\"ard's Impossible Staircase. Results are best
viewed as videos at https://andrewsonga.github.io/gvs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://andrewsonga.github.io/gvs</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Single-Loop First-Order Algorithm for Linearly Constrained Bilevel
  Optimization <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24710v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24710v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Shen, Jiawei Zhang, Minhui Huang, Cong Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study bilevel optimization problems where the lower-level problems are
strongly convex and have coupled linear constraints. To overcome the potential
non-smoothness of the hyper-objective and the computational challenges
associated with the Hessian matrix, we utilize penalty and augmented Lagrangian
methods to reformulate the original problem as a single-level one. Especially,
we establish a strong theoretical connection between the reformulated function
and the original hyper-objective by characterizing the closeness of their
values and derivatives. Based on this reformulation, we propose a single-loop,
first-order algorithm for linearly constrained bilevel optimization (SFLCB). We
provide rigorous analyses of its non-asymptotic convergence rates, showing an
improvement over prior double-loop algorithms -- form
$O(\epsilon^{-3}\log(\epsilon^{-1}))$ to $O(\epsilon^{-3})$. The experiments
corroborate our theoretical findings and demonstrate the practical efficiency
of the proposed SFLCB algorithm. Simulation code is provided at
https://github.com/ShenGroup/SFLCB.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Does Object Binding Naturally Emerge in Large <span class="highlight-title">Pretrain</span>ed Vision
  <span class="highlight-title">Transformer</span>s? <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24709v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24709v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihao Li, Saeed Salehi, Lyle Ungar, Konrad P. Kording
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object binding, the brain's ability to bind the many features that
collectively represent an object into a coherent whole, is central to human
cognition. It groups low-level perceptual features into high-level object
representations, stores those objects efficiently and compositionally in
memory, and supports human reasoning about individual object instances. While
prior work often imposes object-centric attention (e.g., Slot Attention)
explicitly to probe these benefits, it remains unclear whether this ability
naturally emerges in pre-trained Vision Transformers (ViTs). Intuitively, they
could: recognizing which patches belong to the same object should be useful for
downstream prediction and thus guide attention. Motivated by the quadratic
nature of self-attention, we hypothesize that ViTs represent whether two
patches belong to the same object, a property we term IsSameObject. We decode
IsSameObject from patch embeddings across ViT layers using a similarity probe,
which reaches over 90% accuracy. Crucially, this object-binding capability
emerges reliably in self-supervised ViTs (DINO, MAE, CLIP), but markedly weaker
in ImageNet-supervised models, suggesting that binding is not a trivial
architectural artifact, but an ability acquired through specific pretraining
objectives. We further discover that IsSameObject is encoded in a
low-dimensional subspace on top of object features, and that this signal
actively guides attention. Ablating IsSameObject from model activations
degrades downstream performance and works against the learning objective,
implying that emergent object binding naturally serves the pretraining
objective. Our findings challenge the view that ViTs lack object binding and
highlight how symbolic knowledge of "which parts belong together" emerges
naturally in a connectionist system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a Spotlight at NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tongyi DeepResearch Technical Report 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24701v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24701v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         Tongyi DeepResearch Team, Baixuan Li, Bo Zhang, Dingchu Zhang, Fei Huang, Guangyu Li, Guoxin Chen, Huifeng Yin, Jialong Wu, Jingren Zhou, Kuan Li, Liangcai Su, Litu Ou, Liwen Zhang, Pengjun Xie, Rui Ye, Wenbiao Yin, Xinmiao Yu, Xinyu Wang, Xixi Wu, Xuanzhong Chen, Yida Zhao, Zhen Zhang, Zhengwei Tao, Zhongwang Zhang, Zile Qiao, Chenxi Wang, Donglei Yu, Gang Fu, Haiyang Shen, Jiayin Yang, Jun Lin, Junkai Zhang, Kui Zeng, Li Yang, Hailong Yin, Maojia Song, Ming Yan, Peng Xia, Qian Xiao, Rui Min, Ruixue Ding, Runnan Fang, Shaowei Chen, Shen Huang, Shihang Wang, Shihao Cai, Weizhou Shen, Xiaobin Wang, Xin Guan, Xinyu Geng, Yingcheng Shi, Yuning Wu, Zhuo Chen, Zijian Li, Yong Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Tongyi DeepResearch, an agentic large language model, which is
specifically designed for long-horizon, deep information-seeking research
tasks. To incentivize autonomous deep research agency, Tongyi DeepResearch is
developed through an end-to-end training framework that combines agentic
mid-training and agentic post-training, enabling scalable reasoning and
information seeking across complex tasks. We design a highly scalable data
synthesis pipeline that is fully automatic, without relying on costly human
annotation, and empowers all training stages. By constructing customized
environments for each stage, our system enables stable and consistent
interactions throughout. Tongyi DeepResearch, featuring 30.5 billion total
parameters, with only 3.3 billion activated per token, achieves
state-of-the-art performance across a range of agentic deep research
benchmarks, including Humanity's Last Exam, BrowseComp, BrowseComp-ZH,
WebWalkerQA, xbench-DeepSearch, FRAMES and xbench-DeepSearch-2510. We
open-source the model, framework, and complete solutions to empower the
community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://tongyi-agent.github.io/blog</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Greedy Sampling Is Provably Efficient for RLHF <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24700v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24700v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Di Wu, Chengshuai Shi, Jing Yang, Cong Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning from Human Feedback (RLHF) has emerged as a key
technique for post-training large language models. Despite its empirical
success, the theoretical understanding of RLHF is still limited, as learning
the KL-regularized target with only preference feedback poses additional
challenges compared with canonical RL. Existing works mostly study the
reward-based Bradley-Terry (BT) preference model, and extend classical designs
utilizing optimism or pessimism. This work, instead, considers the general
preference model (whose practical relevance has been observed recently) and
obtains performance guarantees with major, order-wise improvements over
existing ones. Surprisingly, these results are derived from algorithms that
directly use the empirical estimates (i.e., greedy sampling), as opposed to
constructing optimistic or pessimistic estimates in previous works. This
insight has a deep root in the unique structural property of the optimal policy
class under the KL-regularized target, and we further specialize it to the BT
model, highlighting the surprising sufficiency of greedy sampling in RLHF.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AgentFold: Long-Horizon Web Agents with Proactive Context Management 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24699v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24699v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Ye, Zhongwang Zhang, Kuan Li, Huifeng Yin, Zhengwei Tao, Yida Zhao, Liangcai Su, Liwen Zhang, Zile Qiao, Xinyu Wang, Pengjun Xie, Fei Huang, Siheng Chen, Jingren Zhou, Yong Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLM-based web agents show immense promise for information seeking, yet their
effectiveness on long-horizon tasks is hindered by a fundamental trade-off in
context management. Prevailing ReAct-based agents suffer from context
saturation as they accumulate noisy, raw histories, while methods that fixedly
summarize the full history at each step risk the irreversible loss of critical
details. Addressing these, we introduce AgentFold, a novel agent paradigm
centered on proactive context management, inspired by the human cognitive
process of retrospective consolidation. AgentFold treats its context as a
dynamic cognitive workspace to be actively sculpted, rather than a passive log
to be filled. At each step, it learns to execute a `folding' operation, which
manages its historical trajectory at multiple scales: it can perform granular
condensations to preserve vital, fine-grained details, or deep consolidations
to abstract away entire multi-step sub-tasks. The results on prominent
benchmarks are striking: with simple supervised fine-tuning (without continual
pre-training or RL), our AgentFold-30B-A3B agent achieves 36.2% on BrowseComp
and 47.3% on BrowseComp-ZH. Notably, this performance not only surpasses or
matches open-source models of a dramatically larger scale, such as the
DeepSeek-V3.1-671B-A37B, but also surpasses leading proprietary agents like
OpenAI's o4-mini.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Drive Safely with Hybrid Options 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24674v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24674v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bram De Cooman, Johan Suykens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Out of the many deep reinforcement learning approaches for autonomous
driving, only few make use of the options (or skills) framework. That is
surprising, as this framework is naturally suited for hierarchical control
applications in general, and autonomous driving tasks in specific. Therefore,
in this work the options framework is applied and tailored to autonomous
driving tasks on highways. More specifically, we define dedicated options for
longitudinal and lateral manoeuvres with embedded safety and comfort
constraints. This way, prior domain knowledge can be incorporated into the
learning process and the learned driving behaviour can be constrained more
easily. We propose several setups for hierarchical control with options and
derive practical algorithms following state-of-the-art reinforcement learning
techniques. By separately selecting actions for longitudinal and lateral
control, the introduced policies over combined and hybrid options obtain the
same expressiveness and flexibility that human drivers have, while being easier
to interpret than classical policies over continuous actions. Of all the
investigated approaches, these flexible policies over hybrid options perform
the best under varying traffic conditions, outperforming the baseline policies
over actions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Eigenfunction Extraction for Ordered Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24672v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24672v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Burak Varıcı, Che-Ping Tsai, Ritabrata Ray, Nicholas M. Boffi, Pradeep Ravikumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in representation learning reveal that widely used
objectives, such as contrastive and non-contrastive, implicitly perform
spectral decomposition of a contextual kernel, induced by the relationship
between inputs and their contexts. Yet, these methods recover only the linear
span of top eigenfunctions of the kernel, whereas exact spectral decomposition
is essential for understanding feature ordering and importance. In this work,
we propose a general framework to extract ordered and identifiable
eigenfunctions, based on modular building blocks designed to satisfy key
desiderata, including compatibility with the contextual kernel and scalability
to modern settings. We then show how two main methodological paradigms,
low-rank approximation and Rayleigh quotient optimization, align with this
framework for eigenfunction extraction. Finally, we validate our approach on
synthetic kernels and demonstrate on real-world image datasets that the
recovered eigenvalues act as effective importance scores for feature selection,
enabling principled efficiency-accuracy tradeoffs via adaptive-dimensional
representations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pearl: A Foundation Model for Placing Every Atom in the Right Location 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24670v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24670v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         Genesis Research Team, Alejandro Dobles, Nina Jovic, Kenneth Leidal, Pranav Murugan, David C. Williams, Drausin Wulsin, Nate Gruver, Christina X. Ji, Korrawat Pruegsanusak, Gianluca Scarpellini, Ansh Sharma, Wojciech Swiderski, Andrea Bootsma, Richard Strong Bowen, Charlotte Chen, Jamin Chen, Marc André Dämgen, Roy Tal Dew, Benjamin DiFrancesco, J. D. Fishman, Alla Ivanova, Zach Kagin, David Li-Bland, Zuli Liu, Igor Morozov, Jeffrey Ouyang-Zhang, Frank C. Pickard IV, Kushal S. Shah, Ben Shor, Gabriel Monteiro da Silva, Maxx Tessmer, Carl Tilbury, Cyr Vetcher, Daniel Zeng, Maruan Al-Shedivat, Aleksandra Faust, Evan N. Feinberg, Michael V. LeVine, Matteus Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately predicting the three-dimensional structures of protein-ligand
complexes remains a fundamental challenge in computational drug discovery that
limits the pace and success of therapeutic design. Deep learning methods have
recently shown strong potential as structural prediction tools, achieving
promising accuracy across diverse biomolecular systems. However, their
performance and utility are constrained by scarce experimental data,
inefficient architectures, physically invalid poses, and the limited ability to
exploit auxiliary information available at inference. To address these issues,
we introduce Pearl (Placing Every Atom in the Right Location), a foundation
model for protein-ligand cofolding at scale. Pearl addresses these challenges
with three key innovations: (1) training recipes that include large-scale
synthetic data to overcome data scarcity; (2) architectures that incorporate an
SO(3)-equivariant diffusion module to inherently respect 3D rotational
symmetries, improving generalization and sample efficiency, and (3)
controllable inference, including a generalized multi-chain templating system
supporting both protein and non-polymeric components as well as dual
unconditional/conditional modes. Pearl establishes a new state-of-the-art
performance in protein-ligand cofolding. On the key metric of generating
accurate (RMSD < 2 \r{A}) and physically valid poses, Pearl surpasses AlphaFold
3 and other open source baselines on the public Runs N' Poses and PoseBusters
benchmarks, delivering 14.5% and 14.2% improvements, respectively, over the
next best model. In the pocket-conditional cofolding regime, Pearl delivers
$3.6\times$ improvement on a proprietary set of challenging, real-world drug
targets at the more rigorous RMSD < 1 \r{A} threshold. Finally, we demonstrate
that model performance correlates directly with synthetic dataset size used in
training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Cost of Robustness: Tighter Bounds on Parameter Complexity for
  Robust Memorization in ReLU Nets <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24643v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24643v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujun Kim, Chaewon Moon, Chulhee Yun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the parameter complexity of robust memorization for $\mathrm{ReLU}$
networks: the number of parameters required to interpolate any given dataset
with $\epsilon$-separation between differently labeled points, while ensuring
predictions remain consistent within a $\mu$-ball around each training sample.
We establish upper and lower bounds on the parameter count as a function of the
robustness ratio $\rho = \mu / \epsilon$. Unlike prior work, we provide a
fine-grained analysis across the entire range $\rho \in (0,1)$ and obtain
tighter upper and lower bounds that improve upon existing results. Our findings
reveal that the parameter complexity of robust memorization matches that of
non-robust memorization when $\rho$ is small, but grows with increasing $\rho$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2025, 72 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Causal Ordering for Structure Learning From Time Series 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24639v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24639v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro P. Sanchez, Damian Machlanski, Steven McDonagh, Sotirios A. Tsaftaris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting causal structure from time series data is crucial for
understanding complex phenomena in physiology, brain connectivity, climate
dynamics, and socio-economic behaviour. Causal discovery in time series is
hindered by the combinatorial complexity of identifying true causal
relationships, especially as the number of variables and time points grow. A
common approach to simplify the task is the so-called ordering-based methods.
Traditional ordering methods inherently limit the representational capacity of
the resulting model. In this work, we fix this issue by leveraging multiple
valid causal orderings, instead of a single one as standard practice. We
propose DOTS (Diffusion Ordered Temporal Structure), using diffusion-based
causal discovery for temporal data. By integrating multiple orderings, DOTS
effectively recovers the transitive closure of the underlying directed acyclic
graph, mitigating spurious artifacts inherent in single-ordering approaches. We
formalise the problem under standard assumptions such as stationarity and the
additive noise model, and leverage score matching with diffusion processes to
enable efficient Hessian estimation. Extensive experiments validate the
approach. Empirical evaluations on synthetic and real-world datasets
demonstrate that DOTS outperforms state-of-the-art baselines, offering a
scalable and robust approach to temporal causal discovery. On synthetic
benchmarks ($d{=}\!3-\!6$ variables, $T{=}200\!-\!5{,}000$ samples), DOTS
improves mean window-graph $F1$ from $0.63$ (best baseline) to $0.81$. On the
CausalTime real-world benchmark ($d{=}20\!-\!36$), while baselines remain the
best on individual datasets, DOTS attains the highest average summary-graph
$F1$ while halving runtime relative to graph-optimisation methods. These
results establish DOTS as a scalable and accurate solution for temporal causal
discovery.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Symbolic Snapshot Ensembles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24633v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24633v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyue Liu, Andrew Cropper
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inductive logic programming (ILP) is a form of logical machine learning. Most
ILP algorithms learn a single hypothesis from a single training run. Ensemble
methods train an ILP algorithm multiple times to learn multiple hypotheses. In
this paper, we train an ILP algorithm only once and save intermediate
hypotheses. We then combine the hypotheses using a minimum description length
weighting scheme. Our experiments on multiple benchmarks, including game
playing and visual reasoning, show that our approach improves predictive
accuracy by 4% with less than 1% computational overhead.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Coreset for Robust Geometric Median: Eliminating Size Dependency on
  Outliers <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24621v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24621v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyi Fang, Lingxiao Huang, Runkai Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the robust geometric median problem in Euclidean space
$\mathbb{R}^d$, with a focus on coreset construction.A coreset is a compact
summary of a dataset $P$ of size $n$ that approximates the robust cost for all
centers $c$ within a multiplicative error $\varepsilon$. Given an outlier count
$m$, we construct a coreset of size $\tilde{O}(\varepsilon^{-2} \cdot
\min\{\varepsilon^{-2}, d\})$ when $n \geq 4m$, eliminating the $O(m)$
dependency present in prior work [Huang et al., 2022 & 2023]. For the special
case of $d = 1$, we achieve an optimal coreset size of
$\tilde{\Theta}(\varepsilon^{-1/2} + \frac{m}{n} \varepsilon^{-1})$, revealing
a clear separation from the vanilla case studied in [Huang et al., 2023;
Afshani and Chris, 2024]. Our results further extend to robust
$(k,z)$-clustering in various metric spaces, eliminating the $m$-dependence
under mild data assumptions. The key technical contribution is a novel
non-component-wise error analysis, enabling substantial reduction of outlier
influence, unlike prior methods that retain them.Empirically, our algorithms
consistently outperform existing baselines in terms of size-accuracy tradeoffs
and runtime, even when data assumptions are violated across a wide range of
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zero-Shot Cross-Lingual Transfer using Prefix-Based Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24619v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24619v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Snegha A, Sayambhu Sen, Piyush Singh Pasi, Abhishek Singhania, Preethi Jyothi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the release of new large language models (LLMs) like Llama and Mistral,
zero-shot cross-lingual transfer has become increasingly feasible due to their
multilingual pretraining and strong generalization capabilities. However,
adapting these decoder-only LLMs to new tasks across languages remains
challenging. While parameter-efficient fine-tuning (PeFT) techniques like
Low-Rank Adaptation (LoRA) are widely used, prefix-based techniques such as
soft prompt tuning, prefix tuning, and Llama Adapter are less explored,
especially for zero-shot transfer in decoder-only models. We present a
comprehensive study of three prefix-based methods for zero-shot cross-lingual
transfer from English to 35+ high- and low-resource languages. Our analysis
further explores transfer across linguistic families and scripts, as well as
the impact of scaling model sizes from 1B to 24B. With Llama 3.1 8B, prefix
methods outperform LoRA-baselines by up to 6% on the Belebele benchmark.
Similar improvements were observed with Mistral v0.3 7B as well. Despite using
only 1.23M learning parameters with prefix tuning, we achieve consistent
improvements across diverse benchmarks. These findings highlight the potential
of prefix-based techniques as an effective and scalable alternative to LoRA,
particularly in low-resource multilingual settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 Pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Statistical physics of deep learning: Optimal learning of a multi-layer
  perceptron near interpolation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24616v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24616v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jean Barbier, Francesco Camilli, Minh-Toan Nguyen, Mauro Pastore, Rudy Skerk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For three decades statistical physics has been providing a framework to
analyse neural networks. A long-standing question remained on its capacity to
tackle deep learning models capturing rich feature learning effects, thus going
beyond the narrow networks or kernel methods analysed until now. We positively
answer through the study of the supervised learning of a multi-layer
perceptron. Importantly, (i) its width scales as the input dimension, making it
more prone to feature learning than ultra wide networks, and more expressive
than narrow ones or with fixed embedding layers; and (ii) we focus on the
challenging interpolation regime where the number of trainable parameters and
data are comparable, which forces the model to adapt to the task. We consider
the matched teacher-student setting. It provides the fundamental limits of
learning random deep neural network targets and helps in identifying the
sufficient statistics describing what is learnt by an optimally trained network
as the data budget increases. A rich phenomenology emerges with various
learning transitions. With enough data optimal performance is attained through
model's "specialisation" towards the target, but it can be hard to reach for
training algorithms which get attracted by sub-optimal solutions predicted by
the theory. Specialisation occurs inhomogeneously across layers, propagating
from shallow towards deep ones, but also across neurons in each layer.
Furthermore, deeper targets are harder to learn. Despite its simplicity, the
Bayesian-optimal setting provides insights on how the depth, non-linearity and
finite (proportional) width influence neural networks in the feature learning
regime that are potentially relevant way beyond it.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 19 figures + appendix. This submission supersedes both
  arXiv:2505.24849 and arXiv:2501.18530</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semi-supervised and unsupervised learning for health indicator
  extraction from guided waves in aerospace composite structures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24614v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24614v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        James Josep Perry, Pablo Garcia-Conde Ortiz, George Konstantinou, Cornelie Vergouwen, Edlyn Santha Kumaran, Morteza Moradi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Health indicators (HIs) are central to diagnosing and prognosing the
condition of aerospace composite structures, enabling efficient maintenance and
operational safety. However, extracting reliable HIs remains challenging due to
variability in material properties, stochastic damage evolution, and diverse
damage modes. Manufacturing defects (e.g., disbonds) and in-service incidents
(e.g., bird strikes) further complicate this process. This study presents a
comprehensive data-driven framework that learns HIs via two learning approaches
integrated with multi-domain signal processing. Because ground-truth HIs are
unavailable, a semi-supervised and an unsupervised approach are proposed: (i) a
diversity deep semi-supervised anomaly detection (Diversity-DeepSAD) approach
augmented with continuous auxiliary labels used as hypothetical damage proxies,
which overcomes the limitation of prior binary labels that only distinguish
healthy and failed states while neglecting intermediate degradation, and (ii) a
degradation-trend-constrained variational autoencoder (DTC-VAE), in which the
monotonicity criterion is embedded via an explicit trend constraint. Guided
waves with multiple excitation frequencies are used to monitor single-stiffener
composite structures under fatigue loading. Time, frequency, and time-frequency
representations are explored, and per-frequency HIs are fused via unsupervised
ensemble learning to mitigate frequency dependence and reduce variance. Using
fast Fourier transform features, the augmented Diversity-DeepSAD model achieved
81.6% performance, while DTC-VAE delivered the most consistent HIs with 92.3%
performance, outperforming existing baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparison of generalised additive models and neural networks in
  applications: A systematic <span class="highlight-title">review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24601v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24601v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jessica Doohan, Lucas Kook, Kevin Burke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks have become a popular tool in predictive modelling, more
commonly associated with machine learning and artificial intelligence than with
statistics. Generalised Additive Models (GAMs) are flexible non-linear
statistical models that retain interpretability. Both are state-of-the-art in
their own right, with their respective advantages and disadvantages. This paper
analyses how these two model classes have performed on real-world tabular data.
Following PRISMA guidelines, we conducted a systematic review of papers that
performed empirical comparisons of GAMs and neural networks. Eligible papers
were identified, yielding 143 papers, with 430 datasets. Key attributes at both
paper and dataset levels were extracted and reported. Beyond summarising
comparisons, we analyse reported performance metrics using mixed-effects
modelling to investigate potential characteristics that can explain and
quantify observed differences, including application area, study year, sample
size, number of predictors, and neural network complexity. Across datasets, no
consistent evidence of superiority was found for either GAMs or neural networks
when considering the most frequently reported metrics (RMSE, $R^2$, and AUC).
Neural networks tended to outperform in larger datasets and in those with more
predictors, but this advantage narrowed over time. Conversely, GAMs remained
competitive, particularly in smaller data settings, while retaining
interpretability. Reporting of dataset characteristics and neural network
complexity was incomplete in much of the literature, limiting transparency and
reproducibility. This review highlights that GAMs and neural networks should be
viewed as complementary approaches rather than competitors. For many tabular
applications, the performance trade-off is modest, and interpretability may
favour GAMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Novel XAI-Enhanced Quantum Adversarial Networks for Velocity
  Dispersion Modeling in MaNGA Galaxies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24598v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24598v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sathwik Narkedimilli, N V Saran Kumar, Aswath Babu H, Manjunath K Vanahalli, Manish M, Vinija Jain, Aman Chadha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current quantum machine learning approaches often face challenges balancing
predictive accuracy, robustness, and interpretability. To address this, we
propose a novel quantum adversarial framework that integrates a hybrid quantum
neural network (QNN) with classical deep learning layers, guided by an
evaluator model with LIME-based interpretability, and extended through quantum
GAN and self-supervised variants. In the proposed model, an adversarial
evaluator concurrently guides the QNN by computing feedback loss, thereby
optimizing both prediction accuracy and model explainability. Empirical
evaluations show that the Vanilla model achieves RMSE = 0.27, MSE = 0.071, MAE
= 0.21, and R^2 = 0.59, delivering the most consistent performance across
regression metrics compared to adversarial counterparts. These results
demonstrate the potential of combining quantum-inspired methods with classical
architectures to develop lightweight, high-performance, and interpretable
predictive models, advancing the applicability of QML beyond current
limitations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physics-Informed Extreme Learning Machine (PIELM): Opportunities and
  Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24577v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24577v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        He Yang, Fei Ren, Hai-Sui Yu, Xiaohui Chen, Pei-Zhi Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We are very delighted to see the fast development of physics-informed extreme
learning machine (PIELM) in recent years for higher computation efficiency and
accuracy in physics-informed machine learning. As a summary or review on PIELM
is currently not available, we would like to take this opportunity to show our
perspective and experience for this promising research direction. We can see
many efforts are made to solve PDEs with sharp gradients, nonlinearities,
high-frequency behavior, hard constraints, uncertainty, multiphysics coupling.
Despite the success, many urgent challenges remain to be tackled, which also
provides us opportunities to develop more robust, interpretable, and
generalizable PIELM frameworks with applications in science and engineering.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DistDF: Time-Series Forecasting Needs Joint-Distribution Wasserstein
  Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24574v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24574v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Wang, Licheng Pan, Yuan Lu, Zhixuan Chu, Xiaoxi Li, Shuting He, Zhichao Chen, Haoxuan Li, Qingsong Wen, Zhouchen Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training time-series forecast models requires aligning the conditional
distribution of model forecasts with that of the label sequence. The standard
direct forecast (DF) approach resorts to minimize the conditional negative
log-likelihood of the label sequence, typically estimated using the mean
squared error. However, this estimation proves to be biased in the presence of
label autocorrelation. In this paper, we propose DistDF, which achieves
alignment by alternatively minimizing a discrepancy between the conditional
forecast and label distributions. Because conditional discrepancies are
difficult to estimate from finite time-series observations, we introduce a
newly proposed joint-distribution Wasserstein discrepancy for time-series
forecasting, which provably upper bounds the conditional discrepancy of
interest. This discrepancy admits tractable, differentiable estimation from
empirical samples and integrates seamlessly with gradient-based training.
Extensive experiments show that DistDF improves the performance diverse
forecast models and achieves the state-of-the-art forecasting performance. Code
is available at https://anonymous.4open.science/r/DistDF-F66B.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LoRA-DA: Data-Aware Initialization for Low-Rank Adaptation via
  Asymptotic Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24561v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24561v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyue Zhang, Chang Chu, Tianren Peng, Qi Li, Xiangyang Luo, Zhihao Jiang, Shao-Lun Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the widespread adoption of LLMs, LoRA has become a dominant method for
PEFT, and its initialization methods have attracted increasing attention.
However, existing methods have notable limitations: many methods do not
incorporate target-domain data, while gradient-based methods exploit data only
at a shallow level by relying on one-step gradient decomposition, which remains
unsatisfactory due to the weak empirical performance of the one-step
fine-tuning model that serves as their basis, as well as the fact that these
methods either lack a rigorous theoretical foundation or depend heavily on
restrictive isotropic assumptions. In this paper, we establish a theoretical
framework for data-aware LoRA initialization based on asymptotic analysis.
Starting from a general optimization objective that minimizes the expectation
of the parameter discrepancy between the fine-tuned and target models, we
derive an optimization problem with two components: a bias term, which is
related to the parameter distance between the fine-tuned and target models, and
is approximated using a Fisher-gradient formulation to preserve anisotropy; and
a variance term, which accounts for the uncertainty introduced by sampling
stochasticity through the Fisher information. By solving this problem, we
obtain an optimal initialization strategy for LoRA. Building on this
theoretical framework, we develop an efficient algorithm, LoRA-DA, which
estimates the terms in the optimization problem from a small set of target
domain samples and obtains the optimal LoRA initialization. Empirical results
across multiple benchmarks demonstrate that LoRA-DA consistently improves final
accuracy over existing initialization methods. Additional studies show faster,
more stable convergence, robustness across ranks, and only a small
initialization overhead for LoRA-DA. The source code will be released upon
publication.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enforcing boundary conditions for physics-informed neural operators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24557v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24557v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niklas Göschel, Sebastian Götschel, Daniel Ruprecht
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine-learning based methods like physics-informed neural networks and
physics-informed neural operators are becoming increasingly adept at solving
even complex systems of partial differential equations. Boundary conditions can
be enforced either weakly by penalizing deviations in the loss function or
strongly by training a solution structure that inherently matches the
prescribed values and derivatives. The former approach is easy to implement but
the latter can provide benefits with respect to accuracy and training times.
However, previous approaches to strongly enforcing Neumann or Robin boundary
conditions require a domain with a fully $C^1$ boundary and, as we demonstrate,
can lead to instability if those boundary conditions are posed on a segment of
the boundary that is piecewise $C^1$ but only $C^0$ globally. We introduce a
generalization of the approach by Sukumar \& Srivastava (doi:
10.1016/j.cma.2021.114333), and a new approach based on orthogonal projections
that overcome this limitation. The performance of these new techniques is
compared against weakly and semi-weakly enforced boundary conditions for the
scalar Darcy flow equation and the stationary Navier-Stokes equations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dual-Mind World Models: A General Framework for Learning in Dynamic
  Wireless Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24546v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24546v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingyi Wang, Rashed Shelim, Walid Saad, Naren Ramakrishnan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the popularity of reinforcement learning (RL) in wireless networks,
existing approaches that rely on model-free RL (MFRL) and model-based RL (MBRL)
are data inefficient and short-sighted. Such RL-based solutions cannot
generalize to novel network states since they capture only statistical patterns
rather than the underlying physics and logic from wireless data. These
limitations become particularly challenging in complex wireless networks with
high dynamics and long-term planning requirements. To address these
limitations, in this paper, a novel dual-mind world model-based learning
framework is proposed with the goal of optimizing completeness-weighted age of
information (CAoI) in a challenging mmWave V2X scenario. Inspired by cognitive
psychology, the proposed dual-mind world model encompasses a pattern-driven
System 1 component and a logic-driven System 2 component to learn dynamics and
logic of the wireless network, and to provide long-term link scheduling over
reliable imagined trajectories. Link scheduling is learned through end-to-end
differentiable imagined trajectories with logical consistency over an extended
horizon rather than relying on wireless data obtained from environment
interactions. Moreover, through imagination rollouts, the proposed world model
can jointly reason network states and plan link scheduling. During intervals
without observations, the proposed method remains capable of making efficient
decisions. Extensive experiments are conducted on a realistic simulator based
on Sionna with real-world physical channel, ray-tracing, and scene objects with
material properties. Simulation results show that the proposed world model
achieves a significant improvement in data efficiency and achieves strong
generalization and adaptation to unseen environments, compared to the
state-of-the-art RL baselines, and the world model approach with only System 1.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Machine-Learning Pipeline for Data-Driven Defect Detection
  and Characterisation: Application to Displacement Cascades 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24523v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24523v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Del Fré, Andrée de Backer, Christophe Domain, Ludovic Thuinet, Charlotte S. Becquart
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neutron irradiation produces, within a few picoseconds, displacement cascades
that are sequences of atomic collisions generating point and extended defects
which subsequently affects the long-term evolution of materials. The diversity
of these defects, characterized morphologically and statistically, defines what
is called the "primary damage". In this work, we present a fully unsupervised
machine learning (ML) workflow that detects and classifies these defects
directly from molecular dynamics data. Local environments are encoded by the
Smooth Overlap of Atomic Positions (SOAP) vector, anomalous atoms are isolated
with autoencoder neural networks (AE), embedded with Uniform Manifold
Approximation and Projection (UMAP) and clustered using Hierarchical
Density-Based Spatial Clustering of Applications with Noise (HDBSCAN). Applied
to 80 keV displacement cascades in Ni, Fe$_7$0Ni$_{10}$Cr$_{20}$, and Zr, the
AE successfully identify the small fraction of outlier atoms that participate
in defect formation. HDBSCAN then partitions the UMAP latent space of
AE-flagged SOAP descriptors into well defined groups representing vacancy- and
interstitial-dominated regions and, within each, separates small from large
aggregates, assigning 99.7 % of outliers to compact physical motifs. A signed
cluster-identification score confirms this separation, and cluster size scales
with net defect counts (R2 > 0.89). Statistical cross analyses between the ML
outlier map and several conventional detectors (centrosymmetry, dislocation
extraction, etc.) reveal strong overlap and complementary coverage, all
achieved without template or threshold tuning. This ML workflow thus provides
an efficient tool for the quantitative mapping of structural anomalies in
materials, particularly those arising from irradiation damage in displacement
cascades.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 1 graphical abstract, 7 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Local Performance vs. Out-of-Distribution Generalization: An Empirical
  Analysis of Personalized Federated Learning in Heterogeneous Data
  Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24503v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24503v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mortesa Hussaini, Jan Theiß, Anthony Stein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the context of Federated Learning with heterogeneous data environments,
local models tend to converge to their own local model optima during local
training steps, deviating from the overall data distributions. Aggregation of
these local updates, e.g., with FedAvg, often does not align with the global
model optimum (client drift), resulting in an update that is suboptimal for
most clients. Personalized Federated Learning approaches address this challenge
by exclusively focusing on the average local performances of clients' models on
their own data distribution. Generalization to out-of-distribution samples,
which is a substantial benefit of FedAvg and represents a significant component
of robustness, appears to be inadequately incorporated into the assessment and
evaluation processes. This study involves a thorough evaluation of Federated
Learning approaches, encompassing both their local performance and their
generalization capabilities. Therefore, we examine different stages within a
single communication round to enable a more nuanced understanding of the
considered metrics. Furthermore, we propose and incorporate a modified approach
of FedAvg, designated as Federated Learning with Individualized Updates (FLIU),
extending the algorithm by a straightforward individualization step with an
adaptive personalization factor. We evaluate and compare the approaches
empirically using MNIST and CIFAR-10 under various distributional conditions,
including benchmark IID and pathological non-IID, as well as additional novel
test environments with Dirichlet distribution specifically developed to stress
the algorithms on complex data heterogeneity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MIMIC-Sepsis: A Curated Benchmark for Modeling and Learning from Sepsis
  Trajectories in the ICU 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24500v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24500v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yong Huang, Zhongqi Yang, Amir Rahmani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sepsis is a leading cause of mortality in intensive care units (ICUs), yet
existing research often relies on outdated datasets, non-reproducible
preprocessing pipelines, and limited coverage of clinical interventions. We
introduce MIMIC-Sepsis, a curated cohort and benchmark framework derived from
the MIMIC-IV database, designed to support reproducible modeling of sepsis
trajectories. Our cohort includes 35,239 ICU patients with time-aligned
clinical variables and standardized treatment data, including vasopressors,
fluids, mechanical ventilation and antibiotics. We describe a transparent
preprocessing pipeline-based on Sepsis-3 criteria, structured imputation
strategies, and treatment inclusion-and release it alongside benchmark tasks
focused on early mortality prediction, length-of-stay estimation, and shock
onset classification. Empirical results demonstrate that incorporating
treatment variables substantially improves model performance, particularly for
Transformer-based architectures. MIMIC-Sepsis serves as a robust platform for
evaluating predictive and sequential models in critical care research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sample-efficient and Scalable Exploration in Continuous-Time RL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24482v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24482v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Klemens Iten, Lenart Treven, Bhavya Sukhija, Florian Dörfler, Andreas Krause
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning algorithms are typically designed for discrete-time
dynamics, even though the underlying real-world control systems are often
continuous in time. In this paper, we study the problem of continuous-time
reinforcement learning, where the unknown system dynamics are represented using
nonlinear ordinary differential equations (ODEs). We leverage probabilistic
models, such as Gaussian processes and Bayesian neural networks, to learn an
uncertainty-aware model of the underlying ODE. Our algorithm, COMBRL, greedily
maximizes a weighted sum of the extrinsic reward and model epistemic
uncertainty. This yields a scalable and sample-efficient approach to
continuous-time model-based RL. We show that COMBRL achieves sublinear regret
in the reward-driven setting, and in the unsupervised RL setting (i.e., without
extrinsic rewards), we provide a sample complexity bound. In our experiments,
we evaluate COMBRL in both standard and unsupervised RL settings and
demonstrate that it scales better, is more sample-efficient than prior methods,
and outperforms baselines across several deep RL tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 6 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Methodology for Comparing Machine Learning Algorithms for Survival
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24473v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24473v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucas Buk Cardoso, Simone Aldrey Angelo, Yasmin Pacheco Gil Bonilha, Fernando Maia, Adeylson Guimarães Ribeiro, Maria Paula Curado, Gisele Aparecida Fernandes, Vanderlei Cunha Parro, Flávio Almeida de Magalhães Cipparrone, Alexandre Dias Porto Chiavegatto Filho, Tatiana Natasha Toporcov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study presents a comparative methodological analysis of six machine
learning models for survival analysis (MLSA). Using data from nearly 45,000
colorectal cancer patients in the Hospital-Based Cancer Registries of S\~ao
Paulo, we evaluated Random Survival Forest (RSF), Gradient Boosting for
Survival Analysis (GBSA), Survival SVM (SSVM), XGBoost-Cox (XGB-Cox),
XGBoost-AFT (XGB-AFT), and LightGBM (LGBM), capable of predicting survival
considering censored data. Hyperparameter optimization was performed with
different samplers, and model performance was assessed using the Concordance
Index (C-Index), C-Index IPCW, time-dependent AUC, and Integrated Brier Score
(IBS). Survival curves produced by the models were compared with predictions
from classification algorithms, and predictor interpretation was conducted
using SHAP and permutation importance. XGB-AFT achieved the best performance
(C-Index = 0.7618; IPCW = 0.7532), followed by GBSA and RSF. The results
highlight the potential and applicability of MLSA to improve survival
prediction and support decision making.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Non-Singularity of the Gradient Descent map for Neural Networks with
  Piecewise Analytic Activations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24466v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24466v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandru Crăciun, Debarghya Ghoshdastidar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The theory of training deep networks has become a central question of modern
machine learning and has inspired many practical advancements. In particular,
the gradient descent (GD) optimization algorithm has been extensively studied
in recent years. A key assumption about GD has appeared in several recent
works: the \emph{GD map is non-singular} -- it preserves sets of measure zero
under preimages. Crucially, this assumption has been used to prove that GD
avoids saddle points and maxima, and to establish the existence of a computable
quantity that determines the convergence to global minima (both for GD and
stochastic GD). However, the current literature either assumes the
non-singularity of the GD map or imposes restrictive assumptions, such as
Lipschitz smoothness of the loss (for example, Lipschitzness does not hold for
deep ReLU networks with the cross-entropy loss) and restricts the analysis to
GD with small step-sizes. In this paper, we investigate the neural network map
as a function on the space of weights and biases. We also prove, for the first
time, the non-singularity of the gradient descent (GD) map on the loss
landscape of realistic neural network architectures (with fully connected,
convolutional, or softmax attention layers) and piecewise analytic activations
(which includes sigmoid, ReLU, leaky ReLU, etc.) for almost all step-sizes. Our
work significantly extends the existing results on the convergence of GD and
SGD by guaranteeing that they apply to practical neural network settings and
has the potential to unlock further exploration of learning dynamics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ARIMA_PLUS: Large-scale, Accurate, Automatic and Interpretable
  In-Database Time Series Forecasting and Anomaly Detection in Google BigQuery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24452v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24452v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xi Cheng, Weijie Shen, Haoming Chen, Chaoyi Shen, Jean Ortega, Jiashang Liu, Steve Thomas, Honglin Zheng, Haoyun Wu, Yuxiang Li, Casey Lichtendahl, Jenny Ortiz, Gang Liu, Haiyang Qi, Omid Fatemieh, Chris Fry, Jing Jing Long
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time series forecasting and anomaly detection are common tasks for
practitioners in industries such as retail, manufacturing, advertising and
energy. Two unique challenges stand out: (1) efficiently and accurately
forecasting time series or detecting anomalies in large volumes automatically;
and (2) ensuring interpretability of results to effectively incorporate
business insights. We present ARIMA_PLUS, a novel framework to overcome these
two challenges by a unique combination of (a) accurate and interpretable time
series models and (b) scalable and fully managed system infrastructure. The
model has a sequential and modular structure to handle different components of
the time series, including holiday effects, seasonality, trend, and anomalies,
which enables high interpretability of the results. Novel enhancements are made
to each module, and a unified framework is established to address both
forecasting and anomaly detection tasks simultaneously. In terms of accuracy,
its comprehensive benchmark on the 42 public datasets in the Monash forecasting
repository shows superior performance over not only well-established
statistical alternatives (such as ETS, ARIMA, TBATS, Prophet) but also newer
neural network models (such as DeepAR, N-BEATS, PatchTST, TimeMixer). In terms
of infrastructure, it is directly built into the query engine of BigQuery in
Google Cloud. It uses a simple SQL interface and automates tedious
technicalities such as data cleaning and model selection. It automatically
scales with managed cloud computational and storage resources, making it
possible to forecast 100 million time series using only 1.5 hours with a
throughput of more than 18000 time series per second. In terms of
interpretability, we present several case studies to demonstrate time series
insights it generates and customizability it offers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Nearest Neighbor Matching as Least Squares Density Ratio Estimation and
  Riesz Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24433v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24433v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masahiro Kato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study proves that Nearest Neighbor (NN) matching can be interpreted as
an instance of Riesz regression for automatic debiased machine learning. Lin et
al. (2023) shows that NN matching is an instance of density-ratio estimation
with their new density-ratio estimator. Chernozhukov et al. (2024) develops
Riesz regression for automatic debiased machine learning, which directly
estimates the Riesz representer (or equivalently, the bias-correction term) by
minimizing the mean squared error. In this study, we first prove that the
density-ratio estimation method proposed in Lin et al. (2023) is essentially
equivalent to Least-Squares Importance Fitting (LSIF) proposed in Kanamori et
al. (2009) for direct density-ratio estimation. Furthermore, we derive Riesz
regression using the LSIF framework. Based on these results, we derive NN
matching from Riesz regression. This study is based on our work Kato (2025a)
and Kato (2025b).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fill in the Blanks: Accelerating Q-Learning with a Handful of
  Demonstrations in Sparse Reward Settings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24432v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24432v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seyed Mahdi Basiri Azad, Joschka Boedecker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) in sparse-reward environments remains a
significant challenge due to the lack of informative feedback. We propose a
simple yet effective method that uses a small number of successful
demonstrations to initialize the value function of an RL agent. By precomputing
value estimates from offline demonstrations and using them as targets for early
learning, our approach provides the agent with a useful prior over promising
actions. The agent then refines these estimates through standard online
interaction. This hybrid offline-to-online paradigm significantly reduces the
exploration burden and improves sample efficiency in sparse-reward settings.
Experiments on benchmark tasks demonstrate that our method accelerates
convergence and outperforms standard baselines, even with minimal or suboptimal
demonstration data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attack on a PUF-based Secure Binary Neural Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24422v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24422v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bijeet Basak, Nupur Patil, Kurian Polachan, Srinivas Vivek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Binarized Neural Networks (BNNs) deployed on memristive crossbar arrays
provide energy-efficient solutions for edge computing but are susceptible to
physical attacks due to memristor nonvolatility. Recently, Rajendran et al.
(IEEE Embedded Systems Letter 2025) proposed a Physical Unclonable Function
(PUF)-based scheme to secure BNNs against theft attacks. Specifically, the
weight and bias matrices of the BNN layers were secured by swapping columns
based on device's PUF key bits.
  In this paper, we demonstrate that this scheme to secure BNNs is vulnerable
to PUF-key recovery attack. As a consequence of our attack, we recover the
secret weight and bias matrices of the BNN. Our approach is motivated by
differential cryptanalysis and reconstructs the PUF key bit-by-bit by observing
the change in model accuracy, and eventually recovering the BNN model
parameters. Evaluated on a BNN trained on the MNIST dataset, our attack could
recover 85% of the PUF key, and recover the BNN model up to 93% classification
accuracy compared to the original model's 96% accuracy. Our attack is very
efficient and it takes a couple of minutes to recovery the PUF key and the
model parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at VLSID 2026. To be published in IEEE Xplore</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ APEX: Approximate-but-exhaustive search for ultra-large combinatorial
  synthesis libraries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24380v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24380v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aryan Pedawi, Jordi Silvestre-Ryan, Bradley Worley, Darren J Hsu, Kushal S Shah, Elias Stehle, Jingrong Zhang, Izhar Wallach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Make-on-demand combinatorial synthesis libraries (CSLs) like Enamine REAL
have significantly enabled drug discovery efforts. However, their large size
presents a challenge for virtual screening, where the goal is to identify the
top compounds in a library according to a computational objective (e.g.,
optimizing docking score) subject to computational constraints under a limited
computational budget. For current library sizes -- numbering in the tens of
billions of compounds -- and scoring functions of interest, a routine virtual
screening campaign may be limited to scoring fewer than 0.1% of the available
compounds, leaving potentially many high scoring compounds undiscovered.
Furthermore, as constraints (and sometimes objectives) change during the course
of a virtual screening campaign, existing virtual screening algorithms
typically offer little room for amortization. We propose the
approximate-but-exhaustive search protocol for CSLs, or APEX. APEX utilizes a
neural network surrogate that exploits the structure of CSLs in the prediction
of objectives and constraints to make full enumeration on a consumer GPU
possible in under a minute, allowing for exact retrieval of approximate top-$k$
sets. To demonstrate APEX's capabilities, we develop a benchmark CSL comprised
of more than 10 million compounds, all of which have been annotated with their
docking scores on five medically relevant targets along with physicohemical
properties measured with RDKit such that, for any objective and set of
constraints, the ground truth top-$k$ compounds can be identified and compared
against the retrievals from any virtual screening algorithm. We show APEX's
consistently strong performance both in retrieval accuracy and runtime compared
to alternative methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comprehensive Evaluation Framework for Synthetic Trip Data Generation
  in Public Transport 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24375v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24375v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanyuan Wu, Zhenlin Qin, Zhenliang Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthetic data offers a promising solution to the privacy and accessibility
challenges of using smart card data in public transport research. Despite rapid
progress in generative modeling, there is limited attention to comprehensive
evaluation, leaving unclear how reliable, safe, and useful synthetic data truly
are. Existing evaluations remain fragmented, typically limited to
population-level representativeness or record-level privacy, without
considering group-level variations or task-specific utility. To address this
gap, we propose a Representativeness-Privacy-Utility (RPU) framework that
systematically evaluates synthetic trip data across three complementary
dimensions and three hierarchical levels (record, group, population). The
framework integrates a consistent set of metrics to quantify similarity,
disclosure risk, and practical usefulness, enabling transparent and balanced
assessment of synthetic data quality. We apply the framework to benchmark
twelve representative generation methods, spanning conventional statistical
models, deep generative networks, and privacy-enhanced variants. Results show
that synthetic data do not inherently guarantee privacy and there is no
"one-size-fits-all" model, the trade-off between privacy and
representativeness/utility is obvious. Conditional Tabular generative
adversarial network (CTGAN) provide the most balanced trade-off and is
suggested for practical applications. The RPU framework provides a systematic
and reproducible basis for researchers and practitioners to compare synthetic
data generation techniques and select appropriate methods in public transport
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Filtering instances and rejecting predictions to obtain reliable models
  in healthcare 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24368v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24368v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maria Gabriela Valeriano, David Kohan Marzagão, Alfredo Montelongo, Carlos Roberto Veiga Kiffer, Natan Katz, Ana Carolina Lorena
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine Learning (ML) models are widely used in high-stakes domains such as
healthcare, where the reliability of predictions is critical. However, these
models often fail to account for uncertainty, providing predictions even with
low confidence. This work proposes a novel two-step data-centric approach to
enhance the performance of ML models by improving data quality and filtering
low-confidence predictions. The first step involves leveraging Instance
Hardness (IH) to filter problematic instances during training, thereby refining
the dataset. The second step introduces a confidence-based rejection mechanism
during inference, ensuring that only reliable predictions are retained. We
evaluate our approach using three real-world healthcare datasets, demonstrating
its effectiveness at improving model reliability while balancing predictive
performance and rejection rate. Additionally, we use alternative criteria -
influence values for filtering and uncertainty for rejection - as baselines to
evaluate the efficiency of the proposed method. The results demonstrate that
integrating IH filtering with confidence-based rejection effectively enhances
model performance while preserving a large proportion of instances. This
approach provides a practical method for deploying ML systems in
safety-critical applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is under review at Machine Learning (Springer)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Perception Learning: A Formal Separation of Sensory Representation
  Learning from Decision Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24356v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24356v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suman Sanyal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Perception Learning (PeL), a paradigm that optimizes an agent's
sensory interface $f_\phi:\mathcal{X}\to\mathcal{Z}$ using task-agnostic
signals, decoupled from downstream decision learning
$g_\theta:\mathcal{Z}\to\mathcal{Y}$. PeL directly targets label-free
perceptual properties, such as stability to nuisances, informativeness without
collapse, and controlled geometry, assessed via objective
representation-invariant metrics. We formalize the separation of perception and
decision, define perceptual properties independent of objectives or
reparameterizations, and prove that PeL updates preserving sufficient
invariants are orthogonal to Bayes task-risk gradients. Additionally, we
provide a suite of task-agnostic evaluation metrics to certify perceptual
quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What do vision-language models see in the context? Investigating
  multimodal in-context learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24331v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24331v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriel O. dos Santos, Esther Colombini, Sandra Avila
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context learning (ICL) enables Large Language Models (LLMs) to learn tasks
from demonstration examples without parameter updates. Although it has been
extensively studied in LLMs, its effectiveness in Vision-Language Models (VLMs)
remains underexplored. In this work, we present a systematic study of ICL in
VLMs, evaluating seven models spanning four architectures on three image
captioning benchmarks. We analyze how prompt design, architectural choices, and
training strategies influence multimodal ICL. To our knowledge, we are the
first to analyze how attention patterns in VLMs vary with an increasing number
of in-context demonstrations. Our results reveal that training on imag-text
interleaved data enhances ICL performance but does not imply effective
integration of visual and textual information from demonstration examples. In
contrast, instruction tuning improves instruction-following but can reduce
reliance on in-context demonstrations, suggesting a trade-off between
instruction alignment and in-context adaptation. Attention analyses further
show that current VLMs primarily focus on textual cues and fail to leverage
visual information, suggesting a limited capacity for multimodal integration.
These findings highlight key limitations in the ICL abilities of current VLMs
and provide insights for enhancing their ability to learn from multimodal
in-context examples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Transformer</span>s can do Bayesian Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24318v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24318v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prajit Bhaskaran, Tom Viering
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian clustering accounts for uncertainty but is computationally demanding
at scale. Furthermore, real-world datasets often contain missing values, and
simple imputation ignores the associated uncertainty, resulting in suboptimal
results. We present Cluster-PFN, a Transformer-based model that extends
Prior-Data Fitted Networks (PFNs) to unsupervised Bayesian clustering. Trained
entirely on synthetic datasets generated from a finite Gaussian Mixture Model
(GMM) prior, Cluster-PFN learns to estimate the posterior distribution over
both the number of clusters and the cluster assignments. Our method estimates
the number of clusters more accurately than handcrafted model selection
procedures such as AIC, BIC and Variational Inference (VI), and achieves
clustering quality competitive with VI while being orders of magnitude faster.
Cluster-PFN can be trained on complex priors that include missing data,
outperforming imputation-based baselines on real-world genomic datasets, at
high missingness. These results show that the Cluster-PFN can provide scalable
and flexible Bayesian clustering.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EDC: Equation Discovery for Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24310v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24310v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guus Toussaint, Arno Knobbe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Equation Discovery techniques have shown considerable success in regression
tasks, where they are used to discover concise and interpretable models
(\textit{Symbolic Regression}). In this paper, we propose a new ED-based binary
classification framework. Our proposed method EDC finds analytical functions of
manageable size that specify the location and shape of the decision boundary.
In extensive experiments on artificial and real-life data, we demonstrate how
EDC is able to discover both the structure of the target equation as well as
the value of its parameters, outperforming the current state-of-the-art
ED-based classification methods in binary classification and achieving
performance comparable to the state of the art in binary classification. We
suggest a grammar of modest complexity that appears to work well on the tested
datasets but argue that the exact grammar -- and thus the complexity of the
models -- is configurable, and especially domain-specific expressions can be
included in the pattern language, where that is required. The presented grammar
consists of a series of summands (additive terms) that include linear,
quadratic and exponential terms, as well as products of two features (producing
hyperbolic curves ideal for capturing XOR-like dependencies). The experiments
demonstrate that this grammar allows fairly flexible decision boundaries while
not so rich to cause overfitting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This preprint has not undergone peer review or any post-submission
  improvements or corrections. The Version of Record of this contribution is
  published in Lecture Notes in Computer Science, and is available online at
  https://doi.org/10.1007/978-3-032-05461-6_9</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Problem-Parameter-Free Decentralized Bilevel Optimization <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24288v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24288v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiwei Zhai, Wenjing Yan, Ying-Jun Angela Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decentralized bilevel optimization has garnered significant attention due to
its critical role in solving large-scale machine learning problems. However,
existing methods often rely on prior knowledge of problem parameters-such as
smoothness, convexity, or communication network topologies-to determine
appropriate stepsizes. In practice, these problem parameters are typically
unavailable, leading to substantial manual effort for hyperparameter tuning. In
this paper, we propose AdaSDBO, a fully problem-parameter-free algorithm for
decentralized bilevel optimization with a single-loop structure. AdaSDBO
leverages adaptive stepsizes based on cumulative gradient norms to update all
variables simultaneously, dynamically adjusting its progress and eliminating
the need for problem-specific hyperparameter tuning. Through rigorous
theoretical analysis, we establish that AdaSDBO achieves a convergence rate of
$\widetilde{\mathcal{O}}\left(\frac{1}{T}\right)$, matching the performance of
well-tuned state-of-the-art methods up to polylogarithmic factors. Extensive
numerical experiments demonstrate that AdaSDBO delivers competitive performance
compared to existing decentralized bilevel optimization methods while
exhibiting remarkable robustness across diverse stepsize configurations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards actionable hypotension prediction -- predicting catecholamine
  therapy initiation in the intensive care unit 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24287v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24287v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Richard Koebe, Noah Saibel, Juan Miguel Lopez Alcaraz, Simon Schäfer, Nils Strodthoff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hypotension in critically ill ICU patients is common and life-threatening.
Escalation to catecholamine therapy marks a key management step, with both
undertreatment and overtreatment posing risks. Most machine learning (ML)
models predict hypotension using fixed MAP thresholds or MAP forecasting,
overlooking the clinical decision behind treatment escalation. Predicting
catecholamine initiation, the start of vasoactive or inotropic agent
administration offers a more clinically actionable target reflecting real
decision-making. Using the MIMIC-III database, we modeled catecholamine
initiation as a binary event within a 15-minute prediction window. Input
features included statistical descriptors from a two-hour sliding MAP context
window, along with demographics, biometrics, comorbidities, and ongoing
treatments. An Extreme Gradient Boosting (XGBoost) model was trained and
interpreted via SHapley Additive exPlanations (SHAP). The model achieved an
AUROC of 0.822 (0.813-0.830), outperforming the hypotension baseline (MAP < 65,
AUROC 0.686 [0.675-0.699]). SHAP analysis highlighted recent MAP values, MAP
trends, and ongoing treatments (e.g., sedatives, electrolytes) as dominant
predictors. Subgroup analysis showed higher performance in males, younger
patients (<53 years), those with higher BMI (>32), and patients without
comorbidities or concurrent medications. Predicting catecholamine initiation
based on MAP dynamics, treatment context, and patient characteristics supports
the critical decision of when to escalate therapy, shifting focus from
threshold-based alarms to actionable decision support. This approach is
feasible across a broad ICU cohort under natural event imbalance. Future work
should enrich temporal and physiological context, extend label definitions to
include therapy escalation, and benchmark against existing hypotension
prediction systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 8 figures, source code under
  https://github.com/AI4HealthUOL/actionable-hypotension</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HergNet: a Fast Neural Surrogate Model for Sound Field Predictions via
  Superposition of Plane Waves 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24279v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24279v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Calafà, Yuanxin Xia, Cheol-Ho Jeong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel neural network architecture for the efficient prediction
of sound fields in two and three dimensions. The network is designed to
automatically satisfy the Helmholtz equation, ensuring that the outputs are
physically valid. Therefore, the method can effectively learn solutions to
boundary-value problems in various wave phenomena, such as acoustics, optics,
and electromagnetism. Numerical experiments show that the proposed strategy can
potentially outperform state-of-the-art methods in room acoustics simulation,
in particular in the range of mid to high frequencies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SALS: Sparse Attention in Latent Space for KV cache Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24273v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24273v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junlin Mu, Hantao Huang, Jihang Zhang, Minghui Yu, Tao Wang, Yidong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models capable of handling extended contexts are in high
demand, yet their inference remains challenging due to substantial Key-Value
cache size and high memory bandwidth requirements. Previous research has
demonstrated that KV cache exhibits low-rank characteristics within the hidden
dimension, suggesting the potential for effective compression. However, due to
the widely adopted Rotary Position Embedding mechanism in modern LLMs, naive
low-rank compression suffers severe accuracy degradation or creates a new speed
bottleneck, as the low-rank cache must first be reconstructed in order to apply
RoPE. In this paper, we introduce two key insights: first, the application of
RoPE to the key vectors increases their variance, which in turn results in a
higher rank; second, after the key vectors are transformed into the latent
space, they largely maintain their representation across most layers. Based on
these insights, we propose the Sparse Attention in Latent Space framework. SALS
projects the KV cache into a compact latent space via low-rank projection, and
performs sparse token selection using RoPE-free query-key interactions in this
space. By reconstructing only a small subset of important tokens, it avoids the
overhead of full KV cache reconstruction. We comprehensively evaluate SALS on
various tasks using two large-scale models: LLaMA2-7b-chat and Mistral-7b, and
additionally verify its scalability on the RULER-128k benchmark with
LLaMA3.1-8B-Instruct. Experimental results demonstrate that SALS achieves SOTA
performance by maintaining competitive accuracy. Under different settings, SALS
achieves 6.4-fold KV cache compression and 5.7-fold speed-up in the attention
operator compared to FlashAttention2 on the 4K sequence. For the end-to-end
throughput performance, we achieves 1.4-fold and 4.5-fold improvement compared
to GPT-fast on 4k and 32K sequences, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UtilGen: Utility-Centric Generative Data Augmentation with Dual-Level
  Task Adaptation <span class="chip">NeurIPS
  2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24262v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24262v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiyu Guo, Shuo Yang, Yiming Huang, Yancheng Long, Xiaobo Xia, Xiu Su, Bo Zhao, Zeke Xie, Liqiang Nie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data augmentation using generative models has emerged as a powerful paradigm
for enhancing performance in computer vision tasks. However, most existing
augmentation approaches primarily focus on optimizing intrinsic data attributes
-- such as fidelity and diversity -- to generate visually high-quality
synthetic data, while often neglecting task-specific requirements. Yet, it is
essential for data generators to account for the needs of downstream tasks, as
training data requirements can vary significantly across different tasks and
network architectures. To address these limitations, we propose UtilGen, a
novel utility-centric data augmentation framework that adaptively optimizes the
data generation process to produce task-specific, high-utility training data
via downstream task feedback. Specifically, we first introduce a weight
allocation network to evaluate the task-specific utility of each synthetic
sample. Guided by these evaluations, UtilGen iteratively refines the data
generation process using a dual-level optimization strategy to maximize the
synthetic data utility: (1) model-level optimization tailors the generative
model to the downstream task, and (2) instance-level optimization adjusts
generation policies -- such as prompt embeddings and initial noise -- at each
generation round. Extensive experiments on eight benchmark datasets of varying
complexity and granularity demonstrate that UtilGen consistently achieves
superior performance, with an average accuracy improvement of 3.87% over
previous SOTA. Further analysis of data influence and distribution reveals that
UtilGen produces more impactful and task-relevant synthetic data, validating
the effectiveness of the paradigm shift from visual characteristics-centric to
task utility-centric data augmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39th Conference on Neural Information Processing Systems (NeurIPS
  2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Memorization to Reasoning in the Spectrum of Loss Curvature 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24256v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24256v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jack Merullo, Srihita Vatsavaya, Lucius Bushnaq, Owen Lewis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We characterize how memorization is represented in transformer models and
show that it can be disentangled in the weights of both language models (LMs)
and vision transformers (ViTs) using a decomposition based on the loss
landscape curvature. This insight is based on prior theoretical and empirical
work showing that the curvature for memorized training points is much sharper
than non memorized, meaning ordering weight components from high to low
curvature can reveal a distinction without explicit labels. This motivates a
weight editing procedure that suppresses far more recitation of untargeted
memorized data more effectively than a recent unlearning method
(BalancedSubnet), while maintaining lower perplexity. Since the basis of
curvature has a natural interpretation for shared structure in model weights,
we analyze the editing procedure extensively on its effect on downstream tasks
in LMs, and find that fact retrieval and arithmetic are specifically and
consistently negatively affected, even though open book fact retrieval and
general logical reasoning is conserved. We posit these tasks rely heavily on
specialized directions in weight space rather than general purpose mechanisms,
regardless of whether those individual datapoints are memorized. We support
this by showing a correspondence between task data's activation strength with
low curvature components that we edit out, and the drop in task performance
after the edit. Our work enhances the understanding of memorization in neural
networks with practical applications towards removing it, and provides evidence
for idiosyncratic, narrowly-used structures involved in solving tasks like math
and fact retrieval.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Forecasting precipitation in the Arctic using probabilistic machine
  learning informed by causal climate drivers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24254v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24254v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Madhurima Panja, Dhiman Das, Tanujit Chakraborty, Arnob Ray, R. Athulya, Chittaranjan Hens, Syamal K. Dana, Nuncio Murukesh, Dibakar Ghosh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding and forecasting precipitation events in the Arctic maritime
environments, such as Bear Island and Ny-{\AA}lesund, is crucial for assessing
climate risk and developing early warning systems in vulnerable marine regions.
This study proposes a probabilistic machine learning framework for modeling and
predicting the dynamics and severity of precipitation. We begin by analyzing
the scale-dependent relationships between precipitation and key atmospheric
drivers (e.g., temperature, relative humidity, cloud cover, and air pressure)
using wavelet coherence, which captures localized dependencies across time and
frequency domains. To assess joint causal influences, we employ
Synergistic-Unique-Redundant Decomposition, which quantifies the impact of
interaction effects among each variable on future precipitation dynamics. These
insights inform the development of data-driven forecasting models that
incorporate both historical precipitation and causal climate drivers. To
account for uncertainty, we employ the conformal prediction method, which
enables the generation of calibrated non-parametric prediction intervals. Our
results underscore the importance of utilizing a comprehensive framework that
combines causal analysis with probabilistic forecasting to enhance the
reliability and interpretability of precipitation predictions in Arctic marine
environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enabling Near-realtime Remote Sensing via Satellite-Ground Collaboration
  of Large Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24242v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24242v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihan Li, Jiahao Yang, Yuxin Zhang, Zhe Chen, Yue Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large vision-language models (LVLMs) have recently demonstrated great
potential in remote sensing (RS) tasks (e.g., disaster monitoring) conducted by
low Earth orbit (LEO) satellites. However, their deployment in real-world LEO
satellite systems remains largely unexplored, hindered by limited onboard
computing resources and brief satellite-ground contacts. We propose Grace, a
satellite-ground collaborative system designed for near-realtime LVLM inference
in RS tasks. Accordingly, we deploy compact LVLM on satellites for realtime
inference, but larger ones on ground stations (GSs) to guarantee end-to-end
performance. Grace is comprised of two main phases that are asynchronous
satellite-GS Retrieval-Augmented Generation (RAG), and a task dispatch
algorithm. Firstly, we still the knowledge archive of GS RAG to satellite
archive with tailored adaptive update algorithm during limited satellite-ground
data exchange period. Secondly, propose a confidence-based test algorithm that
either processes the task onboard the satellite or offloads it to the GS.
Extensive experiments based on real-world satellite orbital data show that
Grace reduces the average latency by 76-95% compared to state-of-the-art
methods, without compromising inference accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Temporal Knowledge Graph Hyperedge Forecasting: Exploring
  Entity-to-Category Link Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24240v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24240v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edward Markai, Sina Molavipour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal Knowledge Graphs have emerged as a powerful way of not only modeling
static relationships between entities but also the dynamics of how relations
evolve over time. As these informational structures can be used to store
information from a real-world setting, such as a news flow, predicting future
graph components to a certain extent equates predicting real-world events. Most
of the research in this field focuses on embedding-based methods, often
leveraging convolutional neural net architectures. These solutions act as black
boxes, limiting insight. In this paper, we explore an extension to an
established rule-based framework, TLogic, that yields a high accuracy in
combination with explainable predictions. This offers transparency and allows
the end-user to critically evaluate the rules applied at the end of the
prediction stage. The new rule format incorporates entity category as a key
component with the purpose of limiting rule application only to relevant
entities. When categories are unknown for building the graph, we propose a
data-driven method to generate them with an LLM-based approach. Additionally,
we investigate the choice of aggregation method for scores of retrieved
entities when performing category prediction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PaTaRM: Bridging Pairwise and Pointwise Signals via Preference-Aware
  Task-Adaptive Reward Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24235v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24235v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ai Jian, Jingqing Ruan, Xing Ma, Dailin Li, QianLin Zhou, Ke Zeng, Xunliang Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reward models (RMs) are central to reinforcement learning from human feedback
(RLHF), providing the critical supervision signals that align large language
models (LLMs) with human preferences. While generative reward models (GRMs)
offer greater interpretability than traditional scalar RMs, current training
paradigms remain limited. Pair-wise methods rely on binary good-versus-bad
labels, which cause mismatches for point-wise inference and necessitate complex
pairing strategies for effective application in RLHF. On the other hand,
point-wise methods require more elaborate absolute labeling with rubric-driven
criteria, resulting in poor adaptability and high annotation costs. In this
work, we propose the Preference-Aware Task-Adaptive Reward Model (PaTaRM), a
unified framework that integrates a preference-aware reward (PAR) mechanism
with dynamic rubric adaptation. PaTaRM leverages relative preference
information from pairwise data to construct robust point-wise training signals,
eliminating the need for explicit point-wise labels. Simultaneously, it employs
a task-adaptive rubric system that flexibly generates evaluation criteria for
both global task consistency and instance-specific fine-grained reasoning. This
design enables efficient, generalizable, and interpretable reward modeling for
RLHF. Extensive experiments show that PaTaRM achieves an average relative
improvement of 4.7% on RewardBench and RMBench across Qwen3-8B and Qwen3-14B
models. Furthermore, PaTaRM boosts downstream RLHF performance, with an average
improvement of 13.6% across IFEval and InFoBench benchmarks, confirming its
effectiveness and robustness. Our code is available at
https://github.com/JaneEyre0530/PaTaRM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sparse Optimistic Information Directed Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24234v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24234v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ludovic Schwartz, Hamish Flynn, Gergely Neu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many high-dimensional online decision-making problems can be modeled as
stochastic sparse linear bandits. Most existing algorithms are designed to
achieve optimal worst-case regret in either the data-rich regime, where
polynomial dependence on the ambient dimension is unavoidable, or the data-poor
regime, where dimension-independence is possible at the cost of worse
dependence on the number of rounds. In contrast, the sparse Information
Directed Sampling (IDS) algorithm satisfies a Bayesian regret bound that has
the optimal rate in both regimes simultaneously. In this work, we explore the
use of Sparse Optimistic Information Directed Sampling (SOIDS) to achieve the
same adaptivity in the worst-case setting, without Bayesian assumptions.
Through a novel analysis that enables the use of a time-dependent learning
rate, we show that SOIDS can optimally balance information and regret. Our
results extend the theoretical guarantees of IDS, providing the first algorithm
that simultaneously achieves optimal worst-case regret in both the data-rich
and data-poor regimes. We empirically demonstrate the good performance of
SOIDS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PRIVET: Privacy Metric Based on Extreme Value Theory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24233v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24233v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antoine Szatkownik, Aurélien Decelle, Beatriz Seoane, Nicolas Bereux, Léo Planche, Guillaume Charpiat, Burak Yelmen, Flora Jay, Cyril Furtlehner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep generative models are often trained on sensitive data, such as genetic
sequences, health data, or more broadly, any copyrighted, licensed or protected
content. This raises critical concerns around privacy-preserving synthetic
data, and more specifically around privacy leakage, an issue closely tied to
overfitting. Existing methods almost exclusively rely on global criteria to
estimate the risk of privacy failure associated to a model, offering only
quantitative non interpretable insights. The absence of rigorous evaluation
methods for data privacy at the sample-level may hinder the practical
deployment of synthetic data in real-world applications. Using extreme value
statistics on nearest-neighbor distances, we propose PRIVET, a generic
sample-based, modality-agnostic algorithm that assigns an individual privacy
leak score to each synthetic sample. We empirically demonstrate that PRIVET
reliably detects instances of memorization and privacy leakage across diverse
data modalities, including settings with very high dimensionality, limited
sample sizes such as genetic data and even under underfitting regimes. We
compare our method to existing approaches under controlled settings and show
its advantage in providing both dataset level and sample level assessments
through qualitative and quantitative outputs. Additionally, our analysis
reveals limitations in existing computer vision embeddings to yield
perceptually meaningful distances when identifying near-duplicate samples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A comparison between joint and dual UKF implementations for state
  estimation and leak localization in water distribution networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24228v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24228v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luis Romero-Ben, Paul Irofti, Florin Stoican, Vicenç Puig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The sustainability of modern cities highly depends on efficient water
distribution management, including effective pressure control and leak
detection and localization. Accurate information about the network hydraulic
state is therefore essential. This article presents a comparison between two
data-driven state estimation methods based on the Unscented Kalman Filter
(UKF), fusing pressure, demand and flow data for head and flow estimation. One
approach uses a joint state vector with a single estimator, while the other
uses a dual-estimator scheme. We analyse their main characteristics, discussing
differences, advantages and limitations, and compare them theoretically in
terms of accuracy and complexity. Finally, we show several estimation results
for the L-TOWN benchmark, allowing to discuss their properties in a real
implementation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to ECC2026 for review. It has 7 pages
  and 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Closing Gaps: An Imputation Analysis of ICU Vital Signs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24217v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24217v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alisher Turubayev, Anna Shopova, Fabian Lange, Mahmut Kamalak, Paul Mattes, Victoria Ayvasky, Bert Arnrich, Bjarne Pfitzner, Robin P. van de Water
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As more Intensive Care Unit (ICU) data becomes available, the interest in
developing clinical prediction models to improve healthcare protocols
increases. However, the lack of data quality still hinders clinical prediction
using Machine Learning (ML). Many vital sign measurements, such as heart rate,
contain sizeable missing segments, leaving gaps in the data that could
negatively impact prediction performance. Previous works have introduced
numerous time-series imputation techniques. Nevertheless, more comprehensive
work is needed to compare a representative set of methods for imputing ICU
vital signs and determine the best practice. In reality, ad-hoc imputation
techniques that could decrease prediction accuracy, like zero imputation, are
still used. In this work, we compare established imputation techniques to guide
researchers in improving the performance of clinical prediction models by
selecting the most accurate imputation technique. We introduce an extensible
and reusable benchmark with currently 15 imputation and 4 amputation methods,
created for benchmarking on major ICU datasets. We hope to provide a
comparative basis and facilitate further ML development to bring more models
into clinical practice.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unlocking Out-of-Distribution Generalization in Dynamics through
  Physics-Guided Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24216v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24216v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Xu, Hao Wu, Kun Wang, Nan Wang, Qingsong Wen, Xian Wu, Wei Gong, Xibin Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In dynamical system modeling, traditional numerical methods are limited by
high computational costs, while modern data-driven approaches struggle with
data scarcity and distribution shifts. To address these fundamental
limitations, we first propose SPARK, a physics-guided quantitative augmentation
plugin. Specifically, SPARK utilizes a reconstruction autoencoder to integrate
physical parameters into a physics-rich discrete state dictionary. This state
dictionary then acts as a structured dictionary of physical states, enabling
the creation of new, physically-plausible training samples via principled
interpolation in the latent space. Further, for downstream prediction, these
augmented representations are seamlessly integrated with a Fourier-enhanced
Graph ODE, a combination designed to robustly model the enriched data
distribution while capturing long-term temporal dependencies. Extensive
experiments on diverse benchmarks demonstrate that SPARK significantly
outperforms state-of-the-art baselines, particularly in challenging
out-of-distribution scenarios and data-scarce regimes, proving the efficacy of
our physics-guided augmentation paradigm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What Can Be Recovered Under Sparse Adversarial Corruption?
  Assumption-Free Theory for Linear Measurements 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24215v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24215v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vishal Halder, Alexandre Reiffers-Masson, Abdeldjalil Aïssa-El-Bey, Gugan Thoppe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Let \(\bm{A} \in \mathbb{R}^{m \times n}\) be an arbitrary, known matrix and
\(\bm{e}\) a \(q\)-sparse adversarial vector. Given \(\bm{y} = \bm{A} x^* +
\bm{e}\) and \(q\), we seek the smallest set containing \(x^*\)-hence the one
conveying maximal information about \(x^*\)-that is uniformly recoverable from
\(\bm{y}\) without knowing \(\bm{e}\). While exact recovery of \(x^*\) via
strong (and often impractical) structural assumptions on \(\bm{A}\) or \(x^*\)
(for example, restricted isometry, sparsity) is well studied, recoverability
for arbitrary \(\bm{A}\) and \(x^*\) remains open. Our main result shows that
the best that one can hope to recover is \(x^* + \ker(\bm{U})\), where
\(\bm{U}\) is the unique projection matrix onto the intersection of rowspaces
of all possible submatrices of \(\bm{A}\) obtained by deleting \(2q\) rows.
Moreover, we prove that every \(x\) that minimizes the \(\ell\_0\)-norm of
\(\bm{y} - \bm{A} x\) lies in \(x^* + \ker(\bm{U})\), which then gives a
constructive approach to recover this set.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Neural Incompatibility: Easing Cross-Scale Knowledge Transfer in
  Large Language Models through Latent Semantic Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24208v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24208v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Gu, Aldeida Aleti, Chunyang Chen, Hongyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) encode vast amounts of knowledge in their
massive parameters, which is accessible to locate, trace, and analyze. Despite
advances in neural interpretability, it is still not clear how to transfer
knowledge in a fine-grained manner, namely parametric knowledge transfer (PKT).
A key problem is enabling effective and efficient knowledge transfer across
LLMs of different scales, which is essential for achieving greater flexibility
and broader applicability in transferring knowledge between LLMs. Due to neural
incompatibility, referring to the architectural and parametric differences
between LLMs of varying scales, existing methods that directly reuse layer
parameters are severely limited. In this paper, we identify the semantic
alignment in latent space as the fundamental prerequisite for LLM cross-scale
knowledge transfer. Instead of directly using the layer parameters, our
approach takes activations as the medium of layer-wise knowledge transfer.
Leveraging the semantics in latent space, our approach is simple and
outperforms prior work, better aligning model behaviors across varying scales.
Evaluations on four benchmarks demonstrate the efficacy of our method. Further
analysis reveals the key factors easing cross-scale knowledge transfer and
provides insights into the nature of latent semantic alignment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>an early-stage version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SPEAR++: Scaling Gradient Inversion via Sparsely-Used Dictionary
  Learning <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24200v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24200v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Bakarsky, Dimitar I. Dimitrov, Maximilian Baader, Martin Vechev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning has seen an increased deployment in real-world scenarios
recently, as it enables the distributed training of machine learning models
without explicit data sharing between individual clients. Yet, the introduction
of the so-called gradient inversion attacks has fundamentally challenged its
privacy-preserving properties. Unfortunately, as these attacks mostly rely on
direct data optimization without any formal guarantees, the vulnerability of
real-world systems remains in dispute and requires tedious testing for each new
federated deployment. To overcome these issues, recently the SPEAR attack was
introduced, which is based on a theoretical analysis of the gradients of linear
layers with ReLU activations. While SPEAR is an important theoretical
breakthrough, the attack's practicality was severely limited by its exponential
runtime in the batch size b. In this work, we fill this gap by applying
State-of-the-Art techniques from Sparsely-Used Dictionary Learning to make the
problem of gradient inversion on linear layers with ReLU activations tractable.
Our experiments demonstrate that our new attack, SPEAR++, retains all desirable
properties of SPEAR, such as robustness to DP noise and FedAvg aggregation,
while being applicable to 10x bigger batch sizes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at the Workshop on Regulatable ML at the 39th Conference on
  Neural Information Processing Systems (NeurIPS 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Blindfolded Experts Generalize Better: Insights from Robotic
  Manipulation and Videogames 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24194v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24194v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ev Zisselman, Mirco Mutti, Shelly Francis-Meretzki, Elisei Shafer, Aviv Tamar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Behavioral cloning is a simple yet effective technique for learning
sequential decision-making from demonstrations. Recently, it has gained
prominence as the core of foundation models for the physical world, where
achieving generalization requires countless demonstrations of a multitude of
tasks. Typically, a human expert with full information on the task demonstrates
a (nearly) optimal behavior. In this paper, we propose to hide some of the
task's information from the demonstrator. This ``blindfolded'' expert is
compelled to employ non-trivial exploration to solve the task. We show that
cloning the blindfolded expert generalizes better to unseen tasks than its
fully-informed counterpart. We conduct experiments of real-world robot peg
insertion tasks with (limited) human demonstrations, alongside videogames from
the Procgen benchmark. Additionally, we support our findings with theoretical
analysis, which confirms that the generalization error scales with
$\sqrt{I/m}$, where $I$ measures the amount of task information available to
the demonstrator, and $m$ is the number of demonstrated tasks. Both theory and
practice indicate that cloning blindfolded experts generalizes better with
fewer demonstrated tasks. Project page with videos and code:
https://sites.google.com/view/blindfoldedexperts/home
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Concordant Perturbations for Linear Bandits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24187v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24187v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucas Lévy, Jean-Lou Valeau, Arya Akhavan, Patrick Rebeschini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the adversarial linear bandits problem and present a unified
algorithmic framework that bridges Follow-the-Regularized-Leader (FTRL) and
Follow-the-Perturbed-Leader (FTPL) methods, extending the known connection
between them from the full-information setting. Within this framework, we
introduce self-concordant perturbations, a family of probability distributions
that mirror the role of self-concordant barriers previously employed in the
FTRL-based SCRiBLe algorithm. Using this idea, we design a novel FTPL-based
algorithm that combines self-concordant regularization with efficient
stochastic exploration. Our approach achieves a regret of $O(d\sqrt{n \ln n})$
on both the $d$-dimensional hypercube and the Euclidean ball. On the Euclidean
ball, this matches the rate attained by existing self-concordant FTRL methods.
For the hypercube, this represents a $\sqrt{d}$ improvement over these methods
and matches the optimal bound up to logarithmic factors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ V-SAT: Video Subtitle Annotation Tool 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24180v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24180v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arpita Kundu, Joyita Chakraborty, Anindita Desarkar, Aritra Sen, Srushti Anil Patil, Vishwanathan Raman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The surge of audiovisual content on streaming platforms and social media has
heightened the demand for accurate and accessible subtitles. However, existing
subtitle generation methods primarily speech-based transcription or OCR-based
extraction suffer from several shortcomings, including poor synchronization,
incorrect or harmful text, inconsistent formatting, inappropriate reading
speeds, and the inability to adapt to dynamic audio-visual contexts. Current
approaches often address isolated issues, leaving post-editing as a
labor-intensive and time-consuming process. In this paper, we introduce V-SAT
(Video Subtitle Annotation Tool), a unified framework that automatically
detects and corrects a wide range of subtitle quality issues. By combining
Large Language Models(LLMs), Vision-Language Models (VLMs), Image Processing,
and Automatic Speech Recognition (ASR), V-SAT leverages contextual cues from
both audio and video. Subtitle quality improved, with the SUBER score reduced
from 9.6 to 3.54 after resolving all language mode issues and F1-scores of
~0.80 for image mode issues. Human-in-the-loop validation ensures high-quality
results, providing the first comprehensive solution for robust subtitle
annotation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EddyFormer: Accelerated Neural Simulations of Three-Dimensional
  Turbulence at Scale <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24173v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24173v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiheng Du, Aditi S. Krishnapriyan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computationally resolving turbulence remains a central challenge in fluid
dynamics due to its multi-scale interactions. Fully resolving large-scale
turbulence through direct numerical simulation (DNS) is computationally
prohibitive, motivating data-driven machine learning alternatives. In this
work, we propose EddyFormer, a Transformer-based spectral-element (SEM)
architecture for large-scale turbulence simulation that combines the accuracy
of spectral methods with the scalability of the attention mechanism. We
introduce an SEM tokenization that decomposes the flow into grid-scale and
subgrid-scale components, enabling capture of both local and global features.
We create a new three-dimensional isotropic turbulence dataset and train
EddyFormer to achieves DNS-level accuracy at 256^3 resolution, providing a 30x
speedup over DNS. When applied to unseen domains up to 4x larger than in
training, EddyFormer preserves accuracy on physics-invariant metrics-energy
spectra, correlation functions, and structure functions-showing domain
generalization. On The Well benchmark suite of diverse turbulent flows,
EddyFormer resolves cases where prior ML models fail to converge, accurately
reproducing complex dynamics across a wide range of physical conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Identifiable learning of dissipative dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24160v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24160v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aiqing Zhu, Beatrice W. Soh, Grigorios A. Pavliotis, Qianxiao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Complex dissipative systems appear across science and engineering, from
polymers and active matter to learning algorithms. These systems operate far
from equilibrium, where energy dissipation and time irreversibility are key to
their behavior, but are difficult to quantify from data. Learning accurate and
interpretable models of such dynamics remains a major challenge: the models
must be expressive enough to describe diverse processes, yet constrained enough
to remain physically meaningful and mathematically identifiable. Here, we
introduce I-OnsagerNet, a neural framework that learns dissipative stochastic
dynamics directly from trajectories while ensuring both interpretability and
uniqueness. I-OnsagerNet extends the Onsager principle to guarantee that the
learned potential is obtained from the stationary density and that the drift
decomposes cleanly into time-reversible and time-irreversible components, as
dictated by the Helmholtz decomposition. Our approach enables us to calculate
the entropy production and to quantify irreversibility, offering a principled
way to detect and quantify deviations from equilibrium. Applications to polymer
stretching in elongational flow and to stochastic gradient Langevin dynamics
reveal new insights, including super-linear scaling of barrier heights and
sub-linear scaling of entropy production rates with the strain rate, and the
suppression of irreversibility with increasing batch size. I-OnsagerNet thus
establishes a general, data-driven framework for discovering and interpreting
non-equilibrium dynamics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Self-supervised</span> Synthetic <span class="highlight-title">Pretrain</span>ing for Inference of Stellar Mass
  Embedded in Dense Gas <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24159v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24159v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keiya Hirashima, Shingo Nozaki, Naoto Harada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stellar mass is a fundamental quantity that determines the properties and
evolution of stars. However, estimating stellar masses in star-forming regions
is challenging because young stars are obscured by dense gas and the regions
are highly inhomogeneous, making spherical dynamical estimates unreliable.
Supervised machine learning could link such complex structures to stellar mass,
but it requires large, high-quality labeled datasets from high-resolution
magneto-hydrodynamical (MHD) simulations, which are computationally expensive.
We address this by pretraining a vision transformer on one million synthetic
fractal images using the self-supervised framework DINOv2, and then applying
the frozen model to limited high-resolution MHD simulations. Our results
demonstrate that synthetic pretraining improves frozen-feature regression
stellar mass predictions, with the pretrained model performing slightly better
than a supervised model trained on the same limited simulations. Principal
component analysis of the extracted features further reveals semantically
meaningful structures, suggesting that the model enables unsupervised
segmentation of star-forming regions without the need for labeled data or
fine-tuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 3 figures, 1 table, accepted for NeurIPS 2025 ML4PS workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fixed Point Neural Acceleration and Inverse Surrogate Model for Battery
  Parameter Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24135v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24135v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hojin Cheon, Hyeongseok Seo, Jihun Jeon, Wooju Lee, Dohyun Jeong, Hongseok Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid expansion of electric vehicles has intensified the need for
accurate and efficient diagnosis of lithium-ion batteries. Parameter
identification of electrochemical battery models is widely recognized as a
powerful method for battery health assessment. However, conventional
metaheuristic approaches suffer from high computational cost and slow
convergence, and recent machine learning methods are limited by their reliance
on constant current data, which may not be available in practice. To overcome
these challenges, we propose deep learning-based framework for parameter
identification of electrochemical battery models. The proposed framework
combines a neural surrogate model of the single particle model with electrolyte
(NeuralSPMe) and a deep learning-based fixed-point iteration method. NeuralSPMe
is trained on realistic EV load profiles to accurately predict lithium
concentration dynamics under dynamic operating conditions while a parameter
update network (PUNet) performs fixed-point iterative updates to significantly
reduce both the evaluation time per sample and the overall number of iterations
required for convergence. Experimental evaluations demonstrate that the
proposed framework accelerates the parameter identification by more than 2000
times, achieves superior sample efficiency and more than 10 times higher
accuracy compared to conventional metaheuristic algorithms, particularly under
dynamic load scenarios encountered in practical applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 11 figures, submitted to Applied Energy</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Causal Convolutional Neural Networks as Finite Impulse Response Filters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24125v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24125v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kiran Bacsa, Wei Liu, Xudong Jian, Huangbin Liang, Eleni Chatzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study investigates the behavior of Causal Convolutional Neural Networks
(CNNs) with quasi-linear activation functions when applied to time-series data
characterized by multimodal frequency content. We demonstrate that, once
trained, such networks exhibit properties analogous to Finite Impulse Response
(FIR) filters, particularly when the convolutional kernels are of extended
length exceeding those typically employed in standard CNN architectures. Causal
CNNs are shown to capture spectral features both implicitly and explicitly,
offering enhanced interpretability for tasks involving dynamic systems.
Leveraging the associative property of convolution, we further show that the
entire network can be reduced to an equivalent single-layer filter resembling
an FIR filter optimized via least-squares criteria. This equivalence yields new
insights into the spectral learning behavior of CNNs trained on signals with
sparse frequency content. The approach is validated on both simulated beam
dynamics and real-world bridge vibration datasets, underlining its relevance
for modeling and identifying physical systems governed by dynamic responses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 19 figures, Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph-Guided Concept Selection for Efficient Retrieval-Augmented
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24120v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24120v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyu Liu, Yijing Liu, Jianfei Yuan, Minzhi Yan, Le Yue, Honghui Xiong, Yi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph-based RAG constructs a knowledge graph (KG) from text chunks to enhance
retrieval in Large Language Model (LLM)-based question answering. It is
especially beneficial in domains such as biomedicine, law, and political
science, where effective retrieval often involves multi-hop reasoning over
proprietary documents. However, these methods demand numerous LLM calls to
extract entities and relations from text chunks, incurring prohibitive costs at
scale. Through a carefully designed ablation study, we observe that certain
words (termed concepts) and their associated documents are more important.
Based on this insight, we propose Graph-Guided Concept Selection (G2ConS). Its
core comprises a chunk selection method and an LLM-independent concept graph.
The former selects salient document chunks to reduce KG construction costs; the
latter closes knowledge gaps introduced by chunk selection at zero cost.
Evaluations on multiple real-world datasets show that G2ConS outperforms all
baselines in construction cost, retrieval effectiveness, and answering quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HistoLens: An Interactive XAI Toolkit for Verifying and Mitigating Flaws
  in Vision-Language Models for Histopathology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24115v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24115v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sandeep Vissapragada, Vikrant Sahu, Gagan Raj Gupta, Vandita Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For doctors to truly trust artificial intelligence, it can't be a black box.
They need to understand its reasoning, almost as if they were consulting a
colleague. We created HistoLens1 to be that transparent, collaborative partner.
It allows a pathologist to simply ask a question in plain English about a
tissue slide--just as they would ask a trainee. Our system intelligently
translates this question into a precise query for its AI engine, which then
provides a clear, structured report. But it doesn't stop there. If a doctor
ever asks, "Why?", HistoLens can instantly provide a 'visual proof' for any
finding--a heatmap that points to the exact cells and regions the AI used for
its analysis. We've also ensured the AI focuses only on the patient's tissue,
just like a trained pathologist would, by teaching it to ignore distracting
background noise. The result is a workflow where the pathologist remains the
expert in charge, using a trustworthy AI assistant to verify their insights and
make faster, more confident diagnoses.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Taming the Tail: NoI Topology Synthesis for Mixed DL Workloads on
  Chiplet-Based Accelerators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24113v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24113v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arnav Shukla, Harsh Sharma, Srikant Bharadwaj, Vinayak Abrol, Sujay Deb
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Heterogeneous chiplet-based systems improve scaling by disag-gregating
CPUs/GPUs and emerging technologies (HBM/DRAM).However this on-package
disaggregation introduces a latency inNetwork-on-Interposer(NoI). We observe
that in modern large-modelinference, parameters and activations routinely move
backand forth from HBM/DRAM, injecting large, bursty flows into theinterposer.
These memory-driven transfers inflate tail latency andviolate Service Level
Agreements (SLAs) across k-ary n-cube base-line NoI topologies. To address this
gap we introduce an InterferenceScore (IS) that quantifies worst-case slowdown
under contention.We then formulate NoI synthesis as a multi-objective
optimization(MOO) problem. We develop PARL (Partition-Aware
ReinforcementLearner), a topology generator that balances throughput,
latency,and power. PARL-generated topologies reduce contention at the memory
cut, meet SLAs, and cut worst-case slowdown to 1.2 times while maintaining
competitive mean throughput relative to link-rich meshes. Overall, this
reframes NoI design for heterogeneouschiplet accelerators with workload-aware
objectives.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing <span class="highlight-title">Pre-train</span>ed Representation Classifiability can Boost its
  Interpretability <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24105v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24105v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shufan Shen, Zhaobo Qi, Junshu Sun, Qingming Huang, Qi Tian, Shuhui Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The visual representation of a pre-trained model prioritizes the
classifiability on downstream tasks, while the widespread applications for
pre-trained visual models have posed new requirements for representation
interpretability. However, it remains unclear whether the pre-trained
representations can achieve high interpretability and classifiability
simultaneously. To answer this question, we quantify the representation
interpretability by leveraging its correlation with the ratio of interpretable
semantics within the representations. Given the pre-trained representations,
only the interpretable semantics can be captured by interpretations, whereas
the uninterpretable part leads to information loss. Based on this fact, we
propose the Inherent Interpretability Score (IIS) that evaluates the
information loss, measures the ratio of interpretable semantics, and quantifies
the representation interpretability. In the evaluation of the representation
interpretability with different classifiability, we surprisingly discover that
the interpretability and classifiability are positively correlated, i.e.,
representations with higher classifiability provide more interpretable
semantics that can be captured in the interpretations. This observation further
supports two benefits to the pre-trained representations. First, the
classifiability of representations can be further improved by fine-tuning with
interpretability maximization. Second, with the classifiability improvement for
the representations, we obtain predictions based on their interpretations with
less accuracy degradation. The discovered positive correlation and
corresponding applications show that practitioners can unify the improvements
in interpretability and classifiability for pre-trained vision models. Codes
are available at https://github.com/ssfgunner/IIS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025 (Spotlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Parameterized Skills from Demonstrations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24095v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24095v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vedant Gupta, Haotian Fu, Calvin Luo, Yiding Jiang, George Konidaris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present DEPS, an end-to-end algorithm for discovering parameterized skills
from expert demonstrations. Our method learns parameterized skill policies
jointly with a meta-policy that selects the appropriate discrete skill and
continuous parameters at each timestep. Using a combination of temporal
variational inference and information-theoretic regularization methods, we
address the challenge of degeneracy common in latent variable models, ensuring
that the learned skills are temporally extended, semantically meaningful, and
adaptable. We empirically show that learning parameterized skills from
multitask expert demonstrations significantly improves generalization to unseen
tasks. Our method outperforms multitask as well as skill learning baselines on
both LIBERO and MetaWorld benchmarks. We also demonstrate that DEPS discovers
interpretable parameterized skills, such as an object grasping skill whose
continuous arguments define the grasp location.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Neurips 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Information-Theoretic Discrete Diffusion <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24088v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24088v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moongyu Jeon, Sangwoo Shin, Dongjae Jeon, Albert No
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present an information-theoretic framework for discrete diffusion models
that yields principled estimators of log-likelihood using score-matching
losses. Inspired by the I-MMSE identity for the Gaussian setup, we derive
analogous results for the discrete setting. Specifically, we introduce the
Information-Minimum Denoising Score Entropy (I-MDSE) relation, which links
mutual information between data and its diffused version to the minimum
denoising score entropy (DSE) loss. We extend this theory to masked diffusion
and establish the Information-Minimum Denoising Cross-Entropy (I-MDCE)
relation, connecting cross-entropy losses to mutual information in discrete
masked processes. These results provide a time-integral decomposition of the
log-likelihood of the data in terms of optimal score-based losses, showing that
commonly used losses such as DSE and DCE are not merely variational bounds but
tight and principled estimators of log-likelihood. The I-MDCE decomposition
further enables practical extensions, including time-free formula, conditional
likelihood estimation in prompt-response tasks, and coupled Monte Carlo
estimation of likelihood ratios. Experiments on synthetic and real-world data
confirm the accuracy, variance stability, and utility of our estimators. The
code is publicly available at https://github.com/Dongjae0324/infodis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Physics-Informed Latent Neural Operator for Real-time Predictions of
  time-dependent parametric PDEs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08428v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08428v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sharmila Karumuri, Lori Graham-Brady, Somdatta Goswami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep operator network (DeepONet) has shown significant promise as surrogate
models for systems governed by partial differential equations (PDEs), enabling
accurate mappings between infinite-dimensional function spaces. However, when
applied to systems with high-dimensional input-output mappings arising from
large numbers of spatial and temporal collocation points, these models often
require heavily overparameterized networks, leading to long training times.
Latent DeepONet addresses some of these challenges by introducing a two-step
approach: first learning a reduced latent space using a separate model,
followed by operator learning within this latent space. While efficient, this
method is inherently data-driven and lacks mechanisms for incorporating
physical laws, limiting its robustness and generalizability in data-scarce
settings. In this work, we propose PI-Latent-NO, a physics-informed latent
neural operator framework that integrates governing physics directly into the
learning process. Our architecture features two coupled DeepONets trained
end-to-end: a Latent-DeepONet that learns a low-dimensional representation of
the solution, and a Reconstruction-DeepONet that maps this latent
representation back to the physical space. By embedding PDE constraints into
the training via automatic differentiation, our method eliminates the need for
labeled training data and ensures physics-consistent predictions. The proposed
framework is both memory and compute-efficient, exhibiting near-constant
scaling with problem size and demonstrating significant speedups over
traditional physics-informed operator models. We validate our approach on a
range of parametric PDEs, showcasing its accuracy, scalability, and suitability
for real-time prediction in complex physical systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeltaPhi: Physical States Residual Learning for Neural Operators in
  Data-Limited PDE Solving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09795v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09795v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xihang Yue, Yi Yang, Linchao Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The limited availability of high-quality training data poses a major obstacle
in data-driven PDE solving, where expensive data collection and resolution
constraints severely impact the ability of neural operator networks to learn
and generalize the underlying physical system. To address this challenge, we
propose DeltaPhi, a novel learning framework that transforms the PDE solving
task from learning direct input-output mappings to learning the residuals
between similar physical states, a fundamentally different approach to neural
operator learning. This reformulation provides implicit data augmentation by
exploiting the inherent stability of physical systems where closer initial
states lead to closer evolution trajectories. DeltaPhi is architecture-agnostic
and can be seamlessly integrated with existing neural operators to enhance
their performance. Extensive experiments demonstrate consistent and significant
improvements across diverse physical systems including regular and irregular
domains, different neural architectures, multiple training data amount, and
cross-resolution scenarios, confirming its effectiveness as a general
enhancement for neural operators in data-limited PDE solving.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Neurips 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Datasheets for Machine Learning Sensors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.08848v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.08848v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Stewart, Yuke Zhang, Pete Warden, Yasmine Omri, Shvetank Prakash, Jacob Huckelberry, Joao Henrique Santos, Shawn Hymel, Benjamin Yeager Brown, Jim MacArthur, Nat Jeffries, Emanuel Moss, Mona Sloane, Brian Plancher, Vijay Janapa Reddi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning (ML) is becoming prevalent in embedded AI sensing systems.
These "ML sensors" enable context-sensitive, real-time data collection and
decision-making across diverse applications ranging from anomaly detection in
industrial settings to wildlife tracking for conservation efforts. As such,
there is a need to provide transparency in the operation of such ML-enabled
sensing systems through comprehensive documentation. This is needed to enable
their reproducibility, to address new compliance and auditing regimes mandated
in regulation and industry-specific policy, and to verify and validate the
responsible nature of their operation. To address this gap, we introduce the
datasheet for ML sensors framework. We provide a comprehensive template,
collaboratively developed in academia-industry partnerships, that captures the
distinct attributes of ML sensors, including hardware specifications, ML model
and dataset characteristics, end-to-end performance metrics, and environmental
impacts. Our framework addresses the continuous streaming nature of sensor
data, real-time processing requirements, and embeds benchmarking methodologies
that reflect real-world deployment conditions, ensuring practical viability.
Aligned with the FAIR principles (Findability, Accessibility, Interoperability,
and Reusability), our approach enhances the transparency and reusability of ML
sensor documentation across academic, industrial, and regulatory domains. To
show the application of our approach, we present two datasheets: the first for
an open-source ML sensor designed in-house and the second for a commercial ML
sensor developed by industry collaborators, both performing computer
vision-based person detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ADMN: A Layer-Wise Adaptive Multimodal Network for Dynamic Input Noise
  and Compute Resources 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07862v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07862v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jason Wu, Yuyang Yuan, Kang Yang, Lance Kaplan, Mani Srivastava
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal deep learning systems are deployed in dynamic scenarios due to the
robustness afforded by multiple sensing modalities. Nevertheless, they struggle
with varying compute resource availability (due to multi-tenancy, device
heterogeneity, etc.) and fluctuating quality of inputs (from sensor feed
corruption, environmental noise, etc.). Statically provisioned multimodal
systems cannot adapt when compute resources change over time, while existing
dynamic networks struggle with strict compute budgets. Additionally, both
systems often neglect the impact of variations in modality quality.
Consequently, modalities suffering substantial corruption may needlessly
consume resources better allocated towards other modalities. We propose ADMN, a
layer-wise Adaptive Depth Multimodal Network capable of tackling both
challenges: it adjusts the total number of active layers across all modalities
to meet strict compute resource constraints and continually reallocates layers
across input modalities according to their modality quality. Our evaluations
showcase ADMN can match the accuracy of state-of-the-art networks while
reducing up to 75% of their floating-point operations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Neurips 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SGFusion: Stochastic Geographic Gradient Fusion in Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23455v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23455v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khoa Nguyen, Khang Tran, NhatHai Phan, Cristian Borcea, Rouming Jin, Issa Khalil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes Stochastic Geographic Gradient Fusion (SGFusion), a novel
training algorithm to leverage the geographic information of mobile users in
Federated Learning (FL). SGFusion maps the data collected by mobile devices
onto geographical zones and trains one FL model per zone, which adapts well to
the data and behaviors of users in that zone. SGFusion models the local
data-based correlation among geographical zones as a hierarchical random graph
(HRG) optimized by Markov Chain Monte Carlo sampling. At each training step,
every zone fuses its local gradient with gradients derived from a small set of
other zones sampled from the HRG. This approach enables knowledge fusion and
sharing among geographical zones in a probabilistic and stochastic gradient
fusion process with self-attention weights, such that "more similar" zones have
"higher probabilities" of sharing gradients with "larger attention weights."
SGFusion remarkably improves model utility without introducing undue
computational cost. Extensive theoretical and empirical results using a
heart-rate prediction dataset collected across 6 countries show that models
trained with SGFusion converge with upper-bounded expected errors and
significantly improve utility in all countries compared to existing approaches
without notable cost in system scalability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hybrid Deep Learning Model to Estimate Cognitive Effort from fNIRS
  Signals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.13883v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.13883v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shayla Sharmin, Roghayeh Leila Barmaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study estimates cognitive effort based on functional near-infrared
spectroscopy data and performance scores using a hybrid DeepNet model. The
estimation of cognitive effort enables educators to modify material to enhance
learning effectiveness and student engagement. In this study, we collected
oxygenated hemoglobin using functional near-infrared spectroscopy during an
educational quiz game. Participants (n=16) responded to 16 questions in a
Unity-based educational game, each within a 30-second response time limit. We
used DeepNet models to predict the performance score from the oxygenated
hemoglobin, and compared traditional machine learning and DeepNet models to
determine which approach provides better accuracy in predicting performance
scores. The result shows that the proposed CNN-GRU gives better performance
with 73% than other models. After the prediction, we used the predicted score
and the oxygenated hemoglobin to observe cognitive effort by calculating
relative neural efficiency and involvement in our test cases. Our result shows
that even with moderate accuracy, the predicted cognitive effort closely follow
the actual trends. This findings can be helpful in designing and improving
learning environments and provide valuable insights into learning materials.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Global Optimization of Gaussian Process Acquisition Functions Using a
  Piecewise-Linear Kernel Approximation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16893v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16893v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilin Xie, Shiqiang Zhang, Joel A. Paulson, Calvin Tsay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian optimization relies on iteratively constructing and optimizing an
acquisition function. The latter turns out to be a challenging, non-convex
optimization problem itself. Despite the relative importance of this step, most
algorithms employ sampling- or gradient-based methods, which do not provably
converge to global optima. This work investigates mixed-integer programming
(MIP) as a paradigm for global acquisition function optimization. Specifically,
our Piecewise-linear Kernel Mixed Integer Quadratic Programming (PK-MIQP)
formulation introduces a piecewise-linear approximation for Gaussian process
kernels and admits a corresponding MIQP representation for acquisition
functions. The proposed method is applicable to uncertainty-based acquisition
functions for any stationary or dot-product kernel. We analyze the theoretical
regret bounds of the proposed approximation, and empirically demonstrate the
framework on synthetic functions, constrained benchmarks, and a hyperparameter
tuning task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 4 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Says Who? Effective Zero-Shot Annotation of Focalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11390v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11390v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rebecca M. M. Hicke, Yuri Bizzoni, Pascale Feldkamp, Ross Deans Kristensen-McLachlan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Focalization describes the way in which access to narrative information is
restricted or controlled based on the knowledge available to knowledge of the
narrator. It is encoded via a wide range of lexico-grammatical features and is
subject to reader interpretation. Even trained annotators frequently disagree
on correct labels, suggesting this task is both qualitatively and
computationally challenging. In this work, we test how well five contemporary
large language model (LLM) families and two baselines perform when annotating
short literary excerpts for focalization. Despite the challenging nature of the
task, we find that LLMs show comparable performance to trained human
annotators, with GPT-4o achieving an average F1 of 84.79%. Further, we
demonstrate that the log probabilities output by GPT-family models frequently
reflect the difficulty of annotating particular excerpts. Finally, we provide a
case study analyzing sixteen Stephen King novels, demonstrating the usefulness
of this approach for computational literary studies and the insights gleaned
from examining focalization at scale.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CHR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TableTime: Reformulating Time Series Classification as Training-Free
  Table Understanding with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.15737v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.15737v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Wang, Mingyue Cheng, Qingyang Mao, Yitong Zhou, Daoyu Wang, Qi Liu, Feiyang Xu, Xin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated their effectiveness in
multivariate time series classification (MTSC). Effective adaptation of LLMs
for MTSC necessitates informative data representations. Existing LLM-based
methods directly encode embeddings for time series within the latent space of
LLMs from scratch to align with semantic space of LLMs. Despite their
effectiveness, we reveal that these methods conceal three inherent bottlenecks:
(1) they struggle to encode temporal and channel-specific information in a
lossless manner, both of which are critical components of multivariate time
series; (2) it is much difficult to align the learned representation space with
the semantic space of the LLMs; (3) they require task-specific retraining,
which is both computationally expensive and labor-intensive. To bridge these
gaps, we propose TableTime, which reformulates MTSC as a table understanding
task. Specifically, TableTime introduces the following strategies: (1) convert
multivariate time series into a tabular form, thus minimizing information loss
to the greatest extent; (2) represent tabular time series in text format to
achieve natural alignment with the semantic space of LLMs; (3) design a
reasoning framework that integrates contextual text information, neighborhood
assistance, multi-path inference and problem decomposition to enhance the
reasoning ability of LLMs and realize zero-shot classification. Extensive
experiments performed on 10 publicly representative datasets from UEA archive
verify the superiorities of the TableTime.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FedMAP: Personalised Federated Learning for Real Large-Scale Healthcare
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19000v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19000v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Zhang, Daniel Kreuter, Carlos Esteve-Yagüe, Sören Dittmer, Javier Fernandez-Marques, Samantha Ip, BloodCounts! Consortium, Norbert C. J. de Wit, Angela Wood, James HF Rudd, Nicholas Lane, Nicholas S Gleadall, Carola-Bibiane Schönlieb, Michael Roberts
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) promises to enable collaborative machine learning
across healthcare sites whilst preserving data privacy. Practical deployment
remains limited by statistical heterogeneity arising from differences in
patient demographics, treatments, and outcomes, and infrastructure constraints.
We introduce FedMAP, a personalised FL (PFL) framework that addresses
heterogeneity through local Maximum a Posteriori (MAP) estimation with Input
Convex Neural Network priors. These priors represent global knowledge gathered
from other sites that guides the model while adapting to local data, and we
provide a formal proof of convergence. Unlike many PFL methods that rely on
fixed regularisation, FedMAP's prior adaptively learns patterns that capture
complex inter-site relationships. We demonstrate improved performance compared
to local training, FedAvg, and several PFL methods across three large-scale
clinical datasets: 10-year cardiovascular risk prediction (CPRD, 387 general
practitioner practices, 258,688 patients), iron deficiency detection (INTERVAL,
4 donor centres, 31,949 blood donors), and mortality prediction (eICU, 150
hospitals, 44,842 patients). FedMAP incorporates a three-tier design that
enables participation across healthcare sites with varying infrastructure and
technical capabilities, from full federated training to inference-only
deployment. Geographical analysis reveals substantial equity improvements, with
underperforming regions achieving up to 14.3% performance gains. This framework
provides the first practical pathway for large-scale healthcare FL deployment,
which ensures clinical sites at all scales can benefit, equity is enhanced, and
privacy is retained.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GST-UNet: A Neural Framework for Spatiotemporal Causal Inference with
  Time-Varying Confounding <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.05295v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.05295v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miruna Oprescu, David K. Park, Xihaier Luo, Shinjae Yoo, Nathan Kallus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating causal effects from spatiotemporal observational data is essential
in public health, environmental science, and policy evaluation, where
randomized experiments are often infeasible. Existing approaches, however,
either rely on strong structural assumptions or fail to handle key challenges
such as interference, spatial confounding, temporal carryover, and time-varying
confounding -- where covariates are influenced by past treatments and, in turn,
affect future ones. We introduce GST-UNet (G-computation Spatio-Temporal UNet),
a theoretically grounded neural framework that combines a U-Net-based
spatiotemporal encoder with regression-based iterative G-computation to
estimate location-specific potential outcomes under complex intervention
sequences. GST-UNet explicitly adjusts for time-varying confounders and
captures non-linear spatial and temporal dependencies, enabling valid causal
inference from a single observed trajectory in data-scarce settings. We
validate its effectiveness in synthetic experiments and in a real-world
analysis of wildfire smoke exposure and respiratory hospitalizations during the
2018 California Camp Fire. Together, these results position GST-UNet as a
principled and ready-to-use framework for spatiotemporal causal inference,
advancing reliable estimation in policy-relevant and scientific domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 6 figures, 6 tables, NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Anomaly Detection in Network Flows with Low-Rank Tensor
  Decompositions and Deep Unrolling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11529v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11529v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Schynol, Marius Pesavento
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anomaly detection (AD) is increasingly recognized as a key component for
ensuring the resilience of future communication systems. While deep learning
has shown state-of-the-art AD performance, its application in critical systems
is hindered by concerns regarding training data efficiency, domain adaptation
and interpretability. This work considers AD in network flows using incomplete
measurements, leveraging a robust tensor decomposition approach and deep
unrolling techniques to address these challenges. We first propose a novel
block-successive convex approximation algorithm based on a regularized
model-fitting objective where the normal flows are modeled as low-rank tensors
and anomalies as sparse. An augmentation of the objective is introduced to
decrease the computational cost. We apply deep unrolling to derive a novel deep
network architecture based on our proposed algorithm, treating the
regularization parameters as learnable weights. Inspired by Bayesian
approaches, we extend the model architecture to perform online adaptation to
per-flow and per-time-step statistics, improving AD performance while
maintaining a low parameter count and preserving the problem's permutation
equivariances. To optimize the deep network weights for detection performance,
we employ a homotopy optimization approach based on an efficient approximation
of the area under the receiver operating characteristic curve. Extensive
experiments on synthetic and real-world data demonstrate that our proposed deep
network architecture exhibits a high training data efficiency, outperforms
reference methods, and adapts seamlessly to varying network topologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Uncertainty Quantification for Self-Evolving Large Language
  Models via Continual Domain <span class="highlight-title">Pretrain</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.22931v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.22931v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaofan Zhou, Lu Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual Learning (CL) is essential for enabling self-evolving large
language models (LLMs) to adapt and remain effective amid rapid knowledge
growth. Yet, despite its importance, little attention has been given to
establishing statistical reliability guarantees for LLMs under CL, particularly
in the setting of continual domain pretraining (CDP). Conformal Prediction (CP)
has shown promise in offering correctness guarantees for LLMs, but it faces
major challenges in CDP: testing data often stems from unknown or shifting
domain distributions, under which CP may no longer provide valid guarantees.
Moreover, when high coverage is required, CP can yield excessively large
prediction sets for unanswerable queries, reducing informativeness. To address
these challenges, we introduce an adaptive rejection and non-exchangeable CP
framework. Our method first estimates the distribution of questions across
domains in the test set using transformer-based clustering, then reweights or
resamples the calibration data accordingly. Building on this, adaptive
rejection CP allows the LLM to selectively abstain from answering when its
confidence or competence shifts significantly. Extensive experiments
demonstrate that our framework enhances both the effectiveness and reliability
of CP under CDP scenarios. Our code is available at:
https://anonymous.4open.science/r/CPCL-8C12/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GraSS: Scalable Data Attribution with Gradient Sparsification and Sparse
  Projection <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.18976v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.18976v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pingbang Hu, Joseph Melkonian, Weijing Tang, Han Zhao, Jiaqi W. Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gradient-based data attribution methods, such as influence functions, are
critical for understanding the impact of individual training samples without
requiring repeated model retraining. However, their scalability is often
limited by the high computational and memory costs associated with per-sample
gradient computation. In this work, we propose GraSS, a novel gradient
compression algorithm and its variants FactGraSS for linear layers
specifically, that explicitly leverage the inherent sparsity of per-sample
gradients to achieve sub-linear space and time complexity. Extensive
experiments demonstrate the effectiveness of our approach, achieving
substantial speedups while preserving data influence fidelity. In particular,
FactGraSS achieves up to 165% faster throughput on billion-scale models
compared to the previous state-of-the-art baselines. Our code is publicly
available at https://github.com/TRAIS-Lab/GraSS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 39th Conference on Neural Information Processing
  Systems (NeurIPS 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Online (Non-)Convex Learning via Tempered Optimism 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07530v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07530v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maxime Haddouche, Olivier Wintenberger, Benjamin Guedj
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimistic Online Learning aims to exploit experts conveying reliable
information to predict the future. However, such implicit optimism may be
challenged when it comes to practical crafting of such experts. A fundamental
example consists in approximating a minimiser of the current problem and use it
as expert. In the context of dynamic environments, such an expert only conveys
partially relevant information as it may lead to overfitting. To tackle this
issue, we introduce in this work the \emph{optimistically tempered} (OT) online
learning framework designed to handle such imperfect experts. As a first
contribution, we show that tempered optimism is a fruitful paradigm for Online
Non-Convex Learning by proposing simple, yet powerful modification of Online
Gradient and Mirror Descent. Second, we derive a second OT algorithm for convex
losses and third, evaluate the practical efficiency of tempered optimism on
real-life datasets and a toy experiment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Real Unsupervised Anomaly Detection Via Confident Meta-Learning <span class="chip">ICCV2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02293v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02293v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Aqeel, Shakiba Sharifi, Marco Cristani, Francesco Setti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  So-called unsupervised anomaly detection is better described as
semi-supervised, as it assumes all training data are nominal. This assumption
simplifies training but requires manual data curation, introducing bias and
limiting adaptability. We propose Confident Meta-learning (CoMet), a novel
training strategy that enables deep anomaly detection models to learn from
uncurated datasets where nominal and anomalous samples coexist, eliminating the
need for explicit filtering. Our approach integrates Soft Confident Learning,
which assigns lower weights to low-confidence samples, and Meta-Learning, which
stabilizes training by regularizing updates based on training validation loss
covariance. This prevents overfitting and enhances robustness to noisy data.
CoMet is model-agnostic and can be applied to any anomaly detection method
trainable via gradient descent. Experiments on MVTec-AD, VIADUCT, and KSDD2
with two state-of-the-art models demonstrate the effectiveness of our approach,
consistently improving over the baseline methods, remaining insensitive to
anomalies in the training set, and setting a new state-of-the-art across all
datasets. Code is available at https://github.com/aqeeelmirza/CoMet
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE/CVF International Conference on Computer Vision
  (ICCV2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ $β$-DQN: Improving Deep Q-Learning By Evolving the Behavior 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.00913v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.00913v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongming Zhang, Fengshuo Bai, Chenjun Xiao, Chao Gao, Bo Xu, Martin Müller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While many sophisticated exploration methods have been proposed, their lack
of generality and high computational cost often lead researchers to favor
simpler methods like $\epsilon$-greedy. Motivated by this, we introduce
$\beta$-DQN, a simple and efficient exploration method that augments the
standard DQN with a behavior function $\beta$. This function estimates the
probability that each action has been taken at each state. By leveraging
$\beta$, we generate a population of diverse policies that balance exploration
between state-action coverage and overestimation bias correction. An adaptive
meta-controller is designed to select an effective policy for each episode,
enabling flexible and explainable exploration. $\beta$-DQN is straightforward
to implement and adds minimal computational overhead to the standard DQN.
Experiments on both simple and challenging exploration domains show that
$\beta$-DQN outperforms existing baseline methods across a wide range of tasks,
providing an effective solution for improving exploration in deep reinforcement
learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>aamas 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Uni-LoRA: One Vector is All You Need <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.00799v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.00799v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaiyang Li, Shaobo Han, Qing Su, Wei Li, Zhipeng Cai, Shihao Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-Rank Adaptation (LoRA) has become the de facto parameter-efficient
fine-tuning (PEFT) method for large language models (LLMs) by constraining
weight updates to low-rank matrices. Recent works such as Tied-LoRA, VeRA, and
VB-LoRA push efficiency further by introducing additional constraints to reduce
the trainable parameter space. In this paper, we show that the parameter space
reduction strategies employed by these LoRA variants can be formulated within a
unified framework, Uni-LoRA, where the LoRA parameter space, flattened as a
high-dimensional vector space $R^D$, can be reconstructed through a projection
from a subspace R^d, with $d \ll D$. We demonstrate that the fundamental
difference among various LoRA methods lies in the choice of the projection
matrix, $P \in R^{D \times d}$.Most existing LoRA variants rely on layer-wise
or structure-specific projections that limit cross-layer parameter sharing,
thereby compromising parameter efficiency. In light of this, we introduce an
efficient and theoretically grounded projection matrix that is isometric,
enabling global parameter sharing and reducing computation overhead.
Furthermore, under the unified view of Uni-LoRA, this design requires only a
single trainable vector to reconstruct LoRA parameters for the entire LLM -
making Uni-LoRA both a unified framework and a "one-vector-only" solution.
Extensive experiments on GLUE, mathematical reasoning, and instruction tuning
benchmarks demonstrate that Uni-LoRA achieves state-of-the-art parameter
efficiency while outperforming or matching prior approaches in predictive
performance. Our code is available at
https://github.com/KaiyangLi1992/Uni-LoRA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025 Spotlight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Group-in-Group Policy Optimization for LLM Agent Training <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10978v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10978v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lang Feng, Zhenghai Xue, Tingcong Liu, Bo An
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in group-based reinforcement learning (RL) have driven
frontier large language models (LLMs) in single-turn tasks like mathematical
reasoning. However, their scalability to multi-turn LLM agent training remains
limited. Unlike static tasks, agent-environment interactions unfold over many
steps and often yield sparse or delayed rewards, making credit assignment
across individual steps significantly more challenging. In this work, we
propose Group-in-Group Policy Optimization (GiGPO), a novel RL algorithm that
achieves fine-grained credit assignment for LLM agents while preserving the
appealing properties of group-based RL: critic-free, low memory, and stable
convergence. GiGPO introduces a two-level structure for estimating relative
advantage: (i) At the episode-level, GiGPO computes macro relative advantages
based on groups of complete trajectories; (ii) At the step-level, GiGPO
introduces an anchor state grouping mechanism that retroactively constructs
step-level groups by identifying repeated environment states across
trajectories. Actions stemming from the same state are grouped together,
enabling micro relative advantage estimation. This hierarchical structure
effectively captures both global trajectory quality and local step
effectiveness without relying on auxiliary models or additional rollouts. We
evaluate GiGPO on challenging agent benchmarks, including ALFWorld and WebShop,
as well as tool-integrated reasoning on search-augmented QA tasks, using
Qwen2.5-1.5B/3B/7B-Instruct. Crucially, GiGPO delivers fine-grained per-step
credit signals, achieves performance gains of > 12% on ALFWorld and > 9% on
WebShop over GRPO, and obtains superior performance on QA tasks (42.1% on 3B
and 47.2% on 7B): all while maintaining the same GPU memory overhead, identical
LLM rollout, and incurring little to no additional time cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TIDMAD: Time Series <span class="highlight-title">Dataset</span> for Discovering Dark Matter with AI
  Denoising <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04378v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04378v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        J. T. Fry, Xinyi Hope Fu, Zhenghao Fu, Kaliroe M. W. Pappas, Lindley Winslow, Aobo Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dark matter makes up approximately 85% of total matter in our universe, yet
it has never been directly observed in any laboratory on Earth. The origin of
dark matter is one of the most important questions in contemporary physics, and
a convincing detection of dark matter would be a Nobel-Prize-level breakthrough
in fundamental science. The ABRACADABRA experiment was specifically designed to
search for dark matter. Although it has not yet made a discovery, ABRACADABRA
has produced several dark matter search results widely endorsed by the physics
community. The experiment generates ultra-long time-series data at a rate of 10
million samples per second, where the dark matter signal would manifest itself
as a sinusoidal oscillation mode within the ultra-long time series. In this
paper, we present the TIDMAD -- a comprehensive data release from the
ABRACADABRA experiment including three key components: an ultra-long time
series dataset divided into training, validation, and science subsets; a
carefully-designed denoising score for direct model benchmarking; and a
complete analysis framework which produces a community-standard dark matter
search result suitable for publication as a physics paper. This data release
enables core AI algorithms to extract the dark matter signal and produce real
physics results thereby advancing fundamental science. The data downloading and
associated analysis scripts are available at
https://github.com/jessicafry/TIDMAD
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2025 (Spotlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mirror Descent and Novel Exponentiated Gradient Algorithms Using
  Trace-Form Entropies and Deformed Logarithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.08748v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.08748v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrzej Cichocki, Toshihisa Tanaka, Frank Nielsen, Sergio Cruces
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a broad class of Mirror Descent (MD) and Generalized
Exponentiated Gradient (GEG) algorithms derived from trace-form entropies
defined via deformed logarithms. Leveraging these generalized entropies yields
MD \& GEG algorithms with improved convergence behavior, robustness to
vanishing and exploding gradients, and inherent adaptability to non-Euclidean
geometries through mirror maps. We establish deep connections between these
methods and Amari's natural gradient, revealing a unified geometric foundation
for additive, multiplicative, and natural gradient updates. Focusing on the
Tsallis, Kaniadakis, Sharma--Taneja--Mittal, and Kaniadakis--Lissia--Scarfone
entropy families, we show that each entropy induces a distinct Riemannian
metric on the parameter space, leading to GEG algorithms that preserve the
natural statistical geometry. The tunable parameters of deformed logarithms
enable adaptive geometric selection, providing enhanced robustness and
convergence over classical Euclidean optimization. Overall, our framework
unifies key first-order MD optimization methods under a single
information-geometric perspective based on generalized Bregman divergences,
where the choice of entropy determines the underlying metric and dual geometric
structure.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multimodal Dreaming: A Global Workspace Approach to World Model-Based
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.21142v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.21142v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Léopold Maytié, Roland Bertin Johannet, Rufin VanRullen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans leverage rich internal models of the world to reason about the future,
imagine counterfactuals, and adapt flexibly to new situations. In Reinforcement
Learning (RL), world models aim to capture how the environment evolves in
response to the agent's actions, facilitating planning and generalization.
However, typical world models directly operate on the environment variables
(e.g. pixels, physical attributes), which can make their training slow and
cumbersome; instead, it may be advantageous to rely on high-level latent
dimensions that capture relevant multimodal variables. Global Workspace (GW)
Theory offers a cognitive framework for multimodal integration and information
broadcasting in the brain, and recent studies have begun to introduce efficient
deep learning implementations of GW. Here, we evaluate the capabilities of an
RL system combining GW with a world model. We compare our GW-Dreamer with
various versions of the standard PPO and the original Dreamer algorithms. We
show that performing the dreaming process (i.e., mental simulation) inside the
GW latent space allows for training with fewer environment steps. As an
additional emergent property, the resulting model (but not its comparison
baselines) displays strong robustness to the absence of one of its observation
modalities (images or simulation attributes). We conclude that the combination
of GW with World Models holds great potential for improving decision-making in
RL agents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Long-Term Mapping of the Douro River Plume with Multi-Agent
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.03534v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.03534v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolò Dal Fabbro, Milad Mesbahi, Renato Mendes, João Borges de Sousa, George J. Pappas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of long-term (multiple days) mapping of a river plume
using multiple autonomous underwater vehicles (AUVs), focusing on the Douro
river representative use-case. We propose an energy - and communication -
efficient multi-agent reinforcement learning approach in which a central
coordinator intermittently communicates with the AUVs, collecting measurements
and issuing commands. Our approach integrates spatiotemporal Gaussian process
regression (GPR) with a multi-head Q-network controller that regulates
direction and speed for each AUV. Simulations using the Delft3D ocean model
demonstrate that our method consistently outperforms both single- and
multi-agent benchmarks, with scaling the number of agents both improving mean
squared error (MSE) and operational endurance. In some instances, our algorithm
demonstrates that doubling the number of AUVs can more than double endurance
while maintaining or improving accuracy, underscoring the benefits of
multi-agent coordination. Our learned policies generalize across unseen
seasonal regimes over different months and years, demonstrating promise for
future developments of data-driven long-term monitoring of dynamic plume
environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploration of Summarization by Generative Language Models for Automated
  Scoring of Long Essays 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.22830v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.22830v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haowei Hua, Hong Jiao, Xinyi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  BERT and its variants are extensively explored for automated scoring.
However, a limit of 512 tokens for these encoder-based models showed the
deficiency in automated scoring of long essays. Thus, this research explores
generative language models for automated scoring of long essays via
summarization and prompting. The results revealed great improvement of scoring
accuracy with QWK increased from 0.822 to 0.8878 for the Learning Agency Lab
Automated Essay Scoring 2.0 dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 5 Tables 7 Figures, Presentation at Artificial Intelligence
  in Measurement and Education Conference (AIME-Con)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AutoJudge: Judge Decoding Without Manual Annotation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.20039v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.20039v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roman Garipov, Fedor Velikonivtsev, Ivan Ermakov, Ruslan Svirschevski, Vage Egiazarian, Max Ryabinin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce AutoJudge, a method that accelerates large language model (LLM)
inference with task-specific lossy speculative decoding. Instead of matching
the original model output distribution token-by-token, we identify which of the
generated tokens affect the downstream quality of the response, relaxing the
distribution match guarantee so that the "unimportant" tokens can be generated
faster. Our approach relies on a semi-greedy search algorithm to test which of
the mismatches between target and draft models should be corrected to preserve
quality and which ones may be skipped. We then train a lightweight classifier
based on existing LLM embeddings to predict, at inference time, which
mismatching tokens can be safely accepted without compromising the final answer
quality. We evaluate the effectiveness of AutoJudge with multiple draft/target
model pairs on mathematical reasoning and programming benchmarks, achieving
significant speedups at the cost of a minor accuracy reduction. Notably, on
GSM8k with the Llama 3.1 70B target model, our approach achieves up to
$\approx2\times$ speedup over speculative decoding at the cost of $\le 1\%$
drop in accuracy. When applied to the LiveCodeBench benchmark, AutoJudge
automatically detects programming-specific important tokens, accepting $\ge 25$
tokens per speculation cycle at $2\%$ drop in Pass@1. Our approach requires no
human annotation and is easy to integrate with modern LLM inference frameworks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Personalized Treatment Plan: Geometrical Model-Agnostic Approach
  to Counterfactual Explanations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.22911v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.22911v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Sin, Milad Toutounchian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In our article, we describe a method for generating counterfactual
explanations in high-dimensional spaces using four steps that involve fitting
our dataset to a model, finding the decision boundary, determining constraints
on the problem, and computing the closest point (counterfactual explanation)
from that boundary. We propose a discretized approach where we find many
discrete points on the boundary and then identify the closest feasible
counterfactual explanation. This method, which we later call $\textit{Segmented
Sampling for Boundary Approximation}$ (SSBA), applies binary search to find
decision boundary points and then searches for the closest boundary point.
Across four datasets of varying dimensionality, we show that our method can
outperform current methods for counterfactual generation with reductions in
distance between $5\%$ to $50\%$ in terms of the $L_2$ norm. Our method can
also handle real-world constraints by restricting changes to immutable and
categorical features, such as age, gender, sex, height, and other related
characteristics such as the case for a health-based dataset. In terms of
runtime, the SSBA algorithm generates decision boundary points on multiple
orders of magnitude in the same given time when we compare to a grid-based
approach. In general, our method provides a simple and effective model-agnostic
method that can compute nearest feasible (i.e. realistic with constraints)
counterfactual explanations. All of our results and code are available at:
https://github.com/dsin85691/SSBA_For_Counterfactuals
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is 15 pages long consisting of multiple sections including
  an abstract, introduction, related works, methodology, results, ablation
  studies, conclusion, future works, and an appendix section. There are 10
  figures and 5 tables in total</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding
  in Vision-Language-Action Policies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.20072v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.20072v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhixuan Liang, Yizhuo Li, Tianshuo Yang, Chengyue Wu, Sitong Mao, Tian Nian, Liuao Pei, Shunbo Zhou, Xiaokang Yang, Jiangmiao Pang, Yao Mu, Ping Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language-Action (VLA) models adapt large vision-language backbones to
map images and instructions into robot actions. However, prevailing VLAs either
generate actions auto-regressively in a fixed left-to-right order or attach
separate MLP or diffusion heads outside the backbone, leading to fragmented
information pathways and specialized training requirements that hinder a
unified, scalable architecture. We present Discrete Diffusion VLA, a
unified-transformer policy that models discretized action chunks with discrete
diffusion. The design retains diffusion's progressive refinement paradigm while
remaining natively compatible with the discrete token interface of VLMs. Our
method achieves an adaptive decoding order that resolves easy action elements
before harder ones and uses secondary re-masking to revisit uncertain
predictions across refinement rounds, which improves consistency and enables
robust error correction. This unified decoder preserves pre-trained
vision-language priors, supports parallel decoding, breaks the autoregressive
bottleneck, and reduces the number of function evaluations. Discrete Diffusion
VLA achieves 96.3% avg. success rates on LIBERO, 71.2% visual matching on
SimplerEnv-Fractal and 54.2% overall on SimplerEnv-Bridge, improving over
autoregressive, MLP decoder and continuous diffusion baselines. These findings
indicate that discrete-diffusion VLA supports precise action modeling and
consistent training, laying groundwork for scaling VLA to larger models and
datasets. Our project page is https://github.com/Liang-ZX/DiscreteDiffusionVLA
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Importance of Being Discrete: Measuring the Impact of Discretization
  in End-to-End Differentially Private Synthetic Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.06923v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.06923v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgi Ganev, Meenatchi Sundaram Muthu Selva Annamalai, Sofiane Mahiou, Emiliano De Cristofaro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Differentially Private (DP) generative marginal models are often used in the
wild to release synthetic tabular datasets in lieu of sensitive data while
providing formal privacy guarantees. These models approximate low-dimensional
marginals or query workloads; crucially, they require the training data to be
pre-discretized, i.e., continuous values need to first be partitioned into
bins. However, as the range of values (or their domain) is often inferred
directly from the training data, with the number of bins and bin edges
typically defined arbitrarily, this approach can ultimately break end-to-end DP
guarantees and may not always yield optimal utility.
  In this paper, we present an extensive measurement study of four
discretization strategies in the context of DP marginal generative models. More
precisely, we design DP versions of three discretizers (uniform, quantile, and
k-means) and reimplement the PrivTree algorithm. We find that optimizing both
the choice of discretizer and bin count can improve utility, on average, by
almost 30% across six DP marginal models, compared to the default strategy and
number of bins, with PrivTree being the best-performing discretizer in the
majority of cases. We demonstrate that, while DP generative models with
non-private discretization remain vulnerable to membership inference attacks,
applying DP during discretization effectively mitigates this risk. Finally, we
improve on an existing approach for automatically selecting the optimal number
of bins, and achieve high utility while reducing both privacy budget
consumption and computational overhead.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Formalism-Implementation Gap in Reinforcement Learning Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.16175v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.16175v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pablo Samuel Castro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The last decade has seen an upswing in interest and adoption of reinforcement
learning (RL) techniques, in large part due to its demonstrated capabilities at
performing certain tasks at "super-human levels". This has incentivized the
community to prioritize research that demonstrates RL agent performance, often
at the expense of research aimed at understanding their learning dynamics.
Performance-focused research runs the risk of overfitting on academic
benchmarks -- thereby rendering them less useful -- which can make it difficult
to transfer proposed techniques to novel problems. Further, it implicitly
diminishes work that does not push the performance-frontier, but aims at
improving our understanding of these techniques. This paper argues two points:
(i) RL research should stop focusing solely on demonstrating agent
capabilities, and focus more on advancing the science and understanding of
reinforcement learning; and (ii) we need to be more precise on how our
benchmarks map to the underlying mathematical formalisms. We use the popular
Arcade Learning Environment (ALE; Bellemare et al., 2013) as an example of a
benchmark that, despite being increasingly considered "saturated", can be
effectively used for developing this understanding, and facilitating the
deployment of RL techniques in impactful real-world problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Why Diffusion Models Don't Memorize: The Role of Implicit Dynamical
  Regularization in Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.17638v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.17638v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tony Bonnaire, Raphaël Urfin, Giulio Biroli, Marc Mézard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have achieved remarkable success across a wide range of
generative tasks. A key challenge is understanding the mechanisms that prevent
their memorization of training data and allow generalization. In this work, we
investigate the role of the training dynamics in the transition from
generalization to memorization. Through extensive experiments and theoretical
analysis, we identify two distinct timescales: an early time
$\tau_\mathrm{gen}$ at which models begin to generate high-quality samples, and
a later time $\tau_\mathrm{mem}$ beyond which memorization emerges. Crucially,
we find that $\tau_\mathrm{mem}$ increases linearly with the training set size
$n$, while $\tau_\mathrm{gen}$ remains constant. This creates a growing window
of training times with $n$ where models generalize effectively, despite showing
strong memorization if training continues beyond it. It is only when $n$
becomes larger than a model-dependent threshold that overfitting disappears at
infinite training times. These findings reveal a form of implicit dynamical
regularization in the training dynamics, which allow to avoid memorization even
in highly overparameterized settings. Our results are supported by numerical
experiments with standard U-Net architectures on realistic and synthetic
datasets, and by a theoretical analysis using a tractable random features model
studied in the high-dimensional limit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as an oral at Neurips 2025. 40 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RWKV-edge: Deeply Compressed RWKV for Resource-Constrained Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.10856v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.10856v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wonkyo Choe, Yangfeng Ji, Felix Xiaozhu Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To deploy LLMs on resource-contained platforms such as mobile robots and
smartphones, non-transformers LLMs have achieved major breakthroughs. Recently,
a novel RNN-based LLM family, Repentance Weighted Key Value (RWKV) has shown
strong computational efficiency; nevertheless, RWKV models still have high
parameter counts which limited their deployment. In this paper, we propose a
suite of compression techniques, ranging from model architecture optimizations
to post-training compression, tailored to the RWKV architecture. Combined, our
techniques reduce the memory footprint of RWKV models by 3.4x -- 5x with only
negligible degradation in accuracy; compared to transformer LLMs with similar
accuracy, our models require 4x less memory footprint.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data Fusion of Deep Learned Molecular Embeddings for Property Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.07297v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.07297v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert J Appleton, Brian C Barnes, Alejandro Strachan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data-driven approaches such as deep learning can result in predictive models
for material properties with exceptional accuracy and efficiency. However, in
many applications, data is sparse, severely limiting their accuracy and
applicability. To improve predictions, techniques such as transfer learning and
multitask learning have been used. The performance of multitask learning models
depends on the strength of the underlying correlations between tasks and the
completeness of the data set. Standard multitask models tend to underperform
when trained on sparse data sets with weakly correlated properties. To address
this gap, we fuse deep-learned embeddings generated by independent pretrained
single-task models, resulting in a multitask model that inherits rich,
property-specific representations. By reusing (rather than retraining) these
embeddings, the resulting fused model outperforms standard multitask models and
can be extended with fewer trainable parameters. We demonstrate this technique
on a widely used benchmark data set of quantum chemistry data for small
molecules as well as a newly compiled sparse data set of experimental data
collected from literature and our own quantum chemistry and thermochemical
calculations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>J. Chem. Inf. Model. 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UniCrossFi: A Unified Framework For Cross-Domain Wi-Fi-based Gesture
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06328v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06328v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ke Xu, Zhiyong Zheng, Hongyuan Zhu, Lei Wang, Jiangtao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Wi-Fi sensing systems are severely hindered by cross domain problem when
deployed in unseen real-world environments. Existing methods typically design
separate frameworks for either domain adaptation or domain generalization,
often relying on extensive labeled data. Existing methods that designed for
domain generalization is often relying on extensive labeled data. However,
real-world scenarios are far more complex, where the deployed model must be
capable of handling generalization under limited labeled source data. To this
end, we propose UniCrossFi, a unified framework designed to mitigate
performance drop in CSI-based sensing across diverse deployment settings. Our
framework not only extends conventional Domain Generalization (DG) to a more
practical Semi-Supervised Domain Generalization (SSDG) setting, where only
partially labeled source data are available, but also introduces a
physics-informed data augmentation strategy, Antenna Response Consistency
(ARC). ARC mitigates the risk of learning superficial shortcuts by exploiting
the intrinsic spatial diversity of multi-antenna systems, treating signals from
different antennas as naturally augmented views of the same event. In addition,
we design a Unified Contrastive Objective to prevent conventional contrastive
learning from pushing apart samples from different domains that share the same
class. We conduct extensive experiments on the public Widar and CSIDA datasets.
The results demonstrate that UniCrossFi consistently establishes a new
state-of-the-art, significantly outperforming existing methods across all
unsupervised domain adaptation, DG, and SSDG benchmarks. UniCrossFi provides a
principled and practical solution to the domain shift challenge, advancing the
feasibility of robust, real-world Wi-Fi sensing systems that can operate
effectively with limited labeled data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Telegrapher's Generative Model via Kac Flows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20641v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20641v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Richard Duong, Jannis Chemseddine, Peter K. Friz, Gabriele Steidl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We break the mold in flow-based generative modeling by proposing a new model
based on the damped wave equation, also known as telegrapher's equation.
Similar to the diffusion equation and Brownian motion, there is a Feynman-Kac
type relation between the telegrapher's equation and the stochastic Kac process
in 1D. The Kac flow evolves stepwise linearly in time, so that the probability
flow is Lipschitz continuous in the Wasserstein distance and, in contrast to
diffusion flows, the norm of the velocity is globally bounded. Furthermore, the
Kac model has the diffusion model as its asymptotic limit. We extend these
considerations to a multi-dimensional stochastic process which consists of
independent 1D Kac processes in each spatial component. We show that this
process gives rise to an absolutely continuous curve in the Wasserstein space
and compute the conditional velocity field starting in a Dirac point
analytically. Using the framework of flow matching, we train a neural network
that approximates the velocity field and use it for sample generation. Our
numerical experiments demonstrate the scalability of our approach, and show its
advantages over diffusion models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Update V2: We added CIFAR experiments. Update V3: The old FID scores
  & CIFAR images of the Kac model corresponded to the schedule g(t) = t. We now
  updated them with both schedules t and t^2. Update V4: We corrected a minor
  implementation error and updated the CIFAR images/table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalized Exponentiated Gradient Algorithms Using the Euler
  Two-Parameter Logarithm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17500v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17500v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrzej Cichocki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  IIn this paper we propose and investigate a new class of Generalized
Exponentiated Gradient (GEG) algorithms using Mirror Descent (MD) updates, and
applying the Bregman divergence with a two--parameter
  deformation of the logarithm as a link function. This link function (referred
here to as the Euler logarithm) is associated with a relatively wide class of
trace--form entropies. In order to derive novel GEG/MD updates, we estimate a
deformed exponential function, which closely approximates the inverse of the
Euler two--parameter deformed logarithm. The characteristic shape and
properties of the Euler logarithm and its inverse--deformed exponential
functions, are tuned by two hyperparameters. By learning these hyperparameters,
we can adapt to the distribution of training data and adjust them to achieve
desired properties of gradient descent algorithms. In the literature, there
exist nowadays more than fifty mathematically well-established entropic
functionals and associated deformed logarithms, so it is impossible to
investigate all of them in one research paper. Therefore, we focus here on a
class of trace-form entropies and the associated deformed two--parameters
logarithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, preprint of Journal paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Linear regression with overparameterized linear neural networks: Tight
  upper and lower bounds for implicit $\ell^1$-regularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.01143v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.01143v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hannes Matt, Dominik Stöger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern machine learning models are often trained in a setting where the
number of parameters exceeds the number of training samples. To understand the
implicit bias of gradient descent in such overparameterized models, prior work
has studied diagonal linear neural networks in the regression setting. These
studies have shown that, when initialized with small weights, gradient descent
tends to favor solutions with minimal $\ell^1$-norm - an effect known as
implicit regularization. In this paper, we investigate implicit regularization
in diagonal linear neural networks of depth $D\ge 2$ for overparameterized
linear regression problems. We focus on analyzing the approximation error
between the limit point of gradient flow trajectories and the solution to the
$\ell^1$-minimization problem. By deriving tight upper and lower bounds on the
approximation error, we precisely characterize how the approximation error
depends on the scale of initialization $\alpha$. Our results reveal a
qualitative difference between depths: for $D \ge 3$, the error decreases
linearly with $\alpha$, whereas for $D=2$, it decreases at rate
$\alpha^{1-\varrho}$, where the parameter $\varrho \in [0,1)$ can be explicitly
characterized. Interestingly, this parameter is closely linked to so-called
null space property constants studied in the sparse recovery literature. We
demonstrate the asymptotic tightness of our bounds through explicit examples.
Numerical experiments corroborate our theoretical findings and suggest that
deeper networks, i.e., $D \ge 3$, may lead to better generalization,
particularly for realistic initialization scales.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Assessing the robustness of heterogeneous treatment effects in survival
  analysis under informative censoring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.13397v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.13397v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Wang, Dennis Frauen, Jonas Schweisthal, Maresa Schröder, Stefan Feuerriegel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dropout is common in clinical studies, with up to half of patients leaving
early due to side effects or other reasons. When dropout is informative (i.e.,
dependent on survival time), it introduces censoring bias, because of which
treatment effect estimates are also biased. In this paper, we propose an
assumption-lean framework to assess the robustness of conditional average
treatment effect (CATE) estimates in survival analysis when facing censoring
bias. Unlike existing works that rely on strong assumptions, such as
non-informative censoring, to obtain point estimation, we use partial
identification to derive informative bounds on the CATE. Thereby, our framework
helps to identify patient subgroups where treatment is effective despite
informative censoring. We further develop a novel meta-learner that estimates
the bounds using arbitrary machine learning models and with favorable
theoretical properties, including double robustness and quasi-oracle
efficiency. We demonstrate the practical value of our meta-learner through
numerical experiments and in an application to a cancer drug trial. Together,
our framework offers a practical tool for assessing the robustness of estimated
treatment effects in the presence of censoring and thus promotes the reliable
use of survival data for evidence generation in medicine and epidemiology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diffusion Models Meet Contextual Bandits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.10028v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.10028v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Imad Aouali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient online decision-making in contextual bandits is challenging, as
methods without informative priors often suffer from computational or
statistical inefficiencies. In this work, we leverage pre-trained diffusion
models as expressive priors to capture complex action dependencies and develop
a practical algorithm that efficiently approximates posteriors under such
priors, enabling both fast updates and sampling. Empirical results demonstrate
the effectiveness and versatility of our approach across diverse contextual
bandit settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Neurips 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Advancing Compositional Awareness in CLIP with Efficient Fine-Tuning <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.24424v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.24424v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amit Peleg, Naman Deep Singh, Matthias Hein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models like CLIP have demonstrated remarkable zero-shot
capabilities in classification and retrieval. However, these models often
struggle with compositional reasoning - the ability to understand the
relationships between concepts. A recent benchmark, SugarCrepe++, reveals that
previous works on improving compositionality have mainly improved lexical
sensitivity but neglected semantic understanding. In addition, downstream
retrieval performance often deteriorates, although one would expect that
improving compositionality should enhance retrieval. In this work, we introduce
CLIC (Compositionally-aware Learning in CLIP), a fine-tuning method based on a
novel training technique combining multiple images and their associated
captions. CLIC improves compositionality across architectures as well as
differently pre-trained CLIP models, both in terms of lexical and semantic
understanding, and achieves consistent gains in retrieval performance. This
even applies to the recent CLIPS, which achieves SOTA retrieval performance.
Nevertheless, the short fine-tuning with CLIC leads to an improvement in
retrieval and to the best compositional CLIP model on SugarCrepe++. All our
models and code are available at https://clic-compositional-clip.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Provable Scaling Laws for the Test-Time Compute of Large Language Models <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19477v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19477v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanxi Chen, Xuchen Pan, Yaliang Li, Bolin Ding, Jingren Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose two simple, principled and practical algorithms that enjoy
provable scaling laws for the test-time compute of large language models
(LLMs). The first one is a two-stage knockout-style algorithm: given an input
problem, it first generates multiple candidate solutions, and then aggregate
them via a knockout tournament for the final output. Assuming that the LLM can
generate a correct solution with non-zero probability and do better than a
random guess in comparing a pair of correct and incorrect solutions, we prove
theoretically that the failure probability of this algorithm decays to zero
exponentially or by a power law (depending on the specific way of scaling) as
its test-time compute grows. The second one is a two-stage league-style
algorithm, where each candidate is evaluated by its average win rate against
multiple opponents, rather than eliminated upon loss to a single opponent.
Under analogous but more robust assumptions, we prove that its failure
probability also decays to zero exponentially with more test-time compute. Both
algorithms require a black-box LLM and nothing else (e.g., no verifier or
reward model) for a minimalistic implementation, which makes them appealing for
practical applications and easy to adapt for different tasks. Through extensive
experiments with diverse models and datasets, we validate the proposed theories
and demonstrate the outstanding scaling properties of both algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025 camera-ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Point Cloud Reinforcement Learning via PCA-Based Canonicalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.20974v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.20974v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Bezick, Vittorio Giammarino, Ahmed H. Qureshi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning (RL) from raw visual input has achieved impressive
successes in recent years, yet it remains fragile to out-of-distribution
variations such as changes in lighting, color, and viewpoint. Point Cloud
Reinforcement Learning (PC-RL) offers a promising alternative by mitigating
appearance-based brittleness, but its sensitivity to camera pose mismatches
continues to undermine reliability in realistic settings. To address this
challenge, we propose PCA Point Cloud (PPC), a canonicalization framework
specifically tailored for downstream robotic control. PPC maps point clouds
under arbitrary rigid-body transformations to a unique canonical pose, aligning
observations to a consistent frame, thereby substantially decreasing
viewpoint-induced inconsistencies. In our experiments, we show that PPC
improves robustness to unseen camera poses across challenging robotic tasks,
providing a principled alternative to domain randomization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Offline Learning and Forgetting for Reasoning with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.11364v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.11364v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianwei Ni, Allen Nie, Sapana Chaudhary, Yao Liu, Huzefa Rangwala, Rasool Fakoor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leveraging inference-time search in large language models has proven
effective in further enhancing a trained model's capability to solve complex
mathematical and reasoning problems. However, this approach significantly
increases computational costs and inference time, as the model must generate
and evaluate multiple candidate solutions to identify a viable reasoning path.
To address this, we propose an effective approach that integrates search
capabilities directly into the model by fine-tuning it on unpaired successful
(learning) and failed reasoning paths (forgetting) derived from diverse search
methods. A key challenge we identify is that naive fine-tuning can degrade the
model's search capability; we show this can be mitigated with a smaller
learning rate. Extensive experiments on the challenging Game-of-24 and
Countdown arithmetic puzzles show that, replacing CoT-generated data with
search-generated data for offline fine-tuning improves success rates by around
23% over inference-time search baselines, while reducing inference time by
180$\times$. On top of this, our learning and forgetting objective consistently
outperforms both supervised fine-tuning and preference-based methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Transactions on Machine Learning Research (TMLR), 2025.
  Code: https://github.com/twni2016/llm-reasoning-uft</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fraud<span class="highlight-title">Transformer</span>: Time-Aware <span class="highlight-title">GPT</span> for Transaction Fraud Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.23712v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.23712v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gholamali Aminian, Andrew Elliott, Tiger Li, Timothy Cheuk Hin Wong, Victor Claude Dehon, Lukasz Szpruch, Carsten Maple, Christopher Read, Martin Brown, Gesine Reinert, Mo Mamouei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting payment fraud in real-world banking streams requires models that
can exploit both the order of events and the irregular time gaps between them.
We introduce FraudTransformer, a sequence model that augments a vanilla
GPT-style architecture with (i) a dedicated time encoder that embeds either
absolute timestamps or inter-event values, and (ii) a learned positional
encoder that preserves relative order. Experiments on a large industrial
dataset -- tens of millions of transactions and auxiliary events -- show that
FraudTransformer surpasses four strong classical baselines (Logistic
Regression, XGBoost and LightGBM) as well as transformer ablations that omit
either the time or positional component. On the held-out test set it delivers
the highest AUROC and PRAUC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in AI-FIND ICAIF'25
  (https://sites.google.com/view/icaif-fraud-detection-workshop/home)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ImageNet-trained CNNs are not biased towards texture: Revisiting feature
  reliance through controlled suppression <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.20234v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.20234v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tom Burgert, Oliver Stoll, Paolo Rota, Begüm Demir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The hypothesis that Convolutional Neural Networks (CNNs) are inherently
texture-biased has shaped much of the discourse on feature use in deep
learning. We revisit this hypothesis by examining limitations in the
cue-conflict experiment by Geirhos et al. To address these limitations, we
propose a domain-agnostic framework that quantifies feature reliance through
systematic suppression of shape, texture, and color cues, avoiding the
confounds of forced-choice conflicts. By evaluating humans and neural networks
under controlled suppression conditions, we find that CNNs are not inherently
texture-biased but predominantly rely on local shape features. Nonetheless,
this reliance can be substantially mitigated through modern training strategies
or architectures (ConvNeXt, ViTs). We further extend the analysis across
computer vision, medical imaging, and remote sensing, revealing that reliance
patterns differ systematically: computer vision models prioritize shape,
medical imaging models emphasize color, and remote sensing models exhibit a
stronger reliance on texture. Code is available at
https://github.com/tomburgert/feature-reliance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2025 (oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Think Just Enough: Sequence-Level Entropy as a Confidence Signal for LLM
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.08146v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.08146v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aman Sharma, Paras Chopra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a simple, yet novel entropy-based framework to drive token
efficiency in large language models during reasoning tasks. Our approach uses
Shannon entropy from token-level logprobs as a confidence signal to enable
early stopping, achieving 25-50% computational savings while maintaining task
accuracy. Crucially, we demonstrate that entropy-based confidence calibration
represents an emergent property of advanced post-training optimization present
in modern reasoning models but notably absent in standard instruction-tuned and
pre-trained models (Llama 3.3 70B). We show that the entropy threshold to stop
reasoning varies from model to model but can be calculated easily in one shot
using only a few examples from existing reasoning datasets. Our results
indicate that advanced reasoning models often know that they've gotten a
correct answer early on, and that this emergent confidence awareness can be
exploited to save tokens and reduce latency. The framework demonstrates
consistent performance across reasoning-optimized model families with 25-50%
computational cost reduction while preserving accuracy, revealing that
confidence mechanisms represent a distinguishing characteristic of modern
post-trained reasoning systems versus their predecessors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LittleBit: Ultra Low-Bit Quantization via Latent Factorization <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13771v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13771v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Banseok Lee, Dongkyu Kim, Youngcheon You, Youngmin Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deploying large language models (LLMs) often faces challenges from
substantial memory and computational costs. Quantization offers a solution, yet
performance degradation in the sub-1-bit regime remains particularly difficult.
This paper introduces LittleBit, a novel method for extreme LLM compression. It
targets levels like 0.1 bits per weight (BPW), achieving nearly 31$\times$
memory reduction, e.g., Llama2-13B to under 0.9 GB. LittleBit represents
weights in a low-rank form using latent matrix factorization, subsequently
binarizing these factors. To counteract information loss from this extreme
precision, it integrates a multi-scale compensation mechanism. This includes
row, column, and an additional latent dimension that learns per-rank
importance. Two key contributions enable effective training: Dual
Sign-Value-Independent Decomposition (Dual-SVID) for quantization-aware
training (QAT) initialization, and integrated Residual Compensation to mitigate
errors. Extensive experiments confirm LittleBit's superiority in sub-1-bit
quantization: e.g., its 0.1 BPW performance on Llama2-7B surpasses the leading
method's 0.7 BPW. LittleBit establishes a new, viable size-performance
trade-off--unlocking a potential 11.6$\times$ speedup over FP16 at the kernel
level--and makes powerful LLMs practical for resource-constrained environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2025. Banseok Lee and Dongkyu Kim contributed
  equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Geo-Sign: Hyperbolic Contrastive Regularisation for Geometrically Aware
  Sign Language Translation <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.00129v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.00129v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edward Fish, Richard Bowden
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress in Sign Language Translation (SLT) has focussed primarily on
improving the representational capacity of large language models to incorporate
Sign Language features. This work explores an alternative direction: enhancing
the geometric properties of skeletal representations themselves. We propose
Geo-Sign, a method that leverages the properties of hyperbolic geometry to
model the hierarchical structure inherent in sign language kinematics. By
projecting skeletal features derived from Spatio-Temporal Graph Convolutional
Networks (ST-GCNs) into the Poincar\'e ball model, we aim to create more
discriminative embeddings, particularly for fine-grained motions like finger
articulations. We introduce a hyperbolic projection layer, a weighted Fr\'echet
mean aggregation scheme, and a geometric contrastive loss operating directly in
hyperbolic space. These components are integrated into an end-to-end
translation framework as a regularisation function, to enhance the
representations within the language model. This work demonstrates the potential
of hyperbolic geometry to improve skeletal representations for Sign Language
Translation, improving on SOTA RGB methods while preserving privacy and
improving computational efficiency. Code available here:
https://github.com/ed-fish/geo-sign.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Trade-offs in Data Memorization via Strong Data Processing Inequalities <span class="chip">COLT 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.01855v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.01855v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vitaly Feldman, Guy Kornowski, Xin Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research demonstrated that training large language models involves
memorization of a significant fraction of training data. Such memorization can
lead to privacy violations when training on sensitive user data and thus
motivates the study of data memorization's role in learning. In this work, we
develop a general approach for proving lower bounds on excess data
memorization, that relies on a new connection between strong data processing
inequalities and data memorization. We then demonstrate that several simple and
natural binary classification problems exhibit a trade-off between the number
of samples available to a learning algorithm, and the amount of information
about the training data that a learning algorithm needs to memorize to be
accurate. In particular, $\Omega(d)$ bits of information about the training
data need to be memorized when $O(1)$ $d$-dimensional examples are available,
which then decays as the number of examples grows at a problem-specific rate.
Further, our lower bounds are generally matched (up to logarithmic factors) by
simple learning algorithms. We also extend our lower bounds to more general
mixture-of-clusters models. Our definitions and results build on the work of
Brown et al. (2021) and address several limitations of the lower bounds in
their work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Appeared in COLT 2025; this revision includes an improved upper bound
  in Theorem 3.1, as well as several minor clarifications and modifications</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Acoustic and Machine Learning Methods for Speech-Based Suicide Risk
  Assessment: A Systematic <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.18195v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.18195v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ambre Marie, Marine Garnier, Thomas Bertin, Laura Machart, Guillaume Dardenne, Gwenolé Quellec, Sofian Berrouiguet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Suicide remains a public health challenge, necessitating improved detection
methods to facilitate timely intervention and treatment. This systematic review
evaluates the role of Artificial Intelligence (AI) and Machine Learning (ML) in
assessing suicide risk through acoustic analysis of speech. Following PRISMA
guidelines, we analyzed 33 articles selected from PubMed, Cochrane, Scopus, and
Web of Science databases. The last search was conducted in February 2025. Risk
of bias was assessed using the PROBAST tool. Studies analyzing acoustic
features between individuals at risk of suicide (RS) and those not at risk
(NRS) were included, while studies lacking acoustic data, a suicide-related
focus, or sufficient methodological details were excluded. Sample sizes varied
widely and were reported in terms of participants or speech segments, depending
on the study. Results were synthesized narratively based on acoustic features
and classifier performance. Findings consistently showed significant acoustic
feature variations between RS and NRS populations, particularly involving
jitter, fundamental frequency (F0), Mel-frequency cepstral coefficients (MFCC),
and power spectral density (PSD). Classifier performance varied based on
algorithms, modalities, and speech elicitation methods, with multimodal
approaches integrating acoustic, linguistic, and metadata features
demonstrating superior performance. Among the 29 classifier-based studies,
reported AUC values ranged from 0.62 to 0.985 and accuracies from 60% to
99.85%. Most datasets were imbalanced in favor of NRS, and performance metrics
were rarely reported separately by group, limiting clear identification of
direction of effect.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint version of a manuscript submitted to the Journal of
  Affective Disorders</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MH-GIN: Multi-scale Heterogeneous Graph-based Imputation Network for AIS
  Data (Extended Version) <span class="chip">VLDB 2026</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.20362v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.20362v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hengyu Liu, Tianyi Li, Yuqiang He, Kristian Torp, Yushuai Li, Christian S. Jensen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Location-tracking data from the Automatic Identification System, much of
which is publicly available, plays a key role in a range of maritime safety and
monitoring applications. However, the data suffers from missing values that
hamper downstream applications. Imputing the missing values is challenging
because the values of different heterogeneous attributes are updated at diverse
rates, resulting in the occurrence of multi-scale dependencies among
attributes. Existing imputation methods that assume similar update rates across
attributes are unable to capture and exploit such dependencies, limiting their
imputation accuracy. We propose MH-GIN, a Multi-scale Heterogeneous Graph-based
Imputation Network that aims improve imputation accuracy by capturing
multi-scale dependencies. Specifically, MH-GIN first extracts multi-scale
temporal features for each attribute while preserving their intrinsic
heterogeneous characteristics. Then, it constructs a multi-scale heterogeneous
graph to explicitly model dependencies between heterogeneous attributes to
enable more accurate imputation of missing values through graph propagation.
Experimental results on two real-world datasets find that MH-GIN is capable of
an average 57% reduction in imputation errors compared to state-of-the-art
methods, while maintaining computational efficiency. The source code and
implementation details of MH-GIN are publicly available
https://github.com/hyLiu1994/MH-GIN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 4 figures; This paper is accepted by PVLDB 2026</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Human-Like Goalkeeping in a Realistic Football Simulation: a
  Sample-Efficient Reinforcement Learning Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23216v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23216v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessandro Sestini, Joakim Bergdahl, Jean-Philippe Barrette-LaPierre, Florian Fuchs, Brady Chen, Michael Jones, Linus Gisslén
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While several high profile video games have served as testbeds for Deep
Reinforcement Learning (DRL), this technique has rarely been employed by the
game industry for crafting authentic AI behaviors. Previous research focuses on
training super-human agents with large models, which is impractical for game
studios with limited resources aiming for human-like agents. This paper
proposes a sample-efficient DRL method tailored for training and fine-tuning
agents in industrial settings such as the video game industry. Our method
improves sample efficiency of value-based DRL by leveraging pre-collected data
and increasing network plasticity. We evaluate our method training a goalkeeper
agent in EA SPORTS FC 25, one of the best-selling football simulations today.
Our agent outperforms the game's built-in AI by 10% in ball saving rate.
Ablation studies show that our method trains agents 50% faster compared to
standard DRL methods. Finally, qualitative evaluation from domain experts
indicates that our approach creates more human-like gameplay compared to
hand-crafted agents. As a testimony of the impact of the approach, the method
is intended to replace the hand-crafted counterpart in next iterations of the
series.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Logical Expressiveness of Temporal GNNs via Two-Dimensional Product
  Logics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.11930v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.11930v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Sälzer, Przemysław Andrzej Wałęga, Martin Lange
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, the expressive power of various neural architectures --
including graph neural networks (GNNs), transformers, and recurrent neural
networks -- has been characterised using tools from logic and formal language
theory. As the capabilities of basic architectures are becoming well
understood, increasing attention is turning to models that combine multiple
architectural paradigms. Among them particularly important, and challenging to
analyse, are temporal extensions of GNNs, which integrate both spatial
(graph-structure) and temporal (evolution over time) dimensions. In this paper,
we initiate the study of logical characterisation of temporal GNNs by
connecting them to two-dimensional product logics. We show that the expressive
power of temporal GNNs depends on how graph and temporal components are
combined. In particular, temporal GNNs that apply static GNNs recursively over
time can capture all properties definable in the product logic of (past)
propositional temporal logic PTL and the modal logic K. In contrast,
architectures such as graph-and-time TGNNs and global TGNNs can only express
restricted fragments of this logic, where the interaction between temporal and
spatial operators is syntactically constrained. These provide us with the first
results on the logical expressiveness of temporal GNNs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MixAT: Combining Continuous and Discrete Adversarial Training for LLMs <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.16947v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.16947v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Csaba Dékány, Stefan Balauca, Robin Staab, Dimitar I. Dimitrov, Martin Vechev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent efforts in Large Language Model (LLM) safety and alignment,
current adversarial attacks on frontier LLMs can still consistently force
harmful generations. Although adversarial training has been widely studied and
shown to significantly improve the robustness of traditional machine learning
models, its strengths and weaknesses in the context of LLMs are less
understood. Specifically, while existing discrete adversarial attacks are
effective at producing harmful content, training LLMs with concrete adversarial
prompts is often computationally expensive, leading to reliance on continuous
relaxations. At the same time, despite their effectiveness and generalization
capabilities, training with continuous perturbations does not always capture
the full spectrum of vulnerabilities exploited by discrete attacks. In this
work, we aim to bridge this gap by introducing MixAT, a novel method that
combines stronger discrete and faster continuous attacks during training. We
rigorously evaluate MixAT across a wide spectrum of state-of-the-art attacks,
proposing the At Least One Attack Success Rate (ALO-ASR) metric to capture the
worst-case vulnerability of models. We show MixAT achieves substantially better
robustness (ALO-ASR < 20%) compared to prior defenses (ALO-ASR > 50%), while
maintaining a runtime comparable to methods based on continuous relaxations. We
further analyze MixAT in realistic deployment settings, exploring how chat
templates, quantization, low-rank adapters, and temperature affect both
adversarial training and evaluation, revealing additional blind spots in
current methodologies. Our results demonstrate that MixAT's discrete-continuous
defense offers a principled and superior robustness-accuracy tradeoff with
minimal computational overhead, highlighting its promise for building safer
LLMs. We provide our code and models at
https://github.com/insait-institute/MixAT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at 39th Conference on Neural Information Processing Systems
  (NeurIPS 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SeeDNorm: Self-Rescaled Dynamic Normalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.22777v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.22777v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenrui Cai, Defa Zhu, Qingjie Liu, Qiyang Min
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Normalization layer constitutes an essential component in neural networks. In
transformers, the predominantly used RMSNorm constrains vectors to a unit
hypersphere, followed by dimension-wise rescaling through a learnable scaling
coefficient $\gamma$ to maintain the representational capacity of the model.
However, RMSNorm discards the input norm information in forward pass and a
static scaling factor $\gamma$ may be insufficient to accommodate the wide
variability of input data and distributional shifts, thereby limiting further
performance improvements, particularly in zero-shot scenarios that large
language models routinely encounter. To address this limitation, we propose
SeeDNorm, which enhances the representational capability of the model by
dynamically adjusting the scaling coefficient based on the current input,
thereby preserving the input norm information and enabling data-dependent,
self-rescaled dynamic normalization. During backpropagation, SeeDNorm retains
the ability of RMSNorm to dynamically adjust gradient according to the input
norm. We provide a detailed analysis of the training optimization for SeedNorm
and proposed corresponding solutions to address potential instability issues
that may arise when applying SeeDNorm. We validate the effectiveness of
SeeDNorm across models of varying sizes in large language model pre-training as
well as supervised and unsupervised computer vision tasks. By introducing a
minimal number of parameters and with neglligible impact on model efficiency,
SeeDNorm achieves consistently superior performance compared to previously
commonly used normalization layers such as RMSNorm and LayerNorm, as well as
element-wise activation alternatives to normalization layers like DyT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 14 figures, 18 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DP-LLM: Runtime Model Adaptation with Dynamic Layer-wise Precision
  Assignment <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.06041v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.06041v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sangwoo Kwon, Seong Hoon Seo, Jae W. Lee, Yeonhong Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How can we effectively handle queries for on-device large language models
(LLMs) with varying runtime constraints, such as latency and accuracy?
Multi-scale quantization addresses this challenge by enabling memory-efficient
runtime model adaptation of LLMs through the overlaying of multiple model
variants quantized to different bitwidths. Meanwhile, an important question
still remains open-ended: how can models be properly configured to match a
target precision or latency? While mixed-precision offers a promising solution,
we take this further by leveraging the key observation that the sensitivity of
each layer dynamically changes across decoding steps. Building on this insight,
we introduce DP-LLM, a novel mechanism that dynamically assigns precision to
each layer based on input values. Experimental results across multiple models
and benchmarks demonstrate that DP-LLM achieves a superior performance-latency
trade-off, outperforming prior approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Minimax Optimal Transfer Learning for Kernel-based Nonparametric
  Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.13966v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.13966v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Wang, Caixing Wang, Xin He, Xingdong Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, transfer learning has garnered significant attention in the
machine learning community. Its ability to leverage knowledge from related
studies to improve generalization performance in a target study has made it
highly appealing. This paper focuses on investigating the transfer learning
problem within the context of nonparametric regression over a reproducing
kernel Hilbert space. The aim is to bridge the gap between practical
effectiveness and theoretical guarantees. We specifically consider two
scenarios: one where the transferable sources are known and another where they
are unknown. For the known transferable source case, we propose a two-step
kernel-based estimator by solely using kernel ridge regression. For the unknown
case, we develop a novel method based on an efficient aggregation algorithm,
which can automatically detect and alleviate the effects of negative sources.
This paper provides the statistical properties of the desired estimators and
establishes the minimax optimal rate. Through extensive numerical experiments
on synthetic data and real examples, we validate our theoretical findings and
demonstrate the effectiveness of our proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Is It Certainly a Deepfake? Reliability Analysis in Detection &
  Generation Ecosystem <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.17550v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.17550v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neslihan Kose, Anthony Rhodes, Umur Aybars Ciftci, Ilke Demir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As generative models are advancing in quality and quantity for creating
synthetic content, deepfakes begin to cause online mistrust. Deepfake detectors
are proposed to counter this effect, however, misuse of detectors claiming fake
content as real or vice versa further fuels this misinformation problem. We
present the first comprehensive uncertainty analysis of deepfake detectors,
systematically investigating how generative artifacts influence prediction
confidence. As reflected in detectors' responses, deepfake generators also
contribute to this uncertainty as their generative residues vary, so we cross
the uncertainty analysis of deepfake detectors and generators. Based on our
observations, the uncertainty manifold holds enough consistent information to
leverage uncertainty for deepfake source detection. Our approach leverages
Bayesian Neural Networks and Monte Carlo dropout to quantify both aleatoric and
epistemic uncertainties across diverse detector architectures. We evaluate
uncertainty on two datasets with nine generators, with four blind and two
biological detectors, compare different uncertainty methods, explore region-
and pixel-based uncertainty, and conduct ablation studies. We conduct and
analyze binary real/fake, multi-class real/fake, source detection, and
leave-one-out experiments between the generator/detector combinations to share
their generalization capability, model calibration, uncertainty, and robustness
against adversarial attacks. We further introduce uncertainty maps that
localize prediction confidence at the pixel level, revealing distinct patterns
correlated with generator-specific artifacts. Our analysis provides critical
insights for deploying reliable deepfake detection systems and establishes
uncertainty quantification as a fundamental requirement for trustworthy
synthetic media detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the ICCV 2025 workshop - STREAM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Clustering-Based Low-Rank Matrix Approximation for Medical Image
  Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08256v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08256v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sisipho Hamlomo, Marcellin Atemkeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical images are inherently high-resolution and contain locally varying
structures crucial for diagnosis. Efficient compression must preserve
diagnostic fidelity while minimizing redundancy. Low-rank matrix approximation
(LoRMA) techniques have shown strong potential for image compression by
capturing global correlations; however, they often fail to adapt to local
structural variations across regions of interest. To address this, we introduce
an adaptive LoRMA, which partitions a medical image into overlapping patches,
groups structurally similar patches into clusters using k-means, and performs
SVD within each cluster. We derive the overall compression factor accounting
for patch overlap and analyze how patch size influences compression efficiency
and computational cost. While applicable to any data with high local variation,
we focus on medical imaging due to its pronounced local variability. We
evaluate and compare our adaptive LoRMA against global SVD across four imaging
modalities: MRI, ultrasound, CT scan, and chest X-ray. Results demonstrate that
adaptive LoRMA effectively preserves structural integrity, edge details, and
diagnostic relevance, measured by PSNR, SSIM, MSE, IoU, and EPI. Adaptive LoRMA
minimizes block artifacts and residual errors, particularly in pathological
regions, consistently outperforming global SVD in PSNR, SSIM, IoU, EPI, and
achieving lower MSE. It prioritizes clinically salient regions while allowing
aggressive compression in non-critical regions, optimizing storage efficiency.
Although adaptive LoRMA requires higher processing time, its diagnostic
fidelity justifies the overhead for high-compression applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MARS-M: When Variance Reduction Meets Matrices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.21800v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.21800v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifeng Liu, Angela Yuan, Quanquan Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Matrix-based preconditioned optimizers, such as Muon, have recently been
shown to be more efficient than scalar-based optimizers for training
large-scale neural networks, including large language models (LLMs). On the
other hand, recent benchmarks on optimizers for LLM pre-training have
demonstrated that variance-reduction techniques such as MARS can achieve
substantial speedups over standard optimizers that do not employ variance
reduction. In this paper, to achieve the best of both worlds, we introduce
MARS-M, a new optimizer that integrates the variance reduction technique in
MARS with Muon. Under standard regularity conditions, we prove that Muon-M
converges to a first-order stationary point at a rate of
$\tilde{\mathcal{O}}(T^{-1/3})$, which improves upon
$\tilde{\mathcal{O}}(T^{-1/4})$ rate attained by Muon. Our empirical results on
language modeling and computer vision tasks demonstrate that MARS-M
consistently yields lower losses and improved performance across various
downstream benchmarks. The implementation of MARS-M is available at
https://github.com/AGI-Arena/MARS/tree/main/MARS_M.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Geometric Mixture Models for Electrolyte Conductivity Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.15403v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.15403v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anyi Li, Jiacheng Cen, Songyou Li, Mingze Li, Yang Yu, Wenbing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate prediction of ionic conductivity in electrolyte systems is crucial
for advancing numerous scientific and technological applications. While
significant progress has been made, current research faces two fundamental
challenges: (1) the lack of high-quality standardized benchmarks, and (2)
inadequate modeling of geometric structure and intermolecular interactions in
mixture systems. To address these limitations, we first reorganize and enhance
the CALiSol and DiffMix electrolyte datasets by incorporating geometric graph
representations of molecules. We then propose GeoMix, a novel geometry-aware
framework that preserves Set-SE(3) equivariance-an essential but challenging
property for mixture systems. At the heart of GeoMix lies the Geometric
Interaction Network (GIN), an equivariant module specifically designed for
intermolecular geometric message passing. Comprehensive experiments demonstrate
that GeoMix consistently outperforms diverse baselines (including MLPs, GNNs,
and geometric GNNs) across both datasets, validating the importance of
cross-molecular geometric interactions and equivariant message passing for
accurate property prediction. This work not only establishes new benchmarks for
electrolyte research but also provides a general geometric learning framework
that advances modeling of mixture systems in energy materials, pharmaceutical
development, and beyond.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Taxonomy and Trends in Reinforcement Learning for Robotics and Control
  Systems: A Structured <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.21758v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.21758v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kumater Ter, Ore-Ofe Ajayi, Daniel Udekwe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) has become a foundational approach for enabling
intelligent robotic behavior in dynamic and uncertain environments. This work
presents an in-depth review of RL principles, advanced deep reinforcement
learning (DRL) algorithms, and their integration into robotic and control
systems. Beginning with the formalism of Markov Decision Processes (MDPs), the
study outlines essential elements of the agent-environment interaction and
explores core algorithmic strategies including actor-critic methods,
value-based learning, and policy gradients. Emphasis is placed on modern DRL
techniques such as DDPG, TD3, PPO, and SAC, which have shown promise in solving
high-dimensional, continuous control tasks. A structured taxonomy is introduced
to categorize RL applications across domains such as locomotion, manipulation,
multi-agent coordination, and human-robot interaction, along with training
methodologies and deployment readiness levels. The review synthesizes recent
research efforts, highlighting technical trends, design patterns, and the
growing maturity of RL in real-world robotics. Overall, this work aims to
bridge theoretical advances with practical implementations, providing a
consolidated perspective on the evolving role of RL in autonomous robotic
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ URB -- Urban Routing Benchmark for RL-equipped Connected Autonomous
  Vehicles <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.17734v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.17734v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmet Onur Akman, Anastasia Psarou, Michał Hoffmann, Łukasz Gorczyca, Łukasz Kowalski, Paweł Gora, Grzegorz Jamróz, Rafał Kucharski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Connected Autonomous Vehicles (CAVs) promise to reduce congestion in future
urban networks, potentially by optimizing their routing decisions. Unlike for
human drivers, these decisions can be made with collective, data-driven
policies, developed using machine learning algorithms. Reinforcement learning
(RL) can facilitate the development of such collective routing strategies, yet
standardized and realistic benchmarks are missing. To that end, we present URB:
Urban Routing Benchmark for RL-equipped Connected Autonomous Vehicles. URB is a
comprehensive benchmarking environment that unifies evaluation across 29
real-world traffic networks paired with realistic demand patterns. URB comes
with a catalog of predefined tasks, multi-agent RL (MARL) algorithm
implementations, three baseline methods, domain-specific performance metrics,
and a modular configuration scheme. Our results show that, despite the lengthy
and costly training, state-of-the-art MARL algorithms rarely outperformed
humans. The experimental results reported in this paper initiate the first
leaderboard for MARL in large-scale urban routing optimization. They reveal
that current approaches struggle to scale, emphasizing the urgent need for
advancements in this domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 39th Conference on Neural Information Processing
  Systems (NeurIPS 2025), Datasets and Benchmarks Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do Language Models Use Their Depth Efficiently? <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13898v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13898v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Róbert Csordás, Christopher D. Manning, Christopher Potts
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern LLMs are increasingly deep, and depth correlates with performance,
albeit with diminishing returns. However, do these models use their depth
efficiently? Do they compose more features to create higher-order computations
that are impossible in shallow models, or do they merely spread the same kinds
of computation out over more layers? To address these questions, we analyze the
residual stream of the Llama 3.1, Qwen 3, and OLMo 2 family of models. We find:
First, comparing the output of the sublayers to the residual stream reveals
that layers in the second half contribute much less than those in the first
half, with a clear phase transition between the two halves. Second, skipping
layers in the second half has a much smaller effect on future computations and
output predictions. Third, for multihop tasks, we are unable to find evidence
that models are using increased depth to compose subresults in examples
involving many hops. Fourth, we seek to directly address whether deeper models
are using their additional layers to perform new kinds of computation. To do
this, we train linear maps from the residual stream of a shallow model to a
deeper one. We find that layers with the same relative depth map best to each
other, suggesting that the larger model simply spreads the same computations
out over its many layers. All this evidence suggests that deeper models are not
using their depth to learn new kinds of computation, but only using the greater
depth to perform more fine-grained adjustments to the residual. This may help
explain why increasing scale leads to diminishing returns for stacked
Transformer architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unlearning Comparator: A Visual Analytics System for Comparative
  Evaluation of Machine Unlearning Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.12730v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.12730v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaeung Lee, Suhyeon Yu, Yurim Jang, Simon S. Woo, Jaemin Jo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine Unlearning (MU) aims to remove target training data from a trained
model so that the removed data no longer influences the model's behavior,
fulfilling "right to be forgotten" obligations under data privacy laws. Yet, we
observe that researchers in this rapidly emerging field face challenges in
analyzing and understanding the behavior of different MU methods, especially in
terms of three fundamental principles in MU: accuracy, efficiency, and privacy.
Consequently, they often rely on aggregate metrics and ad-hoc evaluations,
making it difficult to accurately assess the trade-offs between methods. To
fill this gap, we introduce a visual analytics system, Unlearning Comparator,
designed to facilitate the systematic evaluation of MU methods. Our system
supports two important tasks in the evaluation process: model comparison and
attack simulation. First, it allows the user to compare the behaviors of two
models, such as a model generated by a certain method and a retrained baseline,
at class-, instance-, and layer-levels to better understand the changes made
after unlearning. Second, our system simulates membership inference attacks
(MIAs) to evaluate the privacy of a method, where an attacker attempts to
determine whether specific data samples were part of the original training set.
We evaluate our system through a case study visually analyzing prominent MU
methods and demonstrate that it helps the user not only understand model
behaviors but also gain insights that can inform the improvement of MU methods.
The source code is publicly available at
https://github.com/gnueaj/Machine-Unlearning-Comparator.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE Transactions on Visualization and Computer Graphics
  (TVCG), under review. 15 pages. This work has been submitted to the IEEE for
  possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Two-Stage Learning of Stabilizing Neural Controllers via Zubov Sampling
  and Iterative Domain Expansion <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.01356v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.01356v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Li, Xiangru Zhong, Bin Hu, Huan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning-based neural network (NN) control policies have shown impressive
empirical performance. However, obtaining stability guarantees and estimates of
the region of attraction of these learned neural controllers is challenging due
to the lack of stable and scalable training and verification algorithms.
Although previous works in this area have achieved great success, much
conservatism remains in their frameworks. In this work, we propose a novel
two-stage training framework to jointly synthesize a controller and a Lyapunov
function for continuous-time systems. By leveraging a Zubov-inspired region of
attraction characterization to directly estimate stability boundaries, we
propose a novel training-data sampling strategy and a domain-updating mechanism
that significantly reduces the conservatism in training. Moreover, unlike
existing works on continuous-time systems that rely on an SMT solver to
formally verify the Lyapunov condition, we extend state-of-the-art neural
network verifier $\alpha,\!\beta$-CROWN with the capability of performing
automatic bound propagation through the Jacobian of dynamical systems and a
novel verification scheme that avoids expensive bisection. To demonstrate the
effectiveness of our approach, we conduct numerical experiments by synthesizing
and verifying controllers on several challenging nonlinear systems across
multiple dimensions. We show that our training can yield region of attractions
with volume $5 - 1.5\cdot 10^{5}$ times larger compared to the baselines, and
our verification on continuous systems can be up to $40-10{,}000$ times faster
compared to the traditional SMT solver dReal. Our code is available at
https://github.com/Verified-Intelligence/Two-Stage_Neural_Controller_Training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pairwise Optimal Transports for Training All-to-All Flow-Based Condition
  Transfer Model <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.03188v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.03188v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kotaro Ikeda, Masanori Koyama, Jinzhe Zhang, Kohei Hayashi, Kenji Fukumizu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a flow-based method for learning all-to-all
transfer maps among conditional distributions that approximates pairwise
optimal transport. The proposed method addresses the challenge of handling the
case of continuous conditions, which often involve a large set of conditions
with sparse empirical observations per condition. We introduce a novel cost
function that enables simultaneous learning of optimal transports for all pairs
of conditional distributions. Our method is supported by a theoretical
guarantee that, in the limit, it converges to the pairwise optimal transports
among infinite pairs of conditional distributions. The learned transport maps
are subsequently used to couple data points in conditional flow matching. We
demonstrate the effectiveness of this method on synthetic and benchmark
datasets, as well as on chemical datasets in which continuous physical
properties are defined as conditions. The code for this project can be found at
https://github.com/kotatumuri-room/A2A-FM
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2025, 32 pages, 18 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scalable Exploration via Ensemble++ <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.13195v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.13195v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingru Li, Jiawei Xu, Baoxiang Wang, Zhi-Quan Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Thompson Sampling is a principled method for balancing exploration and
exploitation, but its real-world adoption faces computational challenges in
large-scale or non-conjugate settings. While ensemble-based approaches offer
partial remedies, they typically require prohibitively large ensemble sizes. We
propose Ensemble++, a scalable exploration framework using a novel
shared-factor ensemble architecture with random linear combinations. For linear
bandits, we provide theoretical guarantees showing that Ensemble++ achieves
regret comparable to exact Thompson Sampling with only $\Theta(d \log T)$
ensemble sizes--significantly outperforming prior methods. Crucially, this
efficiency holds across both compact and finite action sets with either
time-invariant or time-varying contexts without configuration changes. We
extend this theoretical foundation to nonlinear rewards by replacing fixed
features with learnable neural representations while preserving the same
incremental update principle, effectively bridging theory and practice for
real-world tasks. Comprehensive experiments across linear, quadratic, neural,
and GPT-based contextual bandits validate our theoretical findings and
demonstrate Ensemble++'s superior regret-computation tradeoff versus
state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FragFM: Hierarchical Framework for Efficient Molecule Generation via
  Fragment-Level Discrete Flow Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15805v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15805v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joongwon Lee, Seonghwan Kim, Seokhyun Moon, Hyunwoo Kim, Woo Youn Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce FragFM, a novel hierarchical framework via fragment-level
discrete flow matching for efficient molecular graph generation. FragFM
generates molecules at the fragment level, leveraging a coarse-to-fine
autoencoder to reconstruct details at the atom level. Together with a
stochastic fragment bag strategy to effectively handle an extensive fragment
space, our framework enables more efficient and scalable molecular generation.
We demonstrate that our fragment-based approach achieves better property
control than the atom-based method and additional flexibility through
conditioning the fragment bag. We also propose a Natural Product Generation
benchmark (NPGen) to evaluate modern molecular graph generative models' ability
to generate natural product-like molecules. Since natural products are
biologically prevalidated and differ from typical drug-like molecules, our
benchmark provides a more challenging yet meaningful evaluation relevant to
drug discovery. We conduct a FragFM comparative study against various models on
diverse molecular generation benchmarks, including NPGen, demonstrating
superior performance. The results highlight the potential of fragment-based
generative modeling for large-scale, property-aware molecular design, paving
the way for more efficient exploration of chemical space.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>49 pages, 29 figures, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PULSE: Practical Evaluation Scenarios for Large Multimodal Model
  Unlearning <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.01271v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.01271v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tatsuki Kawakami, Kazuki Egashira, Atsuyuki Miyai, Go Irie, Kiyoharu Aizawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, unlearning techniques, which are methods for inducing a
model to "forget" previously learned information, have attracted attention as a
way to address privacy and copyright concerns in large language models (LLMs)
and large multimodal models (LMMs). While several unlearning benchmarks have
been established for LLMs, a practical evaluation framework for unlearning in
LMMs has been less explored. Specifically, existing unlearning benchmark for
LMMs considers only scenarios in which the model is required to unlearn
fine-tuned knowledge through a single unlearning operation. In this study, we
introduce PULSE protocol for realistic unlearning scenarios for LMMs by
introducing two critical perspectives: (i) Pre-trained knowledge Unlearning for
analyzing the effect across different knowledge acquisition phases and (ii)
Long-term Sustainability Evaluation to address sequential requests. We then
evaluate existing unlearning methods along these dimensions. Our results reveal
that, although some techniques can successfully unlearn knowledge acquired
through fine-tuning, they struggle to eliminate information learned during
pre-training. Moreover, methods that effectively unlearn a batch of target data
in a single operation exhibit substantial performance degradation when the same
data are split and unlearned sequentially.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2025 Workshop: Evaluating the Evolving LLM
  Lifecycle</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Causal Spatio-Temporal Prediction: An Effective and Efficient
  Multi-Modal Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.17637v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.17637v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuting Huang, Ziquan Fang, Zhihao Zeng, Lu Chen, Yunjun Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatio-temporal prediction plays a crucial role in intelligent
transportation, weather forecasting, and urban planning. While integrating
multi-modal data has shown potential for enhancing prediction accuracy, key
challenges persist: (i) inadequate fusion of multi-modal information, (ii)
confounding factors that obscure causal relations, and (iii) high computational
complexity of prediction models. To address these challenges, we propose
E^2-CSTP, an Effective and Efficient Causal multi-modal Spatio-Temporal
Prediction framework. E^2-CSTP leverages cross-modal attention and gating
mechanisms to effectively integrate multi-modal data. Building on this, we
design a dual-branch causal inference approach: the primary branch focuses on
spatio-temporal prediction, while the auxiliary branch mitigates bias by
modeling additional modalities and applying causal interventions to uncover
true causal dependencies. To improve model efficiency, we integrate GCN with
the Mamba architecture for accelerated spatio-temporal encoding. Extensive
experiments on 4 real-world datasets show that E^2-CSTP significantly
outperforms 9 state-of-the-art methods, achieving up to 9.66% improvements in
accuracy as well as 17.37%-56.11% reductions in computational overhead.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rademacher Meets Colors: More Expressivity, but at What Cost ? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.10101v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.10101v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martin Carrasco, Caio F. Deberaldini Netto, Vahan A. Martirosyan, Aneeqa Mehrab, Ehimare Okoyomon, Caterina Graziani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The expressive power of graph neural networks (GNNs) is typically understood
through their correspondence with graph isomorphism tests such as the
Weisfeiler-Leman (WL) hierarchy. While more expressive GNNs can distinguish a
richer set of graphs, they are also observed to suffer from higher
generalization error. This work provides a theoretical explanation for this
trade-off by linking expressivity and generalization through the lens of
coloring algorithms. Specifically, we show that the number of equivalence
classes induced by WL colorings directly bounds the GNNs Rademacher complexity
-- a key data-dependent measure of generalization. Our analysis reveals that
greater expressivity leads to higher complexity and thus weaker generalization
guarantees. Furthermore, we prove that the Rademacher complexity is stable
under perturbations in the color counts across different samples, ensuring
robustness to sampling variability across datasets. Importantly, our framework
is not restricted to message-passing GNNs or 1-WL, but extends to arbitrary GNN
architectures and expressivity measures that partition graphs into equivalence
classes. These results unify the study of expressivity and generalization in
GNNs, providing a principled understanding of why increasing expressive power
often comes at the cost of generalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interpretable Clustering with Adaptive Heterogeneous Causal Structure
  Learning in Mixed Observational Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04415v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04415v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenrui Li, Qinghao Zhang, Xiaowo Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding causal heterogeneity is essential for scientific discovery in
domains such as biology and medicine. However, existing methods lack causal
awareness, with insufficient modeling of heterogeneity, confounding, and
observational constraints, leading to poor interpretability and difficulty
distinguishing true causal heterogeneity from spurious associations. We propose
an unsupervised framework, HCL (Interpretable Causal Mechanism-Aware Clustering
with Adaptive Heterogeneous Causal Structure Learning), that jointly infers
latent clusters and their associated causal structures from mixed-type
observational data without requiring temporal ordering, environment labels,
interventions or other prior knowledge. HCL relaxes the homogeneity and
sufficiency assumptions by introducing an equivalent representation that
encodes both structural heterogeneity and confounding. It further develops a
bi-directional iterative strategy to alternately refine causal clustering and
structure learning, along with a self-supervised regularization that balance
cross-cluster universality and specificity. Together, these components enable
convergence toward interpretable, heterogeneous causal patterns. Theoretically,
we show identifiability of heterogeneous causal structures under mild
conditions. Empirically, HCL achieves superior performance in both clustering
and structure learning tasks, and recovers biologically meaningful mechanisms
in real-world single-cell perturbation data, demonstrating its utility for
discovering interpretable, mechanism-level causal heterogeneity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.14111v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.14111v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoya Li, Xiaofei Sun, Albert Wang, Jiwei Li, Chris Shum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The exponential growth in demand for GPU computing resources has created an
urgent need for automated CUDA optimization strategies. While recent advances
in LLMs show promise for code generation, current SOTA models achieve low
success rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an
automated reinforcement learning framework for CUDA optimization that employs a
novel contrastive RL algorithm.
  CUDA-L1 achieves significant performance improvements on the CUDA
optimization task: trained on A100, it delivers an average speedup of x3.12
with a median speedup of x1.42 against default baselines over across all 250
CUDA kernels of KernelBench, with peak speedups reaching x120. In addition to
the default baseline provided by KernelBench, CUDA-L1 demonstrates x2.77 over
Torch Compile, x2.88 over Torch Compile with reduce overhead, x2.81 over CUDA
Graph implementations, and remarkably x7.72 over cuDNN libraries. Furthermore,
the model also demonstrates portability across different GPU architectures.
  Beyond these benchmark results, CUDA-L1 demonstrates several properties: it
1) discovers a variety of CUDA optimization techniques and learns to combine
them strategically to achieve optimal performance; 2) uncovers fundamental
principles of CUDA optimization, such as the multiplicative nature of
optimizations; 3) identifies non-obvious performance bottlenecks and rejects
seemingly beneficial optimizations that actually harm performance. The
capabilities demonstrate that, RL can transform an initially poor-performing
LLM into an effective CUDA optimizer through speedup-based reward signals
alone, without human expertise or domain knowledge. This paradigm opens
possibilities for automated optimization of CUDA operations, and holds promise
to substantially promote GPU efficiency and alleviate the rising pressure on
GPU computing resources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://deepreinforce-ai.github.io/cudal1_blog/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mixture-of-Experts Meets In-Context Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05426v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05426v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Wu, Fuhong Liu, Haoru Li, Zican Hu, Daoyi Dong, Chunlin Chen, Zhi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context reinforcement learning (ICRL) has emerged as a promising paradigm
for adapting RL agents to downstream tasks through prompt conditioning.
However, two notable challenges remain in fully harnessing in-context learning
within RL domains: the intrinsic multi-modality of the state-action-reward data
and the diverse, heterogeneous nature of decision tasks. To tackle these
challenges, we propose T2MIR (Token- and Task-wise MoE for In-context RL), an
innovative framework that introduces architectural advances of
mixture-of-experts (MoE) into transformer-based decision models. T2MIR
substitutes the feedforward layer with two parallel layers: a token-wise MoE
that captures distinct semantics of input tokens across multiple modalities,
and a task-wise MoE that routes diverse tasks to specialized experts for
managing a broad task distribution with alleviated gradient conflicts. To
enhance task-wise routing, we introduce a contrastive learning method that
maximizes the mutual information between the task and its router
representation, enabling more precise capture of task-relevant information. The
outputs of two MoE components are concatenated and fed into the next layer.
Comprehensive experiments show that T2MIR significantly facilitates in-context
learning capacity and outperforms various types of baselines. We bring the
potential and promise of MoE to ICRL, offering a simple and scalable
architectural enhancement to advance ICRL one step closer toward achievements
in language and vision communities. Our code is available at
https://github.com/NJU-RL/T2MIR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 13 figures, 17 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An unsupervised tour through the hidden pathways of deep neural networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.21582v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.21582v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Diego Doimo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of this thesis is to improve our understanding of the internal
mechanisms by which deep artificial neural networks create meaningful
representations and are able to generalize. We focus on the challenge of
characterizing the semantic content of the hidden representations with
unsupervised learning tools, partially developed by us and described in this
thesis, which allow harnessing the low-dimensional structure of the data.
Chapter 2. introduces Gride, a method that allows estimating the intrinsic
dimension of the data as an explicit function of the scale without performing
any decimation of the data set. Our approach is based on rigorous
distributional results that enable the quantification of uncertainty of the
estimates. Moreover, our method is simple and computationally efficient since
it relies only on the distances among nearest data points. In Chapter 3, we
study the evolution of the probability density across the hidden layers in some
state-of-the-art deep neural networks. We find that the initial layers generate
a unimodal probability density getting rid of any structure irrelevant to
classification. In subsequent layers, density peaks arise in a hierarchical
fashion that mirrors the semantic hierarchy of the concepts. This process
leaves a footprint in the probability density of the output layer, where the
topography of the peaks allows reconstructing the semantic relationships of the
categories. In Chapter 4, we study the problem of generalization in deep neural
networks: adding parameters to a network that interpolates its training data
will typically improve its generalization performance, at odds with the
classical bias-variance trade-off. We show that wide neural networks learn
redundant representations instead of overfitting to spurious correlation and
that redundant neurons appear only if the network is regularized and the
training error is zero.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>PhD thesis</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Global urban visual perception varies across demographics and
  personalities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12758v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12758v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matias Quintana, Youlong Gu, Xiucheng Liang, Yujun Hou, Koichi Ito, Yihan Zhu, Mahmoud Abdelrahman, Filip Biljecki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding people's preferences is crucial for urban planning, yet current
approaches often combine responses from multi-cultural populations, obscuring
demographic differences and risking amplifying biases. We conducted a
largescale urban visual perception survey of streetscapes worldwide using
street view imagery, examining how demographics -- including gender, age,
income, education, race and ethnicity, and personality traits -- shape
perceptions among 1,000 participants with balanced demographics from five
countries and 45 nationalities. This dataset, Street Perception Evaluation
Considering Socioeconomics (SPECS), reveals demographic- and personality-based
differences across six traditional indicators -- safe, lively, wealthy,
beautiful, boring, depressing -- and four new ones -- live nearby, walk, cycle,
green. Location-based sentiments further shape these preferences. Machine
learning models trained on existing global datasets tend to overestimate
positive indicators and underestimate negative ones compared to human
responses, underscoring the need for local context. Our study aspires to
rectify the myopic treatment of street perception, which rarely considers
demographics or personality traits.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A High-Dimensional Statistical Method for Optimizing Transfer Quantities
  in Multi-Source Transfer Learning <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.04242v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.04242v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyue Zhang, Haohao Fu, Guanbo Huang, Yaoyuan Liang, Chang Chu, Tianren Peng, Yanru Wu, Qi Li, Yang Li, Shao-Lun Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-source transfer learning provides an effective solution to data
scarcity in real-world supervised learning scenarios by leveraging multiple
source tasks. In this field, existing works typically use all available samples
from sources in training, which constrains their training efficiency and may
lead to suboptimal results. To address this, we propose a theoretical framework
that answers the question: what is the optimal quantity of source samples
needed from each source task to jointly train the target model? Specifically,
we introduce a generalization error measure based on K-L divergence, and
minimize it based on high-dimensional statistical analysis to determine the
optimal transfer quantity for each source task. Additionally, we develop an
architecture-agnostic and data-efficient algorithm OTQMS to implement our
theoretical results for target model training in multi-source transfer
learning. Experimental studies on diverse architectures and two real-world
benchmark datasets show that our proposed algorithm significantly outperforms
state-of-the-art approaches in both accuracy and data efficiency. The code and
supplementary materials are available in https://github.com/zqy0126/OTQMS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025 Poster</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PTQTP: Post-Training Quantization to Trit-Planes for Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.16989v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.16989v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        He Xiao, Runming Yang, Qingyao Yang, Wendong Xu, Zhen Li, Yupeng Su, Zhengwu Liu, Hongxia Yang, Ngai Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Post-training quantization (PTQ) of large language models (LLMs) to extremely
low bit-widths remains challenging due to the fundamental trade-off between
computational efficiency and model expressiveness. While existing ultra-low-bit
PTQ methods rely on binary approximations or complex compensation mechanisms,
they suffer from either limited representational capacity or computational
overhead that undermines their efficiency gains. We introduce PTQ to
Trit-Planes (PTQTP), the first ternary-weight PTQ framework that decomposes
weight matrices into structured ternary {-1, 0, 1} trit-planes using 2x1.58-bit
representation. PTQTP achieves multiplication-free inference, identical to
1-bit quantization, while maintaining superior expressiveness through its novel
structured decomposition. Our approach provides: (1) a theoretically grounded
progressive approximation algorithm ensuring global weight consistency; (2)
model-agnostic deployment across diverse modern LLMs without architectural
modifications; and (3) uniform ternary operations that eliminate the need for
mixed-precision or compensation schemes. Comprehensive experiments across
LLaMA3.x and Qwen3 model families (0.6B-70B parameters) demonstrate that PTQTP
significantly outperforms existing low-bit PTQ methods, achieving 82.4%
mathematical reasoning retention versus 0% for competing approaches. PTQTP
approaches and sometimes surpasses 1.58-bit quantization-aware training
performance while requiring only single-hour quantization compared to 10-14 GPU
days for training-based methods. These results establish PTQTP as a practical
solution for efficient LLM deployment in resource-constrained environments. The
code will be available at https://github.com/HeXiao-55/PTQTP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">6</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BLM$_1$: A Boundless Large Model for Cross-Space, Cross-Task, and
  Cross-Embodiment Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24161v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24161v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wentao Tan, Bowen Wang, Heng Zhi, Chenyu Liu, Zhe Li, Jian Liu, Zengrong Lin, Yukun Dai, Yipeng Chen, Wenjie Yang, Enci Xie, Hao Xue, Baixu Ji, Chen Xu, Zhibin Wang, Tianshi Wang, Lei Zhu, Heng Tao Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) have advanced vision-language
reasoning and are increasingly deployed in embodied agents. However,
significant limitations remain: MLLMs generalize poorly across digital-physical
spaces and embodiments; vision-language-action models (VLAs) produce low-level
actions yet lack robust high-level embodied reasoning; and most embodied large
language models (ELLMs) are constrained to digital-space with poor
generalization to the physical world. Thus, unified models that operate
seamlessly across digital and physical spaces while generalizing across
embodiments and tasks remain absent. We introduce the \textbf{Boundless Large
Model (BLM$_1$)}, a multimodal spatial foundation model that preserves
instruction following and reasoning, incorporates embodied knowledge, and
supports robust cross-embodiment control. BLM$_1$ integrates three key
capabilities -- \textit{cross-space transfer, cross-task learning, and
cross-embodiment generalization} -- via a two-stage training paradigm. Stage I
injects embodied knowledge into the MLLM through curated digital corpora while
maintaining language competence. Stage II trains a policy module through an
intent-bridging interface that extracts high-level semantics from the MLLM to
guide control, without fine-tuning the MLLM backbone. This process is supported
by a self-collected cross-embodiment demonstration suite spanning four robot
embodiments and six progressively challenging tasks. Evaluations across digital
and physical benchmarks show that a single BLM$_1$ instance outperforms four
model families -- MLLMs, ELLMs, VLAs, and GMLMs -- achieving
$\sim\!\textbf{6%}$ gains in digital tasks and $\sim\!\textbf{3%}$ in physical
tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Model-Guided Dual-Role Alignment for High-Fidelity Open-Domain
  Video-to-Audio Generation <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24103v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24103v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kang Zhang, Trung X. Pham, Suyeon Lee, Axi Niu, Arda Senocak, Joon Son Chung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present MGAudio, a novel flow-based framework for open-domain
video-to-audio generation, which introduces model-guided dual-role alignment as
a central design principle. Unlike prior approaches that rely on
classifier-based or classifier-free guidance, MGAudio enables the generative
model to guide itself through a dedicated training objective designed for
video-conditioned audio generation. The framework integrates three main
components: (1) a scalable flow-based Transformer model, (2) a dual-role
alignment mechanism where the audio-visual encoder serves both as a
conditioning module and as a feature aligner to improve generation quality, and
(3) a model-guided objective that enhances cross-modal coherence and audio
realism. MGAudio achieves state-of-the-art performance on VGGSound, reducing
FAD to 0.40, substantially surpassing the best classifier-free guidance
baselines, and consistently outperforms existing methods across FD, IS, and
alignment metrics. It also generalizes well to the challenging UnAV-100
benchmark. These results highlight model-guided dual-role alignment as a
powerful and scalable paradigm for conditional video-to-audio generation. Code
is available at: https://github.com/pantheon5100/mgaudio
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Resi-VidTok: An Efficient and Decomposed Progressive Tokenization
  Framework for Ultra-Low-Rate and Lightweight Video Transmission 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.25002v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.25002v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyu Liu, Yi Ma, Rahim Tafazolli, Zhi Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-time transmission of video over wireless networks remains highly
challenging, even with advanced deep models, particularly under severe channel
conditions such as limited bandwidth and weak connectivity. In this paper, we
propose Resi-VidTok, a Resilient Tokenization-Enabled framework designed for
ultra-low-rate and lightweight video transmission that delivers strong
robustness while preserving perceptual and semantic fidelity on commodity
digital hardware. By reorganizing spatio--temporal content into a discrete,
importance-ordered token stream composed of key tokens and refinement tokens,
Resi-VidTok enables progressive encoding, prefix-decodable reconstruction, and
graceful quality degradation under constrained channels. A key contribution is
a resilient 1D tokenization pipeline for video that integrates differential
temporal token coding, explicitly supporting reliable recovery from incomplete
token sets using a single shared framewise decoder--without auxiliary temporal
extractors or heavy generative models. Furthermore, stride-controlled frame
sparsification combined with a lightweight decoder-side interpolator reduces
transmission load while maintaining motion continuity. Finally, a
channel-adaptive source--channel coding and modulation scheme dynamically
allocates rate and protection according to token importance and channel
condition, yielding stable quality across adverse SNRs. Evaluation results
indicate robust visual and semantic consistency at channel bandwidth ratios
(CBR) as low as 0.0004 and real-time reconstruction at over 30 fps,
demonstrating the practicality of Resi-VidTok for energy-efficient,
latency-sensitive, and reliability-critical wireless applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MCIHN: A Hybrid Network Model Based on Multi-path Cross-modal
  Interaction for Multimodal Emotion Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24827v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24827v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyang Zhang, Zhou Yang, Ke Sun, Yucai Pang, Guoliang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal emotion recognition is crucial for future human-computer
interaction. However, accurate emotion recognition still faces significant
challenges due to differences between different modalities and the difficulty
of characterizing unimodal emotional information. To solve these problems, a
hybrid network model based on multipath cross-modal interaction (MCIHN) is
proposed. First, adversarial autoencoders (AAE) are constructed separately for
each modality. The AAE learns discriminative emotion features and reconstructs
the features through a decoder to obtain more discriminative information about
the emotion classes. Then, the latent codes from the AAE of different
modalities are fed into a predefined Cross-modal Gate Mechanism model (CGMM) to
reduce the discrepancy between modalities, establish the emotional relationship
between interacting modalities, and generate the interaction features between
different modalities. Multimodal fusion using the Feature Fusion module (FFM)
for better emotion recognition. Experiments were conducted on publicly
available SIMS and MOSI datasets, demonstrating that MCIHN achieves superior
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper will be published in the MMAsia2025 conference proceedings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mano Technical Report 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.17336v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.17336v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyu Fu, Anyang Su, Chenxu Zhao, Hanning Wang, Minghui Wu, Zhe Yu, Fei Hu, Mingjia Shi, Wei Dong, Jiayao Wang, Yuyang Chen, Ruiyang Yu, Siran Peng, Menglin Li, Nan Huang, Haitian Wei, Jiawei Yu, Yi Xin, Xilin Zhao, Kai Gu, Ping Jiang, Sifan Zhou, Shuo Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graphical user interfaces (GUIs) are the primary medium for human-computer
interaction, yet automating GUI interactions remains challenging due to the
complexity of visual elements, dynamic environments, and the need for
multi-step reasoning. Existing methods based on vision-language models (VLMs)
often suffer from limited resolution, domain mismatch, and insufficient
sequential decisionmaking capability. To address these issues, we propose Mano,
a robust GUI agent built upon a multi-modal foundation model pre-trained on
extensive web and computer system data. Our approach integrates a novel
simulated environment for high-fidelity data generation, a three-stage training
pipeline (supervised fine-tuning, offline reinforcement learning, and online
reinforcement learning), and a verification module for error recovery. Mano
demonstrates state-of-the-art performance on multiple GUI benchmarks, including
Mind2Web and OSWorld, achieving significant improvements in success rate and
operational accuracy. Our work provides new insights into the effective
integration of reinforcement learning with VLMs for practical GUI agent
deployment, highlighting the importance of domain-specific data, iterative
training, and holistic reward design.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Does CLIP perceive art the same way we do? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.05229v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.05229v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Asperti, Leonardo Dessì, Maria Chiara Tonetti, Nico Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  CLIP has emerged as a powerful multimodal model capable of connecting images
and text through joint embeddings, but to what extent does it 'see' the same
way humans do - especially when interpreting artworks? In this paper, we
investigate CLIP's ability to extract high-level semantic and stylistic
information from paintings, including both human-created and AI-generated
imagery. We evaluate its perception across multiple dimensions: content, scene
understanding, artistic style, historical period, and the presence of visual
deformations or artifacts. By designing targeted probing tasks and comparing
CLIP's responses to human annotations and expert benchmarks, we explore its
alignment with human perceptual and contextual understanding. Our findings
reveal both strengths and limitations in CLIP's visual representations,
particularly in relation to aesthetic cues and artistic intent. We further
discuss the implications of these insights for using CLIP as a guidance
mechanism during generative processes, such as style transfer or prompt-based
image synthesis. Our work highlights the need for deeper interpretability in
multimodal systems, especially when applied to creative domains where nuance
and subjectivity play a central role.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-10-27T00:00:00Z">2025-10-27</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">100</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Variational Masked Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23606v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23606v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichi Zhang, Alex Schwing, Zhizhen Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Masked diffusion models have recently emerged as a flexible framework for
discrete generative modeling. However, a key limitation of standard masked
diffusion is its inability to effectively capture dependencies among tokens
that are predicted concurrently, leading to degraded generation quality when
dependencies among tokens are important. To explicitly model dependencies among
tokens, we propose Variational Masked Diffusion (VMD), a framework that
introduces latent variables into the masked diffusion process. Through
controlled experiments on synthetic datasets, we demonstrate that VMD
successfully learns dependencies that conventional masked diffusion fails to
capture. We further validate the effectiveness of our approach on Sudoku
puzzles and text datasets, where learning of dependencies among tokens improves
global consistency. Across these domains, VMD enhances both generation quality
and dependency awareness, highlighting the value of integrating variational
inference into masked diffusion. Our code is available at:
https://riccizz.github.io/VMD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://riccizz.github.io/VMD</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Think Twice: Branch-and-Rethink Reasoning Reward Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23596v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23596v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhu Jiao, Jiaqi Zeng, Julien Veron Vialard, Oleksii Kuchaiev, Jiawei Han, Olivier Delalleau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) increasingly rely on thinking models that
externalize intermediate steps and allocate extra test-time compute, with
think-twice strategies showing that a deliberate second pass can elicit
stronger reasoning. In contrast, most reward models (RMs) still compress many
quality dimensions into a single scalar in one shot, a design that induces
judgment diffusion: attention spreads across evaluation criteria, yielding
diluted focus and shallow analysis. We introduce branch-and-rethink (BR-RM), a
two-turn RM that transfers the think-twice principle to reward modeling. Turn 1
performs adaptive branching, selecting a small set of instance-critical
dimensions (such as factuality and safety) and sketching concise,
evidence-seeking hypotheses. Turn 2 executes branch-conditioned rethinking, a
targeted reread that tests those hypotheses and scrutinizes only what matters
most. We train with GRPO-style reinforcement learning over structured two-turn
traces using a simple binary outcome reward with strict format checks, making
the approach compatible with standard RLHF pipelines. By converting
all-at-oncescoringintofocused, second-lookreasoning,
BR-RMreducesjudgmentdiffusionandimproves sensitivity to subtle yet
consequential errors while remaining practical and scalable. Experimental
results demonstrate that our model achieves state-of-the-art performance on
three challenging reward modeling benchmarks across diverse domains. The code
and the model will be released soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hope Speech Detection in Social Media English Corpora: Performance of
  Traditional and <span class="highlight-title">Transformer</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23585v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23585v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luis Ramos, Hiram Calvo, Olga Kolesnikova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The identification of hope speech has become a promised NLP task, considering
the need to detect motivational expressions of agency and goal-directed
behaviour on social media platforms. This proposal evaluates traditional
machine learning models and fine-tuned transformers for a previously split hope
speech dataset as train, development and test set. On development test, a
linear-kernel SVM and logistic regression both reached a macro-F1 of 0.78; SVM
with RBF kernel reached 0.77, and Na\"ive Bayes hit 0.75. Transformer models
delivered better results, the best model achieved weighted precision of 0.82,
weighted recall of 0.80, weighted F1 of 0.79, macro F1 of 0.79, and 0.80
accuracy. These results suggest that while optimally configured traditional
machine learning models remain agile, transformer architectures detect some
subtle semantics of hope to achieve higher precision and recall in hope speech
detection, suggesting that larges transformers and LLMs could perform better in
small datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReCode: Unify Plan and Action for Universal Granularity Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23564v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23564v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoyang Yu, Jiayi Zhang, Huixue Su, Yufan Zhao, Yifan Wu, Mingyi Deng, Jinyu Xiang, Yizhang Lin, Lingxiao Tang, Yingchao Li, Yuyu Luo, Bang Liu, Chenglin Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world tasks require decisions at varying granularities, and humans excel
at this by leveraging a unified cognitive representation where planning is
fundamentally understood as a high-level form of action. However, current Large
Language Model (LLM)-based agents lack this crucial capability to operate
fluidly across decision granularities. This limitation stems from existing
paradigms that enforce a rigid separation between high-level planning and
low-level action, which impairs dynamic adaptability and limits generalization.
We propose ReCode (Recursive Code Generation), a novel paradigm that addresses
this limitation by unifying planning and action within a single code
representation. In this representation, ReCode treats high-level plans as
abstract placeholder functions, which the agent then recursively decomposes
into finer-grained sub-functions until reaching primitive actions. This
recursive approach dissolves the rigid boundary between plan and action,
enabling the agent to dynamically control its decision granularity.
Furthermore, the recursive structure inherently generates rich,
multi-granularity training data, enabling models to learn hierarchical
decision-making processes. Extensive experiments show ReCode significantly
surpasses advanced baselines in inference performance and demonstrates
exceptional data efficiency in training, validating our core insight that
unifying planning and action through recursive code generation is a powerful
and effective approach to achieving universal granularity control. The code is
available at https://github.com/FoundationAgents/ReCode.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ISA-Bench: Benchmarking Instruction Sensitivity for Large Audio Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23558v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23558v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bohan Li, Wenbin Huang, Yuhang Qiu, Yiwei Guo, Hankun Wang, Zhihan Li, Jing Peng, Ziyang Ma, Xie Chen, Kai Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Audio Language Models (LALMs), which couple acoustic perception with
large language models (LLMs) to extract and understand diverse information from
audio, have attracted intense interest from both academic and industrial
communities. However, existing LALMs are highly sensitive to how instructions
are phrased, affecting both (i) instruction-following rates and (ii) task
performance. Yet, no existing benchmarks offer a systematic and comprehensive
evaluation of this sensitivity. We introduce ISA-Bench, a dynamic benchmark
evaluating instruction sensitivity for LALMs along three axes: instruction
description, output format, and task composition. We assess recent open-source
and proprietary LALMs using ISA-Bench, profiling both compliance and accuracy
under controlled instruction variations. Experimental results reveal that even
state-of-the-art LALMs suffer significant instruction sensitivity, leading to
degraded performance on fundamental audio understanding tasks. To mitigate this
issue, we fine-tune Qwen2-Audio on a specifically constructed complex
instruction-variant dataset, achieving a marked improvement in
instruction-following performance. However, this also induces nontrivial
catastrophic forgetting: the model loses some previously mastered task
capabilities when exposed to new instruction styles. Our benchmark provides a
standardized basis for assessing and improving instruction sensitivity in
LALMs, underscoring the need for instruction-robust audio understanding in
real-world pipelines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to icassp 2026</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A U-Net and <span class="highlight-title">Transformer</span> Pipeline for Multilingual Image Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23554v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23554v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddharth Sahay, Radhika Agarwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an end-to-end multilingual translation pipeline that
integrates a custom U-Net for text detection, the Tesseract engine for text
recognition, and a from-scratch sequence-to-sequence (Seq2Seq) Transformer for
Neural Machine Translation (NMT). Our approach first utilizes a U-Net model,
trained on a synthetic dataset , to accurately segment and detect text regions
from an image. These detected regions are then processed by Tesseract to
extract the source text. This extracted text is fed into a custom Transformer
model trained from scratch on a multilingual parallel corpus spanning 5
languages. Unlike systems reliant on monolithic pre-trained models, our
architecture emphasizes full customization and adaptability. The system is
evaluated on its text detection accuracy, text recognition quality, and
translation performance via BLEU scores. The complete pipeline demonstrates
promising results, validating the viability of a custom-built system for
translating text directly from images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 3 figures, 5 tables, and 2 algorithms. Prepared in IEEE
  double-column format</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LimRank: Less is More for Reasoning-Intensive Information Reranking <span class="chip">EMNLP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23544v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23544v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tingyu Song, Yilun Zhao, Siyue Zhang, Chen Zhao, Arman Cohan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing approaches typically rely on large-scale fine-tuning to adapt LLMs
for information reranking tasks, which is computationally expensive. In this
work, we demonstrate that modern LLMs can be effectively adapted using only
minimal, high-quality supervision. To enable this, we design
LIMRANK-SYNTHESIZER, a reusable and open-source pipeline for generating
diverse, challenging, and realistic reranking examples. Using this synthetic
data, we fine-tune our reranker model, LIMRANK. We evaluate LIMRANK on two
challenging benchmarks, i.e., BRIGHT for reasoning-intensive retrieval and
FollowIR for instruction-following retrieval. Our experiments demonstrate that
LIMRANK achieves competitive performance, while being trained on less than 5%
of the data typically used in prior work. Further ablation studies demonstrate
the effectiveness of LIMRANK-SYNTHESIZER and the strong generalization
capabilities of LIMRANK across downstream tasks, including scientific
literature search and retrieval-augmented generation for knowledge-intensive
problem solving.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2025 Main (Short)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ JanusCoder: Towards a Foundational Visual-Programmatic Interface for
  Code Intelligence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23538v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23538v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiushi Sun, Jingyang Gong, Yang Liu, Qiaosheng Chen, Lei Li, Kai Chen, Qipeng Guo, Ben Kao, Fei Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The scope of neural code intelligence is rapidly expanding beyond text-based
source code to encompass the rich visual outputs that programs generate. This
visual dimension is critical for advanced applications like flexible content
generation and precise, program-driven editing of visualizations. However,
progress has been impeded by the scarcity of high-quality multimodal code data,
a bottleneck stemming from challenges in synthesis and quality assessment. To
address these challenges, we make contributions from both a data and modeling
perspective. We first introduce a complete synthesis toolkit that leverages
reciprocal synergies between data modalities to efficiently produce a
large-scale, high-quality corpus spanning from standard charts to complex
interactive web UIs and code-driven animations. Leveraging this toolkit, we
construct JanusCode-800K, the largest multimodal code corpus to date. This
powers the training of our models, JanusCoder and JanusCoderV, which establish
a visual-programmatic interface for generating code from textual instructions,
visual inputs, or a combination of both. Our unified model is a departure from
existing approaches that build specialized models for isolated tasks. Extensive
experiments on both text-centric and vision-centric coding tasks demonstrate
the superior performance of the JanusCoder series, with our 7B to 14B scale
models approaching or even exceeding the performance of commercial models.
Furthermore, extensive analysis provides key insights into harmonizing
programmatic logic with its visual expression. Our code and checkpoints will
are available at https://github.com/InternLM/JanusCoder.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IPQA: A Benchmark for Core Intent Identification in Personalized
  Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23536v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23536v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jieyong Kim, Maryam Amirizaniani, Soojin Yoon, Dongha Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intent identification serves as the foundation for generating appropriate
responses in personalized question answering (PQA). However, existing
benchmarks evaluate only response quality or retrieval performance without
directly measuring intent identification capabilities. This gap is critical
because without understanding which intents users prioritize, systems cannot
generate responses satisfying individual information needs. To address this, we
introduce the concept of core intents: intents users prioritize when selecting
answers to satisfy their information needs. To evaluate these core intents, we
propose IPQA, a benchmark for core Intent identification in Personalized
Question Answering. Since users do not explicitly state their prioritized
intents, we derive core intents from observable behavior patterns in answer
selection, grounded in satisficing theory where users choose answers meeting
their acceptance thresholds. We construct a dataset with various domains
through systematic filtering, LLM-based annotation, and rigorous quality
control combining automated verification with human validation. Experimental
evaluations across state-of-the-art language models reveal that current systems
struggle with core intent identification in personalized contexts. Models fail
to identify core intents from user histories, with performance degrading as
question complexity increases. The code and dataset will be made publicly
available to facilitate future research in this direction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ M4FC: a Multimodal, Multilingual, Multicultural, Multitask Real-World
  Fact-Checking <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23508v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23508v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahui Geng, Jonathan Tonglet, Iryna Gurevych
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing real-world datasets for multimodal automated fact-checking have
multiple limitations: they contain few instances, focus on only one or two
languages and tasks, suffer from evidence leakage, or depend on external sets
of news articles for sourcing true claims. To address these shortcomings, we
introduce M4FC, a new real-world dataset comprising 4,982 images paired with
6,980 claims. The images, verified by professional fact-checkers from 22
organizations, represent diverse cultural and geographic contexts. Each claim
is available in one or two out of ten languages. M4FC spans six multimodal
fact-checking tasks: visual claim extraction, claimant intent prediction, fake
detection, image contextualization, location verification, and verdict
prediction. We provide baseline results for all tasks and analyze how combining
intermediate tasks influence downstream verdict prediction performance. We make
our dataset and code available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint under review. Code and data available at:
  https://github.com/UKPLab/M4FC</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MMTutorBench: The First Multimodal Benchmark for AI Math Tutoring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23477v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23477v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tengchao Yang, Sichen Guo, Mengzhao Jia, Jiaming Su, Yuanyang Liu, Zhihan Zhang, Meng Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective math tutoring requires not only solving problems but also
diagnosing students' difficulties and guiding them step by step. While
multimodal large language models (MLLMs) show promise, existing benchmarks
largely overlook these tutoring skills. We introduce MMTutorBench, the first
benchmark for AI math tutoring, consisting of 685 problems built around
pedagogically significant key-steps. Each problem is paired with
problem-specific rubrics that enable fine-grained evaluation across six
dimensions, and structured into three tasks-Insight Discovery, Operation
Formulation, and Operation Execution. We evaluate 12 leading MLLMs and find
clear performance gaps between proprietary and open-source systems, substantial
room compared to human tutors, and consistent trends across input variants: OCR
pipelines degrade tutoring quality, few-shot prompting yields limited gains,
and our rubric-based LLM-as-a-Judge proves highly reliable. These results
highlight both the difficulty and diagnostic value of MMTutorBench for
advancing AI tutoring.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Large Language Models for Stance Detection on Financial
  Targets from SEC Filing Reports and Earnings Call Transcripts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23464v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23464v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikesh Gyawali, Doina Caragea, Alex Vasenkov, Cornelia Caragea
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Financial narratives from U.S. Securities and Exchange Commission (SEC)
filing reports and quarterly earnings call transcripts (ECTs) are very
important for investors, auditors, and regulators. However, their length,
financial jargon, and nuanced language make fine-grained analysis difficult.
Prior sentiment analysis in the financial domain required a large, expensive
labeled dataset, making the sentence-level stance towards specific financial
targets challenging. In this work, we introduce a sentence-level corpus for
stance detection focused on three core financial metrics: debt, earnings per
share (EPS), and sales. The sentences were extracted from Form 10-K annual
reports and ECTs, and labeled for stance (positive, negative, neutral) using
the advanced ChatGPT-o3-pro model under rigorous human validation. Using this
corpus, we conduct a systematic evaluation of modern large language models
(LLMs) using zero-shot, few-shot, and Chain-of-Thought (CoT) prompting
strategies. Our results show that few-shot with CoT prompting performs best
compared to supervised baselines, and LLMs' performance varies across the SEC
and ECT datasets. Our findings highlight the practical viability of leveraging
LLMs for target-specific stance in the financial domain without requiring
extensive labeled data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BrowseConf: Confidence-Guided Test-Time Scaling for Web Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23458v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23458v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Litu Ou, Kuan Li, Huifeng Yin, Liwen Zhang, Zhongwang Zhang, Xixi Wu, Rui Ye, Zile Qiao, Yong Jiang, Pengjun Xie, Fei Huang, Jingren Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Confidence in LLMs is a useful indicator of model uncertainty and answer
reliability. Existing work mainly focused on single-turn scenarios, while
research on confidence in complex multi-turn interactions is limited. In this
paper, we investigate whether LLM-based search agents have the ability to
communicate their own confidence through verbalized confidence scores after
long sequences of actions, a significantly more challenging task compared to
outputting confidence in a single interaction. Experimenting on open-source
agentic models, we first find that models exhibit much higher task accuracy at
high confidence while having near-zero accuracy when confidence is low. Based
on this observation, we propose Test-Time Scaling (TTS) methods that use
confidence scores to determine answer quality, encourage the model to try again
until reaching a satisfactory confidence level. Results show that our proposed
methods significantly reduce token consumption while demonstrating competitive
performance compared to baseline fixed budget TTS methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Omni-Reward: Towards Generalist Omni-Modal Reward Modeling with
  Free-Form Preferences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23451v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23451v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoran Jin, Hongbang Yuan, Kejian Zhu, Jiachun Li, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reward models (RMs) play a critical role in aligning AI behaviors with human
preferences, yet they face two fundamental challenges: (1) Modality Imbalance,
where most RMs are mainly focused on text and image modalities, offering
limited support for video, audio, and other modalities; and (2) Preference
Rigidity, where training on fixed binary preference pairs fails to capture the
complexity and diversity of personalized preferences. To address the above
challenges, we propose Omni-Reward, a step toward generalist omni-modal reward
modeling with support for free-form preferences, consisting of: (1) Evaluation:
We introduce Omni-RewardBench, the first omni-modal RM benchmark with free-form
preferences, covering nine tasks across five modalities including text, image,
video, audio, and 3D; (2) Data: We construct Omni-RewardData, a multimodal
preference dataset comprising 248K general preference pairs and 69K
instruction-tuning pairs for training generalist omni-modal RMs; (3) Model: We
propose Omni-RewardModel, which includes both discriminative and generative
RMs, and achieves strong performance on Omni-RewardBench as well as other
widely used reward modeling benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>48 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Neuro-Symbolic Multi-Agent Approach to Legal-Cybersecurity Knowledge
  Integration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23443v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23443v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chiara Bonfanti, Alessandro Druetto, Cataldo Basile, Tharindu Ranasinghe, Marcos Zampieri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing intersection of cybersecurity and law creates a complex
information space where traditional legal research tools struggle to deal with
nuanced connections between cases, statutes, and technical vulnerabilities.
This knowledge divide hinders collaboration between legal experts and
cybersecurity professionals. To address this important gap, this work provides
a first step towards intelligent systems capable of navigating the increasingly
intricate cyber-legal domain. We demonstrate promising initial results on
multilingual tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EMTSF:Extraordinary Mixture of SOTA Models for Time Series Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23396v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23396v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Musleh Alharthi, Kaleel Mahmood, Sarosh Patel, Ausif Mahmood
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The immense success of the Transformer architecture
  in Natural Language Processing has led to its adoption in Time Se ries
Forecasting (TSF), where superior performance has been shown.
  However, a recent important paper questioned their effectiveness by
  demonstrating that a simple single layer linear model outperforms
  Transformer-based models. This was soon shown to be not as valid,
  by a better transformer-based model termed PatchTST. More re cently, TimeLLM
demonstrated even better results by repurposing a
  Large Language Model (LLM) for the TSF domain. Again, a follow
  up paper challenged this by demonstrating that removing the LLM
  component or replacing it with a basic attention layer in fact yields
  better performance. One of the challenges in forecasting is the fact
  that TSF data favors the more recent past, and is sometimes subject
  to unpredictable events. Based upon these recent insights in TSF, we
  propose a strong Mixture of Experts (MoE) framework. Our method
  combines the state-of-the-art (SOTA) models including xLSTM, en hanced
Linear, PatchTST, and minGRU, among others. This set of
  complimentary and diverse models for TSF are integrated in a Trans former
based MoE gating network. Our proposed model outperforms
  all existing TSF models on standard benchmarks, surpassing even the
  latest approaches based on MoE frameworks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detecting Religious Language in Climate Discourse 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23395v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23395v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Evy Beijen, Pien Pieterse, Yusuf Çelik, Willem Th. van Peursen, Sandjai Bhulai, Meike Morren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Religious language continues to permeate contemporary discourse, even in
ostensibly secular domains such as environmental activism and climate change
debates. This paper investigates how explicit and implicit forms of religious
language appear in climate-related texts produced by secular and religious
nongovernmental organizations (NGOs). We introduce a dual methodological
approach: a rule-based model using a hierarchical tree of religious terms
derived from ecotheology literature, and large language models (LLMs) operating
in a zero-shot setting. Using a dataset of more than 880,000 sentences, we
compare how these methods detect religious language and analyze points of
agreement and divergence. The results show that the rule-based method
consistently labels more sentences as religious than LLMs. These findings
highlight not only the methodological challenges of computationally detecting
religious language but also the broader tension over whether religious language
should be defined by vocabulary alone or by contextual meaning. This study
contributes to digital methods in religious studies by demonstrating both the
potential and the limitations of approaches for analyzing how the sacred
persists in climate discourse.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How AI Forecasts AI Jobs: Benchmarking LLM Predictions of Labor Market
  Changes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23358v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23358v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sheri Osborn, Rohit Valecha, H. Raghav Rao, Dan Sass, Anthony Rios
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence is reshaping labor markets, yet we lack tools to
systematically forecast its effects on employment. This paper introduces a
benchmark for evaluating how well large language models (LLMs) can anticipate
changes in job demand, especially in occupations affected by AI. Existing
research has shown that LLMs can extract sentiment, summarize economic reports,
and emulate forecaster behavior, but little work has assessed their use for
forward-looking labor prediction. Our benchmark combines two complementary
datasets: a high-frequency index of sector-level job postings in the United
States, and a global dataset of projected occupational changes due to AI
adoption. We format these data into forecasting tasks with clear temporal
splits, minimizing the risk of information leakage. We then evaluate LLMs using
multiple prompting strategies, comparing task-scaffolded, persona-driven, and
hybrid approaches across model families. We assess both quantitative accuracy
and qualitative consistency over time. Results show that structured task
prompts consistently improve forecast stability, while persona prompts offer
advantages on short-term trends. However, performance varies significantly
across sectors and horizons, highlighting the need for domain-aware prompting
and rigorous evaluation protocols. By releasing our benchmark, we aim to
support future research on labor forecasting, prompt design, and LLM-based
economic reasoning. This work contributes to a growing body of research on how
LLMs interact with real-world economic data, and provides a reproducible
testbed for studying the limits and opportunities of AI as a forecasting tool
in the context of labor markets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages + Limitations + References</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LightKGG: Simple and Efficient Knowledge Graph Generation from Textual
  Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23341v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23341v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Teng Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The scarcity of high-quality knowledge graphs (KGs) remains a critical
bottleneck for downstream AI applications, as existing extraction methods rely
heavily on error-prone pattern-matching techniques or resource-intensive large
language models (LLMs). While recent tools leverage LLMs to generate KGs, their
computational demands limit accessibility for low-resource environments. Our
paper introduces LightKGG, a novel framework that enables efficient KG
extraction from textual data using small-scale language models (SLMs) through
two key technical innovations: (1) Context-integrated Graph extraction
integrates contextual information with nodes and edges into a unified graph
structure, reducing the reliance on complex semantic processing while
maintaining more key information; (2) Topology-enhanced relationship inference
leverages the inherent topology of the extracted graph to efficiently infer
relationships, enabling relationship discovery without relying on complex
language understanding capabilities of LLMs. By enabling accurate KG
construction with minimal hardware requirements, this work bridges the gap
between automated knowledge extraction and practical deployment scenarios while
introducing scientifically rigorous methods for optimizing SLM efficiency in
structured NLP tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Planning Ahead with RSA: Efficient Signalling in Dynamic Environments by
  Projecting User Awareness across Future Timesteps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23340v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23340v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anwesha Das, John Duff, Jörg Hoffmann, Vera Demberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adaptive agent design offers a way to improve human-AI collaboration on
time-sensitive tasks in rapidly changing environments. In such cases, to ensure
the human maintains an accurate understanding of critical task elements, an
assistive agent must not only identify the highest priority information but
also estimate how and when this information can be communicated most
effectively, given that human attention represents a zero-sum cognitive
resource where focus on one message diminishes awareness of other or upcoming
information. We introduce a theoretical framework for adaptive signalling which
meets these challenges by using principles of rational communication,
formalised as Bayesian reference resolution using the Rational Speech Act (RSA)
modelling framework, to plan a sequence of messages which optimise timely
alignment between user belief and a dynamic environment. The agent adapts
message specificity and timing to the particulars of a user and scenario based
on projections of how prior-guided interpretation of messages will influence
attention to the interface and subsequent belief update, across several
timesteps out to a fixed horizon. In a comparison to baseline methods, we show
that this effectiveness depends crucially on combining multi-step planning with
a realistic model of user awareness. As the first application of RSA for
communication in a dynamic environment, and for human-AI interaction in
general, we establish theoretical foundations for pragmatic communication in
human-agent teams, highlighting how insights from cognitive science can be
capitalised to inform the design of assistive agents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BaZi-Based Character Simulation Benchmark: Evaluating AI on Temporal and
  Persona Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23337v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23337v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyuan Zheng, Pai Liu, Xi Chen, Jizheng Dong, Sihan Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human-like virtual characters are crucial for games, storytelling, and
virtual reality, yet current methods rely heavily on annotated data or
handcrafted persona prompts, making it difficult to scale up and generate
realistic, contextually coherent personas. We create the first QA dataset for
BaZi-based persona reasoning, where real human experiences categorized into
wealth, health, kinship, career, and relationships are represented as
life-event questions and answers. Furthermore, we propose the first BaZi-LLM
system that integrates symbolic reasoning with large language models to
generate temporally dynamic and fine-grained virtual personas. Compared with
mainstream LLMs such as DeepSeek-v3 and GPT-5-mini, our method achieves a
30.3%-62.6% accuracy improvement. In addition, when incorrect BaZi information
is used, our model's accuracy drops by 20%-45%, showing the potential of
culturally grounded symbolic-LLM integration for realistic character
simulation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Blockwise Search: Inference-Time Alignment for Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23334v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23334v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Atif Quamar, Mohammad Areeb, Nishant Sharma, Ananth Shreekumar, Jonathan Rosenthal, Muslum Ozgur Ozmen, Mikhail Kuznetsov, Z. Berkay Celik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLM alignment remains a critical challenge. Inference-time methods provide a
flexible alternative to fine-tuning, but their uniform computational effort
often yields suboptimal alignment. We hypothesize that for many alignment
tasks, the initial tokens of a response are disproportionately more critical.
To leverage this principle, we introduce AdaSearch, a novel blockwise search
strategy. It adaptively allocates a fixed computational budget using a sampling
schedule, focusing search effort on these critical tokens. We apply AdaSearch
to sequential decoding and introduce its tree-search counterpart, AdaBeam. Our
comprehensive evaluation across eight LLMs demonstrates that AdaSearch
outperforms strong Best-of-N and fine-tuning baselines. Specifically, win-rates
improve by over 10% for harmlessness generation, controlled sentiment
generation, and for mathematical reasoning tasks relative to Best-of-N.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LibriConvo: Simulating Conversations from Read Literature for ASR and
  Diarization <span class="chip">LREC 2026</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23320v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23320v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Máté Gedeon, Péter Mihajlik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce LibriConvo, a simulated multi-speaker conversational dataset
based on speaker-aware conversation simulation (SASC), designed to support
training and evaluation of speaker diarization and automatic speech recognition
(ASR) systems. Unlike prior resources that mostly rely on semantically
disconnected utterances and implausible temporal gaps, LibriConvo ensures
semantic coherence and realistic conversational timing. Our pipeline leverages
CallHome with external VAD for reliable boundaries, applies compression to
reduce unnaturally long silences, and organizes LibriTTS utterances by book to
maintain contextual consistency. Acoustic realism is enhanced via a novel room
impulse response selection procedure that ranks speaker-microphone
configurations by spatial plausibility, balancing realism and diversity. The
dataset comprises 240.1 hours across 1,496 dialogues with 830 unique speakers,
split in a speaker-disjoint manner for robust evaluation. Baselines show that
the sortformer model outperforms the pyannote pipeline in diarization, while a
fine-tuned Fast Conformer-CTC XLarge with Serialized Output Training achieves
7.29\% WER for ASR, surpassing zero-shot Whisper-large-v3. LibriConvo provides
a valuable resource for advancing multi-speaker speech processing research with
realistic conversational dynamics and controlled experimental conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to LREC 2026</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Arabic Little STT: Arabic Children Speech Recognition <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23319v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23319v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mouhand Alkadri, Dania Desouki, Khloud Al Jallad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The performance of Artificial Intelligence (AI) systems fundamentally depends
on high-quality training data. However, low-resource languages like Arabic
suffer from severe data scarcity. Moreover, the absence of child-specific
speech corpora is an essential gap that poses significant challenges. To
address this gap, we present our created dataset, Arabic Little STT, a dataset
of Levantine Arabic child speech recorded in classrooms, containing 355
utterances from 288 children (ages 6 - 13). We further conduct a systematic
assessment of Whisper, a state-of-the-art automatic speech recognition (ASR)
model, on this dataset and compare its performance with adult Arabic
benchmarks. Our evaluation across eight Whisper variants reveals that even the
best-performing model (Large_v3) struggles significantly, achieving a 0.66 word
error rate (WER) on child speech, starkly contrasting with its sub 0.20 WER on
adult datasets. These results align with other research on English speech.
Results highlight the critical need for dedicated child speech benchmarks and
inclusive training data in ASR development. Emphasizing that such data must be
governed by strict ethical and privacy frameworks to protect sensitive child
information. We hope that this study provides an initial step for future work
on equitable speech technologies for Arabic-speaking children. We hope that our
publicly available dataset enrich the children's demographic representation in
ASR datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DCMM-SQL: Automated Data-Centric Pipeline and Multi-Model Collaboration
  Training for Text-to-SQL Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23284v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23284v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanzhen Xie, Liu Ye, Jiqun Chu, Mochi Gao, Hehuan Liu, Yunzhi Tan, Bo Hu, Zang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-SQL tasks have gained attractive improvements since the release of
ChatGPT. Among them, agent-based frameworks have been widely used in this
field. However, the impact of data-centric strategies on text-to-SQL tasks has
rarely been explored. In this paper, we systemically design a fully automated
data-centric pipeline for text-to-SQL tasks, including \emph{adaptive data
repair}, which can automatically find and fix errors in the training dataset;
and \emph{error data augmentation}, where we specifically diffuse and enhance
erroneous data predicted by the initially trained models. Meanwhile, we propose
a Multi-Model collaboration training schema, aiming to train multiple models
with different augmented data, enabling them to possess distinct capabilities
and work together to complement each other, because it has been found that the
capability of a single fine-tuned model is very limited. Furthermore, we
utilize an ensemble strategy to integrate the capabilities of multiple models
to solve a multiple-choice question, aiming to further improve the accuracy of
text-to-SQL tasks. The experiment results and ablation study have demonstrated
the effectiveness of data-centric pipeline and Multi-Model(MM) interactive
iterative strategies, achieving first place in lightweight text-to-SQL models
(within 70B).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Cocktail-Party Benchmark: Multi-Modal <span class="highlight-title">dataset</span> and Comparative
  Evaluation Results <span class="chip">ICASSP 2026</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23276v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23276v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thai-Binh Nguyen, Katerina Zmolikova, Pingchuan Ma, Ngoc Quan Pham, Christian Fuegen, Alexander Waibel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the task of Multi-Modal Context-Aware Recognition (MCoRec) in
the ninth CHiME Challenge, which addresses the cocktail-party problem of
overlapping conversations in a single-room setting using audio, visual, and
contextual cues. MCoRec captures natural multi-party conversations where the
recordings focus on unscripted, casual group chats, leading to extreme speech
overlap of up to 100% and highly fragmented conversational turns. The task
requires systems to answer the question "Who speaks when, what, and with whom?"
by jointly transcribing each speaker's speech and clustering them into their
respective conversations from audio-visual recordings. Audio-only baselines
exceed 100% word error rate, whereas incorporating visual cues yields
substantial 50% improvements, highlighting the importance of multi-modality. In
this manuscript, we present the motivation behind the task, outline the data
collection process, and report the baseline systems developed for the MCoRec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICASSP 2026</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Code Aesthetics with Agentic Reward Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23272v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23272v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bang Xiao, Lingjie Jiang, Shaohan Huang, Tengchao Lv, Yupan Huang, Xun Wu, Lei Cui, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have become valuable assistants for developers
in code-related tasks. While LLMs excel at traditional programming tasks such
as code generation and bug fixing, they struggle with visually-oriented coding
tasks, often producing suboptimal aesthetics. In this paper, we introduce a new
pipeline to enhance the aesthetic quality of LLM-generated code. We first
construct AesCode-358K, a large-scale instruction-tuning dataset focused on
code aesthetics. Next, we propose agentic reward feedback, a multi-agent system
that evaluates executability, static aesthetics, and interactive aesthetics.
Building on this, we develop GRPO-AR, which integrates these signals into the
GRPO algorithm for joint optimization of functionality and code aesthetics.
Finally, we develop OpenDesign, a benchmark for assessing code aesthetics.
Experimental results show that combining supervised fine-tuning on AesCode-358K
with reinforcement learning using agentic reward feedback significantly
improves performance on OpenDesign and also enhances results on existing
benchmarks such as PandasPlotBench. Notably, our AesCoder-4B surpasses GPT-4o
and GPT-4.1, and achieves performance comparable to large open-source models
with 480B-685B parameters, underscoring the effectiveness of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mubeen AI: A Specialized Arabic Language Model for Heritage Preservation
  and User Intent Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23271v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23271v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammed Aljafari, Ismail Alturki, Ahmed Mori, Yehya Kadumi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mubeen is a proprietary Arabic language model developed by MASARAT SA,
optimized for deep understanding of Arabic linguistics, Islamic studies, and
cultural heritage. Trained on an extensive collection of authentic Arabic
sources significantly expanded by digitizing historical manuscripts via a
proprietary Arabic OCR engine, the model incorporates seminal scholarly works
in linguistics, jurisprudence, hadith, and Quranic exegesis, alongside
thousands of academic theses and peer-reviewed research papers. Conditioned
through a deep linguistic engineering framework, Mubeen masters not just the
meaning but the eloquence of Arabic, enabling precise understanding across
classical texts, contemporary writing, and regional dialects with focus on
comprehending user intent and delivering accurate, contextually relevant
responses. Unlike other Arabic models relying on translated English data that
often fail in intent detection or retrieval-augmented generation (RAG), Mubeen
uses native Arabic sources to ensure cultural authenticity and accuracy. Its
core innovation is the Practical Closure Architecture, designed to solve the
"Utility Gap Crisis" where factually correct answers fail to resolve users'
core needs, forcing them into frustrating cycles of re-prompting. By
prioritizing clarity and decisive guidance, Mubeen transforms from an
information repository into a decisive guide, aligning with Saudi Vision 2030.
The model's architecture combines deep heritage specialization with
multi-disciplinary expert modules, enabling robust performance across both
cultural preservation and general knowledge domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 2 figures, 3 tables. Includes appendices on ethical
  guidelines and training framework. Submitted September 04, 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are ASR foundation models generalized enough to capture features of
  regional dialects for low-resource languages? <span class="chip">AACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23252v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23252v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tawsif Tashwar Dipto, Azmol Hossain, Rubayet Sabbir Faruque, Md. Rezuwan Hassan, Kanij Fatema, Tanmoy Shome, Ruwad Naswan, Md. Foriduzzaman Zihad, Mohaymen Ul Anam, Nazia Tasnim, Hasan Mahmud, Md Kamrul Hasan, Md. Mehedi Hasan Shawon, Farig Sadeque, Tahsin Reasat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional research on speech recognition modeling relies on the canonical
form for most low-resource languages while automatic speech recognition (ASR)
for regional dialects is treated as a fine-tuning task. To investigate the
effects of dialectal variations on ASR we develop a 78-hour annotated Bengali
Speech-to-Text (STT) corpus named Ben-10. Investigation from linguistic and
data-driven perspectives shows that speech foundation models struggle heavily
in regional dialect ASR, both in zero-shot and fine-tuned settings. We observe
that all deep learning methods struggle to model speech data under dialectal
variations but dialect specific model training alleviates the issue. Our
dataset also serves as a out of-distribution (OOD) resource for ASR modeling
under constrained resources in ASR algorithms. The dataset and code developed
for this project are publicly available
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This manuscript contains 11 pages, 5 tables and 16 figures This was
  accepted at International Joint Conference on Natural Language Processing &
  Asia-Pacific Chapter of the Association for Computational Linguistics
  (IJCNLP-AACL) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Process Reward Models for Sentence-Level Verification of LVLM Radiology
  Reports 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23217v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23217v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alois Thomas, Maya Varma, Jean-Benoit Delbrouck, Curtis P. Langlotz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automating radiology report generation with Large Vision-Language Models
(LVLMs) holds great potential, yet these models often produce clinically
critical hallucinations, posing serious risks. Existing hallucination detection
methods frequently lack the necessary sentence-level granularity or robust
generalization across different LVLM generators. We introduce a novel approach:
a sentence-level Process Reward Model (PRM) adapted for this vision-language
task. Our PRM predicts the factual correctness of each generated sentence,
conditioned on clinical context and preceding text. When fine-tuned on
MIMIC-CXR with weakly-supervised labels, a lightweight 0.5B-parameter PRM
outperforms existing verification techniques, demonstrating, for instance,
relative improvements of 7.5% in Matthews Correlation Coefficient and 1.8% in
AUROC over strong white-box baselines on outputs from one LVLM. Unlike methods
reliant on internal model states, our PRM demonstrates strong generalization to
an unseen LVLM. We further show its practical utility: PRM scores effectively
filter low-quality reports, improving F1-CheXbert scores by 4.5% (when
discarding the worst 10% of reports). Moreover, when guiding a novel weighted
best-of-N selection process on the MIMIC-CXR test set, our PRM show relative
improvements in clinical metrics of 7.4% for F1-CheXbert and 0.6% for
BERTScore. These results demonstrate that a lightweight, context-aware PRM
provides a model-agnostic safety layer for clinical LVLMs without access to
internal activations
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PTPP-Aware Adaptation Scaling Laws: Predicting Domain-Adaptation
  Performance at Unseen <span class="highlight-title">Pre-Train</span>ing Budgets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23198v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23198v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Etienne Goffinet, Shane Bergsma, Avraham Sheinin, Natalia Vassilieva, Shaheer Muhammad, Preslav Nakov, Gurpreet Gosal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual pre-training (CPT) for domain adaptation must balance target-domain
gains with stability on the base domain. Existing CPT scaling laws typically
assume a fixed pre-training budget, which limits their ability to forecast
adaptation outcomes for models trained at different tokens-per-parameter
(PTPP). We present \emph{PTPP-aware} adaptation scaling laws that make the
pre-training budget an explicit variable, enabling accurate \emph{prediction}
of adaptation loss at unseen \ptpp. On a multilingual setup (English/Arabic
$\rightarrow$ French), PTPP-aware formulations trained on early stages
(\ptpp{}=\{15,31\}) predict target loss at \ptpp{}=279 and outperform a
PTPP-agnostic \dcpt{} transfer baseline on metrics (Huber-on-log,
MAE$_\mathrm{rel}$, calibration slope); full diagnostics (RMSE, MAPE) are in
the appendix. Beyond forecasting, we show a practical use case: planning replay
ratios and adaptation token budgets that satisfy target and forgetting
constraints under compute limits.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DREaM: Drug-Drug Relation Extraction via Transfer Learning Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23189v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23189v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Fata, Hossein Rahmani, Parinaz Soltanzadeh, Amirhossein Derakhshan, Behrouz Minaei Bidgoli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Relation extraction between drugs plays a crucial role in identifying drug
drug interactions and predicting side effects. The advancement of machine
learning methods in relation extraction, along with the development of large
medical text databases, has enabled the low cost extraction of such relations
compared to other approaches that typically require expert knowledge. However,
to the best of our knowledge, there are limited datasets specifically designed
for drug drug relation extraction currently available. Therefore, employing
transfer learning becomes necessary to apply machine learning methods in this
domain. In this study, we propose DREAM, a method that first employs a trained
relation extraction model to discover relations between entities and then
applies this model to a corpus of medical texts to construct an ontology of
drug relationships. The extracted relations are subsequently validated using a
large language model. Quantitative results indicate that the LLM agreed with 71
of the relations extracted from a subset of PubMed abstracts. Furthermore, our
qualitative analysis indicates that this approach can uncover ambiguities in
the medical domain, highlighting the challenges inherent in relation extraction
in this field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SI-Bench: Benchmarking Social Intelligence of Large Language Models in
  Human-to-Human Conversations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23182v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23182v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuai Huang, Wenxuan Zhao, Jun Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) develop anthropomorphic abilities, they are
increasingly being deployed as autonomous agents to interact with humans.
However, evaluating their performance in realistic and complex social
interactions remains a significant challenge. Most previous research built
datasets through simulated agent-to-agent interactions, which fails to capture
the authentic linguistic styles and relational dynamics found in real human
conversations. To address this gap, we introduce SI-Bench, a novel benchmark
designed to evaluate aspects of social intelligence in LLMs. Grounded in broad
social science theories, SI-Bench contains 2,221 authentic multi-turn dialogues
collected from a social networking application. We further selected a subset of
312 dialogues for manual annotation across 8 major models. The experiments show
that SOTA models have surpassed the human expert in process reasoning under
complex social situations, yet they still fall behind humans in reply quality.
Moreover, introducing Chain-of-Thought (CoT) reasoning may degrade the
performance of LLMs in social dialogue tasks. All datasets are openly available
at https://github.com/SI-Bench/SI-Bench.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MATCH: Task-Driven Code Evaluation through Contrastive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23169v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23169v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marah Ghoummaid, Vladimir Tchuiev, Ofek Glick, Michal Moschkovitz, Dotan Di Castro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI-based code generation is increasingly prevalent, with GitHub Copilot
estimated to generate 46% of the code on GitHub. Accurately evaluating how well
generated code aligns with developer intent remains a critical challenge.
Traditional evaluation methods, such as unit tests, are often unscalable and
costly. Syntactic similarity metrics (e.g., BLEU, ROUGE) fail to capture code
functionality, and metrics like CodeBERTScore require reference code, which is
not always available. To address the gap in reference-free evaluation, with few
alternatives such as ICE-Score, this paper introduces MATCH, a novel
reference-free metric. MATCH uses Contrastive Learning to generate meaningful
embeddings for code and natural language task descriptions, enabling similarity
scoring that reflects how well generated code implements the task. We show that
MATCH achieves stronger correlations with functional correctness and human
preference than existing metrics across multiple programming languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Direct Generation: A Decomposed Approach to Well-Crafted
  Screenwriting with LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23163v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23163v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Lei, Shengyi Zong, Zhaoyan Li, Ziren Zhou, Hao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The screenplay serves as the foundation for television production, defining
narrative structure, character development, and dialogue. While Large Language
Models (LLMs) show great potential in creative writing, direct end-to-end
generation approaches often fail to produce well-crafted screenplays. We argue
this failure stems from forcing a single model to simultaneously master two
disparate capabilities: creative narrative construction and rigid format
adherence. The resulting outputs may mimic superficial style but lack the deep
structural integrity and storytelling substance required for professional use.
To enable LLMs to generate high-quality screenplays, we introduce Dual-Stage
Refinement (DSR), a decomposed framework that decouples creative narrative
generation from format conversion. The first stage transforms a brief outline
into rich, novel-style prose. The second stage refines this narrative into a
professionally formatted screenplay. This separation enables the model to
specialize in one distinct capability at each stage. A key challenge in
implementing DSR is the scarcity of paired outline-to-novel training data. We
address this through hybrid data synthesis: reverse synthesis deconstructs
existing screenplays into structured inputs, while forward synthesis leverages
these inputs to generate high-quality narrative texts as training targets.
Blind evaluations by professional screenwriters show that DSR achieves a 75%
win rate against strong baselines like Gemini-2.5-Pro and reaches 82.7% of
human-level performance. Our work demonstrates that decomposed generation
architecture with tailored data synthesis effectively specializes LLMs in
complex creative domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ENTP: Enhancing Low-Quality SFT Data via Neural-Symbolic Text Purge-Mix 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23160v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23160v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zile Yang, Ling Li, Na Di, Jinlong Pang, Yao Zhou, Hao Cheng, Bo Han, Jiaheng Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Supervised Fine-Tuning (SFT) adapts pre-trained Large Language Models (LLMs)
to domain-specific instructions by training on a carefully curated subset of
high-quality instruction-response pairs, typically drawn from a larger dataset
that often contains many low-quality or noisy samples. However, existing
quality-first paradigms often overlook valuable signals in discarded
low-quality data and rely on imperfect quality filters. We introduce ENTP
(Enhancing low-quality SFT data via Neural-symbolic Text Purge-Mix), a
framework that revitalizes low-quality corpora through symbolic purification
and neural reconstruction. The symbolic module identifies and prunes noisy
samples based on statistical priors, while the neural component synthesizes
enriched instruction-response pairs by leveraging latent representations and
model knowledge. This neural-symbolic synergy enhances data informativeness and
diversity. Experiments show that ENTP-augmented datasets, constructed
exclusively from low-quality data, outperform 13 established data-selection
baselines across five instruction-following benchmarks, and even surpass
fine-tuning on the full original dataset (approximately 300K examples). Our
results highlight the untapped potential of low-quality data and underscore the
importance of intelligent purification and synthesis for efficient instruction
alignment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking GSPO: The Perplexity-Entropy Equivalence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23142v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23142v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chi Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We provide a new perspective on GSPO's length-normalized importance ratios by
establishing their connection to information-theoretic quantities. We show that
GSPO's sequence-level weight $s(\theta) =
(\pi_\theta/\pi_{\theta_{\text{old}}})^{1/|y|}$ can be equivalently expressed
as the inverse perplexity ratio
$\text{PPL}_{\theta_{\text{old}}}/\text{PPL}_\theta$ and as the exponential
cross-entropy change $\exp(\Delta H)$. While the perplexity-entropy
relationship follows from standard definitions, this observation provides a
useful lens for understanding GSPO: the algorithm weights policy gradient
updates by perplexity ratios, offering an information-theoretic interpretation
of the importance weights. This perspective helps explain GSPO's empirical
properties, including log-domain variance reduction through geometric averaging
and stability in training mixture-of-experts models. We validate the
mathematical equivalences and variance predictions through controlled
experiments on mathematical reasoning tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Corpus Frequencies in Morphological Inflection: Do They Matter? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23131v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23131v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomáš Sourada, Jana Straková
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The traditional approach to morphological inflection (the task of modifying a
base word (lemma) to express grammatical categories) has been, for decades, to
consider lexical entries of lemma-tag-form triples uniformly, lacking any
information about their frequency distribution. However, in production
deployment, one might expect the user inputs to reflect a real-world
distribution of frequencies in natural texts. With future deployment in mind,
we explore the incorporation of corpus frequency information into the task of
morphological inflection along three key dimensions during system development:
(i) for train-dev-test split, we combine a lemma-disjoint approach, which
evaluates the model's generalization capabilities, with a frequency-weighted
strategy to better reflect the realistic distribution of items across different
frequency bands in training and test sets; (ii) for evaluation, we complement
the standard type accuracy (often referred to simply as accuracy), which treats
all items equally regardless of frequency, with token accuracy, which assigns
greater weight to frequent words and better approximates performance on running
text; (iii) for training data sampling, we introduce a method novel in the
context of inflection, frequency-aware training, which explicitly incorporates
word frequency into the sampling process. We show that frequency-aware training
outperforms uniform sampling in 26 out of 43 languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in the proceedings of ITAT 2025.15 pages, 1 figure, 4
  tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Higher Rank: Token-wise Input-Output Projections for Efficient
  Low-Rank Adaptation <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23123v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23123v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiwei Li, Xiandi Luo, Haozhao Wang, Xing Tang, Ziqiang Cui, Dugang Liu, Yuhua Li, Xiuqiang He, Ruixuan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-rank adaptation (LoRA) is a parameter-efficient fine-tuning (PEFT) method
widely used in large language models (LLMs). LoRA essentially describes the
projection of an input space into a low-dimensional output space, with the
dimensionality determined by the LoRA rank. In standard LoRA, all input tokens
share the same weights and undergo an identical input-output projection. This
limits LoRA's ability to capture token-specific information due to the inherent
semantic differences among tokens. To address this limitation, we propose
Token-wise Projected Low-Rank Adaptation (TopLoRA), which dynamically adjusts
LoRA weights according to the input token, thereby learning token-wise
input-output projections in an end-to-end manner. Formally, the weights of
TopLoRA can be expressed as $B\Sigma_X A$, where $A$ and $B$ are low-rank
matrices (as in standard LoRA), and $\Sigma_X$ is a diagonal matrix generated
from each input token $X$. Notably, TopLoRA does not increase the rank of LoRA
weights but achieves more granular adaptation by learning token-wise LoRA
weights (i.e., token-wise input-output projections). Extensive experiments
across multiple models and datasets demonstrate that TopLoRA consistently
outperforms LoRA and its variants. The code is available at
https://github.com/Leopold1423/toplora-neurips25.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Flexing in 73 Languages: A Single Small Model for Multilingual
  Inflection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23114v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23114v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomáš Sourada, Jana Straková
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a compact, single-model approach to multilingual inflection, the
task of generating inflected word forms from base lemmas to express grammatical
categories. Our model, trained jointly on data from 73 languages, is
lightweight, robust to unseen words, and outperforms monolingual baselines in
most languages. This demonstrates the effectiveness of multilingual modeling
for inflection and highlights its practical benefits: simplifying deployment by
eliminating the need to manage and retrain dozens of separate monolingual
models. In addition to the standard SIGMORPHON shared task benchmarks, we
evaluate our monolingual and multilingual models on 73 Universal Dependencies
(UD) treebanks, extracting lemma-tag-form triples and their frequency counts.
To ensure realistic data splits, we introduce a novel frequency-weighted,
lemma-disjoint train-dev-test resampling procedure. Our work addresses the lack
of an open-source, general-purpose, multilingual morphological inflection
system capable of handling unseen words across a wide range of languages,
including Czech. All code is publicly released at:
https://github.com/tomsouri/multilingual-inflection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in the proceedings of TSD 2025. 12 pages, 1 figure, 4
  tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Hierarchical Organization for Medical Multi-document
  Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23104v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23104v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi-Li Hsu, Katelyn X. Mei, Lucy Lu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical multi-document summarization (MDS) is a complex task that requires
effectively managing cross-document relationships. This paper investigates
whether incorporating hierarchical structures in the inputs of MDS can improve
a model's ability to organize and contextualize information across documents
compared to traditional flat summarization methods. We investigate two ways of
incorporating hierarchical organization across three large language models
(LLMs), and conduct comprehensive evaluations of the resulting summaries using
automated metrics, model-based metrics, and domain expert evaluation of
preference, understandability, clarity, complexity, relevance, coverage,
factuality, and coherence. Our results show that human experts prefer
model-generated summaries over human-written summaries. Hierarchical approaches
generally preserve factuality, coverage, and coherence of information, while
also increasing human preference for summaries. Additionally, we examine
whether simulated judgments from GPT-4 align with human judgments, finding
higher agreement along more objective evaluation facets. Our findings
demonstrate that hierarchical structures can improve the clarity of medical
summaries generated by models while maintaining content coverage, providing a
practical way to improve human preference for generated summaries.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MAP4TS: A Multi-Aspect <span class="highlight-title">Prompt</span>ing Framework for Time-Series Forecasting
  with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23090v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23090v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suchan Lee, Jihoon Choi, Sohyeon Lee, Minseok Song, Bong-Gyu Jang, Hwanjo Yu, Soyeon Caren Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances have investigated the use of pretrained large language models
(LLMs) for time-series forecasting by aligning numerical inputs with LLM
embedding spaces. However, existing multimodal approaches often overlook the
distinct statistical properties and temporal dependencies that are fundamental
to time-series data. To bridge this gap, we propose MAP4TS, a novel
Multi-Aspect Prompting Framework that explicitly incorporates classical
time-series analysis into the prompt design. Our framework introduces four
specialized prompt components: a Global Domain Prompt that conveys
dataset-level context, a Local Domain Prompt that encodes recent trends and
series-specific behaviors, and a pair of Statistical and Temporal Prompts that
embed handcrafted insights derived from autocorrelation (ACF), partial
autocorrelation (PACF), and Fourier analysis. Multi-Aspect Prompts are combined
with raw time-series embeddings and passed through a cross-modality alignment
module to produce unified representations, which are then processed by an LLM
and projected for final forecasting. Extensive experiments across eight diverse
datasets show that MAP4TS consistently outperforms state-of-the-art LLM-based
methods. Our ablation studies further reveal that prompt-aware designs
significantly enhance performance stability and that GPT-2 backbones, when
paired with structured prompts, outperform larger models like LLaMA in
long-term forecasting tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on LLM Mid-training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23081v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23081v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengying Tu, Xuemiao Zhang, Rongxiang Weng, Rumei Li, Chen Zhang, Yang Bai, Hongfei Yan, Jingang Wang, Xunliang Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in foundation models have highlighted the significant
benefits of multi-stage training, with a particular emphasis on the emergence
of mid-training as a vital stage that bridges pre-training and post-training.
Mid-training is distinguished by its use of intermediate data and computational
resources, systematically enhancing specified capabilities such as mathematics,
coding, reasoning, and long-context extension, while maintaining foundational
competencies. This survey provides a formal definition of mid-training for
large language models (LLMs) and investigates optimization frameworks that
encompass data curation, training strategies, and model architecture
optimization. We analyze mainstream model implementations in the context of
objective-driven interventions, illustrating how mid-training serves as a
distinct and critical stage in the progressive development of LLM capabilities.
By clarifying the unique contributions of mid-training, this survey offers a
comprehensive taxonomy and actionable insights, supporting future research and
innovation in the advancement of LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast-MIA: Efficient and Scalable Membership Inference for LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23074v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23074v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hiromu Takahashi, Shotaro Ishihara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Fast-MIA (https://github.com/Nikkei/fast-mia), a Python library
for efficiently evaluating membership inference attacks (MIA) against Large
Language Models (LLMs). MIA against LLMs has emerged as a crucial challenge due
to growing concerns over copyright, security, and data privacy, and has
attracted increasing research attention. However, the progress of this research
is significantly hindered by two main obstacles: (1) the high computational
cost of inference in LLMs, and (2) the lack of standardized and maintained
implementations of MIA methods, which makes large-scale empirical comparison
difficult. To address these challenges, our library provides fast batch
inference and includes implementations of representative MIA methods under a
unified evaluation framework. This library supports easy implementation of
reproducible benchmarks with simple configuration and extensibility. We release
Fast-MIA as an open-source (Apache License 2.0) tool to support scalable and
transparent research on LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quality-Aware Translation Tagging in Multilingual RAG system <span class="chip">EMNLP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23070v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23070v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hoyeon Moon, Byeolhee Kim, Nikhil Verma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multilingual Retrieval-Augmented Generation (mRAG) often retrieves English
documents and translates them into the query language for low-resource
settings. However, poor translation quality degrades response generation
performance. Existing approaches either assume sufficient translation quality
or utilize the rewriting method, which introduces factual distortion and
hallucinations. To mitigate these problems, we propose Quality-Aware
Translation Tagging in mRAG (QTT-RAG), which explicitly evaluates translation
quality along three dimensions-semantic equivalence, grammatical accuracy, and
naturalness&fluency-and attach these scores as metadata without altering the
original content. We evaluate QTT-RAG against CrossRAG and DKM-RAG as baselines
in two open-domain QA benchmarks (XORQA, MKQA) using six instruction-tuned LLMs
ranging from 2.4B to 14B parameters, covering two low-resource languages
(Korean and Finnish) and one high-resource language (Chinese). QTT-RAG
outperforms the baselines by preserving factual integrity while enabling
generator models to make informed decisions based on translation reliability.
This approach allows for effective usage of cross-lingual documents in
low-resource settings with limited native language documents, offering a
practical and robust solution across multilingual domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2025 MRL Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knocking-Heads Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23052v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23052v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhanchao Zhou, Xiaodong Chen, Haoxing Chen, Zhenzhong Lan, Jianguo Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-head attention (MHA) has become the cornerstone of modern large
language models, enhancing representational capacity through parallel attention
heads. However, increasing the number of heads inherently weakens individual
head capacity, and existing attention mechanisms - whether standard MHA or its
variants like grouped-query attention (GQA) and grouped-tied attention (GTA) -
simply concatenate outputs from isolated heads without strong interaction. To
address this limitation, we propose knocking-heads attention (KHA), which
enables attention heads to "knock" on each other - facilitating cross-head
feature-level interactions before the scaled dot-product attention. This is
achieved by applying a shared, diagonally-initialized projection matrix across
all heads. The diagonal initialization preserves head-specific specialization
at the start of training while allowing the model to progressively learn
integrated cross-head representations. KHA adds only minimal parameters and
FLOPs and can be seamlessly integrated into MHA, GQA, GTA, and other attention
variants. We validate KHA by training a 6.1B parameter MoE model (1.01B
activated) on 1T high-quality tokens. Compared to baseline attention
mechanisms, KHA brings superior and more stable training dynamics, achieving
better performance across downstream tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Incentivizing Agentic Reasoning in LLM Judges via Tool-Integrated
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23038v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23038v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ran Xu, Jingjing Chen, Jiayu Ye, Yu Wu, Jun Yan, Carl Yang, Hongkun Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are widely used as judges to evaluate response
quality, providing a scalable alternative to human evaluation. However, most
LLM judges operate solely on intrinsic text-based reasoning, limiting their
ability to verify complex constraints or perform accurate computation.
Motivated by the success of tool-integrated reasoning (TIR) in numerous tasks,
we propose TIR-Judge, an end-to-end RL framework for training LLM judges that
integrates a code executor for precise evaluation. TIR-Judge is built on three
principles: (i) diverse training across verifiable and non-verifiable domains,
(ii) flexible judgment formats (pointwise, pairwise, listwise), and (iii)
iterative RL that bootstraps directly from the initial model without
distillation. On seven public benchmarks, TIR-Judge surpasses strong
reasoning-based judges by up to 6.4% (pointwise) and 7.7% (pairwise), and
achieves listwise performance comparable to Claude-Opus-4 despite having only
8B parameters. Remarkably, TIR-Judge-Zero - trained entirely without distilled
judge trajectories, matches the performance of distilled variants,
demonstrating that tool-augmented judges can self-evolve through iterative
reinforcement learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in Progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Stable and Effective Reinforcement Learning for
  Mixture-of-Experts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23027v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23027v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Di Zhang, Xun Wu, Shaohan Huang, Yaru Hao, Li Dong, Zewen Chi, Zhifang Sui, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in reinforcement learning (RL) have substantially improved
the training of large-scale language models, leading to significant gains in
generation quality and reasoning ability. However, most existing research
focuses on dense models, while RL training for Mixture-of-Experts (MoE)
architectures remains underexplored. To address the instability commonly
observed in MoE training, we propose a novel router-aware approach to optimize
importance sampling (IS) weights in off-policy RL. Specifically, we design a
rescaling strategy guided by router logits, which effectively reduces gradient
variance and mitigates training divergence. Experimental results demonstrate
that our method significantly improves both the convergence stability and the
final performance of MoE models, highlighting the potential of RL algorithmic
innovations tailored to MoE architectures and providing a promising direction
for efficient training of large-scale expert models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UniAIDet: A Unified and Universal Benchmark for AI-Generated Image
  Content Detection and Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23023v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23023v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huixuan Zhang, Xiaojun Wan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid proliferation of image generative models, the authenticity of
digital images has become a significant concern. While existing studies have
proposed various methods for detecting AI-generated content, current benchmarks
are limited in their coverage of diverse generative models and image
categories, often overlooking end-to-end image editing and artistic images. To
address these limitations, we introduce UniAIDet, a unified and comprehensive
benchmark that includes both photographic and artistic images. UniAIDet covers
a wide range of generative models, including text-to-image, image-to-image,
image inpainting, image editing, and deepfake models. Using UniAIDet, we
conduct a comprehensive evaluation of various detection methods and answer
three key research questions regarding generalization capability and the
relation between detection and localization. Our benchmark and analysis provide
a robust foundation for future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ M$^{3}$T2IBench: A Large-Scale Multi-Category, Multi-Instance,
  Multi-Relation Text-to-Image Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23020v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23020v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huixuan Zhang, Xiaojun Wan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image models are known to struggle with generating images that
perfectly align with textual prompts. Several previous studies have focused on
evaluating image-text alignment in text-to-image generation. However, these
evaluations either address overly simple scenarios, especially overlooking the
difficulty of prompts with multiple different instances belonging to the same
category, or they introduce metrics that do not correlate well with human
evaluation. In this study, we introduce M$^3$T2IBench, a large-scale,
multi-category, multi-instance, multi-relation along with an
object-detection-based evaluation metric, $AlignScore$, which aligns closely
with human evaluation. Our findings reveal that current open-source
text-to-image models perform poorly on this challenging benchmark.
Additionally, we propose the Revise-Then-Enforce approach to enhance image-text
alignment. This training-free post-editing method demonstrates improvements in
image-text alignment across a broad range of diffusion models. \footnote{Our
code and data has been released in supplementary material and will be made
publicly available after the paper is accepted.}
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LangLingual: A Personalised, Exercise-oriented English Language Learning
  Tool Leveraging Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23011v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23011v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sammriddh Gupta, Sonit Singh, Aditya Joshi, Mira Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language educators strive to create a rich experience for learners, while
they may be restricted in the extend of feedback and practice they can provide.
We present the design and development of LangLingual, a conversational agent
built using the LangChain framework and powered by Large Language Models. The
system is specifically designed to provide real-time, grammar-focused feedback,
generate context-aware language exercises and track learner proficiency over
time. The paper discusses the architecture, implementation and evaluation of
LangLingual in detail. The results indicate strong usability, positive learning
outcomes and encouraging learner engagement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Constrained Entropic Unlearning: A Primal-Dual Framework for Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05314v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05314v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taha Entesari, Arman Hatami, Rinat Khaziev, Anil Ramakrishna, Mahyar Fazlyab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) deployed in real-world settings increasingly
face the need to unlearn sensitive, outdated, or proprietary information.
Existing unlearning methods typically formulate forgetting and retention as a
regularized trade-off, combining both objectives into a single scalarized loss.
This often leads to unstable optimization and degraded performance on retained
data, especially under aggressive forgetting. We propose a new formulation of
LLM unlearning as a constrained optimization problem: forgetting is enforced
via a novel logit-margin flattening loss that explicitly drives the output
distribution toward uniformity on a designated forget set, while retention is
preserved through a hard constraint on a separate retain set. Compared to
entropy-based objectives, our loss is softmax-free, numerically stable, and
maintains non-vanishing gradients, enabling more efficient and robust
optimization. We solve the constrained problem using a scalable primal-dual
algorithm that exposes the trade-off between forgetting and retention through
the dynamics of the dual variable, all without any extra computational
overhead. Evaluations on the TOFU and MUSE benchmarks across diverse LLM
architectures demonstrate that our approach consistently matches or exceeds
state-of-the-art baselines, effectively removing targeted information while
preserving downstream utility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The Thirty-Ninth Annual Conference on Neural Information Processing
  Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM4Cell: A <span class="highlight-title">Survey</span> of Large Language and Agentic Models for Single-Cell
  Biology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.07793v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.07793v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sajib Acharjee Dip, Adrika Zafor, Bikash Kumar Paul, Uddip Acharjee Shuvo, Muhit Islam Emon, Xuan Wang, Liqing Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) and emerging agentic frameworks are beginning to
transform single-cell biology by enabling natural-language reasoning,
generative annotation, and multimodal data integration. However, progress
remains fragmented across data modalities, architectures, and evaluation
standards. LLM4Cell presents the first unified survey of 58 foundation and
agentic models developed for single-cell research, spanning RNA, ATAC,
multi-omic, and spatial modalities. We categorize these methods into five
families-foundation, text-bridge, spatial, multimodal, epigenomic, and
agentic-and map them to eight key analytical tasks including annotation,
trajectory and perturbation modeling, and drug-response prediction. Drawing on
over 40 public datasets, we analyze benchmark suitability, data diversity, and
ethical or scalability constraints, and evaluate models across 10 domain
dimensions covering biological grounding, multi-omics alignment, fairness,
privacy, and explainability. By linking datasets, models, and evaluation
domains, LLM4Cell provides the first integrated view of language-driven
single-cell intelligence and outlines open challenges in interpretability,
standardization, and trustworthy model development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages, 5 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SafeMERGE: Preserving Safety Alignment in Fine-Tuned Large Language
  Models via Selective Layer-Wise Model Merging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17239v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17239v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aladin Djuhera, Swanand Ravindra Kadhe, Farhan Ahmed, Syed Zawad, Holger Boche
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning large language models (LLMs) is a common practice to adapt
generalist models to specialized domains. However, recent studies show that
fine-tuning can erode safety alignment, causing LLMs to respond to harmful or
unethical prompts. Many methods to realign safety have been proposed, but often
introduce custom algorithms that are difficult to implement or compromise task
utility. In this work, we propose SafeMERGE, a lightweight, post-fine-tuning
framework that preserves safety while maintaining downstream performance.
SafeMERGE selectively merges fine-tuned with safety-aligned model layers only
when they deviate from safe behavior, measured by a cosine similarity
criterion. Across three LLMs and two tasks, SafeMERGE consistently reduces
harmful outputs compared to other defenses, with negligible or even positive
impact on utility. Our results demonstrate that selective layer-wise merging
offers an effective safeguard against the inadvertent loss of safety during
fine-tuning, establishing SafeMERGE as a simple post-fine-tuning defense.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Superficial Self-Improved Reasoners Benefit from Model Merging <span class="chip">EMNLP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02103v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02103v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangchi Yuan, Chunhui Zhang, Zheyuan Liu, Dachuan Shi, Leyan Pan, Soroush Vosoughi, Wenke Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As scaled language models (LMs) approach human-level reasoning capabilities,
self-improvement emerges as a solution to synthesizing high-quality data
corpus. While previous research has identified model collapse as a risk in
self-improvement, where model outputs become increasingly deterministic, we
discover a more fundamental challenge: the superficial self-improved reasoners
phenomenon. In particular, our analysis reveals that even when LMs show
improved in-domain (ID) reasoning accuracy, they actually compromise their
generalized reasoning capabilities on out-of-domain (OOD) tasks due to
memorization rather than genuine. Through a systematic investigation of LM
architecture, we discover that during self-improvement, LM weight updates are
concentrated in less reasoning-critical layers, leading to superficial
learning. To address this, we propose Iterative Model Merging (IMM), a method
that strategically combines weights from original and self-improved models to
preserve generalization while incorporating genuine reasoning improvements. Our
approach effectively mitigates both LM collapse and superficial learning,
moving towards more stable self-improving systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SafeCOMM: A Study on Safety Degradation in Fine-Tuned Telecom Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.00062v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.00062v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aladin Djuhera, Swanand Ravindra Kadhe, Farhan Ahmed, Syed Zawad, Fernando Koch, Walid Saad, Holger Boche
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning large language models (LLMs) on telecom datasets is a common
practice to adapt general-purpose models to the telecom domain. However, little
attention has been paid to how this process may compromise model safety. Recent
research has shown that even benign fine-tuning can degrade the safety
alignment of LLMs, causing them to respond to harmful or unethical user
queries. In this paper, we investigate this issue by fine-tuning LLMs on three
representative telecom datasets and show that safety degrades even for light
telecom domain adaptation. To this end, we introduce TeleHarm, the first
telecom-specific red-teaming benchmark, which we use alongside established
Direct-Harm and HexPhi datasets to systematically assess harmful behavior. We
further extend our analysis to publicly available TeleLLMs that were
continually pre-trained on large telecom corpora, revealing that safety
alignment is severely lacking, primarily due to the omission of safety-focused
instruction tuning. To address these issues, we evaluate three realignment
defenses: SafeInstruct, SafeLoRA, SafeMERGE. We show that, across all settings,
the proposed defenses can effectively restore safety without compromising
telecom task performance, leading to Safe teleCOMMunication (SafeCOMM) models.
Our work serves as both a diagnostic study and practical guide for safety
realignment in telecom-tuned LLMs, underscoring the need for safety-aware
instruction and fine-tuning in the telecom domain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fixing It in Post: A Comparative Study of LLM Post-Training Data Quality
  and Model Performance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.06522v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.06522v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aladin Djuhera, Swanand Ravindra Kadhe, Syed Zawad, Farhan Ahmed, Heiko Ludwig, Holger Boche
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work on large language models (LLMs) has increasingly focused on
post-training and alignment with datasets curated to enhance instruction
following, world knowledge, and specialized skills. However, most post-training
datasets used in leading open- and closed-source LLMs remain inaccessible to
the public, with limited information about their construction process. This
lack of transparency has motivated the recent development of open-source
post-training corpora. While training on these open alternatives can yield
performance comparable to that of leading models, systematic comparisons remain
challenging due to the significant computational cost of conducting them
rigorously at scale, and are therefore largely absent. As a result, it remains
unclear how specific samples, task types, or curation strategies influence
downstream performance when assessing data quality. In this work, we conduct
the first comprehensive side-by-side analysis of two prominent open
post-training datasets: Tulu-3-SFT-Mix and SmolTalk. Using the Magpie
framework, we annotate each sample with detailed quality metrics, including
turn structure (single-turn vs. multi-turn), task category, input quality, and
response quality, and we derive statistics that reveal structural and
qualitative similarities and differences between the two datasets. Based on
these insights, we design a principled curation recipe that produces a new data
mixture, TuluTalk, which contains 14% fewer samples than either source dataset
while matching or exceeding their performance on key benchmarks. Our findings
offer actionable insights for constructing more effective post-training
datasets that improve model performance within practical resource limits. To
support future research, we publicly release both the annotated source datasets
and our curated TuluTalk mixture.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Human-Aligned Faithfulness in Toxicity Explanations of LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19113v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19113v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ramaravind K. Mothilal, Joanna Roy, Syed Ishtiaque Ahmed, Shion Guha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The discourse around toxicity and LLMs in NLP largely revolves around
detection tasks. This work shifts the focus to evaluating LLMs' reasoning about
toxicity -- from their explanations that justify a stance -- to enhance their
trustworthiness in downstream tasks. Despite extensive research on
explainability, it is not straightforward to adopt existing methods to evaluate
free-form toxicity explanation due to their over-reliance on input text
perturbations, among other challenges. To account for these, we propose a
novel, theoretically-grounded multi-dimensional criterion, Human-Aligned
Faithfulness (HAF), that measures the extent to which LLMs' free-form toxicity
explanations align with those of a rational human under ideal conditions. We
develop six metrics, based on uncertainty quantification, to comprehensively
evaluate HAF of LLMs' toxicity explanations with no human involvement, and
highlight how "non-ideal" the explanations are. We conduct several experiments
on three Llama models (of size up to 70B) and an 8B Ministral model on five
diverse toxicity datasets. Our results show that while LLMs generate plausible
explanations to simple prompts, their reasoning about toxicity breaks down when
prompted about the nuanced relations between the complete set of reasons, the
individual reasons, and their toxicity stances, resulting in inconsistent and
irrelevant responses. We open-source our code at
https://github.com/uofthcdslab/HAF and LLM-generated explanations at
https://huggingface.co/collections/uofthcdslab/haf.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 5 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AttentionRAG: Attention-Guided Context Pruning in Retrieval-Augmented
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.10720v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.10720v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixiong Fang, Tianran Sun, Yuling Shi, Xiaodong Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While RAG demonstrates remarkable capabilities in LLM applications, its
effectiveness is hindered by the ever-increasing length of retrieved contexts,
which introduces information redundancy and substantial computational overhead.
Existing context pruning methods, such as LLMLingua, lack contextual awareness
and offer limited flexibility in controlling compression rates, often resulting
in either insufficient pruning or excessive information loss. In this paper, we
propose AttentionRAG, an attention-guided context pruning method for RAG
systems. The core idea of AttentionRAG lies in its attention focus mechanism,
which reformulates RAG queries into a next-token prediction paradigm. This
mechanism isolates the query's semantic focus to a single token, enabling
precise and efficient attention calculation between queries and retrieved
contexts. Extensive experiments on LongBench and Babilong benchmarks show that
AttentionRAG achieves up to 6.3$\times$ context compression while outperforming
LLMLingua methods by around 10\% in key metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cancer-Myth: Evaluating AI Chatbot on Patient Questions with False
  Presuppositions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.11373v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.11373v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wang Bill Zhu, Tianqi Chen, Xinyan Velocity Yu, Ching Ying Lin, Jade Law, Mazen Jizzini, Jorge J. Nieva, Ruishan Liu, Robin Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cancer patients are increasingly turning to large language models (LLMs) for
medical information, making it critical to assess how well these models handle
complex, personalized questions. However, current medical benchmarks focus on
medical exams or consumer-searched questions and do not evaluate LLMs on real
patient questions with patient details. In this paper, we first have three
hematology-oncology physicians evaluate cancer-related questions drawn from
real patients. While LLM responses are generally accurate, the models
frequently fail to recognize or address false presuppositions in the questions,
posing risks to safe medical decision-making. To study this limitation
systematically, we introduce Cancer-Myth, an expert-verified adversarial
dataset of 585 cancer-related questions with false presuppositions. On this
benchmark, no frontier LLM -- including GPT-5, Gemini-2.5-Pro, and
Claude-4-Sonnet -- corrects these false presuppositions more than $43\%$ of the
time. To study mitigation strategies, we further construct a 150-question
Cancer-Myth-NFP set, in which physicians confirm the absence of false
presuppositions. We find typical mitigation strategies, such as adding
precautionary prompts with GEPA optimization, can raise accuracy on Cancer-Myth
to $80\%$, but at the cost of misidentifying presuppositions in $41\%$ of
Cancer-Myth-NFP questions and causing a $10\%$ relative performance drop on
other medical benchmarks. These findings highlight a critical gap in the
reliability of LLMs, show that prompting alone is not a reliable remedy for
false presuppositions, and underscore the need for more robust safeguards in
medical AI systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Less is More: Local Intrinsic Dimensions of Contextual Language Models <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.01034v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.01034v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Matthias Ruppik, Julius von Rohrscheidt, Carel van Niekerk, Michael Heck, Renato Vukovic, Shutong Feng, Hsien-chin Lin, Nurul Lubis, Bastian Rieck, Marcus Zibrowius, Milica Gašić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the internal mechanisms of large language models (LLMs) remains
a challenging and complex endeavor. Even fundamental questions, such as how
fine-tuning affects model behavior, often require extensive empirical
evaluation. In this paper, we introduce a novel perspective based on the
geometric properties of contextual latent embeddings to study the effects of
training and fine-tuning. To that end, we measure the local dimensions of a
contextual language model's latent space and analyze their shifts during
training and fine-tuning. We show that the local dimensions provide insights
into the model's training dynamics and generalization ability. Specifically,
the mean of the local dimensions predicts when the model's training
capabilities are exhausted, as exemplified in a dialogue state tracking task,
overfitting, as demonstrated in an emotion recognition task, and grokking, as
illustrated with an arithmetic task. Furthermore, our experiments suggest a
practical heuristic: reductions in the mean local dimension tend to accompany
and predict subsequent performance gains. Through this exploration, we aim to
provide practitioners with a deeper understanding of the implications of
fine-tuning on embedding spaces, facilitating informed decisions when
configuring models for specific applications. The results of this work
contribute to the ongoing discourse on the interpretability, adaptability, and
generalizability of LLMs by bridging the gap between intrinsic model mechanisms
and geometric properties in the respective embeddings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 39th Conference on Neural Information Processing
  Systems (NeurIPS 2025; in press). 10 pages, with an additional 17 pages in
  the appendix. Our code is available at
  https://github.com/aidos-lab/Topo_LLM_public and
  https://github.com/aidos-lab/grokking-via-lid</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Computational-Assisted Systematic <span class="highlight-title">Review</span> and Meta-Analysis (CASMA):
  Effect of a Subclass of GnRH-a on Endometriosis Recurrence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.16599v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.16599v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sandro Tsang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Background: Evidence synthesis facilitates evidence-based medicine. This task
becomes increasingly difficult to accomplished with applying computational
solutions, since the medical literature grows at astonishing rates. Objective:
This study evaluates an information retrieval-driven workflow, CASMA, to
enhance the efficiency, transparency, and reproducibility of systematic
reviews. Endometriosis recurrence serves as the ideal case due to its complex
and ambiguous literature. Methods: The hybrid approach integrates PRISMA
guidelines with fuzzy matching and regular expression (regex) to facilitate
semi-automated deduplication and filtered records before manual screening. The
workflow synthesised evidence from randomised controlled trials on the efficacy
of a subclass of gonadotropin-releasing hormone agonists (GnRH-a). A modified
splitting method addressed unit-of-analysis errors in multi-arm trials.
Results: The workflow sharply reduced the screening workload, taking only 11
days to fetch and filter 33,444 records. Seven eligible RCTs were synthesized
(841 patients). The pooled random-effects model yielded a Risk Ratio (RR) of
$0.64$ ($95\%$ CI $0.48$ to $0.86$), demonstrating a $36\%$ reduction in
recurrence, with non-significant heterogeneity ($I^2=0.00\%$, $\tau^2=0.00$).
The findings were robust and stable, as they were backed by sensitivity
analyses. Conclusion: This study demonstrates an application of an
information-retrieval-driven workflow for medical evidence synthesis. The
approach yields valuable clinical results and a generalisable framework to
scale up the evidence synthesis, bridging the gap between clinical research and
computer science.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 12 figures and 4 tables. This work describes an information
  retrieval-driven workflow for medical evidence synthesis, with an application
  to endometriosis recurrence. The method can be generalized to other
  systematic reviews. The preregistered protocol is available:
  https://doi.org/10.17605/OSF.IO/R2DFA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Can We Effectively Expand the Vocabulary of LLMs with 0.01GB of
  Target Language Text? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11477v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11477v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atsuki Yamaguchi, Aline Villavicencio, Nikolaos Aletras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown remarkable capabilities in many
languages beyond English. Yet, LLMs require more inference steps when
generating non-English text due to their reliance on English-centric tokenizers
and vocabulary, resulting in higher usage costs to non-English speakers.
Vocabulary expansion with target language tokens is a widely used cross-lingual
vocabulary adaptation approach to remedy this issue. Despite its effectiveness
in inference speedup, previous work on vocabulary expansion has focused on
high-resource settings assuming access to a substantial amount of target
language data to effectively initialize the embeddings of the new tokens and
adapt the LLM to the target language. However, vocabulary expansion in
low-resource settings has yet to be explored. In this article, we investigate
vocabulary expansion in low-resource settings by considering embedding
initialization methods and continual pre-training strategies. Through extensive
experiments across typologically diverse languages, tasks and models, we
establish a set of strategies to perform vocabulary expansion for faster
inference, while striving to maintain competitive downstream performance to
baselines. This is achieved with only 30K sentences ($\sim$0.01GB text data)
from the target language.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Computational Linguistics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Data-driven ML Approach for Maximizing Performance in LLM-Adapter
  Serving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.08343v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.08343v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ferran Agullo, Joan Oliveras, Chen Wang, Alberto Gutierrez-Torre, Olivier Tardieu, Alaa Youssef, Jordi Torres, Josep Ll. Berral
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid adoption of Large Language Models (LLMs), LLM-adapters have
become increasingly common, providing lightweight specialization of large-scale
models. Serving hundreds or thousands of these adapters on a single GPU allows
request aggregation, increasing throughput, but may also cause request
starvation if GPU memory limits are exceeded. To address this issue, this study
focuses on determining the joint configuration of concurrent and parallel
adapters that maximizes GPU throughput without inducing starvation, given
heterogeneous adapter and traffic properties. We propose a data-driven ML
approach leveraging interpretable models to tackle this caching problem and
introduce the first Digital Twin capable of reproducing an LLM-adapter serving
system, enabling efficient training data generation. Experiments with the vLLM
framework and LoRA adapters show that the Digital Twin reproduces throughput
within 5.1% of real results, while the ML approach predicts optimal numbers of
concurrent and parallel adapters with an error of at most 7.2% under
heterogeneous, real-world workloads.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in a computer science workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Steering Evaluation-Aware Language Models to Act Like They Are Deployed 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.20487v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.20487v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Tian Hua, Andrew Qin, Samuel Marks, Neel Nanda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) can sometimes detect when they are being
evaluated and adjust their behavior to appear more aligned, compromising the
reliability of safety evaluations. In this paper, we show that adding a
steering vector to an LLM's activations can suppress evaluation-awareness and
make the model act like it is deployed during evaluation. To study our steering
technique, we train an LLM to exhibit evaluation-aware behavior using a
two-step training process designed to mimic how this behavior could emerge
naturally. First, we perform continued pretraining on documents with factual
descriptions of the model (1) using Python type hints during evaluation but not
during deployment and (2) recognizing that the presence of a certain evaluation
cue always means that it is being tested. Then, we train the model with expert
iteration to use Python type hints in evaluation settings. The resulting model
is evaluation-aware: it writes type hints in evaluation contexts more than
deployment contexts. We find that activation steering can suppress evaluation
awareness and make the model act like it is deployed even when the cue is
present. Importantly, we constructed our steering vector using the original
model before our additional training. Our results suggest that AI evaluators
could improve the reliability of safety evaluations by steering models to act
like they are deployed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Estimating LLM Consistency: A User Baseline vs Surrogate Metrics <span class="chip">EMNLP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.23799v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.23799v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyuan Wu, Weiran Lin, Omer Akgul, Lujo Bauer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are prone to hallucinations and sensitiveto
prompt perturbations, often resulting in inconsistent or unreliablegenerated
text. Different methods have been proposed to mitigate suchhallucinations and
fragility, one of which is to measure theconsistency of LLM responses -- the
model's confidence in the responseor likelihood of generating a similar
response when resampled. Inprevious work, measuring LLM response consistency
often relied oncalculating the probability of a response appearing within a
pool of resampledresponses, analyzing internal states, or evaluating logits of
resopnses.However, it was not clear how well theseapproaches approximated
users' perceptions of consistency of LLMresponses. To find out, we performed a
user study ($n=2,976$)demonstrating that current methods for measuring LLM
responseconsistency typically do not align well with humans' perceptions of
LLMconsistency. We propose a logit-based ensemble method for estimatingLLM
consistency and show that our method matches the performance of
thebest-performing existing metric in estimating human ratings of
LLMconsistency. Our results suggest that methods for estimating LLMconsistency
without human evaluation are sufficiently imperfect towarrant broader use of
evaluation with human input; this would avoidmisjudging the adequacy of models
because of the imperfections ofautomated consistency metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a main conference paper at EMNLP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Large Language Models Unlock Novel Scientific Research Ideas? <span class="chip">EMNLP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06185v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06185v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sandeep Kumar, Tirthankar Ghosal, Vinayak Goyal, Asif Ekbal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread adoption of Large Language Models (LLMs) and publicly
available ChatGPT have marked a significant turning point in the integration of
Artificial Intelligence (AI) into people's everyday lives. This study examines
the ability of Large Language Models (LLMs) to generate future research ideas
from scientific papers. Unlike tasks such as summarization or translation, idea
generation lacks a clearly defined reference set or structure, making manual
evaluation the default standard. However, human evaluation in this setting is
extremely challenging ie: it requires substantial domain expertise, contextual
understanding of the paper, and awareness of the current research landscape.
This makes it time-consuming, costly, and fundamentally non-scalable,
particularly as new LLMs are being released at a rapid pace. Currently, there
is no automated evaluation metric specifically designed for this task. To
address this gap, we propose two automated evaluation metrics: Idea Alignment
Score (IAScore) and Idea Distinctness Index. We further conducted human
evaluation to assess the novelty, relevance, and feasibility of the generated
future research ideas. This investigation offers insights into the evolving
role of LLMs in idea generation, highlighting both its capability and
limitations. Our work contributes to the ongoing efforts in evaluating and
utilizing language models for generating future research ideas. We make our
datasets and codes publicly available
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2025 (Main)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ClaimGen-CN: A Large-scale Chinese <span class="highlight-title">Dataset</span> for Legal Claim Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.17234v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.17234v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siying Zhou, Yiquan Wu, Hui Chen, Xavier Hu, Kun Kuang, Adam Jatowt, Ming Hu, Chunyan Zheng, Fei Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Legal claims refer to the plaintiff's demands in a case and are essential to
guiding judicial reasoning and case resolution. While many works have focused
on improving the efficiency of legal professionals, the research on helping
non-professionals (e.g., plaintiffs) remains unexplored. This paper explores
the problem of legal claim generation based on the given case's facts. First,
we construct ClaimGen-CN, the first dataset for Chinese legal claim generation
task, from various real-world legal disputes. Additionally, we design an
evaluation metric tailored for assessing the generated claims, which
encompasses two essential dimensions: factuality and clarity. Building on this,
we conduct a comprehensive zero-shot evaluation of state-of-the-art general and
legal-domain large language models. Our findings highlight the limitations of
the current models in factual precision and expressive clarity, pointing to the
need for more targeted development in this domain. To encourage further
exploration of this important task, we will make the dataset publicly
available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Are LLMs Empathetic to All? Investigating the Influence of
  Multi-Demographic Personas on a Model's Empathy <span class="chip">EMNLP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.10328v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.10328v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ananya Malik, Nazanin Sabri, Melissa Karnaze, Mai Elsherief
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models' (LLMs) ability to converse naturally is empowered by
their ability to empathetically understand and respond to their users. However,
emotional experiences are shaped by demographic and cultural contexts. This
raises an important question: Can LLMs demonstrate equitable empathy across
diverse user groups? We propose a framework to investigate how LLMs' cognitive
and affective empathy vary across user personas defined by intersecting
demographic attributes. Our study introduces a novel intersectional analysis
spanning 315 unique personas, constructed from combinations of age, culture,
and gender, across four LLMs. Results show that attributes profoundly shape a
model's empathetic responses. Interestingly, we see that adding multiple
attributes at once can attenuate and reverse expected empathy patterns. We show
that they broadly reflect real-world empathetic trends, with notable
misalignments for certain groups, such as those from Confucian culture. We
complement our quantitative findings with qualitative insights to uncover model
behaviour patterns across different demographic groups. Our findings highlight
the importance of designing empathy-aware LLMs that account for demographic
diversity to promote more inclusive and equitable model behaviour.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures, 4 tables, EMNLP 2025 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bootstrapping Referring Multi-Object Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05039v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05039v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yani Zhang, Dongming Wu, Wencheng Han, Xingping Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Referring understanding is a fundamental task that bridges natural language
and visual content by localizing objects described in free-form expressions.
However, existing works are constrained by limited language expressiveness,
lacking the capacity to model object dynamics in spatial numbers and temporal
states. To address these limitations, we introduce a new and general referring
understanding task, termed referring multi-object tracking (RMOT). Its core
idea is to employ a language expression as a semantic cue to guide the
prediction of multi-object tracking, comprehensively accounting for variations
in object quantity and temporal semantics. Along with RMOT, we introduce a RMOT
benchmark named Refer-KITTI-V2, featuring scalable and diverse language
expressions. To efficiently generate high-quality annotations covering object
dynamics with minimal manual effort, we propose a semi-automatic labeling
pipeline that formulates a total of 9,758 language prompts. In addition, we
propose TempRMOT, an elegant end-to-end Transformer-based framework for RMOT.
At its core is a query-driven Temporal Enhancement Module that represents each
object as a Transformer query, enabling long-term spatial-temporal interactions
with other objects and past frames to efficiently refine these queries.
TempRMOT achieves state-of-the-art performance on both Refer-KITTI and
Refer-KITTI-V2, demonstrating the effectiveness of our approach. The source
code and dataset is available at https://github.com/zyn213/TempRMOT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tiny but Mighty: A Software-Hardware Co-Design Approach for Efficient
  Multimodal Inference on Battery-Powered Small Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.05109v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.05109v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilong Li, Shuai Zhang, Yijing Zeng, Hao Zhang, Xinmiao Xiong, Jingyu Liu, Pan Hu, Suman Banerjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Multimodal Models (LMMs) are inherently modular, consisting of vision
and audio encoders, projectors, and large language models. Yet, they are almost
always executed monolithically, which underutilizes the heterogeneous
accelerators (NPUs, GPUs, DSPs) in modern SoCs and leads to high end-to-end
latency. In this paper, we present NANOMIND, a hardware--software co-design
inference framework for Large Multimodal Models (LMMs) that breaks large models
into modular ``bricks'' (vision, language, audio, etc.) and maps each to its
ideal accelerator. The key insight is that large models can be broken into
modular components and scheduled to run on the most appropriate compute units.
It performs module-level dynamic offloading across accelerators on
unified-memory SoCs. By combining customized hardware design, system-level
scheduling, and optimized low-bit computation kernels, we demonstrate our
framework with a compact, battery-powered device capable of running LMMs
entirely on device. This prototype functions as a self-contained intelligent
assistant that requires no network connectivity, while achieving higher
throughput and superior power efficiency under strict resource constraints. The
design further bypasses CPU bottlenecks and reduces redundant memory usage
through token-aware buffer management and module-level coordination. Our system
outperforms existing implementations in resource efficiency, cutting energy
consumption by 42.3\% and GPU memory usage by 11.2\%. This enables a
battery-powered device to run LLaVA-OneVision with a camera for nearly half a
day and LLaMA-3-8B for voice interactions up to almost 20.8 hours.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SimBench: Benchmarking the Ability of Large Language Models to Simulate
  Human Behaviors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.17516v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.17516v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tiancheng Hu, Joachim Baumann, Lorenzo Lupo, Nigel Collier, Dirk Hovy, Paul Röttger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language model (LLM) simulations of human behavior have the potential
to revolutionize the social and behavioral sciences, if and only if they
faithfully reflect real human behaviors. Current evaluations are fragmented,
based on bespoke tasks and metrics, creating a patchwork of incomparable
results. To address this, we introduce SimBench, the first large-scale,
standardized benchmark for a robust, reproducible science of LLM simulation. By
unifying 20 diverse datasets covering tasks from moral decision-making to
economic choice across a large global participant pool, SimBench provides the
necessary foundation to ask fundamental questions about when, how, and why LLM
simulations succeed or fail. We show that, while even the best LLMs today have
limited simulation ability (score: 40.80/100), performance scales log-linearly
with model size. Simulation performance is not improved by increased
inference-time compute. We demonstrate an alignment-simulation trade-off:
instruction-tuning improves performance on low-entropy (consensus) questions
but degrades it on high-entropy (diverse) ones. Models particularly struggle
when simulating specific demographic groups. Finally, we demonstrate that
simulation ability correlates most strongly with deep, knowledge-intensive
reasoning (MMLU-Pro, r=0.939). By making progress measurable, we aim to
accelerate the development of more faithful LLM simulators.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Website: http://simbench.tiancheng.hu/ Data:
  https://huggingface.co/datasets/pitehu/SimBench</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry
  Scientific Hypotheses <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07076v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07076v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zonglin Yang, Wanhao Liu, Ben Gao, Tong Xie, Yuqiang Li, Wanli Ouyang, Soujanya Poria, Erik Cambria, Dongzhan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientific discovery plays a pivotal role in advancing human society, and
recent progress in large language models (LLMs) suggests their potential to
accelerate this process. However, it remains unclear whether LLMs can
autonomously generate novel and valid hypotheses in chemistry. In this work, we
investigate whether LLMs can discover high-quality chemistry hypotheses given
only a research background-comprising a question and/or a survey-without
restriction on the domain of the question. We begin with the observation that
hypothesis discovery is a seemingly intractable task. To address this, we
propose a formal mathematical decomposition grounded in a fundamental
assumption: that most chemistry hypotheses can be composed from a research
background and a set of inspirations. This decomposition leads to three
practical subtasks-retrieving inspirations, composing hypotheses with
inspirations, and ranking hypotheses - which together constitute a sufficient
set of subtasks for the overall scientific discovery task. We further develop
an agentic LLM framework, MOOSE-Chem, that is a direct implementation of this
mathematical decomposition. To evaluate this framework, we construct a
benchmark of 51 high-impact chemistry papers published and online after January
2024, each manually annotated by PhD chemists with background, inspirations,
and hypothesis. The framework is able to rediscover many hypotheses with high
similarity to the groundtruth, successfully capturing the core
innovations-while ensuring no data contamination since it uses an LLM with
knowledge cutoff date prior to 2024. Finally, based on LLM's surprisingly high
accuracy on inspiration retrieval, a task with inherently out-of-distribution
nature, we propose a bold assumption: that LLMs may already encode latent
scientific knowledge associations not yet recognized by humans.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Prompt</span>ing is not Enough: Exploring Knowledge Integration and
  Controllable Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.19660v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.19660v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tingjia Shen, Hao Wang, Chuan Qin, Ruijun Sun, Yang Song, Defu Lian, Hengshu Zhu, Enhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-domain question answering (OpenQA) represents a cornerstone in natural
language processing (NLP), primarily focused on extracting answers from
unstructured textual data. With the rapid advancements in Large Language Models
(LLMs), LLM-based OpenQA methods have reaped the benefits of emergent
understanding and answering capabilities enabled by massive parameters compared
to traditional methods. However, most of these methods encounter two critical
challenges: how to integrate knowledge into LLMs effectively and how to
adaptively generate results with specific answer formats for various task
situations. To address these challenges, we propose a novel framework named
GenKI, which aims to improve the OpenQA performance by exploring Knowledge
Integration and controllable Generation on LLMs simultaneously. Specifically,
we first train a dense passage retrieval model to retrieve associated knowledge
from a given knowledge base. Subsequently, we introduce a novel knowledge
integration model that incorporates the retrieval knowledge into instructions
during fine-tuning to intensify the model. Furthermore, to enable controllable
generation in LLMs, we leverage a certain fine-tuned LLM and an ensemble based
on text consistency incorporating all coherence, fluency, and answer format
assurance. Finally, extensive experiments conducted on the TriviaQA, MSMARCO,
and CMRC2018 datasets, featuring diverse answer formats, have demonstrated the
effectiveness of GenKI with comparison of state-of-the-art baselines. Moreover,
ablation studies have disclosed a linear relationship between the frequency of
retrieved knowledge and the model's ability to recall knowledge accurately
against the ground truth. Our code of GenKI is available at
https://github.com/USTC-StarTeam/GenKI
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TrajAgent: An LLM-Agent Framework for Trajectory Modeling via
  Large-and-Small Model Collaboration <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20445v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20445v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuwei Du, Jie Feng, Jie Zhao, Yong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trajectory modeling, which includes research on trajectory data pattern
mining and future prediction, has widespread applications in areas such as life
services, urban transportation, and public administration. Numerous methods
have been proposed to address specific problems within trajectory modeling.
However, the heterogeneity of data and the diversity of trajectory tasks make
effective and reliable trajectory modeling an important yet highly challenging
endeavor, even for domain experts. \fix In this paper, we propose
\textit{TrajAgent}, a agent framework powered by large language models (LLMs),
designed to facilitate robust and efficient trajectory modeling through
automation modeling. This framework leverages and optimizes diverse specialized
models to address various trajectory modeling tasks across different datasets
effectively. \unfix~In \textit{TrajAgent}, we first develop \textit{UniEnv}, an
execution environment with a unified data and model interface, to support the
execution and training of various models. Building on \textit{UniEnv}, we
introduce an agentic workflow designed for automatic trajectory modeling across
various trajectory tasks and data. Furthermore, we introduce collaborative
learning schema between LLM-based agents and small speciallized models, to
enhance the performance of the whole framework effectively. Extensive
experiments on four tasks using four real-world datasets demonstrate the
effectiveness of \textit{TrajAgent} in automated trajectory modeling, achieving
a performance improvement of \fix 2.38\%-69.91\% \unfix over baseline methods.
The codes and data can be accessed via
https://github.com/tsinghua-fib-lab/TrajAgent.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2025,
  https://github.com/tsinghua-fib-lab/TrajAgent</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLMs can hide text in other text of the same length 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.20075v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.20075v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonio Norelli, Michael Bronstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A meaningful text can be hidden inside another, completely different yet
still coherent and plausible, text of the same length. For example, a tweet
containing a harsh political critique could be embedded in a tweet that
celebrates the same political leader, or an ordinary product review could
conceal a secret manuscript. This uncanny state of affairs is now possible
thanks to Large Language Models, and in this paper we present a simple and
efficient protocol to achieve it. We show that even modest 8-billion-parameter
open-source LLMs are sufficient to obtain high-quality results, and a message
as long as this abstract can be encoded and decoded locally on a laptop in
seconds. The existence of such a protocol demonstrates a radical decoupling of
text from authorial intent, further eroding trust in written communication,
already shaken by the rise of LLM chatbots. We illustrate this with a concrete
scenario: a company could covertly deploy an unfiltered LLM by encoding its
answers within the compliant responses of a safe model. This possibility raises
urgent questions for AI safety and challenges our understanding of what it
means for a Large Language Model to know something.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, main paper 9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MOOSE-Chem2: Exploring LLM Limits in Fine-Grained Scientific Hypothesis
  Discovery via Hierarchical Search <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.19209v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.19209v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zonglin Yang, Wanhao Liu, Ben Gao, Yujie Liu, Wei Li, Tong Xie, Lidong Bing, Wanli Ouyang, Erik Cambria, Dongzhan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown promise in automating scientific
hypothesis generation, yet existing approaches primarily yield coarse-grained
hypotheses lacking critical methodological and experimental details. We
introduce and formally define the new task of fine-grained scientific
hypothesis discovery, which entails generating detailed, experimentally
actionable hypotheses from coarse initial research directions. We frame this as
a combinatorial optimization problem and investigate the upper limits of LLMs'
capacity to solve it when maximally leveraged. Specifically, we explore four
foundational questions: (1) how to best harness an LLM's internal heuristics to
formulate the fine-grained hypothesis it itself would judge as the most
promising among all the possible hypotheses it might generate, based on its own
internal scoring-thus defining a latent reward landscape over the hypothesis
space; (2) whether such LLM-judged better hypotheses exhibit stronger alignment
with ground-truth hypotheses; (3) whether shaping the reward landscape using an
ensemble of diverse LLMs of similar capacity yields better outcomes than
defining it with repeated instances of the strongest LLM among them; and (4)
whether an ensemble of identical LLMs provides a more reliable reward landscape
than a single LLM. To address these questions, we propose a hierarchical search
method that incrementally proposes and integrates details into the hypothesis,
progressing from general concepts to specific experimental configurations. We
show that this hierarchical process smooths the reward landscape and enables
more effective optimization. Empirical evaluations on a new benchmark of
expert-annotated fine-grained hypotheses from recent literature show that our
method consistently outperforms strong baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Atlas of In-Context Learning: How Attention Heads Shape In-Context
  Retrieval Augmentation <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.15807v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.15807v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrick Kahardipraja, Reduan Achtibat, Thomas Wiegand, Wojciech Samek, Sebastian Lapuschkin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models are able to exploit in-context learning to access
external knowledge beyond their training data through retrieval-augmentation.
While promising, its inner workings remain unclear. In this work, we shed light
on the mechanism of in-context retrieval augmentation for question answering by
viewing a prompt as a composition of informational components. We propose an
attribution-based method to identify specialized attention heads, revealing
in-context heads that comprehend instructions and retrieve relevant contextual
information, and parametric heads that store entities' relational knowledge. To
better understand their roles, we extract function vectors and modify their
attention weights to show how they can influence the answer generation process.
Finally, we leverage the gained insights to trace the sources of knowledge used
during inference, paving the way towards more safe and transparent language
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TaoSR1: The Thinking Model for E-commerce Relevance Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.12365v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.12365v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenhe Dong, Shaowei Yao, Pengkun Jiao, Jianhui Yang, Yiming Jin, Zerui Huang, Xiaojiang Zhou, Dan Ou, Haihong Tang, Bo Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Query-product relevance prediction is a core task in e-commerce search.
BERT-based models excel at semantic matching but lack complex reasoning
capabilities. While Large Language Models (LLMs) are explored, most still use
discriminative fine-tuning or distill to smaller models for deployment. We
propose a framework to directly deploy LLMs for this task, addressing key
challenges: Chain-of-Thought (CoT) error accumulation, discriminative
hallucination, and deployment feasibility. Our framework, TaoSR1, involves
three stages: (1) Supervised Fine-Tuning (SFT) with CoT to instill reasoning;
(2) Offline sampling with a pass@N strategy and Direct Preference Optimization
(DPO) to improve generation quality; and (3) Difficulty-based dynamic sampling
with Group Relative Policy Optimization (GRPO) to mitigate discriminative
hallucination. Additionally, post-CoT processing and a cumulative
probability-based partitioning method enable efficient online deployment.
TaoSR1 significantly outperforms baselines on offline datasets and achieves
substantial gains in online side-by-side human evaluations, introducing a novel
paradigm for applying CoT reasoning to relevance classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Thought Anchors: Which LLM Reasoning Steps Matter? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19143v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19143v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul C. Bogdan, Uzay Macar, Neel Nanda, Arthur Conmy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current frontier large-language models rely on reasoning to achieve
state-of-the-art performance. Many existing interpretability are limited in
this area, as standard methods have been designed to study single forward
passes of a model rather than the multi-token computational steps that unfold
during reasoning. We argue that analyzing reasoning traces at the sentence
level is a promising approach to understanding reasoning processes. We
introduce a black-box method that measures each sentence's counterfactual
importance by repeatedly sampling replacement sentences from the model,
filtering for semantically different ones, and continuing the chain of thought
from that point onwards to quantify the sentence's impact on the distribution
of final answers. We discover that certain sentences can have an outsized
impact on the trajectory of the reasoning trace and final answer. We term these
sentences \textit{thought anchors}. These are generally planning or uncertainty
management sentences, and specialized attention heads consistently attend from
subsequent sentences to thought anchors. We further show that examining
sentence-sentence causal links within a reasoning trace gives insight into a
model's behavior. Such information can be used to predict a problem's
difficulty and the extent different question domains involve sequential or
diffuse reasoning. As a proof-of-concept, we demonstrate that our techniques
together provide a practical toolkit for analyzing reasoning models by
conducting a detailed case study of how the model solves a difficult math
problem, finding that our techniques yield a consistent picture of the
reasoning trace's structure. We provide an open-source tool
(thought-anchors.com) for visualizing the outputs of our methods on further
problems. The convergence across our methods shows the potential of
sentence-level analysis for a deeper understanding of reasoning models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paul C. Bogdan and Uzay Macar contributed equally to this work, and
  their listed order was determined by coinflip. Neel Nanda and Arthur Conmy
  contributed equally to this work as senior authors, and their listed order
  was determined by coinflip</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ThinkBrake: Mitigating Overthinking in Tool Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.00546v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.00546v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minjae Oh, Sangjun Song, Seungkyu Lee, Sungmin Jo, Yohan Jo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Small reasoning models (SRMs) often overthink during tool use: they reach a
correct tool-argument configuration, then continue reasoning and overwrite it
with an incorrect final call. We diagnose overthinking via oracle rollouts that
inject </think> at sentence boundaries. On the Berkeley Function Calling
Leaderboard (BFCL), this oracle termination lifts average accuracy from 85.8\%
to 94.2\% while reducing tokens by 80-94\%, revealing substantial recoverable
headroom and potential redundant reasoning. While prior work on concise
reasoning has largely targeted mathematics, tool reasoning remains
underexplored. We adapt various early-termination baselines to tool use and
introduce ThinkBrake, a training-free decoding heuristic. ThinkBrake monitors
the log-probability margin between </think> and the current top token at
sentence boundaries and triggers termination when this margin becomes small.
Across BFCL's single turn, non-live and live splits, ThinkBrake preserves or
improves accuracy while reducing tokens up to 25\%, outperforming various
baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OpenS2S: Advancing Fully Open-Source End-to-End Empathetic Large Speech
  Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.05177v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.05177v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Wang, Tianyu Peng, Wen Yang, Yinan Bai, Guangfu Wang, Jun Lin, Lanpeng Jia, Lingxiang Wu, Jinqiao Wang, Chengqing Zong, Jiajun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Empathetic interaction is a cornerstone of human-machine communication, due
to the need for understanding speech enriched with paralinguistic cues and
generating emotional and expressive responses. However, the most powerful
empathetic LSLMs are increasingly closed off, leaving the crucial details about
the architecture, data and development opaque to researchers. Given the
critical need for transparent research into the LSLMs and empathetic behavior,
we present OpenS2S, a fully open-source, transparent and end-to-end LSLM
designed to enable empathetic speech interactions. Based on our empathetic
speech-to-text model BLSP-Emo, OpenS2S further employs a streaming interleaved
decoding architecture to achieve low-latency speech generation. To facilitate
end-to-end training, OpenS2S incorporates an automated data construction
pipeline that synthesizes diverse, high-quality empathetic speech dialogues at
low cost. By leveraging large language models to generate empathetic content
and controllable text-to-speech systems to introduce speaker and emotional
variation, we construct a scalable training corpus with rich paralinguistic
diversity and minimal human supervision. We release the fully open-source
OpenS2S model, including the dataset, model weights, pre-training and
fine-tuning codes, to empower the broader research community and accelerate
innovation in empathetic speech systems. The project webpage can be accessed at
https://casia-lm.github.io/OpenS2S
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report, Update on OpenS2S_v1.5</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When Personalization Meets Reality: A Multi-Faceted Analysis of
  Personalized Preference Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19158v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19158v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijiang River Dong, Tiancheng Hu, Yinhong Liu, Ahmet Üstün, Nigel Collier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Reinforcement Learning from Human Feedback (RLHF) is widely used to
align Large Language Models (LLMs) with human preferences, it typically assumes
homogeneous preferences across users, overlooking diverse human values and
minority viewpoints. Although personalized preference learning addresses this
by tailoring separate preferences for individual users, the field lacks
standardized methods to assess its effectiveness. We present a multi-faceted
evaluation framework that measures not only performance but also fairness,
unintended effects, and adaptability across varying levels of preference
divergence. Through extensive experiments comparing eight personalization
methods across three preference datasets, we demonstrate that performance
differences between methods could reach 36% when users strongly disagree, and
personalization can introduce up to 20% safety misalignment. These findings
highlight the critical need for holistic evaluation approaches to advance the
development of more effective and inclusive preference learning systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Input Matters: Evaluating Input Structure's Impact on LLM Summaries of
  Sports Play-by-Play 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.21034v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.21034v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Barkavi Sundararajan, Somayajulu Sripada, Ehud Reiter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A major concern when deploying LLMs in accuracy-critical domains such as
sports reporting is that the generated text may not faithfully reflect the
input data. We quantify how input structure affects hallucinations and other
factual errors in LLM-generated summaries of NBA play-by-play data, across
three formats: row-structured, JSON and unstructured. We manually annotated
3,312 factual errors across 180 game summaries produced by two models,
Llama-3.1-70B and Qwen2.5-72B. Input structure has a strong effect: JSON input
reduces error rates by 69% for Llama and 65% for Qwen compared to unstructured
input, while row-structured input reduces errors by 54% for Llama and 51% for
Qwen. A two-way repeated measures ANOVA shows that input structure accounts for
over 80% of the variance in error rates, with Tukey HSD post hoc tests
confirming statistically significant differences between all input formats.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at INLG 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LinearRAG: Linear Graph Retrieval Augmented Generation on Large-scale
  Corpora 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.10114v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.10114v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luyao Zhuang, Shengyuan Chen, Yilin Xiao, Huachi Zhou, Yujing Zhang, Hao Chen, Qinggang Zhang, Xiao Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) is widely used to mitigate
hallucinations of Large Language Models (LLMs) by leveraging external
knowledge. While effective for simple queries, traditional RAG systems struggle
with large-scale, unstructured corpora where information is fragmented. Recent
advances incorporate knowledge graphs to capture relational structures,
enabling more comprehensive retrieval for complex, multi-hop reasoning tasks.
However, existing graph-based RAG (GraphRAG) methods rely on unstable and
costly relation extraction for graph construction, often producing noisy graphs
with incorrect or inconsistent relations that degrade retrieval quality. In
this paper, we revisit the pipeline of existing GraphRAG systems and propose
LinearRAG (Linear Graph-based Retrieval-Augmented Generation), an efficient
framework that enables reliable graph construction and precise passage
retrieval. Specifically, LinearRAG constructs a relation-free hierarchical
graph, termed Tri-Graph, using only lightweight entity extraction and semantic
linking, avoiding unstable relation modeling. This new paradigm of graph
construction scales linearly with corpus size and incurs no extra token
consumption, providing an economical and reliable indexing of the original
passages. For retrieval, LinearRAG adopts a two-stage strategy: (i) relevant
entity activation via local semantic bridging, followed by (ii) passage
retrieval through global importance aggregation. Extensive experiments on four
datasets demonstrate that LinearRAG significantly outperforms baseline models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-turn Training with Basic Human Feedback Helps Little on LLM
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.21339v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.21339v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiang Liu, Wuganjing Song, Zhenzhou Lin, Feifan Chen, Qiaolong Cai, Chen Li, Yongduo Sui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The reasoning capabilities of Large Language Models (LLMs) are typically
developed through the single-turn reinforcement learning, whereas real-world
applications often involve multi-turn interactions with human feedback, leading
to a potential mismatch between training and deployment conditions. In this
work, we study whether multi-turn training with human feedback is necessary for
reasoning tasks. We compare conventional single-turn training with three
multi-turn strategies and reach contrary conclusions to previous research. We
find that models trained in a single-turn setting generalize effectively to
both single- and multi-turn evaluations, while models trained with multi-turn
strategies exhibit a significant degradation in single-turn reasoning
performance. These results suggest that for tasks with complete information,
robust single-turn training remains more effective and reliable, as multi-turn
training with basic feedback provides limited benefits and can even degrade
reasoning capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ StereoDetect: Detecting Stereotypes and Anti-stereotypes the Correct Way
  Using Social Psychological Underpinnings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.03352v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.03352v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaustubh Shivshankar Shejole, Pushpak Bhattacharyya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stereotypes are known to have very harmful effects, making their detection
critically important. However, current research predominantly focuses on
detecting and evaluating stereotypical biases, thereby leaving the study of
stereotypes in its early stages. Our study revealed that many works have failed
to clearly distinguish between stereotypes and stereotypical biases, which has
significantly slowed progress in advancing research in this area. Stereotype
and Anti-stereotype detection is a problem that requires social knowledge;
hence, it is one of the most difficult areas in Responsible AI. This work
investigates this task, where we propose a five-tuple definition and provide
precise terminologies disentangling stereotypes, anti-stereotypes,
stereotypical bias, and general bias. We provide a conceptual framework
grounded in social psychology for reliable detection. We identify key
shortcomings in existing benchmarks for this task of stereotype and
anti-stereotype detection. To address these gaps, we developed StereoDetect, a
well curated, definition-aligned benchmark dataset designed for this task. We
show that sub-10B language models and GPT-4o frequently misclassify
anti-stereotypes and fail to recognize neutral overgeneralizations. We
demonstrate StereoDetect's effectiveness through multiple qualitative and
quantitative comparisons with existing benchmarks and models fine-tuned on
them. The dataset and code is available at
https://github.com/KaustubhShejole/StereoDetect.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cohort Discovery: A <span class="highlight-title">Survey</span> on LLM-Assisted Clinical Trial Recruitment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15301v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15301v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shrestha Ghosh, Moritz Schneider, Carina Reinicke, Carsten Eickhoff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in LLMs have greatly improved general-domain NLP tasks. Yet,
their adoption in critical domains, such as clinical trial recruitment, remains
limited. As trials are designed in natural language and patient data is
represented as both structured and unstructured text, the task of matching
trials and patients benefits from knowledge aggregation and reasoning abilities
of LLMs. Classical approaches are trial-specific and LLMs with their ability to
consolidate distributed knowledge hold the potential to build a more general
solution. Yet recent applications of LLM-assisted methods rely on proprietary
models and weak evaluation benchmarks. In this survey, we are the first to
analyze the task of trial-patient matching and contextualize emerging LLM-based
approaches in clinical trial recruitment. We critically examine existing
benchmarks, approaches and evaluation frameworks, the challenges to adopting
LLM technologies in clinical research and exciting future directions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Confidence Estimates Decide When Chain-of-Thought Is Necessary for
  LLMs? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.21007v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.21007v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Lewis-Lim, Xingwei Tan, Zhixue Zhao, Nikolaos Aletras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chain-of-thought (CoT) prompting has emerged as a common technique for
enhancing the reasoning abilities of large language models (LLMs). While
extended reasoning can boost accuracy on complex tasks, it is often unnecessary
and substantially increases token usage, limiting the practicality of reasoning
models in many scenarios. Recent models, such as GPT-OSS and Qwen3, expose
controls that enable users to adjust the length of CoT or determine whether it
is used at all. Yet, it remains unclear when CoT should be used: on some tasks
it improves performance, while on others it provides little benefit or even
harms performance. We address this challenge with confidence-gated CoT, where a
model invokes reasoning only when confidence in its direct answer is low. To
this end, we present the first systematic study of training-free confidence
estimation methods for CoT gating. Specifically, we evaluate four training-free
confidence estimation methods and compare them to a random baseline and an
oracle that always knows when CoT is needed. Through extensive experiments, we
show that existing training-free confidence measures can reduce redundant CoT
and outperform randomly invoked CoT. However, the utility of individual
confidence measures is inconsistent, varying with both the dataset and the
model, underscoring the difficulty of deploying confidence-gated CoT in
practice. By analysing both strengths and failure modes, our study highlights
the potential and limitations of current methods and paves the way toward more
reliable adaptive gating of CoT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ First SFT, Second RL, Third UPT: Continual Improving Multi-Modal LLM
  Reasoning via Unsupervised Post-Training <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.22453v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.22453v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lai Wei, Yuting Li, Chen Wang, Yue Wang, Linghe Kong, Weiran Huang, Lichao Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Improving Multi-modal Large Language Models (MLLMs) in the post-training
stage typically relies on supervised fine-tuning (SFT) or reinforcement
learning (RL), which require expensive and manually annotated multi-modal
data--an ultimately unsustainable resource. This limitation has motivated a
growing interest in unsupervised paradigms as a third stage of post-training
after SFT and RL. While recent efforts have explored this direction, their
methods are complex and difficult to iterate. To address this, we propose
MM-UPT, a simple yet effective framework for unsupervised post-training of
MLLMs, enabling continual self-improvement without any external supervision.
The training method of MM-UPT builds upon GRPO, replacing traditional reward
signals with a self-rewarding mechanism based on majority voting over multiple
sampled responses. Our experiments demonstrate that such training method
effectively improves the reasoning ability of Qwen2.5-VL-7B (e.g.,
66.3\%$\rightarrow$72.9\% on MathVista, 62.9\%$\rightarrow$68.7\% on We-Math),
using standard dataset without ground truth labels. To further explore
scalability, we extend our framework to a data self-generation setting,
designing two strategies that prompt the MLLM to synthesize new training
samples on its own. Additional experiments show that combining these synthetic
data with the unsupervised training method can also boost performance,
highlighting a promising approach for scalable self-improvement. Overall,
MM-UPT offers a new paradigm for autonomous enhancement of MLLMs, serving as a
critical third step after initial SFT and RL in the absence of external
supervision. Our code is available at https://github.com/waltonfuture/MM-UPT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeepOmni: Towards Seamless and Smart Speech Interaction with Adaptive
  Modality-Specific MoE 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21864v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21864v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Shao, Heting Gao, Yunhang Shen, Jiawei Chen, Zuwei Long, Dong Yang, Ke Li, Xing Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Native multimodal large language models (MLLMs) restructure a single large
language model (LLM) into a spoken language model (SLM) capable of both speech
and text generation. Compared to modular and aligned MLLMs, native MLLMs
preserve richer paralinguistic features such as emotion and prosody, and
generate speech responses directly within the backbone LLM rather than using a
separate speech decoder. This integration also results in lower response
latency and smoother interaction. However, native MLLMs suffer from
catastrophic forgetting and performance degradation because the available
paired speech-text data is insufficient to support the pretraining of MLLMs
compared to the vast amount of text data required to pretrain text LLMs. To
address this issue, we propose DeepTalk, a framework for adaptive modality
expert learning based on a Mixture of Experts (MoE) architecture. DeepTalk
first adaptively distinguishes modality experts according to their modality
load within the LLM. Each modality expert then undergoes specialized
single-modality training, followed by joint multimodal collaborative training.
As a result, DeepTalk incurs only a 5.5% performance drop compared to the
original LLM, which is significantly lower than the average performance drop of
over 20% typically seen in native MLLMs (such as GLM-4-Voice), and is on par
with modular MLLMs. Meanwhile, the end-to-end dialogue latency remains within
0.5 seconds, ensuring a seamless and intelligent speech interaction experience.
Code and models are released at https://github.com/talkking/DeepTalk.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ COUNTDOWN: Contextually Sparse Activation Filtering Out Unnecessary
  Weights in Down Projection <span class="chip">EMNLP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.17701v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.17701v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaewon Cheon, Pilsung Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing size of large language models has created significant
computational inefficiencies. To address this challenge, sparse activation
methods selectively deactivates non-essential parameters during inference,
reducing computational costs in FFNN layers. While existing methods focus on
non-linear gating mechanisms, we hypothesize that the sparsity of the FFNN
layer lies globally in the form of a linear combination over its internal down
projection matrix. Based on this insight, we propose two methods: M-COUNTDOWN,
leveraging indirect coefficients, and D-COUNTDOWN, utilizing direct
coefficients of the linear combination. Experimental results demonstrate that
D-COUNTDOWN can omit 90% of computations with performance loss as low as 5.5%
ideally, while M-COUNTDOWN provides a predictor-free solution with up to 29.4%
better performance preservation compared to existing methods. Our specialized
kernel implementations effectively realize these theoretical gains into
substantial real-world acceleration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2025 (Main Track)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GraphInstruct: Empowering Large Language Models with Graph Understanding
  and Reasoning Capability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04483v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04483v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihan Luo, Xiran Song, Hong Huang, Jianxun Lian, Chenhao Zhang, Jinqi Jiang, Xing Xie, Hai Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Improving the general capabilities of large language models (LLMs) is an
active research topic. As a common data structure in many real-world domains,
understanding graph data is a crucial part of advancing general intelligence.
To this end, we propose a dynamic benchmark named GraphInstruct in this paper,
which comprehensively includes 21 classical graph reasoning tasks, providing
diverse graph generation pipelines and detailed intermediate reasoning steps
for each sample. Based on GraphInstruct, we develop GraphSolver via efficient
instruction-tuning, which demonstrates prominent graph understanding capability
compared to other open-sourced LLMs. To further endow LLMs with multi-step
graph reasoning capability, we propose a label-mask training strategy and build
GraphSolver+, which leverages masked supervision on intermediate reasoning
tokens to emphasize crucial node-identification signals. As one of the
pioneering efforts to enhance the graph understanding and reasoning abilities
of LLMs, extensive experiments have demonstrated the superiority of GraphSolver
and GraphSolver+ over other LLMs. We sincerely hope GraphInstruct will
facilitate further research on applying LLMs to graph-structured data. Our code
and data are released publicly at: https://github.com/CGCL-codes/GraphInstruct.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Frontiers of Computer Science</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Cross-Lingual Cost: Retrieval Biases in RAG over Arabic-English
  Corpora 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07543v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07543v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Amiraz, Yaroslav Fyodorov, Elad Haramaty, Zohar Karnin, Liane Lewin-Eytan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-lingual retrieval-augmented generation (RAG) is a critical capability
for retrieving and generating answers across languages. Prior work in this
context has mostly focused on generation and relied on benchmarks derived from
open-domain sources, most notably Wikipedia. In such settings, retrieval
challenges often remain hidden due to language imbalances, overlap with
pretraining data, and memorized content. To address this gap, we study
Arabic-English RAG in a domain-specific setting using benchmarks derived from
real-world corporate datasets. Our benchmarks include all combinations of
languages for the user query and the supporting document, drawn independently
and uniformly at random. This enables a systematic study of multilingual
retrieval behavior.
  Our findings reveal that retrieval is a critical bottleneck in cross-lingual
domain-specific scenarios, with substantial performance drops occurring when
the user query and supporting document languages differ. A key insight is that
these failures stem primarily from the retriever's difficulty in ranking
documents across languages. Finally, we propose two simple retrieval strategies
that address this source of failure by enforcing equal retrieval from both
languages or by translating the query, resulting in substantial improvements in
cross-lingual and overall performance. These results highlight meaningful
opportunities for improving multilingual retrieval, particularly in practical,
real-world RAG applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ArabicNLP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ContextAgent: Context-Aware Proactive LLM Agents with Open-World Sensory
  Perceptions <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14668v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14668v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bufang Yang, Lilin Xu, Liekang Zeng, Kaiwei Liu, Siyang Jiang, Wenrui Lu, Hongkai Chen, Xiaofan Jiang, Guoliang Xing, Zhenyu Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Large Language Models (LLMs) have propelled intelligent
agents from reactive responses to proactive support. While promising, existing
proactive agents either rely exclusively on observations from enclosed
environments (e.g., desktop UIs) with direct LLM inference or employ rule-based
proactive notifications, leading to suboptimal user intent understanding and
limited functionality for proactive service. In this paper, we introduce
ContextAgent, the first context-aware proactive agent that incorporates
extensive sensory contexts surrounding humans to enhance the proactivity of LLM
agents. ContextAgent first extracts multi-dimensional contexts from massive
sensory perceptions on wearables (e.g., video and audio) to understand user
intentions. ContextAgent then leverages the sensory contexts and personas from
historical data to predict the necessity for proactive services. When proactive
assistance is needed, ContextAgent further automatically calls the necessary
tools to assist users unobtrusively. To evaluate this new task, we curate
ContextAgentBench, the first benchmark for evaluating context-aware proactive
LLM agents, covering 1,000 samples across nine daily scenarios and twenty
tools. Experiments on ContextAgentBench show that ContextAgent outperforms
baselines by achieving up to 8.5% and 6.0% higher accuracy in proactive
predictions and tool calling, respectively. We hope our research can inspire
the development of more advanced, human-centric, proactive AI assistants. The
code and dataset are publicly available at
https://github.com/openaiotlab/ContextAgent.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ColorEcosystem: Powering Personalized, Standardized, and Trustworthy
  Agentic Service in massive-agent Ecosystem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.21566v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.21566v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangwen Wu, Zheng Wu, Jihong Wang, Yunku Chen, Ruiguang Pei, Heyuan Huang, Xin Liao, Xingyu Lou, Huarong Deng, Zhihui Fu, Weiwen Liu, Zhuosheng Zhang, Weinan Zhang, Jun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of (multimodal) large language model-based agents,
the landscape of agentic service management has evolved from single-agent
systems to multi-agent systems, and now to massive-agent ecosystems. Current
massive-agent ecosystems face growing challenges, including impersonal service
experiences, a lack of standardization, and untrustworthy behavior. To address
these issues, we propose ColorEcosystem, a novel blueprint designed to enable
personalized, standardized, and trustworthy agentic service at scale.
Concretely, ColorEcosystem consists of three key components: agent carrier,
agent store, and agent audit. The agent carrier provides personalized service
experiences by utilizing user-specific data and creating a digital twin, while
the agent store serves as a centralized, standardized platform for managing
diverse agentic services. The agent audit, based on the supervision of
developer and user activities, ensures the integrity and credibility of both
service providers and users. Through the analysis of challenges, transitional
forms, and practical considerations, the ColorEcosystem is poised to power
personalized, standardized, and trustworthy agentic service across
massive-agent ecosystems. Meanwhile, we have also implemented part of
ColorEcosystem's functionality, and the relevant code is open-sourced at
https://github.com/opas-lab/color-ecosystem.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Computational Analysis of Character Development in Holocaust Testimonies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.17063v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.17063v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Esther Shizgal, Eitan Wagner, Renana Keydar, Omri Abend
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work presents a computational approach to analyze character development
along the narrative timeline. The analysis characterizes the inner and outer
changes the protagonist undergoes within a narrative, and the interplay between
them. We consider transcripts of Holocaust survivor testimonies as a test case,
each telling the story of an individual in first-person terms. We focus on the
survivor's religious trajectory, examining the evolution of their disposition
toward religious belief and practice along the testimony. Clustering the
resulting trajectories in the dataset, we identify common sequences in the
data. Our findings highlight multiple common structures of religiosity across
the narratives: in terms of belief, most present a constant disposition, while
for practice, most present an oscillating structure, serving as valuable
material for historical and sociological research. This work demonstrates the
potential of natural language processing techniques for analyzing character
evolution through thematic trajectories in narratives.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FaithLM: Towards Faithful Explanations for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.04678v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.04678v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu-Neng Chuang, Guanchu Wang, Chia-Yuan Chang, Ruixiang Tang, Shaochen Zhong, Fan Yang, Mengnan Du, Xuanting Cai, Vladimir Braverman, Xia Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) increasingly produce natural language
explanations, yet these explanations often lack faithfulness, and they do not
reliably reflect the evidence the model uses to decide. We introduce FaithLM, a
model-agnostic framework that evaluates and improves the faithfulness of LLM
explanations without token masking or task-specific heuristics. FaithLM
formalizes explanation faithfulness as an intervention property: a faithful
explanation should yield a prediction shift when its content is contradicted.
Theoretical analysis shows that the resulting contrary-hint score is a sound
and discriminative estimator of faithfulness. Building on this principle,
FaithLM iteratively refines both the elicitation prompt and the explanation to
maximize the measured score. Experiments on three multi-domain datasets and
multiple LLM backbones demonstrate that FaithLM consistently increases
faithfulness and produces explanations more aligned with human rationales than
strong self-explanation baselines. These findings highlight that
intervention-based evaluation, coupled with iterative optimization, provides a
principled route toward faithful and reliable LLM explanations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Agent KB: Leveraging Cross-Domain Experience for Agentic Problem Solving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.06229v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.06229v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangru Tang, Tianrui Qin, Tianhao Peng, Ziyang Zhou, Daniel Shao, Tingting Du, Xinming Wei, Peng Xia, Fang Wu, He Zhu, Ge Zhang, Jiaheng Liu, Xingyao Wang, Sirui Hong, Chenglin Wu, Hao Cheng, Chi Wang, Wangchunshu Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI agent frameworks operate in isolation, forcing agents to rediscover
solutions and repeat mistakes across different systems. Despite valuable
problem-solving experiences accumulated by frameworks like smolagents,
OpenHands, and OWL, this knowledge remains trapped within individual systems,
preventing the emergence of collective intelligence. Current memory systems
focus on individual agents or framework-specific demonstrations, failing to
enable cross-architecture knowledge transfer. We introduce AGENT KB, a
universal memory infrastructure enabling seamless experience sharing across
heterogeneous agent frameworks without retraining. AGENT KB aggregates
trajectories into a structured knowledge base and serves lightweight APIs. At
inference time, hybrid retrieval operates through two stages: planning seeds
agents with cross-domain workflows, while feedback applies targeted diagnostic
fixes. A disagreement gate ensures retrieved knowledge enhances rather than
disrupts reasoning, addressing knowledge interference in cross-framework
transfer. We validate AGENT KB across major frameworks on GAIA, Humanity's Last
Exam, GPQA, and SWE-bench. Results show substantial improvements across diverse
model families: compared to baseline pass@1, smolagents with AGENT KB achieve
up to 18.7pp gains at pass@3 (55.2% -> 73.9%), while OpenHands improves 4.0pp
on SWE-bench pass@1 (24.3% -> 28.3%). Similar improvements are observed across
all base model families. Ablations confirm that hybrid retrieval and feedback
stages are essential, with automatically generated experiences matching manual
curation. This establishes the foundation for collective agent intelligence
through shared memory infrastructures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Detecting and Rectifying Noisy Labels: A Similarity-based Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.23964v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.23964v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dang Huu-Tien, Minh-Phuong Nguyen, Naoya Inoue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Label noise in datasets could significantly damage the performance and
robustness of deep neural networks (DNNs) trained on these datasets. As the
size of modern DNNs grows, there is a growing demand for automated tools for
detecting such errors. In this paper, we propose post-hoc, model-agnostic noise
detection and rectification methods utilizing the penultimate feature from a
DNN. Our idea is based on the observation that the similarity between the
penultimate feature of a mislabeled data point and its true class data points
is higher than that for data points from other classes, making the probability
of label occurrence within a tight, similar cluster informative for detecting
and rectifying errors. Through theoretical and empirical analyses, we
demonstrate that our approach achieves high detection performance across
diverse, realistic noise scenarios and can automatically rectify these errors
to improve dataset quality. Our implementation is available at
https://anonymous.4open.science/r/noise-detection-and-rectification-AD8E.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">13</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Concerto: Joint 2D-3D <span class="highlight-title">Self-Supervised</span> Learning Emerges Spatial
  Representations <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23607v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23607v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujia Zhang, Xiaoyang Wu, Yixing Lao, Chengyao Wang, Zhuotao Tian, Naiyan Wang, Hengshuang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans learn abstract concepts through multisensory synergy, and once formed,
such representations can often be recalled from a single modality. Inspired by
this principle, we introduce Concerto, a minimalist simulation of human concept
learning for spatial cognition, combining 3D intra-modal self-distillation with
2D-3D cross-modal joint embedding. Despite its simplicity, Concerto learns more
coherent and informative spatial features, as demonstrated by zero-shot
visualizations. It outperforms both standalone SOTA 2D and 3D self-supervised
models by 14.2% and 4.8%, respectively, as well as their feature concatenation,
in linear probing for 3D scene perception. With full fine-tuning, Concerto sets
new SOTA results across multiple scene understanding benchmarks (e.g., 80.7%
mIoU on ScanNet). We further present a variant of Concerto tailored for
video-lifted point cloud spatial understanding, and a translator that linearly
projects Concerto representations into CLIP's language space, enabling
open-world perception. These results highlight that Concerto emerges spatial
representations with superior fine-grained geometric and semantic consistency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025, produced by Pointcept, project page:
  https://pointcept.github.io/Concerto</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Track, Inpaint, Resplat: Subject-driven 3D and 4D Generation with
  Progressive Texture Infilling <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23605v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23605v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuhong Zheng, Ashkan Mirzaei, Igor Gilitschenski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current 3D/4D generation methods are usually optimized for photorealism,
efficiency, and aesthetics. However, they often fail to preserve the semantic
identity of the subject across different viewpoints. Adapting generation
methods with one or few images of a specific subject (also known as
Personalization or Subject-driven generation) allows generating visual content
that align with the identity of the subject. However, personalized 3D/4D
generation is still largely underexplored. In this work, we introduce TIRE
(Track, Inpaint, REsplat), a novel method for subject-driven 3D/4D generation.
It takes an initial 3D asset produced by an existing 3D generative model as
input and uses video tracking to identify the regions that need to be modified.
Then, we adopt a subject-driven 2D inpainting model for progressively infilling
the identified regions. Finally, we resplat the modified 2D multi-view
observations back to 3D while still maintaining consistency. Extensive
experiments demonstrate that our approach significantly improves identity
preservation in 3D/4D generation compared to state-of-the-art methods. Our
project website is available at
https://zsh2000.github.io/track-inpaint-resplat.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025, 38 pages, 22 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PixelRefer: A Unified Framework for Spatio-Temporal Object Referring
  with Arbitrary Granularity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23603v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23603v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuqian Yuan, Wenqiao Zhang, Xin Li, Shihao Wang, Kehan Li, Wentong Li, Jun Xiao, Lei Zhang, Beng Chin Ooi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) have demonstrated strong
general-purpose capabilities in open-world visual comprehension. However, most
existing MLLMs primarily focus on holistic, scene-level understanding, often
overlooking the need for fine-grained, object-centric reasoning. In this paper,
we present PixelRefer, a unified region-level MLLM framework that enables
advanced fine-grained understanding over user-specified regions across both
images and videos. Motivated by the observation that LLM attention
predominantly focuses on object-level tokens, we propose a Scale-Adaptive
Object Tokenizer (SAOT) to generate compact and semantically rich object
representations from free-form regions. Our analysis reveals that global visual
tokens contribute mainly in early LLM layers, inspiring the design of
PixelRefer-Lite, an efficient variant that employs an Object-Centric Infusion
module to pre-fuse global context into object tokens. This yields a lightweight
Object-Only Framework that substantially reduces computational cost while
maintaining high semantic fidelity. To facilitate fine-grained instruction
tuning, we curate PixelRefer-2.2M, a high-quality object-centric instruction
dataset. Extensive experiments across a range of benchmarks validate that
PixelRefer achieves leading performance with fewer training samples, while
PixelRefer-Lite offers competitive accuracy with notable gains in efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PRISM-Bench: A Benchmark of Puzzle-Based Visual Tasks with CoT Error
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23594v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23594v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusu Qian, Cheng Wan, Chao Jia, Yinfei Yang, Qingyu Zhao, Zhe Gan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce \textbf{PRISM-Bench}, a benchmark of puzzle-based visual
challenges designed to evaluate not only whether models can solve problems, but
how their reasoning unfolds. Unlike prior evaluations that measure only
final-answer accuracy, PRISM-Bench introduces a diagnostic task: given a visual
puzzle and a step-by-step chain-of-thought (CoT) containing exactly one error,
models must identify the first incorrect step. This setting enables
fine-grained assessment of logical consistency, error detection, and visual
reasoning. The puzzles in PRISM-Bench require multi-step symbolic, geometric,
and analogical reasoning, resisting shortcuts based on superficial pattern
matching. Evaluations across state-of-the-art MLLMs reveal a persistent gap
between fluent generation and faithful reasoning: models that produce plausible
CoTs often fail to locate simple logical faults. By disentangling answer
generation from reasoning verification, PRISM-Bench offers a sharper lens on
multimodal reasoning competence and underscores the need for diagnostic
evaluation protocols in the development of trustworthy MLLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InFlux: A Benchmark for Self-Calibration of Dynamic Intrinsics of Video
  Cameras <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23589v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23589v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erich Liang, Roma Bhattacharjee, Sreemanti Dey, Rafael Moschopoulos, Caitlin Wang, Michel Liao, Grace Tan, Andrew Wang, Karhan Kayan, Stamatis Alexandropoulos, Jia Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately tracking camera intrinsics is crucial for achieving 3D
understanding from 2D video. However, most 3D algorithms assume that camera
intrinsics stay constant throughout a video, which is often not true for many
real-world in-the-wild videos. A major obstacle in this field is a lack of
dynamic camera intrinsics benchmarks--existing benchmarks typically offer
limited diversity in scene content and intrinsics variation, and none provide
per-frame intrinsic changes for consecutive video frames. In this paper, we
present Intrinsics in Flux (InFlux), a real-world benchmark that provides
per-frame ground truth intrinsics annotations for videos with dynamic
intrinsics. Compared to prior benchmarks, InFlux captures a wider range of
intrinsic variations and scene diversity, featuring 143K+ annotated frames from
386 high-resolution indoor and outdoor videos with dynamic camera intrinsics.
To ensure accurate per-frame intrinsics, we build a comprehensive lookup table
of calibration experiments and extend the Kalibr toolbox to improve its
accuracy and robustness. Using our benchmark, we evaluate existing baseline
methods for predicting camera intrinsics and find that most struggle to achieve
accurate predictions on videos with dynamic intrinsics. For the dataset, code,
videos, and submission, please visit https://influx.cs.princeton.edu/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2025 DB Track, Camera Ready Version.
  Supplementary material included</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FARMER: Flow AutoRegressive <span class="highlight-title">Transformer</span> over Pixels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23588v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23588v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangting Zheng, Qinyu Zhao, Tao Yang, Fei Xiao, Zhijie Lin, Jie Wu, Jiajun Deng, Yanyong Zhang, Rui Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Directly modeling the explicit likelihood of the raw data distribution is key
topic in the machine learning area, which achieves the scaling successes in
Large Language Models by autoregressive modeling. However, continuous AR
modeling over visual pixel data suffer from extremely long sequences and
high-dimensional spaces. In this paper, we present FARMER, a novel end-to-end
generative framework that unifies Normalizing Flows (NF) and Autoregressive
(AR) models for tractable likelihood estimation and high-quality image
synthesis directly from raw pixels. FARMER employs an invertible autoregressive
flow to transform images into latent sequences, whose distribution is modeled
implicitly by an autoregressive model. To address the redundancy and complexity
in pixel-level modeling, we propose a self-supervised dimension reduction
scheme that partitions NF latent channels into informative and redundant
groups, enabling more effective and efficient AR modeling. Furthermore, we
design a one-step distillation scheme to significantly accelerate inference
speed and introduce a resampling-based classifier-free guidance algorithm to
boost image generation quality. Extensive experiments demonstrate that FARMER
achieves competitive performance compared to existing pixel-based generative
models while providing exact likelihoods and scalable training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Bytedance Seed Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Training of INRs via Pruning and Densification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23943v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23943v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Diana Aldana, João Paulo Lima, Daniel Csillag, Daniel Perazzo, Haoan Feng, Luiz Velho, Tiago Novello
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Encoding input coordinates with sinusoidal functions into multilayer
perceptrons (MLPs) has proven effective for implicit neural representations
(INRs) of low-dimensional signals, enabling the modeling of high-frequency
details. However, selecting appropriate input frequencies and architectures
while managing parameter redundancy remains an open challenge, often addressed
through heuristics and heavy hyperparameter optimization schemes. In this
paper, we introduce AIRe ($\textbf{A}$daptive $\textbf{I}$mplicit neural
$\textbf{Re}$presentation), an adaptive training scheme that refines the INR
architecture over the course of optimization. Our method uses a neuron pruning
mechanism to avoid redundancy and input frequency densification to improve
representation capacity, leading to an improved trade-off between network size
and reconstruction quality. For pruning, we first identify less-contributory
neurons and apply a targeted weight decay to transfer their information to the
remaining neurons, followed by structured pruning. Next, the densification
stage adds input frequencies to spectrum regions where the signal underfits,
expanding the representational basis. Through experiments on images and SDFs,
we show that AIRe reduces model size while preserving, or even improving,
reconstruction quality. Code and pretrained models will be released for public
use.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PlanarGS: High-Fidelity Indoor 3D Gaussian Splatting Guided by
  Vision-Language Planar Priors <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23930v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23930v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xirui Jin, Renbiao Jin, Boying Li, Danping Zou, Wenxian Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Three-dimensional Gaussian Splatting (3DGS) has recently emerged as an
efficient representation for novel-view synthesis, achieving impressive visual
quality. However, in scenes dominated by large and low-texture regions, common
in indoor environments, the photometric loss used to optimize 3DGS yields
ambiguous geometry and fails to recover high-fidelity 3D surfaces. To overcome
this limitation, we introduce PlanarGS, a 3DGS-based framework tailored for
indoor scene reconstruction. Specifically, we design a pipeline for
Language-Prompted Planar Priors (LP3) that employs a pretrained vision-language
segmentation model and refines its region proposals via cross-view fusion and
inspection with geometric priors. 3D Gaussians in our framework are optimized
with two additional terms: a planar prior supervision term that enforces planar
consistency, and a geometric prior supervision term that steers the Gaussians
toward the depth and normal cues. We have conducted extensive experiments on
standard indoor benchmarks. The results show that PlanarGS reconstructs
accurate and detailed 3D surfaces, consistently outperforming state-of-the-art
methods by a large margin. Project page: https://planargs.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2025. Project page: https://planargs.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TurboPortrait3D: Single-step diffusion-based fast portrait novel-view
  synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23929v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23929v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emily Kim, Julieta Martinez, Timur Bagautdinov, Jessica Hodgins
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce TurboPortrait3D: a method for low-latency novel-view synthesis
of human portraits. Our approach builds on the observation that existing
image-to-3D models for portrait generation, while capable of producing
renderable 3D representations, are prone to visual artifacts, often lack of
detail, and tend to fail at fully preserving the identity of the subject. On
the other hand, image diffusion models excel at generating high-quality images,
but besides being computationally expensive, are not grounded in 3D and thus
are not directly capable of producing multi-view consistent outputs. In this
work, we demonstrate that image-space diffusion models can be used to
significantly enhance the quality of existing image-to-avatar methods, while
maintaining 3D-awareness and running with low-latency. Our method takes a
single frontal image of a subject as input, and applies a feedforward
image-to-avatar generation pipeline to obtain an initial 3D representation and
corresponding noisy renders. These noisy renders are then fed to a single-step
diffusion model which is conditioned on input image(s), and is specifically
trained to refine the renders in a multi-view consistent way. Moreover, we
introduce a novel effective training strategy that includes pre-training on a
large corpus of synthetic multi-view data, followed by fine-tuning on
high-quality real images. We demonstrate that our approach both qualitatively
and quantitatively outperforms current state-of-the-art for portrait novel-view
synthesis, while being efficient in time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LayerComposer: Interactive Personalized T2I via Spatially-Aware Layered
  Canvas 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.20820v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.20820v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guocheng Gordon Qian, Ruihang Zhang, Tsai-Shien Chen, Yusuf Dalva, Anujraaj Argo Goyal, Willi Menapace, Ivan Skorokhodov, Meng Dong, Arpit Sahni, Daniil Ostashev, Ju Hu, Sergey Tulyakov, Kuan-Chieh Jackson Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite their impressive visual fidelity, existing personalized generative
models lack interactive control over spatial composition and scale poorly to
multiple subjects. To address these limitations, we present LayerComposer, an
interactive framework for personalized, multi-subject text-to-image generation.
Our approach introduces two main contributions: (1) a layered canvas, a novel
representation in which each subject is placed on a distinct layer, enabling
occlusion-free composition; and (2) a locking mechanism that preserves selected
layers with high fidelity while allowing the remaining layers to adapt flexibly
to the surrounding context. Similar to professional image-editing software, the
proposed layered canvas allows users to place, resize, or lock input subjects
through intuitive layer manipulation. Our versatile locking mechanism requires
no architectural changes, relying instead on inherent positional embeddings
combined with a new complementary data sampling strategy. Extensive experiments
demonstrate that LayerComposer achieves superior spatial control and identity
preservation compared to the state-of-the-art methods in multi-subject
personalized image generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, preprint. Project page:
  https://snap-research.github.io/layercomposer/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ReXGroundingCT: A 3D Chest CT <span class="highlight-title">Dataset</span> for Segmentation of Findings from
  Free-Text Reports 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.22030v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.22030v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammed Baharoon, Luyang Luo, Michael Moritz, Abhinav Kumar, Sung Eun Kim, Xiaoman Zhang, Miao Zhu, Mahmoud Hussain Alabbad, Maha Sbayel Alhazmi, Neel P. Mistry, Lucas Bijnens, Kent Ryan Kleinschmidt, Brady Chrisler, Sathvik Suryadevara, Sri Sai Dinesh Jaliparthi, Noah Michael Prudlo, Mark David Marino, Jeremy Palacio, Rithvik Akula, Di Zhou, Hong-Yu Zhou, Ibrahim Ethem Hamamci, Scott J. Adams, Hassan Rayhan AlOmaish, Pranav Rajpurkar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce ReXGroundingCT, the first publicly available dataset linking
free-text findings to pixel-level 3D segmentations in chest CT scans. The
dataset includes 3,142 non-contrast chest CT scans paired with standardized
radiology reports from CT-RATE. Construction followed a structured three-stage
pipeline. First, GPT-4 was used to extract and standardize findings,
descriptors, and metadata from reports originally written in Turkish and
machine-translated into English. Second, GPT-4o-mini categorized each finding
into a hierarchical ontology of lung and pleural abnormalities. Third, 3D
annotations were produced for all CT volumes: the training set was
quality-assured by board-certified radiologists, and the validation and test
sets were fully annotated by board-certified radiologists. Additionally, a
complementary chain-of-thought dataset was created to provide step-by-step
hierarchical anatomical reasoning for localizing findings within the CT volume,
using GPT-4o and localization coordinates derived from organ segmentation
models. ReXGroundingCT contains 16,301 annotated entities across 8,028
text-to-3D-segmentation pairs, covering diverse radiological patterns from
3,142 non-contrast CT scans. About 79% of findings are focal abnormalities and
21% are non-focal. The dataset includes a public validation set of 50 cases and
a private test set of 100 cases, both annotated by board-certified
radiologists. The dataset establishes a foundation for enabling free-text
finding segmentation and grounded radiology report generation in CT imaging.
Model performance on the private test set is hosted on a public leaderboard at
https://rexrank.ai/ReXGroundingCT. The dataset is available at
https://huggingface.co/datasets/rajpurkarlab/ReXGroundingCT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DynCIM: Dynamic Curriculum for Imbalanced Multimodal Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.06456v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.06456v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengxuan Qian, Kai Han, Jiaxin Liu, Zhenlong Yuan, Zhengzhong Zhu, Jingchao Wang, Chongwen Lyu, Jun Chen, Zhe Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal learning integrates complementary information from diverse
modalities to enhance the decision-making process. However, the potential of
multimodal collaboration remains under-exploited due to disparities in data
quality and modality representation capabilities. To address this, we introduce
DynCIM, a novel dynamic curriculum learning framework designed to quantify the
inherent imbalances from both sample and modality perspectives. DynCIM employs
a sample-level curriculum to dynamically assess each sample's difficulty
according to prediction deviation, consistency, and stability, while a
modality-level curriculum measures modality contributions from global and
local. Furthermore, a gating-based dynamic fusion mechanism is introduced to
adaptively adjust modality contributions, minimizing redundancy and optimizing
fusion effectiveness. Extensive experiments on six multimodal benchmarking
datasets, spanning both bimodal and trimodal scenarios, demonstrate that DynCIM
consistently outperforms state-of-the-art methods. Our approach effectively
mitigates modality and sample imbalances while enhancing adaptability and
robustness in multimodal learning tasks. Our code is available at
https://github.com/Raymond-Qiancx/DynCIM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Faces of Fairness: Examining Bias in Facial Expression Recognition
  <span class="highlight-title">Dataset</span>s and Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11049v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11049v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Mehdi Hosseini, Ali Pourramezan Fard, Mohammad H. Mahoor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building AI systems, including Facial Expression Recognition (FER), involves
two critical aspects: data and model design. Both components significantly
influence bias and fairness in FER tasks. Issues related to bias and fairness
in FER datasets and models remain underexplored. This study investigates bias
sources in FER datasets and models. Four common FER datasets--AffectNet, ExpW,
Fer2013, and RAF-DB--are analyzed. The findings demonstrate that AffectNet and
ExpW exhibit high generalizability despite data imbalances. Additionally, this
research evaluates the bias and fairness of six deep models, including three
state-of-the-art convolutional neural network (CNN) models: MobileNet, ResNet,
XceptionNet, as well as three transformer-based models: ViT, CLIP, and
GPT-4o-mini. Experimental results reveal that while GPT-4o-mini and ViT achieve
the highest accuracy scores, they also display the highest levels of bias.
These findings underscore the urgent need for developing new methodologies to
mitigate bias and ensure fairness in datasets and models, particularly in
affective computing applications. See our implementation details at
https://github.com/MMHosseini/bias_in_FER.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">20</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LimRank: Less is More for Reasoning-Intensive Information Reranking <span class="chip">EMNLP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23544v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23544v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tingyu Song, Yilun Zhao, Siyue Zhang, Chen Zhao, Arman Cohan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing approaches typically rely on large-scale fine-tuning to adapt LLMs
for information reranking tasks, which is computationally expensive. In this
work, we demonstrate that modern LLMs can be effectively adapted using only
minimal, high-quality supervision. To enable this, we design
LIMRANK-SYNTHESIZER, a reusable and open-source pipeline for generating
diverse, challenging, and realistic reranking examples. Using this synthetic
data, we fine-tune our reranker model, LIMRANK. We evaluate LIMRANK on two
challenging benchmarks, i.e., BRIGHT for reasoning-intensive retrieval and
FollowIR for instruction-following retrieval. Our experiments demonstrate that
LIMRANK achieves competitive performance, while being trained on less than 5%
of the data typically used in prior work. Further ablation studies demonstrate
the effectiveness of LIMRANK-SYNTHESIZER and the strong generalization
capabilities of LIMRANK across downstream tasks, including scientific
literature search and retrieval-augmented generation for knowledge-intensive
problem solving.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2025 Main (Short)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accurate and Scalable Multimodal Pathology Retrieval via Attentive
  Vision-Language Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23224v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23224v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyi Wang, Zhengjie Zhu, Jiabo Ma, Fang Wang, Yue Shi, Bo Luo, Jili Wang, Qiuyu Cai, Xiuming Zhang, Yen-Wei Chen, Lanfen Lin, Hao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid digitization of histopathology slides has opened up new
possibilities for computational tools in clinical and research workflows. Among
these, content-based slide retrieval stands out, enabling pathologists to
identify morphologically and semantically similar cases, thereby supporting
precise diagnoses, enhancing consistency across observers, and assisting
example-based education. However, effective retrieval of whole slide images
(WSIs) remains challenging due to their gigapixel scale and the difficulty of
capturing subtle semantic differences amid abundant irrelevant content. To
overcome these challenges, we present PathSearch, a retrieval framework that
unifies fine-grained attentive mosaic representations with global-wise slide
embeddings aligned through vision-language contrastive learning. Trained on a
corpus of 6,926 slide-report pairs, PathSearch captures both fine-grained
morphological cues and high-level semantic patterns to enable accurate and
flexible retrieval. The framework supports two key functionalities: (1)
mosaic-based image-to-image retrieval, ensuring accurate and efficient slide
research; and (2) multi-modal retrieval, where text queries can directly
retrieve relevant slides. PathSearch was rigorously evaluated on four public
pathology datasets and three in-house cohorts, covering tasks including
anatomical site retrieval, tumor subtyping, tumor vs. non-tumor discrimination,
and grading across diverse organs such as breast, lung, kidney, liver, and
stomach. External results show that PathSearch outperforms traditional
image-to-image retrieval frameworks. A multi-center reader study further
demonstrates that PathSearch improves diagnostic accuracy, boosts confidence,
and enhances inter-observer agreement among pathologists in real clinical
scenarios. These results establish PathSearch as a scalable and generalizable
retrieval solution for digital pathology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Hierarchical Organization for Medical Multi-document
  Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23104v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23104v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi-Li Hsu, Katelyn X. Mei, Lucy Lu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical multi-document summarization (MDS) is a complex task that requires
effectively managing cross-document relationships. This paper investigates
whether incorporating hierarchical structures in the inputs of MDS can improve
a model's ability to organize and contextualize information across documents
compared to traditional flat summarization methods. We investigate two ways of
incorporating hierarchical organization across three large language models
(LLMs), and conduct comprehensive evaluations of the resulting summaries using
automated metrics, model-based metrics, and domain expert evaluation of
preference, understandability, clarity, complexity, relevance, coverage,
factuality, and coherence. Our results show that human experts prefer
model-generated summaries over human-written summaries. Hierarchical approaches
generally preserve factuality, coverage, and coherence of information, while
also increasing human preference for summaries. Additionally, we examine
whether simulated judgments from GPT-4 align with human judgments, finding
higher agreement along more objective evaluation facets. Our findings
demonstrate that hierarchical structures can improve the clarity of medical
summaries generated by models while maintaining content coverage, providing a
practical way to improve human preference for generated summaries.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Think before Recommendation: Autonomous Reasoning-enhanced Recommender <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23077v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23077v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyu Kong, Junguang Jiang, Bin Liu, Ziru Xu, Han Zhu, Jian Xu, Bo Zheng, Jiancan Wu, Xiang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The core task of recommender systems is to learn user preferences from
historical user-item interactions. With the rapid development of large language
models (LLMs), recent research has explored leveraging the reasoning
capabilities of LLMs to enhance rating prediction tasks. However, existing
distillation-based methods suffer from limitations such as the teacher model's
insufficient recommendation capability, costly and static supervision, and
superficial transfer of reasoning ability. To address these issues, this paper
proposes RecZero, a reinforcement learning (RL)-based recommendation paradigm
that abandons the traditional multi-model and multi-stage distillation
approach. Instead, RecZero trains a single LLM through pure RL to autonomously
develop reasoning capabilities for rating prediction. RecZero consists of two
key components: (1) "Think-before-Recommendation" prompt construction, which
employs a structured reasoning template to guide the model in step-wise
analysis of user interests, item features, and user-item compatibility; and (2)
rule-based reward modeling, which adopts group relative policy optimization
(GRPO) to compute rewards for reasoning trajectories and optimize the LLM.
Additionally, the paper explores a hybrid paradigm, RecOne, which combines
supervised fine-tuning with RL, initializing the model with cold-start
reasoning samples and further optimizing it with RL. Experimental results
demonstrate that RecZero and RecOne significantly outperform existing baseline
methods on multiple benchmark datasets, validating the superiority of the RL
paradigm in achieving autonomous reasoning-enhanced recommender systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025 poster</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Stage Field Extraction of Financial Documents with OCR and Compact
  Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23066v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23066v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichao Jin, Yushuo Wang, Qishuai Zhong, Kent Chiu Jin-Chun, Kenneth Zhu Ke, Donald MacDonald
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Financial documents are essential sources of information for regulators,
auditors, and financial institutions, particularly for assessing the wealth and
compliance of Small and Medium-sized Businesses. However, SMB documents are
often difficult to parse. They are rarely born digital and instead are
distributed as scanned images that are none machine readable. The scans
themselves are low in resolution, affected by skew or rotation, and often
contain noisy backgrounds. These documents also tend to be heterogeneous,
mixing narratives, tables, figures, and multilingual content within the same
report. Such characteristics pose major challenges for automated information
extraction, especially when relying on end to end large Vision Language Models,
which are computationally expensive, sensitive to noise, and slow when applied
to files with hundreds of pages.
  We propose a multistage pipeline that leverages traditional image processing
models and OCR extraction, together with compact VLMs for structured field
extraction of large-scale financial documents. Our approach begins with image
pre-processing, including segmentation, orientation detection, and size
normalization. Multilingual OCR is then applied to recover page-level text.
Upon analyzing the text information, pages are retrieved for coherent sections.
Finally, compact VLMs are operated within these narrowed-down scopes to extract
structured financial indicators.
  Our approach is evaluated using an internal corpus of multi-lingual, scanned
financial documents. The results demonstrate that compact VLMs, together with a
multistage pipeline, achieves 8.8 times higher field level accuracy relative to
directly feeding the whole document into large VLMs, only at 0.7 percent of the
GPU cost and 92.6 percent less end-to-end service latency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Product Search Relevance with EAR-MP: A Solution for the CIKM
  2025 AnalytiCup 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23018v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23018v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        JaeEun Lim, Soomin Kim, Jaeyong Seo, Iori Ono, Qimu Ran, Jae-woong Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multilingual e-commerce search is challenging due to linguistic diversity and
the noise inherent in user-generated queries. This paper documents the solution
employed by our team (EAR-MP) for the CIKM 2025 AnalytiCup, which addresses two
core tasks: Query-Category (QC) relevance and Query-Item (QI) relevance. Our
approach first normalizes the multilingual dataset by translating all text into
English, then mitigates noise through extensive data cleaning and
normalization. For model training, we build on DeBERTa-v3-large and improve
performance with label smoothing, self-distillation, and dropout. In addition,
we introduce task-specific upgrades, including hierarchical token injection for
QC and a hybrid scoring mechanism for QI. Under constrained compute, our method
achieves competitive results, attaining an F1 score of 0.8796 on QC and 0.8744
on QI. These findings underscore the importance of systematic data
preprocessing and tailored training strategies for building robust,
resource-efficient multilingual relevance systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tagging-Augmented Generation: Assisting Language Models in Finding
  Intricate Knowledge In Long Contexts <span class="chip">EMNLP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.22956v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.22956v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anwesan Pal, Karen Hovsepian, Tinghao Guo, Mengnan Zhao, Somendra Tripathi, Nikos Kanakaris, George Mihaila, Sumit Nigam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent investigations into effective context lengths of modern flagship large
language models (LLMs) have revealed major limitations in effective question
answering (QA) and reasoning over long and complex contexts for even the
largest and most impressive cadre of models. While approaches like
retrieval-augmented generation (RAG) and chunk-based re-ranking attempt to
mitigate this issue, they are sensitive to chunking, embedding and retrieval
strategies and models, and furthermore, rely on extensive pre-processing,
knowledge acquisition and indexing steps. In this paper, we propose
Tagging-Augmented Generation (TAG), a lightweight data augmentation strategy
that boosts LLM performance in long-context scenarios, without degrading and
altering the integrity and composition of retrieved documents. We validate our
hypothesis by augmenting two challenging and directly relevant
question-answering benchmarks -- NoLima and NovelQA -- and show that tagging
the context or even just adding tag definitions into QA prompts leads to
consistent performance gains over the baseline -- up to 17% for 32K token
contexts, and 2.9% in complex reasoning question-answering for multi-hop
queries requiring knowledge across a wide span of text. Additional details are
available at https://sites.google.com/view/tag-emnlp.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted at EMNLP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GTR-Mamba: Geometry-to-Tangent Routing for Hyperbolic POI Recommendation <span class="chip">ICDE 2026</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.22942v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.22942v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoxuan Li, Jieyuan Pei, Tangwei Ye, Zhongyuan Lai, Zihan Liu, Fengyuan Xu, Qi Zhang, Liang Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Next Point-of-Interest (POI) recommendation is a critical task in modern
Location-Based Social Networks (LBSNs), aiming to model the complex
decision-making process of human mobility to provide personalized
recommendations for a user's next check-in location. Existing POI
recommendation models, predominantly based on Graph Neural Networks and
sequential models, have been extensively studied. However, these models face a
fundamental limitation: they struggle to simultaneously capture the inherent
hierarchical structure of spatial choices and the dynamics and irregular shifts
of user-specific temporal contexts. To overcome this limitation, we propose
GTR-Mamba, a novel framework for cross-manifold conditioning and routing.
GTR-Mamba leverages the distinct advantages of different mathematical spaces
for different tasks: it models the static, tree-like preference hierarchies in
hyperbolic geometry, while routing the dynamic sequence updates to a novel
Mamba layer in the computationally stable and efficient Euclidean tangent
space. This process is coordinated by a cross-manifold channel that fuses
spatio-temporal information to explicitly steer the State Space Model (SSM),
enabling flexible adaptation to contextual changes. Extensive experiments on
three real-world datasets demonstrate that GTR-Mamba consistently outperforms
state-of-the-art baseline models in next POI recommendation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 8 figures, 4 tables, submitted to ICDE 2026</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MGFRec: Towards Reinforced Reasoning Recommendation with Multiple
  Groundings and Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.22888v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.22888v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shihao Cai, Chongming Gao, Haoyan Liu, Wentao Shi, Jianshan Sun, Ruiming Tang, Fuli Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The powerful reasoning and generative capabilities of large language models
(LLMs) have inspired researchers to apply them to reasoning-based
recommendation tasks, which require in-depth reasoning about user interests and
the generation of recommended items. However, previous reasoning-based
recommendation methods have typically performed inference within the language
space alone, without incorporating the actual item space. This has led to
over-interpreting user interests and deviating from real items. Towards this
research gap, we propose performing multiple rounds of grounding during
inference to help the LLM better understand the actual item space, which could
ensure that its reasoning remains aligned with real items. Furthermore, we
introduce a user agent that provides feedback during each grounding step,
enabling the LLM to better recognize and adapt to user interests. Comprehensive
experiments conducted on three Amazon review datasets demonstrate the
effectiveness of incorporating multiple groundings and feedback. These findings
underscore the critical importance of reasoning within the actual item space,
rather than being confined to the language space, for recommendation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Computational-Assisted Systematic <span class="highlight-title">Review</span> and Meta-Analysis (CASMA):
  Effect of a Subclass of GnRH-a on Endometriosis Recurrence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.16599v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.16599v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sandro Tsang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Background: Evidence synthesis facilitates evidence-based medicine. This task
becomes increasingly difficult to accomplished with applying computational
solutions, since the medical literature grows at astonishing rates. Objective:
This study evaluates an information retrieval-driven workflow, CASMA, to
enhance the efficiency, transparency, and reproducibility of systematic
reviews. Endometriosis recurrence serves as the ideal case due to its complex
and ambiguous literature. Methods: The hybrid approach integrates PRISMA
guidelines with fuzzy matching and regular expression (regex) to facilitate
semi-automated deduplication and filtered records before manual screening. The
workflow synthesised evidence from randomised controlled trials on the efficacy
of a subclass of gonadotropin-releasing hormone agonists (GnRH-a). A modified
splitting method addressed unit-of-analysis errors in multi-arm trials.
Results: The workflow sharply reduced the screening workload, taking only 11
days to fetch and filter 33,444 records. Seven eligible RCTs were synthesized
(841 patients). The pooled random-effects model yielded a Risk Ratio (RR) of
$0.64$ ($95\%$ CI $0.48$ to $0.86$), demonstrating a $36\%$ reduction in
recurrence, with non-significant heterogeneity ($I^2=0.00\%$, $\tau^2=0.00$).
The findings were robust and stable, as they were backed by sensitivity
analyses. Conclusion: This study demonstrates an application of an
information-retrieval-driven workflow for medical evidence synthesis. The
approach yields valuable clinical results and a generalisable framework to
scale up the evidence synthesis, bridging the gap between clinical research and
computer science.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 12 figures and 4 tables. This work describes an information
  retrieval-driven workflow for medical evidence synthesis, with an application
  to endometriosis recurrence. The method can be generalized to other
  systematic reviews. The preregistered protocol is available:
  https://doi.org/10.17605/OSF.IO/R2DFA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Ecosystem for Ontology Interoperability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.12311v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.12311v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhangcheng Qiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ontology interoperability is one of the complicated issues that restricts the
use of ontologies in knowledge graphs (KGs). Different ontologies with
conflicting and overlapping concepts make it difficult to design, develop, and
deploy an interoperable ontology for downstream tasks. We propose an ecosystem
for ontology interoperability. The ecosystem employs three state-of-the-art
semantic techniques in different phases of the ontology engineering life cycle:
ontology design patterns (ODPs) in the design phase, ontology matching and
versioning (OM\&OV) in the develop phase, and ontology-compliant knowledge
graphs (OCKGs) in the deploy phase, to achieve better ontology interoperability
and data integration in real-world applications. A case study of sensor
observation in the building domain validates the usefulness of the proposed
ecosystem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Atlas of In-Context Learning: How Attention Heads Shape In-Context
  Retrieval Augmentation <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.15807v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.15807v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrick Kahardipraja, Reduan Achtibat, Thomas Wiegand, Wojciech Samek, Sebastian Lapuschkin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models are able to exploit in-context learning to access
external knowledge beyond their training data through retrieval-augmentation.
While promising, its inner workings remain unclear. In this work, we shed light
on the mechanism of in-context retrieval augmentation for question answering by
viewing a prompt as a composition of informational components. We propose an
attribution-based method to identify specialized attention heads, revealing
in-context heads that comprehend instructions and retrieve relevant contextual
information, and parametric heads that store entities' relational knowledge. To
better understand their roles, we extract function vectors and modify their
attention weights to show how they can influence the answer generation process.
Finally, we leverage the gained insights to trace the sources of knowledge used
during inference, paving the way towards more safe and transparent language
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TaoSR1: The Thinking Model for E-commerce Relevance Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.12365v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.12365v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenhe Dong, Shaowei Yao, Pengkun Jiao, Jianhui Yang, Yiming Jin, Zerui Huang, Xiaojiang Zhou, Dan Ou, Haihong Tang, Bo Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Query-product relevance prediction is a core task in e-commerce search.
BERT-based models excel at semantic matching but lack complex reasoning
capabilities. While Large Language Models (LLMs) are explored, most still use
discriminative fine-tuning or distill to smaller models for deployment. We
propose a framework to directly deploy LLMs for this task, addressing key
challenges: Chain-of-Thought (CoT) error accumulation, discriminative
hallucination, and deployment feasibility. Our framework, TaoSR1, involves
three stages: (1) Supervised Fine-Tuning (SFT) with CoT to instill reasoning;
(2) Offline sampling with a pass@N strategy and Direct Preference Optimization
(DPO) to improve generation quality; and (3) Difficulty-based dynamic sampling
with Group Relative Policy Optimization (GRPO) to mitigate discriminative
hallucination. Additionally, post-CoT processing and a cumulative
probability-based partitioning method enable efficient online deployment.
TaoSR1 significantly outperforms baselines on offline datasets and achieves
substantial gains in online side-by-side human evaluations, introducing a novel
paradigm for applying CoT reasoning to relevance classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SBAN: A Framework & Multi-Dimensional <span class="highlight-title">Dataset</span> for Large Language Model
  <span class="highlight-title">Pre-Train</span>ing and Software Code Mining 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.18936v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.18936v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamed Jelodar, Mohammad Meymani, Samita Bai, Roozbeh Razavi-Far, Ali A. Ghorbani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces SBAN (Source code, Binary, Assembly, and Natural
Language Description), a large-scale, multi-dimensional dataset designed to
advance the pre-training and evaluation of large language models (LLMs) for
software code analysis. SBAN comprises more than 3 million samples, including
2.9 million benign and 672,000 malware respectively, each represented across
four complementary layers: binary code, assembly instructions, natural language
descriptions, and source code. This unique multimodal structure enables
research on cross-representation learning, semantic understanding of software,
and automated malware detection. Beyond security applications, SBAN supports
broader tasks such as code translation, code explanation, and other software
mining tasks involving heterogeneous data. It is particularly suited for
scalable training of deep models, including transformers and other LLM
architectures. By bridging low-level machine representations and high-level
human semantics, SBAN provides a robust foundation for building intelligent
systems that reason about code. We believe that this dataset opens new
opportunities for mining software behavior, improving security analytics, and
enhancing LLM capabilities in pre-training and fine-tuning tasks for software
code mining.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding Embedding Scaling in Collaborative Filtering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.15709v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.15709v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yicheng He, Zhou Kaiyu, Haoyue Bai, Fengbin Zhu, Yonghui Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling recommendation models into large recommendation models has become one
of the most widely discussed topics. Recent efforts focus on components beyond
the scaling embedding dimension, as it is believed that scaling embedding may
lead to performance degradation. Although there have been some initial
observations on embedding, the root cause of their non-scalability remains
unclear. Moreover, whether performance degradation occurs across different
types of models and datasets is still an unexplored area. Regarding the effect
of embedding dimensions on performance, we conduct large-scale experiments
across 10 datasets with varying sparsity levels and scales, using 4
representative classical architectures. We surprisingly observe two novel
phenomena: double-peak and logarithmic. For the former, as the embedding
dimension increases, performance first improves, then declines, rises again,
and eventually drops. For the latter, it exhibits a perfect logarithmic curve.
Our contributions are threefold. First, we discover two novel phenomena when
scaling collaborative filtering models. Second, we gain an understanding of the
underlying causes of the double-peak phenomenon. Lastly, we theoretically
analyze the noise robustness of collaborative filtering models, with results
matching empirical observations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Cross-Lingual Cost: Retrieval Biases in RAG over Arabic-English
  Corpora 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07543v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07543v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Amiraz, Yaroslav Fyodorov, Elad Haramaty, Zohar Karnin, Liane Lewin-Eytan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-lingual retrieval-augmented generation (RAG) is a critical capability
for retrieving and generating answers across languages. Prior work in this
context has mostly focused on generation and relied on benchmarks derived from
open-domain sources, most notably Wikipedia. In such settings, retrieval
challenges often remain hidden due to language imbalances, overlap with
pretraining data, and memorized content. To address this gap, we study
Arabic-English RAG in a domain-specific setting using benchmarks derived from
real-world corporate datasets. Our benchmarks include all combinations of
languages for the user query and the supporting document, drawn independently
and uniformly at random. This enables a systematic study of multilingual
retrieval behavior.
  Our findings reveal that retrieval is a critical bottleneck in cross-lingual
domain-specific scenarios, with substantial performance drops occurring when
the user query and supporting document languages differ. A key insight is that
these failures stem primarily from the retriever's difficulty in ranking
documents across languages. Finally, we propose two simple retrieval strategies
that address this source of failure by enforcing equal retrieval from both
languages or by translating the query, resulting in substantial improvements in
cross-lingual and overall performance. These results highlight meaningful
opportunities for improving multilingual retrieval, particularly in practical,
real-world RAG applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ArabicNLP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CMIE: Combining MLLM Insights with External Evidence for Explainable
  Out-of-Context Misinformation Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.23449v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.23449v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fanxiao Li, Jiaying Wu, Canyuan He, Wei Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) have demonstrated impressive
capabilities in visual reasoning and text generation. While previous studies
have explored the application of MLLM for detecting out-of-context (OOC)
misinformation, our empirical analysis reveals two persisting challenges of
this paradigm. Evaluating the representative GPT-4o model on direct reasoning
and evidence augmented reasoning, results indicate that MLLM struggle to
capture the deeper relationships-specifically, cases in which the image and
text are not directly connected but are associated through underlying semantic
links. Moreover, noise in the evidence further impairs detection accuracy. To
address these challenges, we propose CMIE, a novel OOC misinformation detection
framework that incorporates a Coexistence Relationship Generation (CRG)
strategy and an Association Scoring (AS) mechanism. CMIE identifies the
underlying coexistence relationships between images and text, and selectively
utilizes relevant evidence to enhance misinformation detection. Experimental
results demonstrate that our approach outperforms existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Membership Inference Attacks on Recommender System: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.11080v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.11080v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiajie He, Xintong Chen, Xinyang Fang, Min-Chun Chen, Yuechun Gu, Keke Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems (RecSys) have been widely applied to various
applications, including E-commerce, finance, healthcare, social media and have
become increasingly influential in shaping user behavior and decision-making,
highlighting their growing impact in various domains. However, recent studies
have shown that RecSys are vulnerable to membership inference attacks (MIAs),
which aim to infer whether user interaction record was used to train a target
model or not. MIAs on RecSys models can directly lead to a privacy breach. For
example, via identifying the fact that a purchase record that has been used to
train a RecSys associated with a specific user, an attacker can infer that
user's special quirks. In recent years, MIAs have been shown to be effective on
other ML tasks, e.g., classification models and natural language processing.
However, traditional MIAs are ill-suited for RecSys due to the unseen posterior
probability. Although MIAs on RecSys form a newly emerging and rapidly growing
research area, there has been no systematic survey on this topic yet. In this
article, we conduct the first comprehensive survey on RecSys MIAs. This survey
offers a comprehensive review of the latest advancements in RecSys MIAs,
exploring the design principles, challenges, attack and defense associated with
this emerging field. We provide a unified taxonomy that categorizes different
RecSys MIAs based on their characterizations and discuss their pros and cons.
Based on the limitations and gaps identified in this survey, we point out
several promising future research directions to inspire the researchers who
wish to follow this area. This survey not only serves as a reference for the
research community but also provides a clear description for researchers
outside this research domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From ID-based to ID-free: Rethinking ID Effectiveness in Multimodal
  Collaborative Filtering Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.05715v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.05715v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guohao Li, Li Jing, Jia Wu, Xuefei Li, Kai Zhu, Yue He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most existing multimodal collaborative filtering recommendation (MCFRec)
methods rely heavily on ID features and multimodal content to enhance
recommendation performance. However, this paper reveals that ID features are
effective but have limited benefits in multimodal collaborative filtering
recommendation. Therefore, this paper systematically deconstruct the pros and
cons of ID features: (i) they provide initial embedding but lack semantic
richness, (ii) they provide a unique identifier for each user and item but
hinder generalization to untrained data, and (iii) they assist in aligning and
fusing multimodal features but may lead to representation shift. Based on these
insights, this paper proposes IDFREE, an ID-free multimodal collaborative
Filtering REcommEndation baseline. IDFREE replaces ID features with multimodal
features and positional encodings to generate semantically meaningful ID-free
embeddings. For ID-free multimodal collaborative filtering, it further proposes
an adaptive similarity graph module to construct dynamic user-user and
item-item graphs based on multimodal features. Then, an augmented user-item
graph encoder is proposed to construct more effective user and item encoding.
Finally, IDFREE achieves inter-multimodal alignment based on the contrastive
learning and uses Softmax loss as recommendation loss. Basic experiments on
three public datasets demonstrate that IDFREE outperforms existing ID-based
MCFRec methods, achieving an average performance gain of 72.24% across standard
metrics (Recall@5, 10, 20, 50 and NDCG@5, 10, 20, 50). Exploratory and extended
experiments further validate our findings on the limitations of ID features in
MCFRec. The code is released at https://github.com/G-H-Li/IDFREE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We identified that our current approach achieves its reported
  performance only under specific data conditions, and its robustness is weaker
  than we initially expected</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LIME: Link-based user-item Interaction Modeling with decoupled xor
  attention for Efficient test time scaling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.18239v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.18239v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunjiang Jiang, Ayush Agarwal, Yang Liu, Bi Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling large recommendation systems requires advancing three major
frontiers: processing longer user histories, expanding candidate sets, and
increasing model capacity. While promising, transformers' computational cost
scales quadratically with the user sequence length and linearly with the number
of candidates. This trade-off makes it prohibitively expensive to expand
candidate sets or increase sequence length at inference, despite the
significant performance improvements.
  We introduce \textbf{LIME}, a novel architecture that resolves this
trade-off. Through two key innovations, LIME fundamentally reduces
computational complexity. First, low-rank ``link embeddings" enable
pre-computation of attention weights by decoupling user and candidate
interactions, making the inference cost nearly independent of candidate set
size. Second, a linear attention mechanism, \textbf{LIME-XOR}, reduces the
complexity with respect to user sequence length from quadratic ($O(N^2)$) to
linear ($O(N)$).
  Experiments on public and industrial datasets show LIME achieves near-parity
with state-of-the-art transformers but with a 10$\times$ inference speedup on
large candidate sets or long sequence lengths. When tested on a major
recommendation platform, LIME improved user engagement while maintaining
minimal inference costs with respect to candidate set size and user history
length, establishing a new paradigm for efficient and expressive recommendation
systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">25</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Variational Masked Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23606v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23606v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichi Zhang, Alex Schwing, Zhizhen Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Masked diffusion models have recently emerged as a flexible framework for
discrete generative modeling. However, a key limitation of standard masked
diffusion is its inability to effectively capture dependencies among tokens
that are predicted concurrently, leading to degraded generation quality when
dependencies among tokens are important. To explicitly model dependencies among
tokens, we propose Variational Masked Diffusion (VMD), a framework that
introduces latent variables into the masked diffusion process. Through
controlled experiments on synthetic datasets, we demonstrate that VMD
successfully learns dependencies that conventional masked diffusion fails to
capture. We further validate the effectiveness of our approach on Sudoku
puzzles and text datasets, where learning of dependencies among tokens improves
global consistency. Across these domains, VMD enhances both generation quality
and dependency awareness, highlighting the value of integrating variational
inference into masked diffusion. Our code is available at:
https://riccizz.github.io/VMD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://riccizz.github.io/VMD</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Track, Inpaint, Resplat: Subject-driven 3D and 4D Generation with
  Progressive Texture Infilling <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23605v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23605v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuhong Zheng, Ashkan Mirzaei, Igor Gilitschenski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current 3D/4D generation methods are usually optimized for photorealism,
efficiency, and aesthetics. However, they often fail to preserve the semantic
identity of the subject across different viewpoints. Adapting generation
methods with one or few images of a specific subject (also known as
Personalization or Subject-driven generation) allows generating visual content
that align with the identity of the subject. However, personalized 3D/4D
generation is still largely underexplored. In this work, we introduce TIRE
(Track, Inpaint, REsplat), a novel method for subject-driven 3D/4D generation.
It takes an initial 3D asset produced by an existing 3D generative model as
input and uses video tracking to identify the regions that need to be modified.
Then, we adopt a subject-driven 2D inpainting model for progressively infilling
the identified regions. Finally, we resplat the modified 2D multi-view
observations back to 3D while still maintaining consistency. Extensive
experiments demonstrate that our approach significantly improves identity
preservation in 3D/4D generation compared to state-of-the-art methods. Our
project website is available at
https://zsh2000.github.io/track-inpaint-resplat.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025, 38 pages, 22 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lightweight Robust Direct Preference Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23590v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23590v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheol Woo Kim, Shresth Verma, Mauricio Tec, Milind Tambe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Direct Preference Optimization (DPO) has become a popular method for
fine-tuning large language models (LLMs) due to its stability and simplicity.
However, it is also known to be sensitive to noise in the data and prone to
overfitting. Recent works have proposed using distributionally robust
optimization (DRO) to address potential noise and distributional shift in the
data. However, these methods often suffer from excessive conservatism and high
computational cost. We propose DPO-PRO (DPO with Preference Robustness), a
robust fine-tuning algorithm based on DPO which accounts for uncertainty in the
preference distribution through a lightweight DRO formulation. Unlike prior
DRO-based variants, DPO-PRO focuses solely on uncertainty in preferences,
avoiding unnecessary conservatism and incurring negligible computational
overhead. We further show that DPO-PRO is equivalent to a regularized DPO
objective that penalizes model overconfidence under weak preference signals. We
evaluate DPO-PRO on standard alignment benchmarks and a real-world public
health task. Experimental results show that our method consistently improves
robustness to noisy preference signals compared to existing DPO variants.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lookahead Anchoring: Preserving Character Identity in Audio-Driven Human
  Animation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23581v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23581v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyoung Seo, Rodrigo Mira, Alexandros Haliassos, Stella Bounareli, Honglie Chen, Linh Tran, Seungryong Kim, Zoe Landgraf, Jie Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-driven human animation models often suffer from identity drift during
temporal autoregressive generation, where characters gradually lose their
identity over time. One solution is to generate keyframes as intermediate
temporal anchors that prevent degradation, but this requires an additional
keyframe generation stage and can restrict natural motion dynamics. To address
this, we propose Lookahead Anchoring, which leverages keyframes from future
timesteps ahead of the current generation window, rather than within it. This
transforms keyframes from fixed boundaries into directional beacons: the model
continuously pursues these future anchors while responding to immediate audio
cues, maintaining consistent identity through persistent guidance. This also
enables self-keyframing, where the reference image serves as the lookahead
target, eliminating the need for keyframe generation entirely. We find that the
temporal lookahead distance naturally controls the balance between expressivity
and consistency: larger distances allow for greater motion freedom, while
smaller ones strengthen identity adherence. When applied to three recent human
animation models, Lookahead Anchoring achieves superior lip synchronization,
identity preservation, and visual quality, demonstrating improved temporal
conditioning across several different architectures. Video results are
available at the following link: https://lookahead-anchoring.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://lookahead-anchoring.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RobotArena $\infty$: Scalable Robot Benchmarking via Real-to-Sim
  Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23571v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23571v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yash Jangir, Yidi Zhang, Kashu Yamazaki, Chenyu Zhang, Kuan-Hsun Tu, Tsung-Wei Ke, Lei Ke, Yonatan Bisk, Katerina Fragkiadaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The pursuit of robot generalists - instructable agents capable of performing
diverse tasks across diverse environments - demands rigorous and scalable
evaluation. Yet real-world testing of robot policies remains fundamentally
constrained: it is labor-intensive, slow, unsafe at scale, and difficult to
reproduce. Existing simulation benchmarks are similarly limited, as they train
and test policies within the same synthetic domains and cannot assess models
trained from real-world demonstrations or alternative simulation environments.
As policies expand in scope and complexity, these barriers only intensify,
since defining "success" in robotics often hinges on nuanced human judgments of
execution quality. In this paper, we introduce a new benchmarking framework
that overcomes these challenges by shifting VLA evaluation into large-scale
simulated environments augmented with online human feedback. Leveraging
advances in vision-language models, 2D-to-3D generative modeling, and
differentiable rendering, our approach automatically converts video
demonstrations from widely used robot datasets into simulated counterparts.
Within these digital twins, we assess VLA policies using both automated
VLM-guided scoring and scalable human preference judgments collected from
crowdworkers, transforming human involvement from tedious scene setup,
resetting, and safety supervision into lightweight preference comparisons. To
measure robustness, we systematically perturb simulated environments along
multiple axes, such as textures and object placements, stress-testing policy
generalization under controlled variation. The result is a continuously
evolving, reproducible, and scalable benchmark for real-world trained robot
manipulation policies, addressing a critical missing capability in today's
robotics landscape.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Website: https://robotarenainf.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReCode: Unify Plan and Action for Universal Granularity Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23564v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23564v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoyang Yu, Jiayi Zhang, Huixue Su, Yufan Zhao, Yifan Wu, Mingyi Deng, Jinyu Xiang, Yizhang Lin, Lingxiao Tang, Yingchao Li, Yuyu Luo, Bang Liu, Chenglin Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world tasks require decisions at varying granularities, and humans excel
at this by leveraging a unified cognitive representation where planning is
fundamentally understood as a high-level form of action. However, current Large
Language Model (LLM)-based agents lack this crucial capability to operate
fluidly across decision granularities. This limitation stems from existing
paradigms that enforce a rigid separation between high-level planning and
low-level action, which impairs dynamic adaptability and limits generalization.
We propose ReCode (Recursive Code Generation), a novel paradigm that addresses
this limitation by unifying planning and action within a single code
representation. In this representation, ReCode treats high-level plans as
abstract placeholder functions, which the agent then recursively decomposes
into finer-grained sub-functions until reaching primitive actions. This
recursive approach dissolves the rigid boundary between plan and action,
enabling the agent to dynamically control its decision granularity.
Furthermore, the recursive structure inherently generates rich,
multi-granularity training data, enabling models to learn hierarchical
decision-making processes. Extensive experiments show ReCode significantly
surpasses advanced baselines in inference performance and demonstrates
exceptional data efficiency in training, validating our core insight that
unifying planning and action through recursive code generation is a powerful
and effective approach to achieving universal granularity control. The code is
available at https://github.com/FoundationAgents/ReCode.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Minimizing Human Intervention in Online Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23557v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23557v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Réveillard, Vasileios Saketos, Alexandre Proutiere, Richard Combes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce and study an online problem arising in question answering
systems. In this problem, an agent must sequentially classify user-submitted
queries represented by $d$-dimensional embeddings drawn i.i.d. from an unknown
distribution. The agent may consult a costly human expert for the correct
label, or guess on her own without receiving feedback. The goal is to minimize
regret against an oracle with free expert access. When the time horizon $T$ is
at least exponential in the embedding dimension $d$, one can learn the geometry
of the class regions: in this regime, we propose the Conservative Hull-based
Classifier (CHC), which maintains convex hulls of expert-labeled queries and
calls the expert as soon as a query lands outside all known hulls. CHC attains
$\mathcal{O}(\log^d T)$ regret in $T$ and is minimax optimal for $d=1$.
Otherwise, the geometry cannot be reliably learned without additional
distributional assumptions. We show that when the queries are drawn from a
subgaussian mixture, for $T \le e^d$, a Center-based Classifier (CC) achieves
regret proportional to $N\log{N}$ where $N$ is the number of labels. To bridge
these regimes, we introduce the Generalized Hull-based Classifier (GHC), a
practical extension of CHC that allows for more aggressive guessing via a
tunable threshold parameter. Our approach is validated with experiments,
notably on real-world question-answering datasets using embeddings derived from
state-of-the-art large language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>49 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A U-Net and <span class="highlight-title">Transformer</span> Pipeline for Multilingual Image Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23554v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23554v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddharth Sahay, Radhika Agarwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an end-to-end multilingual translation pipeline that
integrates a custom U-Net for text detection, the Tesseract engine for text
recognition, and a from-scratch sequence-to-sequence (Seq2Seq) Transformer for
Neural Machine Translation (NMT). Our approach first utilizes a U-Net model,
trained on a synthetic dataset , to accurately segment and detect text regions
from an image. These detected regions are then processed by Tesseract to
extract the source text. This extracted text is fed into a custom Transformer
model trained from scratch on a multilingual parallel corpus spanning 5
languages. Unlike systems reliant on monolithic pre-trained models, our
architecture emphasizes full customization and adaptability. The system is
evaluated on its text detection accuracy, text recognition quality, and
translation performance via BLEU scores. The complete pipeline demonstrates
promising results, validating the viability of a custom-built system for
translating text directly from images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 3 figures, 5 tables, and 2 algorithms. Prepared in IEEE
  double-column format</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sequential Multi-Agent Dynamic Algorithm Configuration <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23535v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23535v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Lu, Ke Xue, Lei Yuan, Yao Wang, Yaoyuan Wang, Sheng Fu, Chao Qian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamic algorithm configuration (DAC) is a recent trend in automated machine
learning, which can dynamically adjust the algorithm's configuration during the
execution process and relieve users from tedious trial-and-error tuning tasks.
Recently, multi-agent reinforcement learning (MARL) approaches have improved
the configuration of multiple heterogeneous hyperparameters, making various
parameter configurations for complex algorithms possible. However, many complex
algorithms have inherent inter-dependencies among multiple parameters (e.g.,
determining the operator type first and then the operator's parameter), which
are, however, not considered in previous approaches, thus leading to
sub-optimal results. In this paper, we propose the sequential multi-agent DAC
(Seq-MADAC) framework to address this issue by considering the inherent
inter-dependencies of multiple parameters. Specifically, we propose a
sequential advantage decomposition network, which can leverage action-order
information through sequential advantage decomposition. Experiments from
synthetic functions to the configuration of multi-objective optimization
algorithms demonstrate Seq-MADAC's superior performance over state-of-the-art
MARL methods and show strong generalization across problem classes. Seq-MADAC
establishes a new paradigm for the widespread dependency-aware automated
algorithm configuration. Our code is available at
https://github.com/lamda-bbo/seq-madac.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Direct Debiased Machine Learning via Bregman Divergence Minimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23534v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23534v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masahiro Kato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop a direct debiased machine learning framework comprising Neyman
targeted estimation and generalized Riesz regression. Our framework unifies
Riesz regression for automatic debiased machine learning, covariate balancing,
targeted maximum likelihood estimation (TMLE), and density-ratio estimation. In
many problems involving causal effects or structural models, the parameters of
interest depend on regression functions. Plugging regression functions
estimated by machine learning methods into the identifying equations can yield
poor performance because of first-stage bias. To reduce such bias, debiased
machine learning employs Neyman orthogonal estimating equations. Debiased
machine learning typically requires estimation of the Riesz representer and the
regression function. For this problem, we develop a direct debiased machine
learning framework with an end-to-end algorithm. We formulate estimation of the
nuisance parameters, the regression function and the Riesz representer, as
minimizing the discrepancy between Neyman orthogonal scores computed with known
and unknown nuisance parameters, which we refer to as Neyman targeted
estimation. Neyman targeted estimation includes Riesz representer estimation,
and we measure discrepancies using the Bregman divergence. The Bregman
divergence encompasses various loss functions as special cases, where the
squared loss yields Riesz regression and the Kullback-Leibler divergence yields
entropy balancing. We refer to this Riesz representer estimation as generalized
Riesz regression. Neyman targeted estimation also yields TMLE as a special case
for regression function estimation. Furthermore, for specific pairs of models
and Riesz representer estimation methods, we can automatically obtain the
covariate balancing property without explicitly solving the covariate balancing
objective.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ When No Paths Lead to Rome: Benchmarking Systematic Neural Relational
  Reasoning <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23532v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23532v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anirban Das, Irtaza Khalid, Rafael Peñaloza, Steven Schockaert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Designing models that can learn to reason in a systematic way is an important
and long-standing challenge. In recent years, a wide range of solutions have
been proposed for the specific case of systematic relational reasoning,
including Neuro-Symbolic approaches, variants of the Transformer architecture,
and specialised Graph Neural Networks. However, existing benchmarks for
systematic relational reasoning focus on an overly simplified setting, based on
the assumption that reasoning can be reduced to composing relational paths. In
fact, this assumption is hard-baked into the architecture of several recent
models, leading to approaches that can perform well on existing benchmarks but
are difficult to generalise to other settings. To support further progress in
the field of systematic relational reasoning with neural networks, we introduce
NoRA, a new benchmark which adds several levels of difficulty and requires
models to go beyond path-based reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at NeurIPS 2025 D&B track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Linearity in Audio Consistency Autoencoders via Implicit
  Regularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23530v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23530v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bernardo Torres, Manuel Moussallam, Gabriel Meseguer-Brocal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio autoencoders learn useful, compressed audio representations, but their
non-linear latent spaces prevent intuitive algebraic manipulation such as
mixing or scaling. We introduce a simple training methodology to induce
linearity in a high-compression Consistency Autoencoder (CAE) by using data
augmentation, thereby inducing homogeneity (equivariance to scalar gain) and
additivity (the decoder preserves addition) without altering the model's
architecture or loss function. When trained with our method, the CAE exhibits
linear behavior in both the encoder and decoder while preserving reconstruction
fidelity. We test the practical utility of our learned space on music source
composition and separation via simple latent arithmetic. This work presents a
straightforward technique for constructing structured latent spaces, enabling
more intuitive and efficient audio processing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Toward Carbon-Neutral Human AI: Rethinking Data, Computation, and
  Learning Paradigms for Sustainable Intelligence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23524v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23524v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        KC Santosh, Rodrigue Rizk, Longwei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of Artificial Intelligence (AI) has led to
unprecedented computational demands, raising significant environmental and
ethical concerns. This paper critiques the prevailing reliance on large-scale,
static datasets and monolithic training paradigms, advocating for a shift
toward human-inspired, sustainable AI solutions. We introduce a novel
framework, Human AI (HAI), which emphasizes incremental learning, carbon-aware
optimization, and human-in-the-loop collaboration to enhance adaptability,
efficiency, and accountability. By drawing parallels with biological cognition
and leveraging dynamic architectures, HAI seeks to balance performance with
ecological responsibility. We detail the theoretical foundations, system
design, and operational principles that enable AI to learn continuously and
contextually while minimizing carbon footprints and human annotation costs. Our
approach addresses pressing challenges in active learning, continual
adaptation, and energy-efficient model deployment, offering a pathway toward
responsible, human-centered artificial intelligence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Deep Latent Factor Graph Clustering with Fairness-Utility Trade-off
  Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23507v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23507v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siamak Ghodsi, Amjad Seyedi, Tai Le Quy, Fariba Karimi, Eirini Ntoutsi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fair graph clustering seeks partitions that respect network structure while
maintaining proportional representation across sensitive groups, with
applications spanning community detection, team formation, resource allocation,
and social network analysis. Many existing approaches enforce rigid constraints
or rely on multi-stage pipelines (e.g., spectral embedding followed by
$k$-means), limiting trade-off control, interpretability, and scalability. We
introduce \emph{DFNMF}, an end-to-end deep nonnegative tri-factorization
tailored to graphs that directly optimizes cluster assignments with a soft
statistical-parity regularizer. A single parameter $\lambda$ tunes the
fairness--utility balance, while nonnegativity yields parts-based factors and
transparent soft memberships. The optimization uses sparse-friendly alternating
updates and scales near-linearly with the number of edges. Across synthetic and
real networks, DFNMF achieves substantially higher group balance at comparable
modularity, often dominating state-of-the-art baselines on the Pareto front.
The code is available at https://github.com/SiamakGhodsi/DFNMF.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE Big-Data 2025 main research track. The paper is 10
  main pages and 4 pages of Appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Constrained Entropic Unlearning: A Primal-Dual Framework for Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05314v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05314v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taha Entesari, Arman Hatami, Rinat Khaziev, Anil Ramakrishna, Mahyar Fazlyab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) deployed in real-world settings increasingly
face the need to unlearn sensitive, outdated, or proprietary information.
Existing unlearning methods typically formulate forgetting and retention as a
regularized trade-off, combining both objectives into a single scalarized loss.
This often leads to unstable optimization and degraded performance on retained
data, especially under aggressive forgetting. We propose a new formulation of
LLM unlearning as a constrained optimization problem: forgetting is enforced
via a novel logit-margin flattening loss that explicitly drives the output
distribution toward uniformity on a designated forget set, while retention is
preserved through a hard constraint on a separate retain set. Compared to
entropy-based objectives, our loss is softmax-free, numerically stable, and
maintains non-vanishing gradients, enabling more efficient and robust
optimization. We solve the constrained problem using a scalable primal-dual
algorithm that exposes the trade-off between forgetting and retention through
the dynamics of the dual variable, all without any extra computational
overhead. Evaluations on the TOFU and MUSE benchmarks across diverse LLM
architectures demonstrate that our approach consistently matches or exceeds
state-of-the-art baselines, effectively removing targeted information while
preserving downstream utility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The Thirty-Ninth Annual Conference on Neural Information Processing
  Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UNDREAM: Bridging Differentiable Rendering and Photorealistic Simulation
  for End-to-end Adversarial Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.16923v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.16923v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mansi Phute, Matthew Hull, Haoran Wang, Alec Helbling, ShengYun Peng, Willian Lunardi, Martin Andreoni, Wenke Lee, Duen Horng Chau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning models deployed in safety critical applications like autonomous
driving use simulations to test their robustness against adversarial attacks in
realistic conditions. However, these simulations are non-differentiable,
forcing researchers to create attacks that do not integrate simulation
environmental factors, reducing attack success. To address this limitation, we
introduce UNDREAM, the first software framework that bridges the gap between
photorealistic simulators and differentiable renderers to enable end-to-end
optimization of adversarial perturbations on any 3D objects. UNDREAM enables
manipulation of the environment by offering complete control over weather,
lighting, backgrounds, camera angles, trajectories, and realistic human and
object movements, thereby allowing the creation of diverse scenes. We showcase
a wide array of distinct physically plausible adversarial objects that UNDREAM
enables researchers to swiftly explore in different configurable environments.
This combination of photorealistic simulation and differentiable optimization
opens new avenues for advancing research of physical adversarial attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ESCA: Contextualizing Embodied Agents via Scene-Graph Generation <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.15963v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.15963v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiani Huang, Amish Sethi, Matthew Kuo, Mayank Keoliya, Neelay Velingker, JungHo Jung, Ser-Nam Lim, Ziyang Li, Mayur Naik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal large language models (MLLMs) are making rapid progress toward
general-purpose embodied agents. However, existing MLLMs do not reliably
capture fine-grained links between low-level visual features and high-level
textual semantics, leading to weak grounding and inaccurate perception. To
overcome this challenge, we propose ESCA, a framework that contextualizes
embodied agents by grounding their perception in spatial-temporal scene graphs.
At its core is SGCLIP, a novel, open-domain, promptable foundation model for
generating scene graphs that is based on CLIP. SGCLIP is trained on 87K+
open-domain videos using a neurosymbolic pipeline that aligns automatically
generated captions with scene graphs produced by the model itself, eliminating
the need for human-labeled annotations. We demonstrate that SGCLIP excels in
both prompt-based inference and task-specific fine-tuning, achieving
state-of-the-art results on scene graph generation and action localization
benchmarks. ESCA with SGCLIP improves perception for embodied agents based on
both open-source and commercial MLLMs, achieving state of-the-art performance
across two embodied environments. Notably, ESCA significantly reduces agent
perception errors and enables open-source models to surpass proprietary
baselines. We release the source code for SGCLIP model training at
https://github.com/video-fm/LASER and for the embodied agent at
https://github.com/video-fm/ESCA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a Spotlight Paper at NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Now you see me! Attribution Distributions Reveal What is Truly Important
  for a Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.07346v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.07346v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nils Philipp Walter, Jilles Vreeken, Jonas Fischer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks are regularly employed in high-stakes decision-making, where
understanding and transparency is key. Attribution methods have been developed
to gain understanding into which input features neural networks use for a
specific prediction. Although widely used in computer vision, these methods
often result in unspecific saliency maps that fail to identify the relevant
information that led to a decision, supported by different benchmarks results.
Here, we revisit the common attribution pipeline and identify one cause for the
lack of specificity in attributions as the computation of attribution of
isolated logits. Instead, we suggest to combine attributions of multiple class
logits in analogy to how the softmax combines the information across logits. By
computing probability distributions of attributions over classes for each
spatial location in the image, we unleash the true capabilities of existing
attribution methods, revealing better object- and instance-specificity and
uncovering discriminative as well as shared features between classes. On common
benchmarks, including the grid-pointing game and randomization-based sanity
checks, we show that this reconsideration of how and where we compute
attributions across the network improves established attribution methods while
staying agnostic to model architectures. We make the code publicly available:
https://github.com/nilspwalter/var.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SafeCOMM: A Study on Safety Degradation in Fine-Tuned Telecom Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.00062v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.00062v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aladin Djuhera, Swanand Ravindra Kadhe, Farhan Ahmed, Syed Zawad, Fernando Koch, Walid Saad, Holger Boche
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning large language models (LLMs) on telecom datasets is a common
practice to adapt general-purpose models to the telecom domain. However, little
attention has been paid to how this process may compromise model safety. Recent
research has shown that even benign fine-tuning can degrade the safety
alignment of LLMs, causing them to respond to harmful or unethical user
queries. In this paper, we investigate this issue by fine-tuning LLMs on three
representative telecom datasets and show that safety degrades even for light
telecom domain adaptation. To this end, we introduce TeleHarm, the first
telecom-specific red-teaming benchmark, which we use alongside established
Direct-Harm and HexPhi datasets to systematically assess harmful behavior. We
further extend our analysis to publicly available TeleLLMs that were
continually pre-trained on large telecom corpora, revealing that safety
alignment is severely lacking, primarily due to the omission of safety-focused
instruction tuning. To address these issues, we evaluate three realignment
defenses: SafeInstruct, SafeLoRA, SafeMERGE. We show that, across all settings,
the proposed defenses can effectively restore safety without compromising
telecom task performance, leading to Safe teleCOMMunication (SafeCOMM) models.
Our work serves as both a diagnostic study and practical guide for safety
realignment in telecom-tuned LLMs, underscoring the need for safety-aware
instruction and fine-tuning in the telecom domain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Graph Neural Networks: A Mutual Learning Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.19223v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.19223v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Agbaje, Arkajyoti Mitra, Afia Anjum, Pranali Khose, Ebelechukwu Nwafor, Habeeb Olufowobi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge distillation (KD) techniques have emerged as a powerful tool for
transferring expertise from complex teacher models to lightweight student
models, particularly beneficial for deploying high-performance models in
resource-constrained devices. This approach has been successfully applied to
graph neural networks (GNNs), harnessing their expressive capabilities to
generate node embeddings that capture structural and feature-related
information. In this study, we depart from the conventional KD approach by
exploring the potential of collaborative learning among GNNs. In the absence of
a pre-trained teacher model, we show that relatively simple and shallow GNN
architectures can synergetically learn efficient models capable of performing
better during inference, particularly in tackling multiple tasks. We propose a
collaborative learning framework where ensembles of student GNNs mutually teach
each other throughout the training process. We introduce an adaptive logit
weighting unit to facilitate efficient knowledge exchange among models and an
entropy enhancement technique to improve mutual learning. These components
dynamically empower the models to adapt their learning strategies during
training, optimizing their performance for downstream tasks. Extensive
experiments conducted on three datasets each for node and graph classification
demonstrate the effectiveness of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Stability of Graph Convolutional Neural Networks: A Probabilistic
  Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.01213v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.01213v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ning Zhang, Henry Kenlay, Li Zhang, Mihai Cucuringu, Xiaowen Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph convolutional neural networks (GCNNs) have emerged as powerful tools
for analyzing graph-structured data, achieving remarkable success across
diverse applications. However, the theoretical understanding of the stability
of these models, i.e., their sensitivity to small changes in the graph
structure, remains in rather limited settings, hampering the development and
deployment of robust and trustworthy models in practice. To fill this gap, we
study how perturbations in the graph topology affect GCNN outputs and propose a
novel formulation for analyzing model stability. Unlike prior studies that
focus only on worst-case perturbations, our distribution-aware formulation
characterizes output perturbations across a broad range of input data. This
way, our framework enables, for the first time, a probabilistic perspective on
the interplay between the statistical properties of the node data and
perturbations in the graph topology. We conduct extensive experiments to
validate our theoretical findings and demonstrate their benefits over existing
baselines, in terms of both representation stability and adversarial attacks on
downstream tasks. Our results demonstrate the practical significance of the
proposed formulation and highlight the importance of incorporating data
distribution into stability analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ProSpero: Active Learning for Robust Protein Design Beyond Wild-Type
  Neighborhoods <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.22494v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.22494v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michal Kmicikiewicz, Vincent Fortuin, Ewa Szczurek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Designing protein sequences of both high fitness and novelty is a challenging
task in data-efficient protein engineering. Exploration beyond wild-type
neighborhoods often leads to biologically implausible sequences or relies on
surrogate models that lose fidelity in novel regions. Here, we propose
ProSpero, an active learning framework in which a frozen pre-trained generative
model is guided by a surrogate updated from oracle feedback. By integrating
fitness-relevant residue selection with biologically-constrained Sequential
Monte Carlo sampling, our approach enables exploration beyond wild-type
neighborhoods while preserving biological plausibility. We show that our
framework remains effective even when the surrogate is misspecified. ProSpero
consistently outperforms or matches existing methods across diverse protein
engineering tasks, retrieving sequences of both high fitness and novelty.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Structure of Stationary Solutions to McKean-Vlasov Equations with
  Applications to Noisy <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.20094v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.20094v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Krishnakumar Balasubramanian, Sayan Banerjee, Philippe Rigollet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study stationary solutions of McKean-Vlasov equations on the circle. Our
main contributions stem from observing an exact equivalence between solutions
of the stationary McKean-Vlasov equation and an infinite-dimensional quadratic
system of equations over Fourier coefficients, which allows explicit
characterization of the stationary states in a sequence space rather than a
function space. This framework provides a transparent description of local
bifurcations, characterizing their periodicity, and resonance structures, while
accommodating singular potentials. We derive analytic expressions that
characterize the emergence, form and shape (supercritical, critical,
subcritical or transcritical) of bifurcations involving possibly multiple
Fourier modes and connect them with discontinuous phase transitions. We also
characterize, under suitable assumptions, the detailed structure of the
stationary bifurcating solutions that are accurate upto an arbitrary number of
Fourier modes. At the global level, we establish regularity and concavity
properties of the free energy landscape, proving existence, compactness, and
coexistence of globally minimizing stationary measures, further identifying
discontinuous phase transitions with points of non-differentiability of the
minimum free energy map. As an application, we specialize the theory to the
Noisy Mean-Field Transformer model, where we show how changing the inverse
temperature parameter $\beta$ affects the geometry of the infinitely many
bifurcations from the uniform measure. We also explain how increasing $\beta$
can lead to a rich class of approximate multi-mode stationary solutions which
can be seen as `metastable states'. Further, a sharp transition from continuous
to discontinuous (first-order) phase behavior is observed as $\beta$ increases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>46 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WhaleVAD-BPN: Improving Baleen Whale Call Detection with Boundary
  Proposal Networks and Post-processing Optimisation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.21280v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.21280v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christiaan M. Geldenhuys, Günther Tonitz, Thomas R. Niesler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While recent sound event detection (SED) systems can identify baleen whale
calls in marine audio, challenges related to false positive and minority-class
detection persist. We propose the boundary proposal network (BPN), which
extends an existing lightweight SED system. The BPN is inspired by work in
image object detection and aims to reduce the number of false positive
detections. It achieves this by using intermediate latent representations
computed within the backbone classification model to gate the final output.
When added to an existing SED system, the BPN achieves a 16.8 % absolute
increase in precision, as well as 21.3 % and 9.4 % improvements in the F1-score
for minority-class d-calls and bp-calls, respectively. We further consider two
approaches to the selection of post-processing hyperparameters: a
forward-search and a backward-search. By separately optimising event-level and
frame-level hyperparameters, these two approaches lead to considerable
performance improvements over parameters selected using empirical methods. The
complete WhaleVAD-BPN system achieves a cross-validated development F1-score of
0.475, which is a 9.8 % absolute improvement over the baseline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DmC: Nearest Neighbor Guidance Diffusion Model for Offline Cross-domain
  Reinforcement Learning <span class="chip">ECAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.20499v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.20499v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linh Le Pham Van, Minh Hoang Nguyen, Duc Kieu, Hung Le, Hung The Tran, Sunil Gupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-domain offline reinforcement learning (RL) seeks to enhance sample
efficiency in offline RL by utilizing additional offline source datasets. A key
challenge is to identify and utilize source samples that are most relevant to
the target domain. Existing approaches address this challenge by measuring
domain gaps through domain classifiers, target transition dynamics modeling, or
mutual information estimation using contrastive loss. However, these methods
often require large target datasets, which is impractical in many real-world
scenarios. In this work, we address cross-domain offline RL under a limited
target data setting, identifying two primary challenges: (1) Dataset imbalance,
which is caused by large source and small target datasets and leads to
overfitting in neural network-based domain gap estimators, resulting in
uninformative measurements; and (2) Partial domain overlap, where only a subset
of the source data is closely aligned with the target domain. To overcome these
issues, we propose DmC, a novel framework for cross-domain offline RL with
limited target samples. Specifically, DmC utilizes $k$-nearest neighbor
($k$-NN) based estimation to measure domain proximity without neural network
training, effectively mitigating overfitting. Then, by utilizing this domain
proximity, we introduce a nearest-neighbor-guided diffusion model to generate
additional source samples that are better aligned with the target domain, thus
enhancing policy learning with more effective source samples. Through
theoretical analysis and extensive experiments in diverse MuJoCo environments,
we demonstrate that DmC significantly outperforms state-of-the-art cross-domain
offline RL methods, achieving substantial performance gains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at ECAI 2025; offline cross-domain reinforcement learning
  with a guided diffusion model;</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">8</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MMSD3.0: A Multi-Image Benchmark for Real-World Multimodal Sarcasm
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23299v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23299v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haochen Zhao, Yuyao Kong, Yongxiu Xu, Gaopeng Gou, Hongbo Xu, Yubin Wang, Haoliang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite progress in multimodal sarcasm detection, existing datasets and
methods predominantly focus on single-image scenarios, overlooking potential
semantic and affective relations across multiple images. This leaves a gap in
modeling cases where sarcasm is triggered by multi-image cues in real-world
settings. To bridge this gap, we introduce MMSD3.0, a new benchmark composed
entirely of multi-image samples curated from tweets and Amazon reviews. We
further propose the Cross-Image Reasoning Model (CIRM), which performs targeted
cross-image sequence modeling to capture latent inter-image connections. In
addition, we introduce a relevance-guided, fine-grained cross-modal fusion
mechanism based on text-image correspondence to reduce information loss during
integration. We establish a comprehensive suite of strong and representative
baselines and conduct extensive experiments, showing that MMSD3.0 is an
effective and reliable benchmark that better reflects real-world conditions.
Moreover, CIRM demonstrates state-of-the-art performance across MMSD, MMSD2.0
and MMSD3.0, validating its effectiveness in both single-image and multi-image
scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enabling American Sign Language Communication Under Low Data Rates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.23056v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.23056v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Panneer Selvam Santhalingam, Swann Thantsin, Ahmad Kamari, Parth Pathak, Kenneth De Haan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, video conferencing applications have become increasingly
prevalent, relying heavily on high-speed internet connectivity. When such
connectivity is lacking, users often default to audio-only communication, a
mode that significantly disadvantages American Sign Language (ASL) users, whose
communication relies on hand gestures, body movement, and facial expressions.
In this work, we introduce VC4ASL, a system designed to enable ASL
communication over the audio channel of existing video conferencing
applications, even in the absence of reliable video. VC4ASL integrates
seamlessly with current platforms without requiring any modifications. Our
approach establishes a communication channel through audio by encoding and
transmitting human pose information, which is then rendered to reconstruct
signed content. We propose novel receive-side error detection and correction
mechanisms that exploit the inherent structural constraints of human pose data.
To evaluate the system, we simulate network-degraded environments, generate
pose-based ASL video sequences, and conduct user studies to assess
comprehension among ASL users. Experimental results demonstrate that VC4ASL
effectively facilitates intelligible ASL communication over audio in
low-bandwidth scenarios where video transmission is impaired.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TEn-CATG:Text-Enriched Audio-Visual Video Parsing with Multi-Scale
  Category-Aware Temporal Graph 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04086v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04086v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaru Chen, Faegheh Sardari, Peiliang Zhang, Ruohao Guo, Yang Xiang, Zhenbo Li, Wenwu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-visual video parsing (AVVP) aims to detect event categories and their
temporal boundaries in videos, typically under weak supervision. Existing
methods mainly focus on (i) improving temporal modeling using attention-based
architectures or (ii) generating richer pseudo-labels to address the absence of
frame-level annotations. However, attention-based models often overfit noisy
pseudo-labels, leading to cumulative training errors, while pseudo-label
generation approaches distribute attention uniformly across frames, weakening
temporal localization accuracy. To address these challenges, we propose
TEn-CATG, a text-enriched AVVP framework that combines semantic calibration
with category-aware temporal reasoning. More specifically, we design a
bi-directional text fusion (BiT) module by leveraging audio-visual features as
semantic anchors to refine text embeddings, which departs from conventional
text-to-feature alignment, thereby mitigating noise and enhancing cross-modal
consistency. Furthermore, we introduce the category-aware temporal graph (CATG)
module to model temporal relationships by selecting multi-scale temporal
neighbors and learning category-specific temporal decay factors, enabling
effective event-dependent temporal reasoning. Extensive experiments demonstrate
that TEn-CATG achieves state-of-the-art results across multiple evaluation
metrics on benchmark datasets LLP and UnAV-100, highlighting its robustness and
superior ability to capture complex temporal and semantic dependencies in
weakly supervised AVVP tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Modality-incomplete Anomaly Detection: A Modality-instructive
  Framework with Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01737v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01737v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bingchen Miao, Wenqiao Zhang, Juncheng Li, Wangyu Wu, Siliang Tang, Zhaocheng Li, Haochen Shi, Jun Xiao, Yueting Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Industrial Anomaly Detection (MIAD), which utilizes 3D point
clouds and 2D RGB images to identify abnormal regions in products, plays a
crucial role in industrial quality inspection. However, traditional MIAD
settings assume that all 2D and 3D modalities are paired, ignoring the fact
that multimodal data collected from the real world is often imperfect due to
missing modalities. Additionally, models trained on modality-incomplete data
are prone to overfitting. Therefore, MIAD models that demonstrate robustness
against modality-incomplete data are highly desirable in practice. To address
this, we introduce a pioneering study that comprehensively investigates
Modality-Incomplete Industrial Anomaly Detection (MIIAD), and under the
guidance of experts, we construct the MIIAD Bench with rich modality-missing
settings to account for imperfect learning environments with incomplete
multimodal information. As expected, we find that most existing MIAD methods
perform poorly on the MIIAD Bench, leading to significant performance
degradation. To tackle this challenge, we propose a novel two-stage Robust
modAlity-aware fusing and Detecting framewoRk, abbreviated as RADAR.
Specifically: i) We propose Modality-incomplete Instruction to guide the
multimodal Transformer to robustly adapt to various modality-incomplete
scenarios, and implement adaptive parameter learning based on HyperNetwork. ii)
Then, we construct a Double-Pseudo Hybrid Module to highlight the uniqueness of
modality combinations, mitigating overfitting issues and further enhancing the
robustness of the MIIAD model. Our experimental results demonstrate that the
proposed RADAR significantly outperforms traditional MIAD methods on our newly
created MIIAD dataset, proving its practical application value.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CMIE: Combining MLLM Insights with External Evidence for Explainable
  Out-of-Context Misinformation Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.23449v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.23449v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fanxiao Li, Jiaying Wu, Canyuan He, Wei Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) have demonstrated impressive
capabilities in visual reasoning and text generation. While previous studies
have explored the application of MLLM for detecting out-of-context (OOC)
misinformation, our empirical analysis reveals two persisting challenges of
this paradigm. Evaluating the representative GPT-4o model on direct reasoning
and evidence augmented reasoning, results indicate that MLLM struggle to
capture the deeper relationships-specifically, cases in which the image and
text are not directly connected but are associated through underlying semantic
links. Moreover, noise in the evidence further impairs detection accuracy. To
address these challenges, we propose CMIE, a novel OOC misinformation detection
framework that incorporates a Coexistence Relationship Generation (CRG)
strategy and an Association Scoring (AS) mechanism. CMIE identifies the
underlying coexistence relationships between images and text, and selectively
utilizes relevant evidence to enhance misinformation detection. Experimental
results demonstrate that our approach outperforms existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From ID-based to ID-free: Rethinking ID Effectiveness in Multimodal
  Collaborative Filtering Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.05715v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.05715v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guohao Li, Li Jing, Jia Wu, Xuefei Li, Kai Zhu, Yue He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most existing multimodal collaborative filtering recommendation (MCFRec)
methods rely heavily on ID features and multimodal content to enhance
recommendation performance. However, this paper reveals that ID features are
effective but have limited benefits in multimodal collaborative filtering
recommendation. Therefore, this paper systematically deconstruct the pros and
cons of ID features: (i) they provide initial embedding but lack semantic
richness, (ii) they provide a unique identifier for each user and item but
hinder generalization to untrained data, and (iii) they assist in aligning and
fusing multimodal features but may lead to representation shift. Based on these
insights, this paper proposes IDFREE, an ID-free multimodal collaborative
Filtering REcommEndation baseline. IDFREE replaces ID features with multimodal
features and positional encodings to generate semantically meaningful ID-free
embeddings. For ID-free multimodal collaborative filtering, it further proposes
an adaptive similarity graph module to construct dynamic user-user and
item-item graphs based on multimodal features. Then, an augmented user-item
graph encoder is proposed to construct more effective user and item encoding.
Finally, IDFREE achieves inter-multimodal alignment based on the contrastive
learning and uses Softmax loss as recommendation loss. Basic experiments on
three public datasets demonstrate that IDFREE outperforms existing ID-based
MCFRec methods, achieving an average performance gain of 72.24% across standard
metrics (Recall@5, 10, 20, 50 and NDCG@5, 10, 20, 50). Exploratory and extended
experiments further validate our findings on the limitations of ID features in
MCFRec. The code is released at https://github.com/G-H-Li/IDFREE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We identified that our current approach achieves its reported
  performance only under specific data conditions, and its robustness is weaker
  than we initially expected</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ControlText: Unlocking Controllable Fonts in Multilingual Text Rendering
  without Font Annotations <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10999v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10999v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Jiang, Yuan Yuan, Xinyi Bai, Zhuoqun Hao, Alyson Yin, Yaojie Hu, Wenyu Liao, Lyle Ungar, Camillo J. Taylor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work demonstrates that diffusion models can achieve font-controllable
multilingual text rendering using just raw images without font label
annotations.Visual text rendering remains a significant challenge. While recent
methods condition diffusion on glyphs, it is impossible to retrieve exact font
annotations from large-scale, real-world datasets, which prevents
user-specified font control. To address this, we propose a data-driven solution
that integrates the conditional diffusion model with a text segmentation model,
utilizing segmentation masks to capture and represent fonts in pixel space in a
self-supervised manner, thereby eliminating the need for any ground-truth
labels and enabling users to customize text rendering with any multilingual
font of their choice. The experiment provides a proof of concept of our
algorithm in zero-shot text and font editing across diverse fonts and
languages, providing valuable insights for the community and industry toward
achieving generalized visual text rendering. Code is available at
github.com/bowen-upenn/ControlText.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The 2025 Conference on Empirical Methods in Natural Language
  Processing (EMNLP) Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive 3D Mesh Steganography Based on Feature-Preserving Distortion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.08884v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.08884v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yushu Zhang, Jiahao Zhu, Mignfu Xue, Xinpeng Zhang, Xiaochun Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current 3D mesh steganography algorithms relying on geometric modification
are prone to detection by steganalyzers. In traditional steganography, adaptive
steganography has proven to be an efficient means of enhancing steganography
security. Taking inspiration from this, we propose a highly adaptive embedding
algorithm, guided by the principle of minimizing a carefully crafted distortion
through efficient steganography codes. Specifically, we tailor a
payload-limited embedding optimization problem for 3D settings and devise a
feature-preserving distortion (FPD) to measure the impact of message embedding.
The distortion takes on an additive form and is defined as a weighted
difference of the effective steganalytic subfeatures utilized by the current 3D
steganalyzers. With practicality in mind, we refine the distortion to enhance
robustness and computational efficiency. By minimizing the FPD, our algorithm
can preserve mesh features to a considerable extent, including steganalytic and
geometric features, while achieving a high embedding capacity. During the
practical embedding phase, we employ the Q-layered syndrome trellis code (STC).
However, calculating the bit modification probability (BMP) for each layer of
the Q-layered STC, given the variation of Q, can be cumbersome. To address this
issue, we design a universal and automatic approach for the BMP calculation.
The experimental results demonstrate that our algorithm achieves
state-of-the-art performance in countering 3D steganalysis. Code is available
at https://github.com/zjhJOJO/3D-steganography-based-on-FPD.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE TVCG, corresponding author Jiahao Zhu, code is available at
  https://github.com/zjhJOJO/3D-steganography-based-on-FPD.git</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-10-26T00:00:00Z">2025-10-26</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">13</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Civic Ground Truth in News Recommenders: A Method for Public Value
  Scoring <span class="chip">RecSys 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.22865v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.22865v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        James Meese, Kyle Herbertson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research in news recommendation systems (NRS) continues to explore the best
ways to integrate normative goals such as editorial objectives and public
service values into existing systems. Prior efforts have incorporated expert
input or audience feedback to quantify these values, laying the groundwork for
more civic-minded recommender systems. This paper contributes to that
trajectory, introducing a method for embedding civic values into NRS through
large-scale, structured audience evaluations. The proposed civic ground truth
approach aims to generate value-based labels through a nationally
representative survey that are generalisable across a wider news corpus, using
automated metadata enrichment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at NORMalize 2025: The Third Workshop on the Normative
  Design and Evaluation of Recommender Systems, co-located with the ACM
  Conference on Recommender Systems 2025 (RecSys 2025), Prague</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ REVISION:Reflective Intent Mining and Online Reasoning Auxiliary for
  E-commerce Visual Search System Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.22739v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.22739v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwen Tang, Qiuyu Zhao, Zenghui Sun, Jinsong Lan, Xiaoyong Zhu, Bo Zheng, Kaifu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Taobao e-commerce visual search, user behavior analysis reveals a large
proportion of no-click requests, suggesting diverse and implicit user intents.
These intents are expressed in various forms and are difficult to mine and
discover, thereby leading to the limited adaptability and lag in platform
strategies. This greatly restricts users' ability to express diverse intents
and hinders the scalability of the visual search system. This mismatch between
user implicit intent expression and system response defines the User-SearchSys
Intent Discrepancy. To alleviate the issue, we propose a novel framework
REVISION. This framework integrates offline reasoning mining with online
decision-making and execution, enabling adaptive strategies to solve implicit
user demands. In the offline stage, we construct a periodic pipeline to mine
discrepancies from historical no-click requests. Leveraging large models, we
analyze implicit intent factors and infer optimal suggestions by jointly
reasoning over query and product metadata. These inferred suggestions serve as
actionable insights for refining platform strategies. In the online stage,
REVISION-R1-3B, trained on the curated offline data, performs holistic analysis
over query images and associated historical products to generate optimization
plans and adaptively schedule strategies across the search pipeline. Our
framework offers a streamlined paradigm for integrating large models with
traditional search systems, enabling end-to-end intelligent optimization across
information aggregation and user interaction. Experimental results demonstrate
that our approach improves the efficiency of implicit intent mining from
large-scale search logs and significantly reduces the no-click rate.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ $\text{E}^2\text{Rank}$: Your Text Embedding can Also be an Effective
  and Efficient Listwise Reranker 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.22733v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.22733v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Liu, Yanzhao Zhang, Mingxin Li, Dingkun Long, Pengjun Xie, Jiaxin Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text embedding models serve as a fundamental component in real-world search
applications. By mapping queries and documents into a shared embedding space,
they deliver competitive retrieval performance with high efficiency. However,
their ranking fidelity remains limited compared to dedicated rerankers,
especially recent LLM-based listwise rerankers, which capture fine-grained
query-document and document-document interactions. In this paper, we propose a
simple yet effective unified framework $\text{E}^2\text{Rank}$, means Efficient
Embedding-based Ranking (also means Embedding-to-Rank), which extends a single
text embedding model to perform both high-quality retrieval and listwise
reranking through continued training under a listwise ranking objective,
thereby achieving strong effectiveness with remarkable efficiency. By applying
cosine similarity between the query and document embeddings as a unified
ranking function, the listwise ranking prompt, which is constructed from the
original query and its candidate documents, serves as an enhanced query
enriched with signals from the top-K documents, akin to pseudo-relevance
feedback (PRF) in traditional retrieval models. This design preserves the
efficiency and representational quality of the base embedding model while
significantly improving its reranking performance. Empirically,
$\textrm{E}^2\text{Rank}$ achieves state-of-the-art results on the BEIR
reranking benchmark and demonstrates competitive performance on the
reasoning-intensive BRIGHT benchmark, with very low reranking latency. We also
show that the ranking training process improves embedding performance on the
MTEB benchmark. Our findings indicate that a single embedding model can
effectively unify retrieval and reranking, offering both computational
efficiency and competitive ranking accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and models are avaliable at https://alibaba-nlp.github.io/E2Rank</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ATLAS: Actor-Critic Task-Completion with Look-ahead Action Simulation <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.22732v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.22732v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiali Cheng, Anjishnu Kumar, Roshan Lal, Rishi Rajasekaran, Hani Ramezani, Omar Zia Khan, Oleg Rokhlenko, Sunny Chiu-Webster, Gang Hua, Hadi Amiri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We observe that current state-of-the-art web-agents are unable to effectively
adapt to new environments without neural network fine-tuning, without which
they produce inefficient execution plans due to a lack of awareness of the
structure and dynamics of the new environment. To address this limitation, we
introduce ATLAS (Actor-Critic Task-completion with Look-ahead Action
Simulation), a memory-augmented agent that is able to make plans grounded in a
model of the environment by simulating the consequences of those actions in
cognitive space. Our agent starts by building a "cognitive map" by performing a
lightweight curiosity driven exploration of the environment. The planner
proposes candidate actions; the simulator predicts their consequences in
cognitive space; a critic analyzes the options to select the best roll-out and
update the original plan; and a browser executor performs the chosen action. On
the WebArena-Lite Benchmark, we achieve a 63% success rate compared to 53.9%
success rate for the previously published state-of-the-art. Unlike previous
systems, our modular architecture requires no website-specific LLM fine-tuning.
Ablations show sizable drops without the world-model, hierarchical planner, and
look-ahead-based replanner confirming their complementary roles within the
design of our system
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, NeurIPS 2025 Workshop on Language Agents and World Models</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Windsock is Dancing: Adaptive Multimodal Retrieval-Augmented Generation <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.22694v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.22694v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shu Zhao, Tianyi Shen, Nilesh Ahuja, Omesh Tickoo, Vijaykrishnan Narayanan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Retrieval-Augmented Generation (MRAG) has emerged as a promising
method to generate factual and up-to-date responses of Multimodal Large
Language Models (MLLMs) by incorporating non-parametric knowledge from external
knowledge bases. However, existing MRAG approaches suffer from static retrieval
strategies, inflexible modality selection, and suboptimal utilization of
retrieved information, leading to three critical challenges: determining when
to retrieve, what modality to incorporate, and how to utilize retrieved
information effectively. To address these challenges, we introduce Windsock, a
query-dependent module making decisions on retrieval necessity and modality
selection, effectively reducing computational overhead and improving response
quality. Additionally, we propose Dynamic Noise-Resistance (DANCE) Instruction
Tuning, an adaptive training strategy that enhances MLLMs' ability to utilize
retrieved information while maintaining robustness against noise. Moreover, we
adopt a self-assessment approach leveraging knowledge within MLLMs to convert
question-answering datasets to MRAG training datasets. Extensive experiments
demonstrate that our proposed method significantly improves the generation
quality by 17.07% while reducing 8.95% retrieval times.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2025 UniReps Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diversification as Risk Minimization <span class="chip">WSDM 2026</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.22681v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.22681v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rikiya Takehi, Fernando Diaz, Tetsuya Sakai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Users tend to remember failures of a search session more than its many
successes. This observation has led to work on search robustness, where systems
are penalized if they perform very poorly on some queries. However, this
principle of robustness has been overlooked within a single query. An ambiguous
or underspecified query (e.g., ``jaguar'') can have several user intents, where
popular intents often dominate the ranking, leaving users with minority intents
unsatisfied. Although the diversification literature has long recognized this
issue, existing metrics only model the average relevance across intents and
provide no robustness guarantees. More surprisingly, we show theoretically and
empirically that many well-known diversification algorithms are no more robust
than a naive, non-diversified algorithm. To address this critical gap, we
propose to frame diversification as a risk-minimization problem. We introduce
VRisk, which measures the expected risk faced by the least-served fraction of
intents in a query. Optimizing VRisk produces a robust ranking, reducing the
likelihood of poor user experiences. We then propose VRisker, a fast greedy
re-ranker with provable approximation guarantees. Finally, experiments on NTCIR
INTENT-2, TREC Web 2012, and MovieLens show the vulnerability of existing
methods. VRisker reduces worst-case intent failures by up to 33% with a minimal
2% drop in average performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint, accepted at WSDM 2026 (Full Paper). 16 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tools are under-documented: Simple Document Expansion Boosts Tool
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.22670v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.22670v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuan Lu, Haohang Huang, Rui Meng, Yaohui Jin, Wenjun Zeng, Xiaoyu Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have recently demonstrated strong capabilities
in tool use, yet progress in tool retrieval remains hindered by incomplete and
heterogeneous tool documentation. To address this challenge, we introduce
Tool-DE, a new benchmark and framework that systematically enriches tool
documentation with structured fields to enable more effective tool retrieval,
together with two dedicated models, Tool-Embed and Tool-Rank. We design a
scalable document expansion pipeline that leverages both open- and
closed-source LLMs to generate, validate, and refine enriched tool profiles at
low cost, producing large-scale corpora with 50k instances for embedding-based
retrievers and 200k for rerankers. On top of this data, we develop two models
specifically tailored for tool retrieval: Tool-Embed, a dense retriever, and
Tool-Rank, an LLM-based reranker. Extensive experiments on ToolRet and Tool-DE
demonstrate that document expansion substantially improves retrieval
performance, with Tool-Embed and Tool-Rank achieving new state-of-the-art
results on both benchmarks. We further analyze the contribution of individual
fields to retrieval effectiveness, as well as the broader impact of document
expansion on both training and evaluation. Overall, our findings highlight both
the promise and limitations of LLM-driven document expansion, positioning
Tool-DE, along with the proposed Tool-Embed and Tool-Rank, as a foundation for
future research in tool retrieval.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ATOM: AdapTive and OptiMized dynamic temporal knowledge graph
  construction using LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.22590v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.22590v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yassir Lairgi, Ludovic Moncla, Khalid Benabdeslem, Rémy Cazabet, Pierre Cléau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In today's rapidly expanding data landscape, knowledge extraction from
unstructured text is vital for real-time analytics, temporal inference, and
dynamic memory frameworks. However, traditional static knowledge graph (KG)
construction often overlooks the dynamic and time-sensitive nature of
real-world data, limiting adaptability to continuous changes. Moreover, recent
zero- or few-shot approaches that avoid domain-specific fine-tuning or reliance
on prebuilt ontologies often suffer from instability across multiple runs, as
well as incomplete coverage of key facts. To address these challenges, we
introduce ATOM (AdapTive and OptiMized), a few-shot and scalable approach that
builds and continuously updates Temporal Knowledge Graphs (TKGs) from
unstructured texts. ATOM splits input documents into minimal, self-contained
"atomic" facts, improving extraction exhaustivity and stability. Then, it
constructs atomic TKGs from these facts while employing a dual-time modeling
that distinguishes when information is observed from when it is valid. The
resulting atomic TKGs are subsequently merged in parallel. Empirical
evaluations demonstrate that ATOM achieves ~18% higher exhaustivity, ~17%
better stability, and over 90% latency reduction compared to baseline methods,
demonstrating a strong scalability potential for dynamic TKG construction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Open Multimodal Retrieval-Augmented Factual Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.22521v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.22521v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Tian, Fan Liu, Jingyuan Zhang, Wei Bi, Yupeng Hu, Liqiang Nie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Multimodal Models (LMMs) have achieved remarkable progress in
generating photorealistic and prompt-aligned images, but they often produce
outputs that contradict verifiable knowledge, especially when prompts involve
fine-grained attributes or time-sensitive events. Conventional
retrieval-augmented approaches attempt to address this issue by introducing
external information, yet they are fundamentally incapable of grounding
generation in accurate and evolving knowledge due to their reliance on static
sources and shallow evidence integration. To bridge this gap, we introduce
ORIG, an agentic open multimodal retrieval-augmented framework for Factual
Image Generation (FIG), a new task that requires both visual realism and
factual grounding. ORIG iteratively retrieves and filters multimodal evidence
from the web and incrementally integrates the refined knowledge into enriched
prompts to guide generation. To support systematic evaluation, we build
FIG-Eval, a benchmark spanning ten categories across perceptual, compositional,
and temporal dimensions. Experiments demonstrate that ORIG substantially
improves factual consistency and overall image quality over strong baselines,
highlighting the potential of open multimodal retrieval for factual image
generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scaling Down, Serving Fast: Compressing and Deploying Efficient LLMs for
  Recommendation Systems <span class="chip">EMNLP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14305v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14305v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kayhan Behdin, Ata Fatahibaarzi, Qingquan Song, Yun Dai, Aman Gupta, Zhipeng Wang, Shao Tang, Hejian Sang, Gregory Dexter, Sirou Zhu, Siyu Zhu, Tejas Dharamsi, Vignesh Kothapalli, Zhoutong Fu, Yihan Cao, Pin-Lun Hsu, Fedor Borisyuk, Natesh Pillai, Luke Simon, Rahul Mazumder
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated remarkable performance across
a wide range of industrial applications, from search and recommendation systems
to generative tasks. Although scaling laws indicate that larger models
generally yield better generalization and performance, their substantial
computational requirements often render them impractical for many real-world
scenarios at scale. In this paper, we present a comprehensive set of insights
for training and deploying small language models (SLMs) that deliver high
performance for a variety of industry use cases. We focus on two key
techniques: (1) knowledge distillation and (2) model compression via structured
pruning and quantization. These approaches enable SLMs to retain much of the
quality of their larger counterparts while significantly reducing
training/serving costs and latency. We detail the impact of these techniques on
a variety of use cases in a large professional social network platform and
share deployment lessons, including hardware optimization strategies that
improve speed and throughput for both predictive and reasoning-based
applications in Recommendation Systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2025 Industry Track - Oral Presentation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Simple but Effective Elaborative Query Reformulation Approach for
  Natural Language Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.02656v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.02656v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianfeng Wen, Yifan Liu, Justin Cui, Joshua Zhang, Anton Korikov, George-Kirollos Saad, Scott Sanner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural Language (NL) recommender systems aim to retrieve relevant items from
free-form user queries and item descriptions. Existing systems often rely on
dense retrieval (DR), which struggles to interpret challenging queries that
express broad (e.g., "cities for youth friendly activities") or indirect (e.g.,
"cities for a high school graduation trip") user intents. While query
reformulation (QR) has been widely adopted to improve such systems, existing QR
methods tend to focus only on expanding the range of query subtopics (breadth)
or elaborating on the potential meaning of a query (depth), but not both. In
this paper, we propose EQR (Elaborative Subtopic Query Reformulation), a large
language model-based QR method that combines both breadth and depth by
generating potential query subtopics with information-rich elaborations. We
also introduce three new natural language recommendation benchmarks in travel,
hotel, and restaurant domains to establish evaluation of NL recommendation with
challenging queries. Experiments show EQR substantially outperforms
state-of-the-art QR methods in various evaluation metrics, highlighting that a
simple yet effective QR approach can significantly improve NL recommender
systems for queries with broad and indirect user intents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Worse than Zero-shot? A Fact-Checking <span class="highlight-title">Dataset</span> for Evaluating the
  Robustness of RAG Against Misleading Retrievals <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.16101v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.16101v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linda Zeng, Rithwik Gupta, Divij Motwani, Diji Yang, Yi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) has shown impressive capabilities in
mitigating hallucinations in large language models (LLMs). However, LLMs
struggle to maintain consistent reasoning when exposed to misleading or
conflicting evidence, especially in real-world domains such as politics, where
information is polarized or selectively framed. Mainstream RAG benchmarks
evaluate models under clean retrieval settings, where systems generate answers
from gold-standard documents, or under synthetically perturbed settings, where
documents are artificially injected with noise. These assumptions fail to
reflect real-world conditions, often leading to an overestimation of RAG system
performance. To address this gap, we introduce RAGuard, the first benchmark to
evaluate the robustness of RAG systems against misleading retrievals. Unlike
prior benchmarks that rely on synthetic noise, our fact-checking dataset
captures naturally occurring misinformation by constructing its retrieval
corpus from Reddit discussions. It categorizes retrieved evidence into three
types: supporting, misleading, and unrelated, providing a realistic and
challenging testbed for assessing how well RAG systems navigate different types
of evidence. Our experiments reveal that, when exposed to potentially
misleading retrievals, all tested LLM-powered RAG systems perform worse than
their zero-shot baselines (i.e., no retrieval at all), while human annotators
consistently perform better, highlighting LLMs' susceptibility to noisy
environments. To our knowledge, RAGuard is the first benchmark to
systematically assess the robustness of the RAG against misleading evidence. We
expect this benchmark to drive future research toward improving RAG systems
beyond idealized datasets, making them more reliable for real-world
applications. The dataset is available at
https://huggingface.co/datasets/UCSC-IRKM/RAGuard.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Advances in Neural Information Processing Systems (NeurIPS 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CPRet: A <span class="highlight-title">Dataset</span>, Benchmark, and Model for Retrieval in Competitive
  Programming <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12925v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12925v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Deng, Yuan Meng, Shixiang Tang, Wanli Ouyang, Xinzhu Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Competitive programming benchmarks are widely used in scenarios such as
programming contests and large language model assessments. However, the growing
presence of duplicate or highly similar problems raises concerns not only about
competition fairness, but also about the validity of competitive programming as
a benchmark for model evaluation. In this paper, we propose a new problem,
similar question retrieval, to tackle this issue. Due to the lack of both data
and models, solving this problem is challenging. To this end, we introduce
CPRet, a retrieval-oriented benchmark suite for competitive programming,
covering four retrieval tasks: two code-centric (i.e., Text-to-Code,
Code-to-Code) and two newly proposed problem-centric tasks (i.e.,
Problem-to-Duplicate, Simplified-to-Full) built from a combination of
automatically crawled problem-solution data and manually curated annotations.
Our contribution includes both high-quality training data and temporally
separated test sets for reliable evaluation. Besides, we further develop two
task-specialized retrievers based on this dataset: CPRetriever-Code, trained
with a novel Group-InfoNCE loss for problem-code alignment, and
CPRetriever-Prob, fine-tuned for identifying problem-level similarity. Both
models achieve strong results and are open-sourced for local use. Finally, we
analyze LiveCodeBench and find that high-similarity problems inflate model pass
rates and reduce differentiation, underscoring the need for similarity-aware
evaluation in future benchmarks.
  Github: https://github.com/coldchair/CPRet
  Online Demo: https://www.cpret.online/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2025 Dataset and Benchmark Track</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">11</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM-based Fusion of Multi-modal Features for Commercial Memorability
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.22829v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.22829v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aleksandar Pramov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the prediction of commercial (brand) memorability as
part of "Subtask 2: Commercial/Ad Memorability" within the "Memorability:
Predicting movie and commercial memorability" task at the MediaEval 2025
workshop competition. We propose a multimodal fusion system with a Gemma-3 LLM
backbone that integrates pre-computed visual (ViT) and textual (E5) features by
multi-modal projections. The model is adapted using Low-Rank Adaptation (LoRA).
A heavily-tuned ensemble of gradient boosted trees serves as a baseline. A key
contribution is the use of LLM-generated rationale prompts, grounded in
expert-derived aspects of memorability, to guide the fusion model. The results
demonstrate that the LLM-based system exhibits greater robustness and
generalization performance on the final test set, compared to the baseline.
  The paper's codebase can be found at
https://github.com/dsgt-arc/mediaeval-2025-memorability
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Region-Adaptive Learned Hierarchical Encoding for 3D Gaussian Splatting
  Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.22812v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.22812v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shashank N. Sridhara, Birendra Kathariya, Fangjun Pu, Peng Yin, Eduardo Pavez, Antonio Ortega
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Region-Adaptive Learned Hierarchical Encoding (RALHE) for 3D
Gaussian Splatting (3DGS) data. While 3DGS has recently become popular for
novel view synthesis, the size of trained models limits its deployment in
bandwidth-constrained applications such as volumetric media streaming. To
address this, we propose a learned hierarchical latent representation that
builds upon the principles of "overfitted" learned image compression (e.g.,
Cool-Chic and C3) to efficiently encode 3DGS attributes. Unlike images, 3DGS
data have irregular spatial distributions of Gaussians (geometry) and consist
of multiple attributes (signals) defined on the irregular geometry. Our codec
is designed to account for these differences between images and 3DGS.
Specifically, we leverage the octree structure of the voxelized 3DGS geometry
to obtain a hierarchical multi-resolution representation. Our approach overfits
latents to each Gaussian attribute under a global rate constraint. These
latents are decoded independently through a lightweight decoder network. To
estimate the bitrate during training, we employ an autoregressive probability
model that leverages octree-derived contexts from the 3D point structure. The
multi-resolution latents, decoder, and autoregressive entropy coding networks
are jointly optimized for each Gaussian attribute. Experiments demonstrate that
the proposed RALHE compression framework achieves a rendering PSNR gain of up
to 2dB at low bitrates (less than 1 MB) compared to the baseline 3DGS
compression methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 Pages, 5 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding What Is Not Said:Referring Remote Sensing Image
  Segmentation with Scarce Expressions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.22760v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.22760v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Ye, Bowen Liu, Jianghang Lin, Jiayi Ji, Pingyang Dai, Liujuan Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Referring Remote Sensing Image Segmentation (RRSIS) aims to segment instances
in remote sensing images according to referring expressions. Unlike Referring
Image Segmentation on general images, acquiring high-quality referring
expressions in the remote sensing domain is particularly challenging due to the
prevalence of small, densely distributed objects and complex backgrounds. This
paper introduces a new learning paradigm, Weakly Referring Expression Learning
(WREL) for RRSIS, which leverages abundant class names as weakly referring
expressions together with a small set of accurate ones to enable efficient
training under limited annotation conditions. Furthermore, we provide a
theoretical analysis showing that mixed-referring training yields a provable
upper bound on the performance gap relative to training with fully annotated
referring expressions, thereby establishing the validity of this new setting.
We also propose LRB-WREL, which integrates a Learnable Reference Bank (LRB) to
refine weakly referring expressions through sample-specific prompt embeddings
that enrich coarse class-name inputs. Combined with a teacher-student
optimization framework using dynamically scheduled EMA updates, LRB-WREL
stabilizes training and enhances cross-modal generalization under noisy weakly
referring supervision. Extensive experiments on our newly constructed benchmark
with varying weakly referring data ratios validate both the theoretical
insights and the practical effectiveness of WREL and LRB-WREL, demonstrating
that they can approach or even surpass models trained with fully annotated
referring expressions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TVMC: Time-Varying Mesh Compression via Multi-Stage Anchor Mesh
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.22646v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.22646v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        He Huang, Qi Yang, Yiling Xu, Zhu Li, Jenq-Neng Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time-varying meshes, characterized by dynamic connectivity and varying vertex
counts, hold significant promise for applications such as augmented reality.
However, their practical utilization remains challenging due to the substantial
data volume required for high-fidelity representation. While various
compression methods attempt to leverage temporal redundancy between consecutive
mesh frames, most struggle with topological inconsistency and motion-induced
artifacts. To address these issues, we propose Time-Varying Mesh Compression
(TVMC), a novel framework built on multi-stage coarse-to-fine anchor mesh
generation for inter-frame prediction. Specifically, the anchor mesh is
progressively constructed in three stages: initial, coarse, and fine. The
initial anchor mesh is obtained through fast topology alignment to exploit
temporal coherence. A Kalman filter-based motion estimation module then
generates a coarse anchor mesh by accurately compensating inter-frame motions.
Subsequently, a Quadric Error Metric-based refinement step optimizes vertex
positions to form a fine anchor mesh with improved geometric fidelity. Based on
the refined anchor mesh, the inter-frame motions relative to the reference base
mesh are encoded, while the residual displacements between the subdivided fine
anchor mesh and the input mesh are adaptively quantized and compressed. This
hierarchical strategy preserves consistent connectivity and high-quality
surface approximation, while achieving an efficient and compact representation
of dynamic geometry. Extensive experiments on standard MPEG dynamic mesh
sequences demonstrate that TVMC achieves state-of-the-art compression
performance. Compared to the latest V-DMC standard, it delivers a significant
BD-rate gain of 10.2% ~ 16.9%, while preserving high reconstruction quality.
The code is available at https://github.com/H-Huang774/TVMC.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepfakeBench-MM: A Comprehensive Benchmark for Multimodal Deepfake
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.22622v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.22622v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kangran Zhao, Yupeng Chen, Xiaoyu Zhang, Yize Chen, Weinan Guan, Baicheng Chen, Chengzhe Sun, Soumyya Kanti Datta, Qingshan Liu, Siwei Lyu, Baoyuan Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The misuse of advanced generative AI models has resulted in the widespread
proliferation of falsified data, particularly forged human-centric audiovisual
content, which poses substantial societal risks (e.g., financial fraud and
social instability). In response to this growing threat, several works have
preliminarily explored countermeasures. However, the lack of sufficient and
diverse training data, along with the absence of a standardized benchmark,
hinder deeper exploration. To address this challenge, we first build Mega-MMDF,
a large-scale, diverse, and high-quality dataset for multimodal deepfake
detection. Specifically, we employ 21 forgery pipelines through the combination
of 10 audio forgery methods, 12 visual forgery methods, and 6 audio-driven face
reenactment methods. Mega-MMDF currently contains 0.1 million real samples and
1.1 million forged samples, making it one of the largest and most diverse
multimodal deepfake datasets, with plans for continuous expansion. Building on
it, we present DeepfakeBench-MM, the first unified benchmark for multimodal
deepfake detection. It establishes standardized protocols across the entire
detection pipeline and serves as a versatile platform for evaluating existing
methods as well as exploring novel approaches. DeepfakeBench-MM currently
supports 5 datasets and 11 multimodal deepfake detectors. Furthermore, our
comprehensive evaluations and in-depth analyses uncover several key findings
from multiple perspectives (e.g., augmentation, stacked forgery). We believe
that DeepfakeBench-MM, together with our large-scale Mega-MMDF, will serve as
foundational infrastructures for advancing multimodal deepfake detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ STATUS Bench: A Rigorous Benchmark for Evaluating Object State
  Understanding in Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.22571v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.22571v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahiro Ukai, Shuhei Kurita, Nakamasa Inoue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object state recognition aims to identify the specific condition of objects,
such as their positional states (e.g., open or closed) and functional states
(e.g., on or off). While recent Vision-Language Models (VLMs) are capable of
performing a variety of multimodal tasks, it remains unclear how precisely they
can identify object states. To alleviate this issue, we introduce the STAte and
Transition UnderStanding Benchmark (STATUS Bench), the first benchmark for
rigorously evaluating the ability of VLMs to understand subtle variations in
object states in diverse situations. Specifically, STATUS Bench introduces a
novel evaluation scheme that requires VLMs to perform three tasks
simultaneously: object state identification (OSI), image retrieval (IR), and
state change identification (SCI). These tasks are defined over our fully
hand-crafted dataset involving image pairs, their corresponding object state
descriptions and state change descriptions. Furthermore, we introduce a
large-scale training dataset, namely STATUS Train, which consists of 13 million
semi-automatically created descriptions. This dataset serves as the largest
resource to facilitate further research in this area. In our experiments, we
demonstrate that STATUS Bench enables rigorous consistency evaluation and
reveal that current state-of-the-art VLMs still significantly struggle to
capture subtle object state distinctions. Surprisingly, under the proposed
rigorous evaluation scheme, most open-weight VLMs exhibited chance-level
zero-shot performance. After fine-tuning on STATUS Train, Qwen2.5-VL achieved
performance comparable to Gemini 2.0 Flash. These findings underscore the
necessity of STATUS Bench and Train for advancing object state recognition in
VLM research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Event-guided Exposure-agnostic Video Frame Interpolation via
  Adaptive Feature Blending <span class="chip">BMVC2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.22565v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.22565v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junsik Jung, Yoonki Cho, Woo Jae Kim, Lin Wang, Sune-eui Yoon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exposure-agnostic video frame interpolation (VFI) is a challenging task that
aims to recover sharp, high-frame-rate videos from blurry, low-frame-rate
inputs captured under unknown and dynamic exposure conditions. Event cameras
are sensors with high temporal resolution, making them especially advantageous
for this task. However, existing event-guided methods struggle to produce
satisfactory results on severely low-frame-rate blurry videos due to the lack
of temporal constraints. In this paper, we introduce a novel event-guided
framework for exposure-agnostic VFI, addressing this limitation through two key
components: a Target-adaptive Event Sampling (TES) and a Target-adaptive
Importance Mapping (TIM). Specifically, TES samples events around the target
timestamp and the unknown exposure time to better align them with the
corresponding blurry frames. TIM then generates an importance map that
considers the temporal proximity and spatial relevance of consecutive features
to the target. Guided by this map, our framework adaptively blends consecutive
features, allowing temporally aligned features to serve as the primary cues
while spatially relevant ones offer complementary support. Extensive
experiments on both synthetic and real-world datasets demonstrate the
effectiveness of our approach in exposure-agnostic VFI scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for BMVC2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MagicMotion: Controllable Video Generation with Dense-to-Sparse
  Trajectory Guidance <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.16421v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.16421v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quanhao Li, Zhen Xing, Rui Wang, Hui Zhang, Qi Dai, Zuxuan Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in video generation have led to remarkable improvements in
visual quality and temporal coherence. Upon this, trajectory-controllable video
generation has emerged to enable precise object motion control through
explicitly defined spatial paths. However, existing methods struggle with
complex object movements and multi-object motion control, resulting in
imprecise trajectory adherence, poor object consistency, and compromised visual
quality. Furthermore, these methods only support trajectory control in a single
format, limiting their applicability in diverse scenarios. Additionally, there
is no publicly available dataset or benchmark specifically tailored for
trajectory-controllable video generation, hindering robust training and
systematic evaluation. To address these challenges, we introduce MagicMotion, a
novel image-to-video generation framework that enables trajectory control
through three levels of conditions from dense to sparse: masks, bounding boxes,
and sparse boxes. Given an input image and trajectories, MagicMotion seamlessly
animates objects along defined trajectories while maintaining object
consistency and visual quality. Furthermore, we present MagicData, a
large-scale trajectory-controlled video dataset, along with an automated
pipeline for annotation and filtering. We also introduce MagicBench, a
comprehensive benchmark that assesses both video quality and trajectory control
accuracy across different numbers of objects. Extensive experiments demonstrate
that MagicMotion outperforms previous methods across various metrics. Our
project page are publicly available at
https://quanhaol.github.io/magicmotion-site.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Principled Multimodal Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17343v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17343v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaohao Liu, Xiaobo Xia, See-Kiong Ng, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal representation learning seeks to create a unified representation
space by integrating diverse data modalities to improve multimodal
understanding. Traditional methods often depend on pairwise contrastive
learning, which relies on a predefined anchor modality, restricting alignment
across all modalities. Recent advances have investigated the simultaneous
alignment of multiple modalities, yet several challenges remain, such as
limitations imposed by fixed anchor points and instability arising from
optimizing the product of singular values. To address the challenges, in this
paper, we propose Principled Multimodal Representation Learning (PMRL), a novel
framework that achieves simultaneous alignment of multiple modalities without
anchor dependency in a more stable manner. Specifically, grounded in the
theoretical insight that full alignment corresponds to a rank-1 Gram matrix,
PMRL optimizes the dominant singular value of the representation matrix to
align modalities along a shared leading direction. We propose a softmax-based
loss function that treats singular values as logits to prioritize the largest
singular value. Besides, instance-wise contrastive regularization on the
leading eigenvectors maintains inter-instance separability and prevents
representation collapse. Extensive experiments across diverse tasks demonstrate
PMRL's superiority compared to baseline methods. The source code will be
publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Corrected typos and updated experimental results. 32 pages, 9
  figures, 10 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NVS-SQA: Exploring <span class="highlight-title">Self-Supervised</span> Quality Representation Learning for
  Neurally Synthesized Scenes without References 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06488v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06488v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiang Qu, Yiran Shen, Xiaoming Chen, Yuk Ying Chung, Weidong Cai, Tongliang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural View Synthesis (NVS), such as NeRF and 3D Gaussian Splatting,
effectively creates photorealistic scenes from sparse viewpoints, typically
evaluated by quality assessment methods like PSNR, SSIM, and LPIPS. However,
these full-reference methods, which compare synthesized views to reference
views, may not fully capture the perceptual quality of neurally synthesized
scenes (NSS), particularly due to the limited availability of dense reference
views. Furthermore, the challenges in acquiring human perceptual labels hinder
the creation of extensive labeled datasets, risking model overfitting and
reduced generalizability. To address these issues, we propose NVS-SQA, a NSS
quality assessment method to learn no-reference quality representations through
self-supervision without reliance on human labels. Traditional self-supervised
learning predominantly relies on the "same instance, similar representation"
assumption and extensive datasets. However, given that these conditions do not
apply in NSS quality assessment, we employ heuristic cues and quality scores as
learning objectives, along with a specialized contrastive pair preparation
process to improve the effectiveness and efficiency of learning. The results
show that NVS-SQA outperforms 17 no-reference methods by a large margin (i.e.,
on average 109.5% in SRCC, 98.6% in PLCC, and 91.5% in KRCC over the second
best) and even exceeds 16 full-reference methods across all evaluation metrics
(i.e., 22.9% in SRCC, 19.1% in PLCC, and 18.6% in KRCC over the second best).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by TPAMI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GS-ProCams: Gaussian Splatting-based Projector-Camera Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.11762v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.11762v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyue Deng, Jijiang Li, Haibin Ling, Bingyao Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present GS-ProCams, the first Gaussian Splatting-based framework for
projector-camera systems (ProCams). GS-ProCams is not only view-agnostic but
also significantly enhances the efficiency of projection mapping (PM) that
requires establishing geometric and radiometric mappings between the projector
and the camera. Previous CNN-based ProCams are constrained to a specific
viewpoint, limiting their applicability to novel perspectives. In contrast,
NeRF-based ProCams support view-agnostic projection mapping, however, they
require an additional co-located light source and demand significant
computational and memory resources. To address this issue, we propose
GS-ProCams that employs 2D Gaussian for scene representations, and enables
efficient view-agnostic ProCams applications. In particular, we explicitly
model the complex geometric and photometric mappings of ProCams using projector
responses, the projection surface's geometry and materials represented by
Gaussians, and the global illumination component. Then, we employ
differentiable physically-based rendering to jointly estimate them from
captured multi-view projections. Compared to state-of-the-art NeRF-based
methods, our GS-ProCams eliminates the need for additional devices, achieving
superior ProCams simulation quality. It also uses only 1/10 of the GPU memory
for training and is 900 times faster in inference speed. Please refer to our
project page for the code and dataset:
https://realqingyue.github.io/GS-ProCams/.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-10-25T00:00:00Z">2025-10-25</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">11</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FAIR-RAG: Faithful Adaptive Iterative Refinement for Retrieval-Augmented
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.22344v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.22344v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Aghajani Asl, Majid Asgari-Bidhendi, Behrooz Minaei-Bidgoli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Retrieval-Augmented Generation (RAG) mitigates hallucination and
knowledge staleness in Large Language Models (LLMs), existing frameworks often
falter on complex, multi-hop queries that require synthesizing information from
disparate sources. Current advanced RAG methods, employing iterative or
adaptive strategies, lack a robust mechanism to systematically identify and
fill evidence gaps, often propagating noise or failing to gather a
comprehensive context. We introduce FAIR-RAG, a novel agentic framework that
transforms the standard RAG pipeline into a dynamic, evidence-driven reasoning
process. At its core is an Iterative Refinement Cycle governed by a module we
term Structured Evidence Assessment (SEA). The SEA acts as an analytical gating
mechanism: it deconstructs the initial query into a checklist of required
findings and audits the aggregated evidence to identify confirmed facts and,
critically, explicit informational gaps. These gaps provide a precise signal to
an Adaptive Query Refinement agent, which generates new, targeted sub-queries
to retrieve missing information. This cycle repeats until the evidence is
verified as sufficient, ensuring a comprehensive context for a final, strictly
faithful generation. We conducted experiments on challenging multi-hop QA
benchmarks, including HotpotQA, 2WikiMultiHopQA, and MusiQue. In a unified
experimental setup, FAIR-RAG significantly outperforms strong baselines. On
HotpotQA, it achieves an F1-score of 0.453 -- an absolute improvement of 8.3
points over the strongest iterative baseline -- establishing a new
state-of-the-art for this class of methods on these benchmarks. Our work
demonstrates that a structured, evidence-driven refinement process with
explicit gap analysis is crucial for unlocking reliable and accurate reasoning
in advanced RAG systems for complex, knowledge-intensive tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 5 figures, 5 tables. Keywords: Retrieval-Augmented
  Generation (RAG), Large Language Models (LLMs), Agentic AI, Multi-hop
  Question Answering, Faithfulness</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PatenTEB: A Comprehensive Benchmark and Model Family for Patent Text
  Embedding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.22264v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.22264v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Iliass Ayaou, Denis Cavallucci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Patent text embeddings enable prior art search, technology landscaping, and
patent analysis, yet existing benchmarks inadequately capture patent-specific
challenges. We introduce PatenTEB, a comprehensive benchmark comprising 15
tasks across retrieval, classification, paraphrase, and clustering, with 2.06
million examples. PatenTEB employs domain-stratified splits, domain specific
hard negative mining, and systematic coverage of asymmetric
fragment-to-document matching scenarios absent from general embedding
benchmarks. We develop the patembed model family through multi-task training,
spanning 67M to 344M parameters with context lengths up to 4096 tokens.
External validation shows strong generalization: patembed-base achieves
state-of-the-art on MTEB BigPatentClustering.v2 (0.494 V-measure vs. 0.445
previous best), while patembed-large achieves 0.377 NDCG@100 on DAPFAM.
Systematic ablations reveal that multi-task training improves external
generalization despite minor benchmark costs, and that domain-pretrained
initialization provides consistent advantages across task families. All
resources will be made available at https://github.com/iliass-y/patenteb.
Keywords: patent retrieval, sentence embeddings, multi-task learning,
asymmetric retrieval, benchmark evaluation, contrastive learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PaperAsk: A Benchmark for Reliability Evaluation of LLMs in Paper Search
  and Reading 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.22242v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.22242v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yutao Wu, Xiao Liu, Yunhao Feng, Jiale Ding, Xingjun Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) increasingly serve as research assistants, yet
their reliability in scholarly tasks remains under-evaluated. In this work, we
introduce PaperAsk, a benchmark that systematically evaluates LLMs across four
key research tasks: citation retrieval, content extraction, paper discovery,
and claim verification. We evaluate GPT-4o, GPT-5, and Gemini-2.5-Flash under
realistic usage conditions-via web interfaces where search operations are
opaque to the user. Through controlled experiments, we find consistent
reliability failures: citation retrieval fails in 48-98% of multi-reference
queries, section-specific content extraction fails in 72-91% of cases, and
topical paper discovery yields F1 scores below 0.32, missing over 60% of
relevant literature. Further human analysis attributes these failures to the
uncontrolled expansion of retrieved context and the tendency of LLMs to
prioritize semantically relevant text over task instructions. Across basic
tasks, the LLMs display distinct failure behaviors: ChatGPT often withholds
responses rather than risk errors, whereas Gemini produces fluent but
fabricated answers. To address these issues, we develop lightweight reliability
classifiers trained on PaperAsk data to identify unreliable outputs. PaperAsk
provides a reproducible and diagnostic framework for advancing the reliability
evaluation of LLM-based scholarly assistance systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hybrid-Vector Retrieval for Visually Rich Documents: Combining
  Single-Vector Efficiency and Multi-Vector Accuracy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.22215v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.22215v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juyeon Kim, Geon Lee, Dongwon Choi, Taeuk Kim, Kijung Shin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval over visually rich documents is essential for tasks such as legal
discovery, scientific search, and enterprise knowledge management. Existing
approaches fall into two paradigms: single-vector retrieval, which is efficient
but coarse, and multi-vector retrieval, which is accurate but computationally
expensive. To address this trade-off, we propose HEAVEN, a two-stage
hybrid-vector framework. In the first stage, HEAVEN efficiently retrieves
candidate pages using a single-vector method over Visually-Summarized Pages
(VS-Pages), which assemble representative visual layouts from multiple pages.
In the second stage, it reranks candidates with a multi-vector method while
filtering query tokens by linguistic importance to reduce redundant
computations. To evaluate retrieval systems under realistic conditions, we also
introduce ViMDOC, the first benchmark for visually rich, multi-document, and
long-document retrieval. Across four benchmarks, HEAVEN attains 99.87% of the
Recall@1 performance of multi-vector models on average while reducing per-query
computation by 99.82%, achieving efficiency and accuracy. Our code and datasets
are available at: https://github.com/juyeonnn/HEAVEN
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling Up Efficient Small Language Models Serving and Deployment for
  Semantic Job Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.22101v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.22101v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kayhan Behdin, Qingquan Song, Sriram Vasudevan, Jian Sheng, Xiaojing Ma, Z Zhou, Chuanrui Zhu, Guoyao Li, Chanh Nguyen, Sayan Ghosh, Hejian Sang, Ata Fatahi Baarzi, Sundara Raman Ramachandran, Xiaoqing Wang, Qing Lan, Vinay Y S, Qi Guo, Caleb Johnson, Zhipeng Wang, Fedor Borisyuk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated impressive quality when
applied to predictive tasks such as relevance ranking and semantic search.
However, deployment of such LLMs remains prohibitively expensive for industry
applications with strict latency and throughput requirements. In this work, we
present lessons and efficiency insights from developing a purely text-based
decoder-only Small Language Model (SLM) for a semantic search application at
LinkedIn. Particularly, we discuss model compression techniques such as pruning
that allow us to reduce the model size by up to $40\%$ while maintaining the
accuracy. Additionally, we present context compression techniques that allow us
to reduce the input context length by up to $10$x with minimal loss of
accuracy. Finally, we present practical lessons from optimizing the serving
infrastructure for deploying such a system on GPUs at scale, serving millions
of requests per second. Taken together, this allows us to increase our system's
throughput by $10$x in a real-world deployment, while meeting our quality bar.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> of Long-Document Retrieval in the PLM and LLM Era 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.07759v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.07759v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minghan Li, Miyang Luo, Tianrui Lv, Yishuai Zhang, Siqi Zhao, Ercong Nie, Guodong Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of long-form documents presents a fundamental challenge to
information retrieval (IR), as their length, dispersed evidence, and complex
structures demand specialized methods beyond standard passage-level techniques.
This survey provides the first comprehensive treatment of long-document
retrieval (LDR), consolidating methods, challenges, and applications across
three major eras. We systematize the evolution from classical lexical and early
neural models to modern pre-trained (PLM) and large language models (LLMs),
covering key paradigms like passage aggregation, hierarchical encoding,
efficient attention, and the latest LLM-driven re-ranking and retrieval
techniques. Beyond the models, we review domain-specific applications,
specialized evaluation resources, and outline critical open challenges such as
efficiency trade-offs, multimodal alignment, and faithfulness. This survey aims
to provide both a consolidated reference and a forward-looking agenda for
advancing long-document retrieval in the era of foundation models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Query Expansion in the Age of <span class="highlight-title">Pre-train</span>ed and Large Language Models: A
  Comprehensive <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.07794v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.07794v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minghan Li, Xinxuan Lv, Junjie Zou, Tongna Chen, Chao Zhang, Suchao An, Ercong Nie, Guodong Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern information retrieval (IR) must reconcile short, ambiguous queries
with increasingly diverse and dynamic corpora. Query expansion (QE) remains
central to alleviating vocabulary mismatch, yet the design space has shifted
with pre-trained and large language models (PLMs, LLMs). In this survey, we
organize recent work along four complementary dimensions: the point of
injection (implicit/embedding vs. selection-based explicit), grounding and
interaction (from zero-grounding prompts to multi-round retrieve-expand loops),
learning and alignment (SFT/PEFT/DPO), and knowledge-graph integration. A
model-centric taxonomy is also outlined, spanning encoder-only,
encoder-decoder, decoder-only, instruction-tuned, and domain or multilingual
variants, with affordances for QE such as contextual disambiguation,
controllable generation, and zero-shot or few-shot reasoning. Practice-oriented
guidance specifies where neural QE helps most: first-stage retrieval,
multi-query fusion, re-ranking, and retrieval-augmented generation (RAG). The
survey compares traditional and neural QE across seven aspects and maps
applications in web search, biomedicine, e-commerce, open-domain question
answering/RAG, conversational and code search, and cross-lingual settings. The
survey concludes with an agenda focused on reliable, safe, efficient, and
adaptive QE, offering a principled blueprint for deploying and combining
techniques under real-world constraints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages,3 figures,3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unifying Search and Recommendation with Dual-View Representation
  Learning in a Generative Paradigm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.06714v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.06714v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jujia Zhao, Wenjie Wang, Chen Xu, Xiuying Chen, Zhaochun Ren, Suzan Verberne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems and search engines serve as foundational elements of
online platforms, with the former delivering information proactively and the
latter enabling users to seek information actively. Unifying both tasks in a
shared model is promising since it can enhance user modeling and item
understanding. Previous approaches mainly follow a discriminative paradigm,
utilizing shared encoders to process input features and task-specific heads to
perform each task. However, this paradigm encounters two key challenges:
gradient conflict and manual design complexity. From the information theory
perspective, these challenges potentially both stem from the same issue -- low
mutual information between the input features and task-specific outputs during
the optimization process.
  To tackle these issues, we propose GenSR, a novel generative paradigm for
unifying search and recommendation (S&R), which leverages task-specific prompts
to partition the model's parameter space into subspaces, thereby enhancing
mutual information. To construct effective subspaces for each task, GenSR first
prepares informative representations for each subspace and then optimizes both
subspaces in one unified model. Specifically, GenSR consists of two main
modules: (1) Dual Representation Learning, which independently models
collaborative and semantic historical information to derive expressive item
representations; and (2) S&R Task Unifying, which utilizes contrastive learning
together with instruction tuning to generate task-specific outputs effectively.
Extensive experiments on two public datasets show GenSR outperforms
state-of-the-art methods across S&R tasks. Our work introduces a new generative
paradigm compared with previous discriminative methods and establishes its
superiority from the mutual information perspective.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WildClaims: Information Access Conversations in the Wild(Chat) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.17442v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.17442v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hideaki Joko, Shakiba Amirshahi, Charles L. A. Clarke, Faegheh Hasibi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of Large Language Models (LLMs) has transformed
conversational systems into practical tools used by millions. However, the
nature and necessity of information retrieval in real-world conversations
remain largely unexplored, as research has focused predominantly on
traditional, explicit information access conversations. The central question
is: What do real-world information access conversations look like? To this end,
we first conduct an observational study on the WildChat dataset, large-scale
user-ChatGPT conversations, finding that users' access to information occurs
implicitly as check-worthy factual assertions made by the system, even when the
conversation's primary intent is non-informational, such as creative writing.
To enable the systematic study of this phenomenon, we release the WildClaims
dataset, a novel resource consisting of 121,905 extracted factual claims from
7,587 utterances in 3,000 WildChat conversations, each annotated for
check-worthiness. Our preliminary analysis of this resource reveals that
conservatively 18% to 51% of conversations contain check-worthy assertions,
depending on the methods employed, and less conservatively, as many as 76% may
contain such assertions. This high prevalence underscores the importance of
moving beyond the traditional understanding of explicit information access, to
address the implicit information access that arises in real-world user-system
conversations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FACE: A Fine-grained Reference Free Evaluator for Conversational
  Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.00314v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.00314v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hideaki Joko, Faegheh Hasibi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A systematic, reliable, and low-cost evaluation of Conversational Recommender
Systems (CRSs) remains an open challenge. Existing automatic CRS evaluation
methods are proven insufficient for evaluating the dynamic nature of
recommendation conversations. This work proposes FACE: a Fine-grained,
Aspect-based Conversation Evaluation method that provides evaluation scores for
diverse turn and dialogue level qualities of recommendation conversations. FACE
is reference-free and shows strong correlation with human judgments, achieving
system correlation of 0.9 and turn/dialogue-level of 0.5, outperforming
state-of-the-art CRS evaluation methods by a large margin. Additionally, unlike
existing LLM-based methods that provide single uninterpretable scores, FACE
provides insights into the system performance and enables identifying and
locating problems within conversations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Audio Does Matter: Importance-Aware Multi-Granularity Fusion for Video
  Moment Retrieval <span class="chip">ACM MM 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.04273v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.04273v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junan Lin, Daizong Liu, Xianke Chen, Xiaoye Qu, Xun Yang, Jixiang Zhu, Sanyuan Zhang, Jianfeng Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video Moment Retrieval (VMR) aims to retrieve a specific moment semantically
related to the given query. To tackle this task, most existing VMR methods
solely focus on the visual and textual modalities while neglecting the
complementary but important audio modality. Although a few recent works try to
tackle the joint audio-vision-text reasoning, they treat all modalities equally
and simply embed them without fine-grained interaction for moment retrieval.
These designs are counter-practical as: Not all audios are helpful for video
moment retrieval, and the audio of some videos may be complete noise or
background sound that is meaningless to the moment determination. To this end,
we propose a novel Importance-aware Multi-Granularity fusion model (IMG), which
learns to dynamically and selectively aggregate the audio-vision-text contexts
for VMR. Specifically, after integrating the textual guidance with vision and
audio separately, we first design a pseudo-label-supervised audio importance
predictor that predicts the importance score of the audio, and accordingly
assigns weights to mitigate the interference caused by noisy audio. Then, we
design a multi-granularity audio fusion module that adaptively fuses audio and
visual modalities at local-, event-, and global-level, fully capturing their
complementary contexts. We further propose a cross-modal knowledge distillation
strategy to address the challenge of missing audio modality during inference.
To evaluate our method, we further construct a new VMR dataset, i.e.,
Charades-AudioMatter, where audio-related samples are manually selected and
re-organized from the original Charades-STA to validate the model's capability
in utilizing audio modality. Extensive experiments validate the effectiveness
of our method, achieving state-of-the-art with audio-video fusion in VMR
methods. Our code is available at https://github.com/HuiGuanLab/IMG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACM MM 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Frequency-Spatial Interaction Driven Network for Low-Light Image
  Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.22154v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.22154v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunhong Tao, Wenbing Tao, Xiang Xiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-light image enhancement (LLIE) aims at improving the perception or
interpretability of an image captured in an environment with poor illumination.
With the advent of deep learning, the LLIE technique has achieved significant
breakthroughs. However, existing LLIE methods either ignore the important role
of frequency domain information or fail to effectively promote the propagation
and flow of information, limiting the LLIE performance. In this paper, we
develop a novel frequency-spatial interaction-driven network (FSIDNet) for LLIE
based on two-stage architecture. To be specific, the first stage is designed to
restore the amplitude of low-light images to improve the lightness, and the
second stage devotes to restore phase information to refine fine-grained
structures. Considering that Frequency domain and spatial domain information
are complementary and both favorable for LLIE, we further develop two
frequency-spatial interaction blocks which mutually amalgamate the
complementary spatial and frequency information to enhance the capability of
the model. In addition, we construct the Information Exchange Module (IEM) to
associate two stages by adequately incorporating cross-stage and cross-scale
features to effectively promote the propagation and flow of information in the
two-stage network structure. Finally, we conduct experiments on several widely
used benchmark datasets (i.e., LOL-Real, LSRW-Huawei, etc.), which demonstrate
that our method achieves the excellent performance in terms of visual results
and quantitative metrics while preserving good model efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Audio Does Matter: Importance-Aware Multi-Granularity Fusion for Video
  Moment Retrieval <span class="chip">ACM MM 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.04273v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.04273v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junan Lin, Daizong Liu, Xianke Chen, Xiaoye Qu, Xun Yang, Jixiang Zhu, Sanyuan Zhang, Jianfeng Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video Moment Retrieval (VMR) aims to retrieve a specific moment semantically
related to the given query. To tackle this task, most existing VMR methods
solely focus on the visual and textual modalities while neglecting the
complementary but important audio modality. Although a few recent works try to
tackle the joint audio-vision-text reasoning, they treat all modalities equally
and simply embed them without fine-grained interaction for moment retrieval.
These designs are counter-practical as: Not all audios are helpful for video
moment retrieval, and the audio of some videos may be complete noise or
background sound that is meaningless to the moment determination. To this end,
we propose a novel Importance-aware Multi-Granularity fusion model (IMG), which
learns to dynamically and selectively aggregate the audio-vision-text contexts
for VMR. Specifically, after integrating the textual guidance with vision and
audio separately, we first design a pseudo-label-supervised audio importance
predictor that predicts the importance score of the audio, and accordingly
assigns weights to mitigate the interference caused by noisy audio. Then, we
design a multi-granularity audio fusion module that adaptively fuses audio and
visual modalities at local-, event-, and global-level, fully capturing their
complementary contexts. We further propose a cross-modal knowledge distillation
strategy to address the challenge of missing audio modality during inference.
To evaluate our method, we further construct a new VMR dataset, i.e.,
Charades-AudioMatter, where audio-related samples are manually selected and
re-organized from the original Charades-STA to validate the model's capability
in utilizing audio modality. Extensive experiments validate the effectiveness
of our method, achieving state-of-the-art with audio-video fusion in VMR
methods. Our code is available at https://github.com/HuiGuanLab/IMG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACM MM 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-10-24T00:00:00Z">2025-10-24</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AstaBench: Rigorous Benchmarking of AI Agents with a Scientific Research
  Suite 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.21652v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.21652v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Bragg, Mike D'Arcy, Nishant Balepur, Dan Bareket, Bhavana Dalvi, Sergey Feldman, Dany Haddad, Jena D. Hwang, Peter Jansen, Varsha Kishore, Bodhisattwa Prasad Majumder, Aakanksha Naik, Sigal Rahamimov, Kyle Richardson, Amanpreet Singh, Harshit Surana, Aryeh Tiktinsky, Rosni Vasu, Guy Wiener, Chloe Anastasiades, Stefan Candra, Jason Dunkelberger, Dan Emery, Rob Evans, Malachi Hamada, Regan Huff, Rodney Kinney, Matt Latzke, Jaron Lochner, Ruben Lozano-Aguilera, Cecile Nguyen, Smita Rao, Amber Tanaka, Brooke Vlahos, Peter Clark, Doug Downey, Yoav Goldberg, Ashish Sabharwal, Daniel S. Weld
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI agents hold the potential to revolutionize scientific productivity by
automating literature reviews, replicating experiments, analyzing data, and
even proposing new directions of inquiry; indeed, there are now many such
agents, ranging from general-purpose "deep research" systems to specialized
science-specific agents, such as AI Scientist and AIGS. Rigorous evaluation of
these agents is critical for progress. Yet existing benchmarks fall short on
several fronts: they (1) fail to provide holistic, product-informed measures of
real-world use cases such as science research; (2) lack reproducible agent
tools necessary for a controlled comparison of core agentic capabilities; (3)
do not account for confounding variables such as model cost and tool access;
(4) do not provide standardized interfaces for quick agent prototyping and
evaluation; and (5) lack comprehensive baseline agents necessary to identify
true advances. In response, we define principles and tooling for more
rigorously benchmarking agents. Using these, we present AstaBench, a suite that
provides the first holistic measure of agentic ability to perform scientific
research, comprising 2400+ problems spanning the entire scientific discovery
process and multiple scientific domains, and including many problems inspired
by actual user requests to deployed Asta agents. Our suite comes with the first
scientific research environment with production-grade search tools that enable
controlled, reproducible evaluation, better accounting for confounders.
Alongside, we provide a comprehensive suite of nine science-optimized classes
of Asta agents and numerous baselines. Our extensive evaluation of 57 agents
across 22 agent classes reveals several interesting findings, most importantly
that despite meaningful progress on certain individual aspects, AI remains far
from solving the challenge of science research assistance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Knee-Deep in C-RASP: A <span class="highlight-title">Transformer</span> Depth Hierarchy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.16055v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.16055v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andy Yang, Michaël Cadilhac, David Chiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It has been observed that transformers with greater depth (that is, more
layers) have more capabilities, but can we establish formally which
capabilities are gained? We answer this question with a theoretical proof
followed by an empirical study. First, we consider transformers that round to
fixed precision except inside attention. We show that this subclass of
transformers is expressively equivalent to the programming language C-RASP and
this equivalence preserves depth. Second, we prove that deeper C-RASP programs
are more expressive than shallower C-RASP programs, implying that deeper
transformers are more expressive than shallower transformers (within the
subclass mentioned above). The same is also proven for transformers with
positional encodings (like RoPE and ALiBi). These results are established by
studying a temporal logic with counting operators equivalent to C-RASP.
Finally, we provide empirical evidence that our theory predicts the depth
required for transformers without positional encodings to length-generalize on
a family of sequential dependency tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SimuRA: A World-Model-Driven Simulative Reasoning Architecture for
  General Goal-Oriented Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.23773v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.23773v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingkai Deng, Jinyu Hou, Zhiting Hu, Eric Xing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI agents built on foundation models hold enormous promise. Current practice,
however, focuses on a one-task-one-agent approach, which not only falls short
of scalability and generality, but also faces practical limitations from
black-box autoregressive reasoning, where decisions unfold token by token
without explicit simulation or counterfactual evaluation of outcomes. Humans,
on the other hand, reason and plan by mentally simulating the consequences of
actions within an internal model of the world -- a capability that supports
flexible, goal-directed behavior across diverse contexts. Moving towards a more
general and powerful AI agent, we introduce SimuRA, a goal-oriented
architecture for generalized agentic reasoning. Based on a principled
formulation of an optimal agent in any general environment, SimuRA addresses
the limitations of black-box autoregressive reasoning by incorporating the
world model for planning via simulation. Our prototype world model is
implemented using LLMs as a substrate, leveraging the natural language as a
discrete, hierarchical representation grounded in concepts for planning, while
remaining model-agnostic. On complex web-browsing tasks such as flight search,
SimuRA improves the success rate from 0% to 32.2% compared to a representative
open-web agent baseline. Across tasks, world-model-based planning achieves up
to 124% higher task completion rates than a matched black-box autoregressive
baseline, demonstrating the advantages of simulative reasoning. We release
ReasonerAgent-Web, a web-browsing agent built on SimuRA, as an open-source
research demo.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This submission has been updated to adjust the scope and presentation
  of the work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RECODE-H: A Benchmark for Research Code Development with Interactive
  Human Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.06186v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.06186v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunyu Miao, Henry Peng Zou, Yangning Li, Yankai Chen, Yibo Wang, Fangxin Wang, Yifan Li, Wooseong Yang, Bowei He, Xinni Zhang, Dianzhi Yu, Hanchen Yang, Hoang H Nguyen, Yue Zhou, Jie Yang, Jizhou Guo, Wenzhe Fan, Chin-Yuan Yeh, Panpan Meng, Liancheng Fang, Jinhu Qi, Wei-Chieh Huang, Zhengyao Gu, Yuwei Han, Langzhou He, Yuyao Yang, Yinghui Li, Hai-Tao Zheng, Xue Liu, Irwin King, Philip S. Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) show the promise in supporting scientific
research implementation, yet their ability to generate correct and executable
code remains limited. Existing works largely adopt one-shot settings, ignoring
the iterative and feedback-driven nature of realistic workflows of scientific
research development. To address this gap, we present RECODE-H, a benchmark of
102 tasks from research papers and repositories that evaluates LLM agents
through multi-turn interactions with LLM-simulated human feedback. It includes
structured instructions,unit tests, and a five-level feedback hierarchy to
reflect realistic researcher-agent collaboration. We further present
ReCodeAgent, a framework that integrates feedback into iterative code
generation. Experiments with leading LLMs, including GPT-5, Claude-Sonnet-4,
DeepSeek-V3.1, and Gemini 2.5, show substantial performance gains with richer
feedback, while also highlighting ongoing challenges in the generation of
complex research code. RECODE-H establishes a foundation for developing
adaptive, feedback-driven LLM agents in scientific research implementation
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and dataset are available at github.com/ChunyuMiao98/RECODE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Alleviating Forgetfulness of Linear Attention by Hybrid Sparse Attention
  and Contextualized Learnable Token Eviction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.20787v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.20787v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mutian He, Philip N. Garner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Linear-attention models that compress the entire input sequence into a
fixed-size recurrent state offer an efficient alternative to Transformers, but
their finite memory induces forgetfulness that harms retrieval-intensive tasks.
To mitigate the issue, we explore a series of hybrid models that restore direct
access to past tokens. We interleave token mixers with intermediate time and
space complexity between linear and full attention, including sparse attention
with token eviction, and the query-aware native sparse attention. Particularly,
we propose a novel learnable token eviction approach. Combined with
sliding-window attention, an end-to-end trainable lightweight CNN aggregates
information from both past and future adjacent tokens to adaptively retain a
limited set of critical KV-pairs per head, maintaining linear attention's
constant time and space complexity. Efficient Triton kernels for the sparse
attention mechanisms are provided. Empirical evaluations on retrieval-intensive
benchmarks support the effectiveness of our approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">25</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated Detection of Visual Attribute Reliance with a Self-Reflective
  Agent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.21704v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.21704v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christy Li, Josep Lopez Camuñas, Jake Thomas Touchet, Jacob Andreas, Agata Lapedriza, Antonio Torralba, Tamar Rott Shaham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When a vision model performs image recognition, which visual attributes drive
its predictions? Detecting unintended reliance on specific visual features is
critical for ensuring model robustness, preventing overfitting, and avoiding
spurious correlations. We introduce an automated framework for detecting such
dependencies in trained vision models. At the core of our method is a
self-reflective agent that systematically generates and tests hypotheses about
visual attributes that a model may rely on. This process is iterative: the
agent refines its hypotheses based on experimental outcomes and uses a
self-evaluation protocol to assess whether its findings accurately explain
model behavior. When inconsistencies arise, the agent self-reflects over its
findings and triggers a new cycle of experimentation. We evaluate our approach
on a novel benchmark of 130 models designed to exhibit diverse visual attribute
dependencies across 18 categories. Our results show that the agent's
performance consistently improves with self-reflection, with a significant
performance increase over non-reflective baselines. We further demonstrate that
the agent identifies real-world visual attribute dependencies in
state-of-the-art models, including CLIP's vision encoder and the YOLOv8 object
detector.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages, 10 figures, Neurips 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visual Diffusion Models are Geometric Solvers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.21697v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.21697v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nir Goren, Shai Yehezkel, Omer Dahary, Andrey Voynov, Or Patashnik, Daniel Cohen-Or
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we show that visual diffusion models can serve as effective
geometric solvers: they can directly reason about geometric problems by working
in pixel space. We first demonstrate this on the Inscribed Square Problem, a
long-standing problem in geometry that asks whether every Jordan curve contains
four points forming a square. We then extend the approach to two other
well-known hard geometric problems: the Steiner Tree Problem and the Simple
Polygon Problem.
  Our method treats each problem instance as an image and trains a standard
visual diffusion model that transforms Gaussian noise into an image
representing a valid approximate solution that closely matches the exact one.
The model learns to transform noisy geometric structures into correct
configurations, effectively recasting geometric reasoning as image generation.
  Unlike prior work that necessitates specialized architectures and
domain-specific adaptations when applying diffusion to parametric geometric
representations, we employ a standard visual diffusion model that operates on
the visual representation of the problem. This simplicity highlights a
surprising bridge between generative modeling and geometric problem solving.
Beyond the specific problems studied here, our results point toward a broader
paradigm: operating in image space provides a general and practical framework
for approximating notoriously hard problems, and opens the door to tackling a
far wider class of challenging geometric tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://kariander1.github.io/visual-geo-solver/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BachVid: Training-Free Video Generation with Consistent Background and
  Character 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.21696v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.21696v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Yan, Xibin Song, Yifu Wang, Hongdong Li, Pan Ji, Chao Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion Transformers (DiTs) have recently driven significant progress in
text-to-video (T2V) generation. However, generating multiple videos with
consistent characters and backgrounds remains a significant challenge. Existing
methods typically rely on reference images or extensive training, and often
only address character consistency, leaving background consistency to
image-to-video models. We introduce BachVid, the first training-free method
that achieves consistent video generation without needing any reference images.
Our approach is based on a systematic analysis of DiT's attention mechanism and
intermediate features, revealing its ability to extract foreground masks and
identify matching points during the denoising process. Our method leverages
this finding by first generating an identity video and caching the intermediate
variables, and then inject these cached variables into corresponding positions
in newly generated videos, ensuring both foreground and background consistency
across multiple videos. Experimental results demonstrate that BachVid achieves
robust consistency in generated videos without requiring additional training,
offering a novel and efficient solution for consistent video generation without
relying on reference images or additional training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://wolfball.github.io/bachvid</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Thin Ice: Towards Explainable Conservation Monitoring via Attribution
  and Perturbations <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.21689v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.21689v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayi Zhou, Günel Aghakishiyeva, Saagar Arya, Julian Dale, James David Poling, Holly R. Houliston, Jamie N. Womble, Gregory D. Larsen, David W. Johnston, Brinnae Bent
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computer vision can accelerate ecological research and conservation
monitoring, yet adoption in ecology lags in part because of a lack of trust in
black-box neural-network-based models. We seek to address this challenge by
applying post-hoc explanations to provide evidence for predictions and document
limitations that are important to field deployment. Using aerial imagery from
Glacier Bay National Park, we train a Faster R-CNN to detect pinnipeds (harbor
seals) and generate explanations via gradient-based class activation mapping
(HiResCAM, LayerCAM), local interpretable model-agnostic explanations (LIME),
and perturbation-based explanations. We assess explanations along three axes
relevant to field use: (i) localization fidelity: whether high-attribution
regions coincide with the animal rather than background context; (ii)
faithfulness: whether deletion/insertion tests produce changes in detector
confidence; and (iii) diagnostic utility: whether explanations reveal
systematic failure modes. Explanations concentrate on seal torsos and contours
rather than surrounding ice/rock, and removal of the seals reduces detection
confidence, providing model-evidence for true positives. The analysis also
uncovers recurrent error sources, including confusion between seals and black
ice and rocks. We translate these findings into actionable next steps for model
development, including more targeted data curation and augmentation. By pairing
object detection with post-hoc explainability, we can move beyond "black-box"
predictions toward auditable, decision-supporting tools for conservation
monitoring.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS Imageomics Workshop 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WorldGrow: Generating Infinite 3D World 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.21682v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.21682v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sikuang Li, Chen Yang, Jiemin Fang, Taoran Yi, Jia Lu, Jiazhong Cen, Lingxi Xie, Wei Shen, Qi Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We tackle the challenge of generating the infinitely extendable 3D world --
large, continuous environments with coherent geometry and realistic appearance.
Existing methods face key challenges: 2D-lifting approaches suffer from
geometric and appearance inconsistencies across views, 3D implicit
representations are hard to scale up, and current 3D foundation models are
mostly object-centric, limiting their applicability to scene-level generation.
Our key insight is leveraging strong generation priors from pre-trained 3D
models for structured scene block generation. To this end, we propose
WorldGrow, a hierarchical framework for unbounded 3D scene synthesis. Our
method features three core components: (1) a data curation pipeline that
extracts high-quality scene blocks for training, making the 3D structured
latent representations suitable for scene generation; (2) a 3D block inpainting
mechanism that enables context-aware scene extension; and (3) a coarse-to-fine
generation strategy that ensures both global layout plausibility and local
geometric/textural fidelity. Evaluated on the large-scale 3D-FRONT dataset,
WorldGrow achieves SOTA performance in geometry reconstruction, while uniquely
supporting infinite scene generation with photorealistic and structurally
consistent outputs. These results highlight its capability for constructing
large-scale virtual environments and potential for building future world
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://world-grow.github.io/ Code:
  https://github.com/world-grow/WorldGrow</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Foundation Models in Dermatopathology: Skin Tissue Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.21664v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.21664v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riya Gupta, Yiwei Zong, Dennis H. Murphree
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid generation of whole-slide images (WSIs) in dermatopathology
necessitates automated methods for efficient processing and accurate
classification. This study evaluates the performance of two foundation models,
UNI and Virchow2, as feature extractors for classifying WSIs into three
diagnostic categories: melanocytic, basaloid, and squamous lesions. Patch-level
embeddings were aggregated into slide-level features using a mean-aggregation
strategy and subsequently used to train multiple machine learning classifiers,
including logistic regression, gradient-boosted trees, and random forest
models. Performance was assessed using precision, recall, true positive rate,
false positive rate, and the area under the receiver operating characteristic
curve (AUROC) on the test set. Results demonstrate that patch-level features
extracted using Virchow2 outperformed those extracted via UNI across most
slide-level classifiers, with logistic regression achieving the highest
accuracy (90%) for Virchow2, though the difference was not statistically
significant. The study also explored data augmentation techniques and image
normalization to enhance model robustness and generalizability. The
mean-aggregation approach provided reliable slide-level feature
representations. All experimental results and metrics were tracked and
visualized using WandB.ai, facilitating reproducibility and interpretability.
This research highlights the potential of foundation models for automated WSI
classification, providing a scalable and effective approach for
dermatopathological diagnosis while paving the way for future advancements in
slide-level representation learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Self-Supervised</span> Learning of Synapse Types from EM Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.21663v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.21663v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aarav Shetty, Gary B Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Separating synapses into different classes based on their appearance in EM
images has many applications in biology. Examples may include assigning a
neurotransmitter to a particular class, or separating synapses whose strength
can be modulated from those whose strength is fixed. Traditionally, this has
been done in a supervised manner, giving the classification algorithm examples
of the different classes. Here we instead separate synapses into classes based
only on the observation that nearby synapses in the same neuron are likely more
similar than synapses chosen randomly from different cells. We apply our
methodology to data from {\it Drosophila}. Our approach has the advantage that
the number of synapse types does not need to be known in advance. It may also
provide a principled way to select ground-truth that spans the range of synapse
structure.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Long-tailed Species Recognition in the NACTI Wildlife <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.21657v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.21657v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zehua Liu, Tilo Burghardt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As most ''in the wild'' data collections of the natural world, the North
America Camera Trap Images (NACTI) dataset shows severe long-tailed class
imbalance, noting that the largest 'Head' class alone covers >50% of the 3.7M
images in the corpus. Building on the PyTorch Wildlife model, we present a
systematic study of Long-Tail Recognition methodologies for species recognition
on the NACTI dataset covering experiments on various LTR loss functions plus
LTR-sensitive regularisation. Our best configuration achieves 99.40% Top-1
accuracy on our NACTI test data split, substantially improving over a 95.51%
baseline using standard cross-entropy with Adam. This also improves on
previously reported top performance in MLWIC2 at 96.8% albeit using partly
unpublished (potentially different) partitioning, optimiser, and evaluation
protocols. To evaluate domain shifts (e.g. night-time captures, occlusion,
motion-blur) towards other datasets we construct a Reduced-Bias Test set from
the ENA-Detection dataset where our experimentally optimised long-tail enhanced
model achieves leading 52.55% accuracy (up from 51.20% with WCE loss),
demonstrating stronger generalisation capabilities under distribution shift. We
document the consistent improvements of LTR-enhancing scheduler choices in this
NACTI wildlife domain, particularly when in tandem with state-of-the-art LTR
losses. We finally discuss qualitative and quantitative shortcomings that LTR
methods cannot sufficiently address, including catastrophic breakdown for
'Tail' classes under severe domain shift. For maximum reproducibility we
publish all dataset splits, key code, and full network weights.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Group Inertial Poser: Multi-Person Pose and Global Translation from
  Sparse Inertial Sensors and Ultra-Wideband Ranging <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.21654v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.21654v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ying Xue, Jiaxi Jiang, Rayan Armani, Dominik Hollidt, Yi-Chi Liao, Christian Holz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tracking human full-body motion using sparse wearable inertial measurement
units (IMUs) overcomes the limitations of occlusion and instrumentation of the
environment inherent in vision-based approaches. However, purely IMU-based
tracking compromises translation estimates and accurate relative positioning
between individuals, as inertial cues are inherently self-referential and
provide no direct spatial reference for others. In this paper, we present a
novel approach for robustly estimating body poses and global translation for
multiple individuals by leveraging the distances between sparse wearable
sensors - both on each individual and across multiple individuals. Our method
Group Inertial Poser estimates these absolute distances between pairs of
sensors from ultra-wideband ranging (UWB) and fuses them with inertial
observations as input into structured state-space models to integrate temporal
motion patterns for precise 3D pose estimation. Our novel two-step optimization
further leverages the estimated distances for accurately tracking people's
global trajectories through the world. We also introduce GIP-DB, the first
IMU+UWB dataset for two-person tracking, which comprises 200 minutes of motion
recordings from 14 participants. In our evaluation, Group Inertial Poser
outperforms previous state-of-the-art methods in accuracy and robustness across
synthetic and real-world data, showing the promise of IMU+UWB-based multi-human
motion capture in the wild. Code, models, dataset:
https://github.com/eth-siplab/GroupInertialPoser
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICCV 2025, Code:
  https://github.com/eth-siplab/GroupInertialPoser</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Dynamic Knowledge Distillation Method Based on the Gompertz Curve 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.21649v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.21649v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Yang, Guangjun Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel dynamic knowledge distillation framework,
Gompertz-CNN, which integrates the Gompertz growth model into the training
process to address the limitations of traditional knowledge distillation.
Conventional methods often fail to capture the evolving cognitive capacity of
student models, leading to suboptimal knowledge transfer. To overcome this, we
propose a stage-aware distillation strategy that dynamically adjusts the weight
of distillation loss based on the Gompertz curve, reflecting the student's
learning progression: slow initial growth, rapid mid-phase improvement, and
late-stage saturation. Our framework incorporates Wasserstein distance to
measure feature-level discrepancies and gradient matching to align backward
propagation behaviors between teacher and student models. These components are
unified under a multi-loss objective, where the Gompertz curve modulates the
influence of distillation losses over time. Extensive experiments on CIFAR-10
and CIFAR-100 using various teacher-student architectures (e.g., ResNet50 and
MobileNet_v2) demonstrate that Gompertz-CNN consistently outperforms
traditional distillation methods, achieving up to 8% and 4% accuracy gains on
CIFAR-10 and CIFAR-100, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DAP-MAE: Domain-Adaptive Point Cloud Masked Autoencoder for Effective
  Cross-Domain Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.21635v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.21635v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqi Gao, Qiufu Li, Linlin Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compared to 2D data, the scale of point cloud data in different domains
available for training, is quite limited. Researchers have been trying to
combine these data of different domains for masked autoencoder (MAE)
pre-training to leverage such a data scarcity issue. However, the prior
knowledge learned from mixed domains may not align well with the downstream 3D
point cloud analysis tasks, leading to degraded performance. To address such an
issue, we propose the Domain-Adaptive Point Cloud Masked Autoencoder (DAP-MAE),
an MAE pre-training method, to adaptively integrate the knowledge of
cross-domain datasets for general point cloud analysis. In DAP-MAE, we design a
heterogeneous domain adapter that utilizes an adaptation mode during
pre-training, enabling the model to comprehensively learn information from
point clouds across different domains, while employing a fusion mode in the
fine-tuning to enhance point cloud features. Meanwhile, DAP-MAE incorporates a
domain feature generator to guide the adaptation of point cloud features to
various downstream tasks. With only one pre-training, DAP-MAE achieves
excellent performance across four different point cloud analysis tasks,
reaching 95.18% in object classification on ScanObjectNN and 88.45% in facial
expression recognition on Bosphorus.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 7 figures, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Epipolar Geometry Improves Video Generation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.21615v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.21615v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Orest Kupyn, Fabian Manhardt, Federico Tombari, Christian Rupprecht
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video generation models have progressed tremendously through large latent
diffusion transformers trained with rectified flow techniques. Yet these models
still struggle with geometric inconsistencies, unstable motion, and visual
artifacts that break the illusion of realistic 3D scenes. 3D-consistent video
generation could significantly impact numerous downstream applications in
generation and reconstruction tasks. We explore how epipolar geometry
constraints improve modern video diffusion models. Despite massive training
data, these models fail to capture fundamental geometric principles underlying
visual content. We align diffusion models using pairwise epipolar geometry
constraints via preference-based optimization, directly addressing unstable
camera trajectories and geometric artifacts through mathematically principled
geometric enforcement. Our approach efficiently enforces geometric principles
without requiring end-to-end differentiability. Evaluation demonstrates that
classical geometric constraints provide more stable optimization signals than
modern learned metrics, which produce noisy targets that compromise alignment
quality. Training on static scenes with dynamic cameras ensures high-quality
measurements while the model generalizes effectively to diverse dynamic
content. By bridging data-driven deep learning with classical geometric
computer vision, we present a practical method for generating spatially
consistent videos without compromising visual quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modest-Align: Data-Efficient Alignment for Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.21606v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.21606v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxiang Liu, Yuan Wang, Jiawei Du, Joey Tianyi Zhou, Mingkun Xu, Zuozhu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-modal alignment aims to map heterogeneous modalities into a shared
latent space, as exemplified by models like CLIP, which benefit from
large-scale image-text pretraining for strong recognition capabilities.
However, when operating in resource-constrained settings with limited or
low-quality data, these models often suffer from overconfidence and degraded
performance due to the prevalence of ambiguous or weakly correlated image-text
pairs. Current contrastive learning approaches, which rely on single positive
pairs, further exacerbate this issue by reinforcing overconfidence on uncertain
samples. To address these challenges, we propose Modest-Align, a lightweight
alignment framework designed for robustness and efficiency. Our approach
leverages two complementary strategies -- Random Perturbation, which introduces
controlled noise to simulate uncertainty, and Embedding Smoothing, which
calibrates similarity distributions in the embedding space. These mechanisms
collectively reduce overconfidence and improve performance on noisy or weakly
aligned samples. Extensive experiments across multiple benchmark datasets
demonstrate that Modest-Align outperforms state-of-the-art methods in retrieval
tasks, achieving competitive results with over 100x less training data and 600x
less GPU time than CLIP. Our method offers a practical and scalable solution
for cross-modal alignment in real-world, low-resource scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ S3OD: Towards Generalizable Salient Object Detection with Synthetic Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.21605v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.21605v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Orest Kupyn, Hirokatsu Kataoka, Christian Rupprecht
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Salient object detection exemplifies data-bounded tasks where expensive
pixel-precise annotations force separate model training for related subtasks
like DIS and HR-SOD. We present a method that dramatically improves
generalization through large-scale synthetic data generation and
ambiguity-aware architecture. We introduce S3OD, a dataset of over 139,000
high-resolution images created through our multi-modal diffusion pipeline that
extracts labels from diffusion and DINO-v3 features. The iterative generation
framework prioritizes challenging categories based on model performance. We
propose a streamlined multi-mask decoder that naturally handles the inherent
ambiguity in salient object detection by predicting multiple valid
interpretations. Models trained solely on synthetic data achieve 20-50% error
reduction in cross-dataset generalization, while fine-tuned versions reach
state-of-the-art performance across DIS and HR-SOD benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated interictal epileptic spike detection from simple and noisy
  annotations in MEG data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.21596v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.21596v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pauline Mouches, Julien Jung, Armand Demasson, Agnès Guinard, Romain Bouet, Rosalie Marchal, Romain Quentin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In drug-resistant epilepsy, presurgical evaluation of epilepsy can be
considered. Magnetoencephalography (MEG) has been shown to be an effective exam
to inform the localization of the epileptogenic zone through the localization
of interictal epileptic spikes. Manual detection of these pathological
biomarkers remains a fastidious and error-prone task due to the high
dimensionality of MEG recordings, and interrater agreement has been reported to
be only moderate. Current automated methods are unsuitable for clinical
practice, either requiring extensively annotated data or lacking robustness on
non-typical data. In this work, we demonstrate that deep learning models can be
used for detecting interictal spikes in MEG recordings, even when only temporal
and single-expert annotations are available, which represents real-world
clinical practice. We propose two model architectures: a feature-based
artificial neural network (ANN) and a convolutional neural network (CNN),
trained on a database of 59 patients, and evaluated against a state-of-the-art
model to classify short time windows of signal. In addition, we employ an
interactive machine learning strategy to iteratively improve our data
annotation quality using intermediary model outputs. Both proposed models
outperform the state-of-the-art model (F1-scores: CNN=0.46, ANN=0.44) when
tested on 10 holdout test patients. The interactive machine learning strategy
demonstrates that our models are robust to noisy annotations. Overall, results
highlight the robustness of models with simple architectures when analyzing
complex and imperfectly annotated data. Our method of interactive machine
learning offers great potential for faster data annotation, while our models
represent useful and efficient tools for automated interictal spikes detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 7 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Geometric Approach to Steerable Convolutions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.18813v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.18813v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soumyabrata Kundu, Risi Kondor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In contrast to the somewhat abstract, group theoretical approach adopted by
many papers, our work provides a new and more intuitive derivation of steerable
convolutional neural networks in $d$ dimensions. This derivation is based on
geometric arguments and fundamental principles of pattern matching. We offer an
intuitive explanation for the appearance of the Clebsch--Gordan decomposition
and spherical harmonic basis functions. Furthermore, we suggest a novel way to
construct steerable convolution layers using interpolation kernels that improve
upon existing implementation, and offer greater robustness to noisy data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DynamicPAE: Generating Scene-Aware Physical Adversarial Examples in
  Real-Time 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08053v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08053v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin Hu, Xianglong Liu, Jiakai Wang, Junkai Zhang, Xianqi Yang, Haotong Qin, Yuqing Ma, Ke Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Physical adversarial examples (PAEs) are regarded as whistle-blowers of
real-world risks in deep-learning applications, thus worth further
investigation. However, current PAE generation studies show limited adaptive
attacking ability to diverse and varying scenes, revealing the urgent
requirement of dynamic PAEs that are generated in real time and conditioned on
the observation from the attacker. The key challenge in generating dynamic PAEs
is learning the sparse relation between PAEs and the observation of attackers
under the noisy feedback of attack training. To address the challenge, we
present DynamicPAE, the first generative framework that enables scene-aware
real-time physical attacks. Specifically, to address the noisy feedback problem
that obfuscates the exploration of scene-related PAEs, we introduce the
residual-guided adversarial pattern exploration technique. Residual-guided
training, which relaxes the attack training with a reconstruction task, is
proposed to enrich the feedback information, thereby achieving a more
comprehensive exploration of PAEs. To address the alignment problem between the
trained generator and the real-world scenario, we introduce the
distribution-matched attack scenario alignment, consisting of the
conditional-uncertainty-aligned data module and the skewness-aligned objective
re-weighting module. The former aligns the training environment with the
incomplete observation of the real-world attacker. The latter facilitates
consistent stealth control across different attack targets with the skewness
controller. Extensive digital and physical evaluations demonstrate the superior
attack performance of DynamicPAE, attaining a 2.07 $\times$ boost (58.8%
average AP drop under attack) on representative object detectors (e.g., DETR)
over state-of-the-art static PAE generating methods. Overall, our work opens
the door to end-to-end modeling of dynamic PAEs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mixture of Experts in Image Classification: What's the Sweet Spot? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.18322v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.18322v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mathurin Videau, Alessandro Leite, Marc Schoenauer, Olivier Teytaud
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mixture-of-Experts (MoE) models have shown promising potential for
parameter-efficient scaling across domains. However, their application to image
classification remains limited, often requiring billion-scale datasets to be
competitive. In this work, we explore the integration of MoE layers into image
classification architectures using open datasets. We conduct a systematic
analysis across different MoE configurations and model scales. We find that
moderate parameter activation per sample provides the best trade-off between
performance and efficiency. However, as the number of activated parameters
increases, the benefits of MoE diminish. Our analysis yields several practical
insights for vision MoE design. First, MoE layers most effectively strengthen
tiny and mid-sized models, while gains taper off for large-capacity networks
and do not redefine state-of-the-art ImageNet performance. Second, a Last-2
placement heuristic offers the most robust cross-architecture choice, with
Every-2 slightly better for Vision Transform (ViT), and both remaining
effective as data and model scale increase. Third, larger datasets (e.g.,
ImageNet-21k) allow more experts, up to 16, for ConvNeXt to be utilized
effectively without changing placement, as increased data reduces overfitting
and promotes broader expert specialization. Finally, a simple linear router
performs best, suggesting that additional routing complexity yields no
consistent benefit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Transactions on Machine Learning Research</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Metropolis-Hastings Sampling for 3D Gaussian Reconstruction <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12945v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12945v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyunjin Kim, Haebeom Jung, Jaesik Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose an adaptive sampling framework for 3D Gaussian Splatting (3DGS)
that leverages comprehensive multi-view photometric error signals within a
unified Metropolis-Hastings approach. Vanilla 3DGS heavily relies on
heuristic-based density-control mechanisms (e.g., cloning, splitting, and
pruning), which can lead to redundant computations or premature removal of
beneficial Gaussians. Our framework overcomes these limitations by
reformulating densification and pruning as a probabilistic sampling process,
dynamically inserting and relocating Gaussians based on aggregated multi-view
errors and opacity scores. Guided by Bayesian acceptance tests derived from
these error-based importance scores, our method substantially reduces reliance
on heuristics, offers greater flexibility, and adaptively infers Gaussian
distributions without requiring predefined scene complexity. Experiments on
benchmark datasets, including Mip-NeRF360, Tanks and Temples and Deep Blending,
show that our approach reduces the number of Gaussians needed, achieving faster
convergence while matching or modestly surpassing the view-synthesis quality of
state-of-the-art models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025. Project Page: https://hjhyunjinkim.github.io/MH-3DGS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lightweight Facial Landmark Detection in Thermal Images via Multi-Level
  Cross-Modal Knowledge Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.11128v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.11128v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiyi Tong, Olivia Nocentini, Marta Lagomarsino, Kuanqi Cai, Marta Lorenzini, Arash Ajoudani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facial Landmark Detection (FLD) in thermal imagery is critical for
applications in challenging lighting conditions, but it is hampered by the lack
of rich visual cues. Conventional cross-modal solutions, like feature fusion or
image translation from RGB data, are often computationally expensive or
introduce structural artifacts, limiting their practical deployment. To address
this, we propose Multi-Level Cross-Modal Knowledge Distillation (MLCM-KD), a
novel framework that decouples high-fidelity RGB-to-thermal knowledge transfer
from model compression to create both accurate and efficient thermal FLD
models. A central challenge during knowledge transfer is the profound modality
gap between RGB and thermal data, where traditional unidirectional distillation
fails to enforce semantic consistency across disparate feature spaces. To
overcome this, we introduce Dual-Injected Knowledge Distillation (DIKD), a
bidirectional mechanism designed specifically for this task. DIKD establishes a
connection between modalities: it not only guides the thermal student with rich
RGB features but also validates the student's learned representations by
feeding them back into the frozen teacher's prediction head. This closed-loop
supervision forces the student to learn modality-invariant features that are
semantically aligned with the teacher, ensuring a robust and profound knowledge
transfer. Experiments show that our approach sets a new state-of-the-art on
public thermal FLD benchmarks, notably outperforming previous methods while
drastically reducing computational overhead.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FORLA: Federated Object-centric Representation Learning with Slot
  Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.02964v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.02964v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guiqiu Liao, Matjaz Jogan, Eric Eaton, Daniel A. Hashimoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning efficient visual representations across heterogeneous unlabeled
datasets remains a central challenge in federated learning. Effective federated
representations require features that are jointly informative across clients
while disentangling domain-specific factors without supervision. We introduce
FORLA, a novel framework for federated object-centric representation learning
and feature adaptation across clients using unsupervised slot attention. At the
core of our method is a shared feature adapter, trained collaboratively across
clients to adapt features from foundation models, and a shared slot attention
module that learns to reconstruct the adapted features. To optimize this
adapter, we design a two-branch student-teacher architecture. In each client, a
student decoder learns to reconstruct full features from foundation models,
while a teacher decoder reconstructs their adapted, low-dimensional
counterpart. The shared slot attention module bridges cross-domain learning by
aligning object-level representations across clients. Experiments in multiple
real-world datasets show that our framework not only outperforms centralized
baselines on object discovery but also learns a compact, universal
representation that generalizes well across domains. This work highlights
federated slot attention as an effective tool for scalable, unsupervised visual
representation learning from cross-domain data with distributed concepts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Neurips2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SegMASt3R: Geometry Grounded Segment Matching <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.05051v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.05051v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohit Jayanti, Swayam Agrawal, Vansh Garg, Siddharth Tourani, Muhammad Haris Khan, Sourav Garg, Madhava Krishna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Segment matching is an important intermediate task in computer vision that
establishes correspondences between semantically or geometrically coherent
regions across images. Unlike keypoint matching, which focuses on localized
features, segment matching captures structured regions, offering greater
robustness to occlusions, lighting variations, and viewpoint changes. In this
paper, we leverage the spatial understanding of 3D foundation models to tackle
wide-baseline segment matching, a challenging setting involving extreme
viewpoint shifts. We propose an architecture that uses the inductive bias of
these 3D foundation models to match segments across image pairs with up to 180
degree view-point change rotation. Extensive experiments show that our approach
outperforms state-of-the-art methods, including the SAM2 video propagator and
local feature matching methods, by up to 30% on the AUPRC metric, on ScanNet++
and Replica datasets. We further demonstrate benefits of the proposed model on
relevant downstream tasks, including 3D instance mapping and object-relative
navigation. Project Page: https://segmast3r.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to The 39th Annual Conference on Neural Information
  Processing Systems (NeurIPS 2025) as a Spotlight (top 3.5%)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Narrow Gate: Localized Image-Text Communication in Native Multimodal
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.06646v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.06646v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessandro Serra, Francesco Ortu, Emanuele Panizon, Lucrezia Valeriani, Lorenzo Basile, Alessio Ansuini, Diego Doimo, Alberto Cazzaniga
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in multimodal training have significantly improved the
integration of image understanding and generation within a unified model. This
study investigates how vision-language models (VLMs) handle image-understanding
tasks, focusing on how visual information is processed and transferred to the
textual domain. We compare native multimodal VLMs, models trained from scratch
on multimodal data to generate both text and images, and non-native multimodal
VLMs, models adapted from pre-trained large language models or capable of
generating only text, highlighting key differences in information flow. We find
that in native multimodal VLMs, image and text embeddings are more separated
within the residual stream. Moreover, VLMs differ in how visual information
reaches text: non-native multimodal VLMs exhibit a distributed communication
pattern, where information is exchanged through multiple image tokens, whereas
models trained natively for joint image and text generation tend to rely on a
single post-image token that acts as a narrow gate for visual information. We
show that ablating this single token significantly deteriorates
image-understanding performance, whereas targeted, token-level interventions
reliably steer image semantics and downstream text with fine-grained control.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Operational Change Detection for Geographical Information: <span class="highlight-title">Overview</span> and
  Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.14109v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.14109v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Gonthier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rapid evolution of territories due to climate change and human impact
requires prompt and effective updates to geospatial databases maintained by the
National Mapping Agency. This paper presents a comprehensive overview of change
detection methods tailored for the operational updating of large-scale
geographic databases. This review first outlines the fundamental definition of
change, emphasizing its multifaceted nature, from temporal to semantic
characterization. It categorizes automatic change detection methods into four
main families: rule-based, statistical, machine learning, and simulation
methods. The strengths, limitations, and applicability of every family are
discussed in the context of various input data. Then, key applications for
National Mapping Agencies are identified, particularly the optimization of
geospatial database updating, change-based phenomena, and dynamics monitoring.
Finally, the paper highlights the current challenges for leveraging change
detection such as the variability of change definition, the missing of relevant
large-scale datasets, the diversity of input data, the unstudied no-change
detection, the human in the loop integration and the operational constraints.
The discussion underscores the necessity for ongoing innovation in change
detection techniques to address the future needs of geographic information
systems for national mapping agencies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Video-RTS: Rethinking Reinforcement Learning and Test-Time Scaling for
  Efficient and Enhanced Video Reasoning <span class="chip">EMNLP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.06485v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.06485v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyang Wang, Jaehong Yoon, Shoubin Yu, Md Mohaiminul Islam, Gedas Bertasius, Mohit Bansal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite advances in reinforcement learning (RL)-based video reasoning with
large language models (LLMs), data collection and fine-tuning remain
significant challenges. These methods often rely on large-scale supervised
fine-tuning (SFT) with extensive video data and long Chain-of-Thought (CoT)
annotations, making them costly and hard to scale. To address this, we present
Video-RTS, a new approach to improve video reasoning capability with
drastically improved data efficiency by combining data-efficient RL with a
video-adaptive test-time scaling (TTS) strategy. Building on observations about
the data scaling, we skip the resource-intensive SFT step and employ efficient
pure-RL training with output-based rewards, requiring no additional annotations
or extensive fine-tuning. Furthermore, to utilize computational resources more
efficiently, we introduce a sparse-to-dense video TTS strategy that improves
inference by iteratively adding frames based on output consistency. We validate
our approach on multiple video reasoning benchmarks, showing that Video-RTS
surpasses existing video reasoning models by 2.4% in accuracy using only 3.6%
training samples. Specifically, Video-RTS achieves a 4.2% improvement on
Video-Holmes, a recent and challenging video reasoning benchmark. Notably, our
pure RL training and adaptive video TTS offer complementary strengths, enabling
Video-RTS's strong reasoning performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2025. The first two authors contributed equally. Project page:
  https://sites.google.com/cs.unc.edu/videorts2025/</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">19</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Data-Centric Approach to Multilingual E-Commerce Product Search: Case
  Study on Query-Category and Query-Item Relevance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.21671v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.21671v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yabo Yin, Yang Xi, Jialong Wang, Shanqi Wang, Jiateng Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multilingual e-commerce search suffers from severe data imbalance across
languages, label noise, and limited supervision for low-resource
languages--challenges that impede the cross-lingual generalization of relevance
models despite the strong capabilities of large language models (LLMs). In this
work, we present a practical, architecture-agnostic, data-centric framework to
enhance performance on two core tasks: Query-Category (QC) relevance (matching
queries to product categories) and Query-Item (QI) relevance (matching queries
to product titles). Rather than altering the model, we redesign the training
data through three complementary strategies: (1) translation-based augmentation
to synthesize examples for languages absent in training, (2) semantic negative
sampling to generate hard negatives and mitigate class imbalance, and (3)
self-validation filtering to detect and remove likely mislabeled instances.
Evaluated on the CIKM AnalytiCup 2025 dataset, our approach consistently yields
substantial F1 score improvements over strong LLM baselines, achieving
competitive results in the official competition. Our findings demonstrate that
systematic data engineering can be as impactful as--and often more deployable
than--complex model modifications, offering actionable guidance for building
robust multilingual search systems in the real-world e-commerce settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepAgent: A General Reasoning Agent with Scalable Toolsets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.21618v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.21618v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoxi Li, Wenxiang Jiao, Jiarui Jin, Guanting Dong, Jiajie Jin, Yinuo Wang, Hao Wang, Yutao Zhu, Ji-Rong Wen, Yuan Lu, Zhicheng Dou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large reasoning models have demonstrated strong problem-solving abilities,
yet real-world tasks often require external tools and long-horizon
interactions. Existing agent frameworks typically follow predefined workflows,
which limit autonomous and global task completion. In this paper, we introduce
DeepAgent, an end-to-end deep reasoning agent that performs autonomous
thinking, tool discovery, and action execution within a single, coherent
reasoning process. To address the challenges of long-horizon interactions,
particularly the context length explosion from multiple tool calls and the
accumulation of interaction history, we introduce an autonomous memory folding
mechanism that compresses past interactions into structured episodic, working,
and tool memories, reducing error accumulation while preserving critical
information. To teach general-purpose tool use efficiently and stably, we
develop an end-to-end reinforcement learning strategy, namely ToolPO, that
leverages LLM-simulated APIs and applies tool-call advantage attribution to
assign fine-grained credit to the tool invocation tokens. Extensive experiments
on eight benchmarks, including general tool-use tasks (ToolBench, API-Bank,
TMDB, Spotify, ToolHop) and downstream applications (ALFWorld, WebShop, GAIA,
HLE), demonstrate that DeepAgent consistently outperforms baselines across both
labeled-tool and open-set tool retrieval scenarios. This work takes a step
toward more general and capable agents for real-world applications. The code
and demo are available at https://github.com/RUC-NLPIR/DeepAgent.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Doc-Researcher: A Unified System for Multimodal Document Parsing and
  Deep Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.21603v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.21603v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kuicai Dong, Shurui Huang, Fangda Ye, Wei Han, Zhi Zhang, Dexun Li, Wenjun Li, Qu Yang, Gang Wang, Yichao Wang, Chen Zhang, Yong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Research systems have revolutionized how LLMs solve complex questions
through iterative reasoning and evidence gathering. However, current systems
remain fundamentally constrained to textual web data, overlooking the vast
knowledge embedded in multimodal documents Processing such documents demands
sophisticated parsing to preserve visual semantics (figures, tables, charts,
and equations), intelligent chunking to maintain structural coherence, and
adaptive retrieval across modalities, which are capabilities absent in existing
systems. In response, we present Doc-Researcher, a unified system that bridges
this gap through three integrated components: (i) deep multimodal parsing that
preserves layout structure and visual semantics while creating multi-granular
representations from chunk to document level, (ii) systematic retrieval
architecture supporting text-only, vision-only, and hybrid paradigms with
dynamic granularity selection, and (iii) iterative multi-agent workflows that
decompose complex queries, progressively accumulate evidence, and synthesize
comprehensive answers across documents and modalities. To enable rigorous
evaluation, we introduce M4DocBench, the first benchmark for Multi-modal,
Multi-hop, Multi-document, and Multi-turn deep research. Featuring 158
expert-annotated questions with complete evidence chains across 304 documents,
M4DocBench tests capabilities that existing benchmarks cannot assess.
Experiments demonstrate that Doc-Researcher achieves 50.6% accuracy, 3.4xbetter
than state-of-the-art baselines, validating that effective document research
requires not just better retrieval, but fundamentally deep parsing that
preserve multimodal integrity and support iterative research. Our work
establishes a new paradigm for conducting deep research on multimodal document
collections.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Redefining Retrieval Evaluation in the Era of LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.21440v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.21440v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giovanni Trappolini, Florin Cuconasu, Simone Filice, Yoelle Maarek, Fabrizio Silvestri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional Information Retrieval (IR) metrics, such as nDCG, MAP, and MRR,
assume that human users sequentially examine documents with diminishing
attention to lower ranks. This assumption breaks down in Retrieval Augmented
Generation (RAG) systems, where search results are consumed by Large Language
Models (LLMs), which, unlike humans, process all retrieved documents as a whole
rather than sequentially. Additionally, traditional IR metrics do not account
for related but irrelevant documents that actively degrade generation quality,
rather than merely being ignored. Due to these two major misalignments, namely
human vs. machine position discount and human relevance vs. machine utility,
classical IR metrics do not accurately predict RAG performance. We introduce a
utility-based annotation schema that quantifies both the positive contribution
of relevant passages and the negative impact of distracting ones. Building on
this foundation, we propose UDCG (Utility and Distraction-aware Cumulative
Gain), a metric using an LLM-oriented positional discount to directly optimize
the correlation with the end-to-end answer accuracy. Experiments on five
datasets and six LLMs demonstrate that UDCG improves correlation by up to 36%
compared to traditional metrics. Our work provides a critical step toward
aligning IR evaluation with LLM consumers and enables more reliable assessment
of RAG components
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SciNUP: Natural Language User Interest Profiles for Scientific
  Literature Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.21352v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.21352v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mariam Arustashvili, Krisztian Balog
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The use of natural language (NL) user profiles in recommender systems offers
greater transparency and user control compared to traditional representations.
However, there is scarcity of large-scale, publicly available test collections
for evaluating NL profile-based recommendation. To address this gap, we
introduce SciNUP, a novel synthetic dataset for scholarly recommendation that
leverages authors' publication histories to generate NL profiles and
corresponding ground truth items. We use this dataset to conduct a comparison
of baseline methods, ranging from sparse and dense retrieval approaches to
state-of-the-art LLM-based rerankers. Our results show that while baseline
methods achieve comparable performance, they often retrieve different items,
indicating complementary behaviors. At the same time, considerable headroom for
improvement remains, highlighting the need for effective NL-based
recommendation approaches. The SciNUP dataset thus serves as a valuable
resource for fostering future research and development in this area.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CausalRec: A CausalBoost Attention Model for Sequential Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.21333v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.21333v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunbo Hou, Tianle Yang, Ruijie Li, Li He, Liang Wang, Weiping Li, Bo Zheng, Guojie Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in correlation-based sequential recommendation systems have
demonstrated substantial success. Specifically, the attention-based model
outperforms other RNN-based and Markov chains-based models by capturing both
short- and long-term dependencies more effectively. However, solely focusing on
item co-occurrences overlooks the underlying motivations behind user behaviors,
leading to spurious correlations and potentially inaccurate recommendations. To
address this limitation, we present a novel framework that integrates causal
attention for sequential recommendation, CausalRec. It incorporates a causal
discovery block and a CausalBooster. The causal discovery block learns the
causal graph in user behavior sequences, and we provide a theory to guarantee
the identifiability of the learned causal graph. The CausalBooster utilizes the
discovered causal graph to refine the attention mechanism, prioritizing
behaviors with causal significance. Experimental evaluations on real-world
datasets indicate that CausalRec outperforms several state-of-the-art methods,
with average improvements of 7.21% in Hit Rate (HR) and 8.65% in Normalized
Discounted Cumulative Gain (NDCG). To the best of our knowledge, this is the
first model to incorporate causality through the attention mechanism in
sequential recommendation, demonstrating the value of causality in generating
more accurate and reliable recommendations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pctx: Tokenizing Personalized Context for Generative Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.21276v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.21276v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiyong Zhong, Jiajie Su, Yunshan Ma, Julian McAuley, Yupeng Hou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative recommendation (GR) models tokenize each action into a few
discrete tokens (called semantic IDs) and autoregressively generate the next
tokens as predictions, showing advantages such as memory efficiency,
scalability, and the potential to unify retrieval and ranking. Despite these
benefits, existing tokenization methods are static and non-personalized. They
typically derive semantic IDs solely from item features, assuming a universal
item similarity that overlooks user-specific perspectives. However, under the
autoregressive paradigm, semantic IDs with the same prefixes always receive
similar probabilities, so a single fixed mapping implicitly enforces a
universal item similarity standard across all users. In practice, the same item
may be interpreted differently depending on user intentions and preferences. To
address this issue, we propose a personalized context-aware tokenizer that
incorporates a user's historical interactions when generating semantic IDs.
This design allows the same item to be tokenized into different semantic IDs
under different user contexts, enabling GR models to capture multiple
interpretive standards and produce more personalized predictions. Experiments
on three public datasets demonstrate up to 11.44% improvement in NDCG@10 over
non-personalized action tokenization baselines. Our code is available at
https://github.com/YoungZ365/Pctx.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bi-Level Optimization for Generative Recommendation: Bridging
  Tokenization and Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.21242v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.21242v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yimeng Bai, Chang Liu, Yang Zhang, Dingxian Wang, Frank Yang, Andrew Rabinovich, Wenge Rong, Fuli Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative recommendation is emerging as a transformative paradigm by
directly generating recommended items, rather than relying on matching.
Building such a system typically involves two key components: (1) optimizing
the tokenizer to derive suitable item identifiers, and (2) training the
recommender based on those identifiers. Existing approaches often treat these
components separately--either sequentially or in alternation--overlooking their
interdependence. This separation can lead to misalignment: the tokenizer is
trained without direct guidance from the recommendation objective, potentially
yielding suboptimal identifiers that degrade recommendation performance.
  To address this, we propose BLOGER, a Bi-Level Optimization for GEnerative
Recommendation framework, which explicitly models the interdependence between
the tokenizer and the recommender in a unified optimization process. The lower
level trains the recommender using tokenized sequences, while the upper level
optimizes the tokenizer based on both the tokenization loss and recommendation
loss. We adopt a meta-learning approach to solve this bi-level optimization
efficiently, and introduce gradient surgery to mitigate gradient conflicts in
the upper-level updates, thereby ensuring that item identifiers are both
informative and recommendation-aligned. Extensive experiments on real-world
datasets demonstrate that BLOGER consistently outperforms state-of-the-art
generative recommendation methods while maintaining practical efficiency with
no significant additional computational overhead, effectively bridging the gap
between item tokenization and autoregressive generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VOGUE: A Multimodal <span class="highlight-title">Dataset</span> for Conversational Recommendation in Fashion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.21151v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.21151v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Guo, Minqi Sun, Yilun Jiang, Jiazhou Liang, Scott Sanner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal conversational recommendation has emerged as a promising paradigm
for delivering personalized experiences through natural dialogue enriched by
visual and contextual grounding. Yet, current multimodal conversational
recommendation datasets remain limited: existing resources either simulate
conversations, omit user history, or fail to collect sufficiently detailed
feedback, all of which constrain the types of research and evaluation they
support.
  To address these gaps, we introduce VOGUE, a novel dataset of 60 humanhuman
dialogues in realistic fashion shopping scenarios. Each dialogue is paired with
a shared visual catalogue, item metadata, user fashion profiles and histories,
and post-conversation ratings from both Seekers and Assistants. This design
enables rigorous evaluation of conversational inference, including not only
alignment between predicted and ground-truth preferences, but also calibration
against full rating distributions and comparison with explicit and implicit
user satisfaction signals.
  Our initial analyses of VOGUE reveal distinctive dynamics of visually
grounded dialogue. For example, recommenders frequently suggest items
simultaneously in feature-based groups, which creates distinct conversational
phases bridged by Seeker critiques and refinements. Benchmarking multimodal
large language models against human recommenders shows that while MLLMs
approach human-level alignment in aggregate, they exhibit systematic
distribution errors in reproducing human ratings and struggle to generalize
preference inference beyond explicitly discussed items. These findings
establish VOGUE as both a unique resource for studying multimodal
conversational systems and as a challenge dataset beyond the current
recommendation capabilities of existing top-tier multimodal foundation models
such as GPT-4o-mini, GPT-5-mini, and Gemini-2.5-Flash.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Benchmark for Open-Domain Numerical Fact-Checking Enhanced by Claim
  Decomposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.22055v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.22055v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        V Venktesh, Deepali Prabhu, Avishek Anand
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fact-checking numerical claims is critical as the presence of numbers provide
mirage of veracity despite being fake potentially causing catastrophic impacts
on society. The prior works in automatic fact verification do not primarily
focus on natural numerical claims. A typical human fact-checker first retrieves
relevant evidence addressing the different numerical aspects of the claim and
then reasons about them to predict the veracity of the claim. Hence, the search
process of a human fact-checker is a crucial skill that forms the foundation of
the verification process. Emulating a real-world setting is essential to aid in
the development of automated methods that encompass such skills. However,
existing benchmarks employ heuristic claim decomposition approaches augmented
with weakly supervised web search to collect evidences for verifying claims.
This sometimes results in less relevant evidences and noisy sources with
temporal leakage rendering a less realistic retrieval setting for claim
verification. Hence, we introduce QuanTemp++: a dataset consisting of natural
numerical claims, an open domain corpus, with the corresponding relevant
evidence for each claim. The evidences are collected through a claim
decomposition process approximately emulating the approach of human
fact-checker and veracity labels ensuring there is no temporal leakage. Given
this dataset, we also characterize the retrieval performance of key claim
decomposition paradigms. Finally, we observe their effect on the outcome of the
verification pipeline and draw insights. The code for data pipeline along with
link to data can be found at https://github.com/VenkteshV/QuanTemp_Plus
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Massive Memorization with Hundreds of Trillions of Parameters for
  Sequential Transducer Generative Recommenders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.22049v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.22049v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhimin Chen, Chenyu Zhao, Ka Chun Mo, Yunjiang Jiang, Jane H. Lee, Shouwei Chen, Khushhall Chandra Mahajan, Ning Jiang, Kai Ren, Jinhui Li, Wen-Yun Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern large-scale recommendation systems rely heavily on user interaction
history sequences to enhance the model performance. The advent of large
language models and sequential modeling techniques, particularly
transformer-like architectures, has led to significant advancements recently
(e.g., HSTU, SIM, and TWIN models). While scaling to ultra-long user histories
(10k to 100k items) generally improves model performance, it also creates
significant challenges on latency, queries per second (QPS) and GPU cost in
industry-scale recommendation systems. Existing models do not adequately
address these industrial scalability issues. In this paper, we propose a novel
two-stage modeling framework, namely VIrtual Sequential Target Attention
(VISTA), which decomposes traditional target attention from a candidate item to
user history items into two distinct stages: (1) user history summarization
into a few hundred tokens; followed by (2) candidate item attention to those
tokens. These summarization token embeddings are then cached in storage system
and then utilized as sequence features for downstream model training and
inference. This novel design for scalability enables VISTA to scale to lifelong
user histories (up to one million items) while keeping downstream training and
inference costs fixed, which is essential in industry. Our approach achieves
significant improvements in offline and online metrics and has been
successfully deployed on an industry leading recommendation platform serving
billions of users.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Item Scoring for Natural Language Recommendation via Gaussian
  Process Regression with LLM Relevance Judgments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.22023v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.22023v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Liu, Qianfeng Wen, Jiazhou Liang, Mark Zhao, Justin Cui, Anton Korikov, Armin Torogh, Junyoung Kim, Scott Sanner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural Language Recommendation (NLRec) generates item suggestions based on
the relevance between user-issued NL requests and NL item description passages.
Existing NLRec approaches often use Dense Retrieval (DR) to compute item
relevance scores from aggregation of inner products between user request
embeddings and relevant passage embeddings. However, DR views the request as
the sole relevance label, thus leading to a unimodal scoring function centered
on the query embedding that is often a weak proxy for query relevance. To
better capture the potential multimodal distribution of the relevance scoring
function that may arise from complex NLRec data, we propose GPR-LLM that uses
Gaussian Process Regression (GPR) with LLM relevance judgments for a subset of
candidate passages. Experiments on four NLRec datasets and two LLM backbones
demonstrate that GPR-LLM with an RBF kernel, capable of modeling multimodal
relevance scoring functions, consistently outperforms simpler unimodal kernels
(dot product, cosine similarity), as well as baseline methods including DR,
cross-encoder, and pointwise LLM-based relevance scoring by up to 65%. Overall,
GPR-LLM provides an efficient and effective approach to NLRec within a minimal
LLM labeling budget.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages,20 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Temporal Graph Theoretic Analysis of Geopolitical Dynamics in the U.S.
  Entity List 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.21962v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.21962v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunsen Lei, Kexin Bai, Quan Li, H. Howie Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Export controls have become one of America's most prominent tools of economic
statecraft. They aim to block rival countries' access to sensitive
technologies, safeguard U.S. supply chains, protect national security, and
shape geopolitical competition. Among various instruments, the U.S. Entity List
has emerged as the most salient, yet its dynamics remain underexplored. This
paper introduces a novel temporal graph framework that transforms the Entity
List documents from a static registry of foreign entities of concern into a
dynamic representation of geopolitical strategy. We construct the first
event-based dataset of U.S. government foreign entity designations and model
them as a temporal bipartite graph. Building on this representation, we develop
a multi-level analytical approach that reveals shifting roles, enforcement
strategy, and broader sanction ecosystems. Applied to 25 years of data, the
framework uncovers dynamic patterns of escalation, persistence, and
coordination that static views cannot capture. More broadly, our study
demonstrates how temporal graph analysis offers systematic computational
insights into the geopolitical dynamics of export controls.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 9 figures. Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Faster and Memory-Efficient Training of Sequential Recommendation Models
  for Large Catalogs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.09682v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.09682v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maxim Zhelnin, Dmitry Redko, Volkov Daniil, Anna Volodkevich, Petr Sokerin, Valeriy Shevchenko, Egor Shvetsov, Alexey Vasilev, Darya Denisova, Ruslan Izmailov, Alexey Zaytsev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommendations (SR) with transformer-based architectures are
widely adopted in real-world applications, where SR models require frequent
retraining to adapt to ever-changing user preferences. However, training
transformer-based SR models often encounters a high computational cost
associated with scoring extensive item catalogs, often exceeding thousands of
items. This occurs mainly due to the use of cross-entropy loss, where peak
memory scales proportionally to catalog size, batch size, and sequence length.
Recognizing this, practitioners in the field of recommendation systems
typically address memory consumption by integrating the cross-entropy (CE) loss
with negative sampling, thereby reducing the explicit memory demands of the
final layer. However, a small number of negative samples would degrade model
performance, and as we demonstrate in our work, increasing the number of
negative samples and the batch size further improves the model's performance,
but rapidly starts to exceed industrial GPUs' size (~40Gb).
  In this work, we introduce the CCE- method, which offers a GPU-efficient
implementation of the CE loss with negative sampling. Our method accelerates
training by up to two times while reducing memory consumption by more than 10
times. Leveraging the memory savings afforded by using CCE- for model training,
it becomes feasible to enhance its accuracy on datasets with a large item
catalog compared to those trained with original PyTorch-implemented loss
functions. Finally, we perform an analysis of key memory-related
hyperparameters and highlight the necessity of a delicate balance among these
factors. We demonstrate that scaling both the number of negative samples and
batch size leads to better results rather than maximizing only one of them. To
facilitate further adoption of CCE-, we release a Triton kernel that
efficiently implements the proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SimLab: A Platform for Simulation-based Evaluation of Conversational
  Information Access Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.04888v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.04888v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nolwenn Bernard, Sharath Chandra Etagi Suresh, Krisztian Balog, ChengXiang Zhai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Progress in conversational information access (CIA) systems has been hindered
by the difficulty of evaluating such systems with reproducible experiments.
While user simulation offers a promising solution, the lack of infrastructure
and tooling to support this evaluation paradigm remains a significant barrier.
To address this gap, we introduce SimLab, the first cloud-based platform
providing a centralized solution for the community to benchmark both
conversational systems and user simulators in a controlled and reproducible
setting. We articulate the requirements for such a platform and propose a
general infrastructure to meet them. We then present the design and
implementation of an initial version of SimLab and showcase its features
through an initial simulation-based evaluation task in conversational movie
recommendation. Furthermore, we discuss the platform's sustainability and
future opportunities for development, inviting the community to drive further
progress in the fields of CIA and user simulation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AcuRank: Uncertainty-Aware Adaptive Computation for Listwise Reranking <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.18512v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.18512v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soyoung Yoon, Gyuwan Kim, Gyu-Hwung Cho, Seung-won Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Listwise reranking with large language models (LLMs) enhances top-ranked
results in retrieval-based applications. Due to the limit in context size and
high inference cost of long context, reranking is typically performed over a
fixed size of small subsets, with the final ranking aggregated from these
partial results. This fixed computation disregards query difficulty and
document distribution, leading to inefficiencies. We propose AcuRank, an
adaptive reranking framework that dynamically adjusts both the amount and
target of computation based on uncertainty estimates over document relevance.
Using a Bayesian TrueSkill model, we iteratively refine relevance estimates
until reaching sufficient confidence levels, and our explicit modeling of
ranking uncertainty enables principled control over reranking behavior and
avoids unnecessary updates to confident predictions. Results on the TREC-DL and
BEIR benchmarks show that our method consistently achieves a superior
accuracy-efficiency trade-off and scales better with compute than
fixed-computation baselines. These results highlight the effectiveness and
generalizability of our method across diverse retrieval tasks and LLM-based
reranking models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2025. The first two authors contributed equally.
  Author order is randomly determined via coin toss</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Context-aware Reasoning-enhanced Generative Searching in
  E-commerce 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.16925v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.16925v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiding Liu, Ben Chen, Mingyue Cheng, Enhong Chen, Li Li, Chenyi Lei, Wenwu Ou, Han Li, Kun Gai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Search-based recommendation is one of the most critical application scenarios
in e-commerce platforms. Users' complex search contexts--such as spatiotemporal
factors, historical interactions, and current query's information--constitute
an essential part of their decision-making, reflecting implicit preferences
that complement explicit query terms. Modeling such rich contextual signals and
their intricate associations with candidate items remains a key challenge.
Although numerous efforts have been devoted to building more effective search
methods, existing approaches still show limitations in integrating contextual
information, which hinders their ability to fully capture user intent.
  To address these challenges, we propose a context-aware reasoning-enhanced
generative search framework for better \textbf{understanding the complicated
context}. Specifically, the framework first unifies heterogeneous user and item
contexts into textual representations or text-based semantic identifiers and
aligns them. To overcome the lack of explicit reasoning trajectories, we
introduce a self-evolving post-training paradigm that iteratively combines
supervised fine-tuning and reinforcement learning to progressively enhance the
model's reasoning capability. In addition, we identify potential biases in
existing RL algorithms when applied to search scenarios and present a debiased
variant of GRPO to improve ranking performance. Extensive experiments on search
log data collected from a real-world e-commerce platform demonstrate that our
approach achieves superior performance compared with strong baselines,
validating its effectiveness for search-based recommendation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rank-GRPO: Training LLM-based Conversational Recommender Systems with
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.20150v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.20150v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaochen Zhu, Harald Steck, Dawen Liang, Yinhan He, Vito Ostuni, Jundong Li, Nathan Kallus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are reshaping the recommender system paradigm by
enabling users to express preferences and receive recommendations through
conversations. Yet, aligning LLMs to the recommendation task remains
challenging: pretrained LLMs often generate out-of-catalog items, violate
required output formats, and their ranking quality degrades sharply toward the
end of the generated list. To this end, we propose ConvRec-R1, a two-stage
framework for end-to-end training of LLM-based conversational recommender
systems. In Stage 1, we construct a behavioral-cloning dataset with a
Remap-Reflect-Adjust pipeline, which produces high-quality, catalog-grounded
demonstrations from powerful blackbox LLMs to warm-start the RL training. In
Stage 2, we propose Rank-GRPO, a principled extension of group relative policy
optimization (GRPO) tailored to tasks with rank-style outputs. Rank-GRPO treats
each rank in the recommendation list as the unit instead of token (too
fine-grained) or sequence (too coarse), redefining rewards to remove non-causal
credit assignment and introducing a rank-level importance ratio based on the
geometric mean of rank-wise token probabilities to stabilize policy updates.
Experiments on the public Reddit-v2 dataset show that ConvRec-R1 converges
faster and achieves higher Recall and NDCG than GRPO-style baselines. Code and
datasets are released at https://github.com/yaochenzhu/Rank-GRPO.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Engram Memory Encoding and Retrieval: A Neurocomputational Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.01659v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.01659v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Szelogowski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite substantial research into the biological basis of memory, the precise
mechanisms by which experiences are encoded, stored, and retrieved in the brain
remain incompletely understood. A growing body of evidence supports the engram
theory, which posits that sparse populations of neurons undergo lasting
physical and biochemical changes to support long-term memory. Yet, a
comprehensive computational framework that integrates biological findings with
mechanistic models remains elusive. This work synthesizes insights from
cellular neuroscience and computational modeling to address key challenges in
engram research: how engram neurons are identified and manipulated; how
synaptic plasticity mechanisms contribute to stable memory traces; and how
sparsity promotes efficient, interference-resistant representations. Relevant
computational approaches -- such as sparse regularization, engram gating, and
biologically inspired architectures like Sparse Distributed Memory and spiking
neural networks -- are also examined. Together, these findings suggest that
memory efficiency, capacity, and stability emerge from the interaction of
plasticity and sparsity constraints. By integrating neurobiological and
computational perspectives, this paper provides a comprehensive theoretical
foundation for engram research and proposes a roadmap for future inquiry into
the mechanisms underlying memory, with implications for the diagnosis and
treatment of memory-related disorders.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 7 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">32</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Equivariance by Contrast: Identifiable Equivariant Embeddings from
  Unlabeled Finite Group Actions <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.21706v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.21706v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobias Schmidt, Steffen Schneider, Matthias Bethge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Equivariance by Contrast (EbC) to learn equivariant embeddings
from observation pairs $(\mathbf{y}, g \cdot \mathbf{y})$, where $g$ is drawn
from a finite group acting on the data. Our method jointly learns a latent
space and a group representation in which group actions correspond to
invertible linear maps -- without relying on group-specific inductive biases.
We validate our approach on the infinite dSprites dataset with structured
transformations defined by the finite group $G:= (R_m \times \mathbb{Z}_n
\times \mathbb{Z}_n)$, combining discrete rotations and periodic translations.
The resulting embeddings exhibit high-fidelity equivariance, with group
operations faithfully reproduced in latent space. On synthetic data, we further
validate the approach on the non-abelian orthogonal group $O(n)$ and the
general linear group $GL(n)$. We also provide a theoretical proof for
identifiability. While broad evaluation across diverse group types on
real-world data remains future work, our results constitute the first
successful demonstration of general-purpose encoder-only equivariant learning
from group action observations alone, including non-trivial non-abelian groups
and a product group motivated by modeling affine equivariances in computer
vision.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2025. The last two authors contributed equally.
  Code is available at https://github.com/dynamical-inference/ebc</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visual Diffusion Models are Geometric Solvers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.21697v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.21697v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nir Goren, Shai Yehezkel, Omer Dahary, Andrey Voynov, Or Patashnik, Daniel Cohen-Or
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we show that visual diffusion models can serve as effective
geometric solvers: they can directly reason about geometric problems by working
in pixel space. We first demonstrate this on the Inscribed Square Problem, a
long-standing problem in geometry that asks whether every Jordan curve contains
four points forming a square. We then extend the approach to two other
well-known hard geometric problems: the Steiner Tree Problem and the Simple
Polygon Problem.
  Our method treats each problem instance as an image and trains a standard
visual diffusion model that transforms Gaussian noise into an image
representing a valid approximate solution that closely matches the exact one.
The model learns to transform noisy geometric structures into correct
configurations, effectively recasting geometric reasoning as image generation.
  Unlike prior work that necessitates specialized architectures and
domain-specific adaptations when applying diffusion to parametric geometric
representations, we employ a standard visual diffusion model that operates on
the visual representation of the problem. This simplicity highlights a
surprising bridge between generative modeling and geometric problem solving.
Beyond the specific problems studied here, our results point toward a broader
paradigm: operating in image space provides a general and practical framework
for approximating notoriously hard problems, and opens the door to tackling a
far wider class of challenging geometric tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://kariander1.github.io/visual-geo-solver/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mechanistic Interpretability for Neural TSP Solvers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.21693v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.21693v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reuben Narad, Leonard Boussioux, Michael Wagner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks have advanced combinatorial optimization, with
Transformer-based solvers achieving near-optimal solutions on the Traveling
Salesman Problem (TSP) in milliseconds. However, these models operate as black
boxes, providing no insight into the geometric patterns they learn or the
heuristics they employ during tour construction. We address this opacity by
applying sparse autoencoders (SAEs), a mechanistic interpretability technique,
to a Transformer-based TSP solver, representing the first application of
activation-based interpretability methods to operations research models. We
train a pointer network with reinforcement learning on 100-node instances, then
fit an SAE to the encoder's residual stream to discover an overcomplete
dictionary of interpretable features. Our analysis reveals that the solver
naturally develops features mirroring fundamental TSP concepts: boundary
detectors that activate on convex-hull nodes, cluster-sensitive features
responding to locally dense regions, and separator features encoding geometric
partitions. These findings provide the first model-internal account of what
neural TSP solvers compute before node selection, demonstrate that geometric
structure emerges without explicit supervision, and suggest pathways toward
transparent hybrid systems that combine neural efficiency with algorithmic
interpretability. Interactive feature explorer:
https://reubennarad.github.io/TSP_interp
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Uncertainty Calibration for Equivariant Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.21691v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.21691v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edward Berman, Jacob Ginesin, Marco Pacini, Robin Walters
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data-sparse settings such as robotic manipulation, molecular physics, and
galaxy morphology classification are some of the hardest domains for deep
learning. For these problems, equivariant networks can help improve modeling
across undersampled parts of the input space, and uncertainty estimation can
guard against overconfidence. However, until now, the relationships between
equivariance and model confidence, and more generally equivariance and model
calibration, has yet to be studied. Since traditional classification and
regression error terms show up in the definitions of calibration error, it is
natural to suspect that previous work can be used to help understand the
relationship between equivariance and calibration error. In this work, we
present a theory relating equivariance to uncertainty estimation. By proving
lower and upper bounds on uncertainty calibration errors (ECE and ENCE) under
various equivariance conditions, we elucidate the generalization limits of
equivariant models and illustrate how symmetry mismatch can result in
miscalibration in both classification and regression. We complement our
theoretical framework with numerical experiments that clarify the relationship
between equivariance and uncertainty using a variety of real and simulated
datasets, and we comment on trends with symmetry mismatch, group size, and
aleatoric and epistemic uncertainties.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review at Transactions on Machine Learning Research (TMLR).
  Code is available at https://github.com/EdwardBerman/EquiUQ . Excited to
  share this paper, comments welcome :D</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal <span class="highlight-title">Dataset</span>s with Controllable Mutual Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.21686v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.21686v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raheem Karim Hashmani, Garrett W. Merz, Helen Qu, Mariel Pettee, Kyle Cranmer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a framework for generating highly multimodal datasets with
explicitly calculable mutual information between modalities. This enables the
construction of benchmark datasets that provide a novel testbed for systematic
studies of mutual information estimators and multimodal self-supervised
learning techniques. Our framework constructs realistic datasets with known
mutual information using a flow-based generative model and a structured causal
framework for generating correlated latent variables.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 4 figures, 1 table. Our code is publicly available at
  https://github.com/RKHashmani/MmMi-Datasets</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal Graph Clustering without Edge Density Signals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.21669v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.21669v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maximilien Dreveton, Elaine Siyu Liu, Matthias Grossglauser, Patrick Thiran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper establishes the theoretical limits of graph clustering under the
Popularity-Adjusted Block Model (PABM), addressing limitations of existing
models. In contrast to the Stochastic Block Model (SBM), which assumes uniform
vertex degrees, and to the Degree-Corrected Block Model (DCBM), which applies
uniform degree corrections across clusters, PABM introduces separate popularity
parameters for intra- and inter-cluster connections. Our main contribution is
the characterization of the optimal error rate for clustering under PABM, which
provides novel insights on clustering hardness: we demonstrate that unlike SBM
and DCBM, cluster recovery remains possible in PABM even when traditional
edge-density signals vanish, provided intra- and inter-cluster popularity
coefficients differ. This highlights a dimension of degree heterogeneity
captured by PABM but overlooked by DCBM: local differences in connectivity
patterns can enhance cluster separability independently of global edge
densities. Finally, because PABM exhibits a richer structure, its expected
adjacency matrix has rank between $k$ and $k^2$, where $k$ is the number of
clusters. As a result, spectral embeddings based on the top $k$ eigenvectors
may fail to capture important structural information. Our numerical experiments
on both synthetic and real datasets confirm that spectral clustering algorithms
incorporating $k^2$ eigenvectors outperform traditional spectral approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DEEDEE: Fast and Scalable Out-of-Distribution Dynamics Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.21638v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.21638v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tala Aljaafari, Varun Kanade, Philip Torr, Christian Schroeder de Witt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deploying reinforcement learning (RL) in safety-critical settings is
constrained by brittleness under distribution shift. We study
out-of-distribution (OOD) detection for RL time series and introduce DEEDEE, a
two-statistic detector that revisits representation-heavy pipelines with a
minimal alternative. DEEDEE uses only an episodewise mean and an RBF kernel
similarity to a training summary, capturing complementary global and local
deviations. Despite its simplicity, DEEDEE matches or surpasses contemporary
detectors across standard RL OOD suites, delivering a 600-fold reduction in
compute (FLOPs / wall-time) and an average 5% absolute accuracy gain over
strong baselines. Conceptually, our results indicate that diverse anomaly types
often imprint on RL trajectories through a small set of low-order statistics,
suggesting a compact foundation for OOD detection in complex environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Few-Shot Knowledge Distillation of LLMs With Counterfactual Explanations <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.21631v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.21631v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Faisal Hamman, Pasan Dissanayake, Yanjun Fu, Sanghamitra Dutta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge distillation is a promising approach to transfer capabilities from
complex teacher models to smaller, resource-efficient student models that can
be deployed easily, particularly in task-aware scenarios. However, existing
methods of task-aware distillation typically require substantial quantities of
data which may be unavailable or expensive to obtain in many practical
scenarios. In this paper, we address this challenge by introducing a novel
strategy called Counterfactual-explanation-infused Distillation CoD for
few-shot task-aware knowledge distillation by systematically infusing
counterfactual explanations. Counterfactual explanations (CFEs) refer to inputs
that can flip the output prediction of the teacher model with minimum
perturbation. Our strategy CoD leverages these CFEs to precisely map the
teacher's decision boundary with significantly fewer samples. We provide
theoretical guarantees for motivating the role of CFEs in distillation, from
both statistical and geometric perspectives. We mathematically show that CFEs
can improve parameter estimation by providing more informative examples near
the teacher's decision boundary. We also derive geometric insights on how CFEs
effectively act as knowledge probes, helping the students mimic the teacher's
decision boundaries more effectively than standard data. We perform experiments
across various datasets and LLMs to show that CoD outperforms standard
distillation approaches in few-shot regimes (as low as 8-512 samples). Notably,
CoD only uses half of the original samples used by the baselines, paired with
their corresponding CFEs and still improves performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepAgent: A General Reasoning Agent with Scalable Toolsets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.21618v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.21618v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoxi Li, Wenxiang Jiao, Jiarui Jin, Guanting Dong, Jiajie Jin, Yinuo Wang, Hao Wang, Yutao Zhu, Ji-Rong Wen, Yuan Lu, Zhicheng Dou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large reasoning models have demonstrated strong problem-solving abilities,
yet real-world tasks often require external tools and long-horizon
interactions. Existing agent frameworks typically follow predefined workflows,
which limit autonomous and global task completion. In this paper, we introduce
DeepAgent, an end-to-end deep reasoning agent that performs autonomous
thinking, tool discovery, and action execution within a single, coherent
reasoning process. To address the challenges of long-horizon interactions,
particularly the context length explosion from multiple tool calls and the
accumulation of interaction history, we introduce an autonomous memory folding
mechanism that compresses past interactions into structured episodic, working,
and tool memories, reducing error accumulation while preserving critical
information. To teach general-purpose tool use efficiently and stably, we
develop an end-to-end reinforcement learning strategy, namely ToolPO, that
leverages LLM-simulated APIs and applies tool-call advantage attribution to
assign fine-grained credit to the tool invocation tokens. Extensive experiments
on eight benchmarks, including general tool-use tasks (ToolBench, API-Bank,
TMDB, Spotify, ToolHop) and downstream applications (ALFWorld, WebShop, GAIA,
HLE), demonstrate that DeepAgent consistently outperforms baselines across both
labeled-tool and open-set tool retrieval scenarios. This work takes a step
toward more general and capable agents for real-world applications. The code
and demo are available at https://github.com/RUC-NLPIR/DeepAgent.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative Correlation Manifolds: Generating Synthetic Data with
  Preserved Higher-Order Correlations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.21610v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.21610v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jens E. d'Hondt, Wieger R. Punter, Odysseas Papapetrou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing need for data privacy and the demand for robust machine
learning models have fueled the development of synthetic data generation
techniques. However, current methods often succeed in replicating simple
summary statistics but fail to preserve both the pairwise and higher-order
correlation structure of the data that define the complex, multi-variable
interactions inherent in real-world systems. This limitation can lead to
synthetic data that is superficially realistic but fails when used for
sophisticated modeling tasks. In this white paper, we introduce Generative
Correlation Manifolds (GCM), a computationally efficient method for generating
synthetic data. The technique uses Cholesky decomposition of a target
correlation matrix to produce datasets that, by mathematical proof, preserve
the entire correlation structure -- from simple pairwise relationships to
higher-order interactions -- of the source dataset. We argue that this method
provides a new approach to synthetic data generation with potential
applications in privacy-preserving data sharing, robust model training, and
simulation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Tactile-based Reinforcement Learning for Robotic Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.21609v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.21609v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elle Miller, Trevor McInroe, David Abel, Oisin Mac Aodha, Sethu Vijayakumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Achieving safe, reliable real-world robotic manipulation requires agents to
evolve beyond vision and incorporate tactile sensing to overcome sensory
deficits and reliance on idealised state information. Despite its potential,
the efficacy of tactile sensing in reinforcement learning (RL) remains
inconsistent. We address this by developing self-supervised learning (SSL)
methodologies to more effectively harness tactile observations, focusing on a
scalable setup of proprioception and sparse binary contacts. We empirically
demonstrate that sparse binary tactile signals are critical for dexterity,
particularly for interactions that proprioceptive control errors do not
register, such as decoupled robot-object motions. Our agents achieve superhuman
dexterity in complex contact tasks (ball bouncing and Baoding ball rotation).
Furthermore, we find that decoupling the SSL memory from the on-policy memory
can improve performance. We release the Robot Tactile Olympiad (RoTO) benchmark
to standardise and promote future research in tactile-based manipulation.
Project page: https://elle-miller.github.io/tactile_rl
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Causal Climate Emulation with Bayesian Filtering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.09891v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.09891v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian Hickman, Ilija Trajkovic, Julia Kaltenborn, Francis Pelletier, Alex Archibald, Yaniv Gurwicz, Peer Nowack, David Rolnick, Julien Boussard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional models of climate change use complex systems of coupled equations
to simulate physical processes across the Earth system. These simulations are
highly computationally expensive, limiting our predictions of climate change
and analyses of its causes and effects. Machine learning has the potential to
quickly emulate data from climate models, but current approaches are not able
to incorporate physically-based causal relationships. Here, we develop an
interpretable climate model emulator based on causal representation learning.
We derive a novel approach including a Bayesian filter for stable long-term
autoregressive emulation. We demonstrate that our emulator learns accurate
climate dynamics, and we show the importance of each one of its components on a
realistic synthetic dataset and data from two widely deployed climate models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages, 26 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Intrinsic Goals for Autonomous Agents: Model-Based Exploration in
  Virtual Zebrafish Predicts Ethological Behavior and Whole-Brain Dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.00138v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.00138v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reece Keller, Alyn Kirsch, Felix Pei, Xaq Pitkow, Leo Kozachkov, Aran Nayebi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomy is a hallmark of animal intelligence, enabling adaptive and
intelligent behavior in complex environments without relying on external reward
or task structure. Existing reinforcement learning approaches to exploration in
reward-free environments, including a class of methods known as model-based
intrinsic motivation, exhibit inconsistent exploration patterns and do not
converge to an exploratory policy, thus failing to capture robust autonomous
behaviors observed in animals. Moreover, systems neuroscience has largely
overlooked the neural basis of autonomy, focusing instead on experimental
paradigms where animals are motivated by external reward rather than engaging
in ethological, naturalistic and task-independent behavior. To bridge these
gaps, we introduce a novel model-based intrinsic drive explicitly designed
after the principles of autonomous exploration in animals. Our method
(3M-Progress) achieves animal-like exploration by tracking divergence between
an online world model and a fixed prior learned from an ecological niche. To
the best of our knowledge, we introduce the first autonomous embodied agent
that predicts brain data entirely from self-supervised optimization of an
intrinsic goal -- without any behavioral or neural training data --
demonstrating that 3M-Progress agents capture the explainable variance in
behavioral patterns and whole-brain neural-glial dynamics recorded from
autonomously behaving larval zebrafish, thereby providing the first
goal-driven, population-level model of neural-glial computation. Our findings
establish a computational framework connecting model-based intrinsic motivation
to naturalistic behavior, providing a foundation for building artificial agents
with animal-like autonomy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ System-Embedded Diffusion Bridge Models <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.23726v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.23726v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bartlomiej Sobieski, Matthew Tivnan, Yuang Wang, Siyeop Yoon, Pengfei Jin, Dufan Wu, Quanzheng Li, Przemyslaw Biecek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Solving inverse problems -- recovering signals from incomplete or noisy
measurements -- is fundamental in science and engineering. Score-based
generative models (SGMs) have recently emerged as a powerful framework for this
task. Two main paradigms have formed: unsupervised approaches that adapt
pretrained generative models to inverse problems, and supervised bridge methods
that train stochastic processes conditioned on paired clean and corrupted data.
While the former typically assume knowledge of the measurement model, the
latter have largely overlooked this structural information. We introduce System
embedded Diffusion Bridge Models (SDBs), a new class of supervised bridge
methods that explicitly embed the known linear measurement system into the
coefficients of a matrix-valued SDE. This principled integration yields
consistent improvements across diverse linear inverse problems and demonstrates
robust generalization under system misspecification between training and
deployment, offering a promising solution to real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SimuRA: A World-Model-Driven Simulative Reasoning Architecture for
  General Goal-Oriented Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.23773v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.23773v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingkai Deng, Jinyu Hou, Zhiting Hu, Eric Xing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI agents built on foundation models hold enormous promise. Current practice,
however, focuses on a one-task-one-agent approach, which not only falls short
of scalability and generality, but also faces practical limitations from
black-box autoregressive reasoning, where decisions unfold token by token
without explicit simulation or counterfactual evaluation of outcomes. Humans,
on the other hand, reason and plan by mentally simulating the consequences of
actions within an internal model of the world -- a capability that supports
flexible, goal-directed behavior across diverse contexts. Moving towards a more
general and powerful AI agent, we introduce SimuRA, a goal-oriented
architecture for generalized agentic reasoning. Based on a principled
formulation of an optimal agent in any general environment, SimuRA addresses
the limitations of black-box autoregressive reasoning by incorporating the
world model for planning via simulation. Our prototype world model is
implemented using LLMs as a substrate, leveraging the natural language as a
discrete, hierarchical representation grounded in concepts for planning, while
remaining model-agnostic. On complex web-browsing tasks such as flight search,
SimuRA improves the success rate from 0% to 32.2% compared to a representative
open-web agent baseline. Across tasks, world-model-based planning achieves up
to 124% higher task completion rates than a matched black-box autoregressive
baseline, demonstrating the advantages of simulative reasoning. We release
ReasonerAgent-Web, a web-browsing agent built on SimuRA, as an open-source
research demo.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This submission has been updated to adjust the scope and presentation
  of the work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Federated Unlearning Made Practical: Seamless Integration via Negated
  Pseudo-Gradients 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.05822v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.05822v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessio Mora, Carlo Mazzocca, Rebecca Montanari, Paolo Bellavista
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The right to be forgotten is a fundamental principle of privacy-preserving
regulations and extends to Machine Learning (ML) paradigms such as Federated
Learning (FL). While FL enhances privacy by enabling collaborative model
training without sharing private data, trained models still retain the
influence of training data. Federated Unlearning (FU) methods recently proposed
often rely on impractical assumptions for real-world FL deployments, such as
storing client update histories or requiring access to a publicly available
dataset. To address these constraints, this paper introduces a novel method
that leverages negated Pseudo-gradients Updates for Federated Unlearning (PUF).
Our approach only uses standard client model updates, which are employed during
regular FL rounds, and interprets them as pseudo-gradients. When a client needs
to be forgotten, we apply the negation of their pseudo-gradients, appropriately
scaled, to the global model. Unlike state-of-the-art mechanisms, PUF seamlessly
integrates with FL workflows, incurs no additional computational and
communication overhead beyond standard FL rounds, and supports concurrent
unlearning requests. We extensively evaluated the proposed method on two
well-known benchmark image classification datasets (CIFAR-10 and CIFAR-100) and
a real-world medical imaging dataset for segmentation (ProstateMRI), using
three different neural architectures: two residual networks and a vision
transformer. The experimental results across various settings demonstrate that
PUF achieves state-of-the-art forgetting effectiveness and recovery time,
without relying on any additional assumptions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reinforcement Learning with Action Chunking <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07969v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07969v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiyang Li, Zhiyuan Zhou, Sergey Levine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Q-chunking, a simple yet effective recipe for improving
reinforcement learning (RL) algorithms for long-horizon, sparse-reward tasks.
Our recipe is designed for the offline-to-online RL setting, where the goal is
to leverage an offline prior dataset to maximize the sample-efficiency of
online learning. Effective exploration and sample-efficient learning remain
central challenges in this setting, as it is not obvious how the offline data
should be utilized to acquire a good exploratory policy. Our key insight is
that action chunking, a technique popularized in imitation learning where
sequences of future actions are predicted rather than a single action at each
timestep, can be applied to temporal difference (TD)-based RL methods to
mitigate the exploration challenge. Q-chunking adopts action chunking by
directly running RL in a 'chunked' action space, enabling the agent to (1)
leverage temporally consistent behaviors from offline data for more effective
online exploration and (2) use unbiased $n$-step backups for more stable and
efficient TD learning. Our experimental results demonstrate that Q-chunking
exhibits strong offline performance and online sample efficiency, outperforming
prior best offline-to-online methods on a range of long-horizon, sparse-reward
manipulation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The Thirty-Ninth Annual Conference on Neural Information Processing
  Systems (NeurIPS 2025); 36 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quantum Temporal Fusion <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.04048v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.04048v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Krishnakanta Barik, Goutam Paul
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The \textit{Temporal Fusion Transformer} (TFT), proposed by Lim \textit{et
al.}, published in \textit{International Journal of Forecasting} (2021), is a
state-of-the-art attention-based deep neural network architecture specifically
designed for multi-horizon time series forecasting. It has demonstrated
significant performance improvements over existing benchmarks. In this work, we
introduce the Quantum Temporal Fusion Transformer (QTFT), a quantum-enhanced
hybrid quantum-classical architecture that extends the capabilities of the
classical TFT framework. The core idea of this work is inspired by the
foundation studies, \textit{The Power of Quantum Neural Networks} by Amira
Abbas \textit{et al.} and \textit{Quantum Vision Transformers} by El Amine
Cherrat \textit{et al.}, published in \textit{ Nature Computational Science}
(2021) and \textit{Quantum} (2024), respectively. A key advantage of our
approach lies in its foundation on a variational quantum algorithm, enabling
implementation on current noisy intermediate-scale quantum (NISQ) devices
without strict requirements on the number of qubits or circuit depth. Our
results demonstrate that QTFT is successfully trained on the forecasting
datasets and is capable of accurately predicting future values. In particular,
our experimental results on two different datasets display that the model
outperforms its classical counterpart in terms of both training and test loss.
These results indicate the prospect of using quantum computing to boost deep
learning architectures in complex machine learning tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Gravity-informed Spatiotemporal <span class="highlight-title">Transformer</span> for Human Activity
  Intensity Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13678v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13678v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Wang, Zhenghong Wang, Fan Zhang, Chaogui Kang, Sijie Ruan, Di Zhu, Chengling Tang, Zhongfu Ma, Weiyu Zhang, Yu Zheng, Philip S. Yu, Yu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human activity intensity prediction is crucial to many location-based
services. Despite tremendous progress in modeling dynamics of human activity,
most existing methods overlook physical constraints of spatial interaction,
leading to uninterpretable spatial correlations and over-smoothing phenomenon.
To address these limitations, this work proposes a physics-informed deep
learning framework, namely Gravity-informed Spatiotemporal Transformer
(Gravityformer) by integrating the universal law of gravitation to refine
transformer attention. Specifically, it (1) estimates two spatially explicit
mass parameters based on spatiotemporal embedding feature, (2) models the
spatial interaction in end-to-end neural network using proposed adaptive
gravity model to learn the physical constraint, and (3) utilizes the learned
spatial interaction to guide and mitigate the over-smoothing phenomenon in
transformer attention. Moreover, a parallel spatiotemporal graph convolution
transformer is proposed for achieving a balance between coupled spatial and
temporal learning. Systematic experiments on six real-world large-scale
activity datasets demonstrate the quantitative and qualitative superiority of
our model over state-of-the-art benchmarks. Additionally, the learned gravity
attention matrix can be not only disentangled and interpreted based on
geographical laws, but also improved the generalization in zero-shot
cross-region inference. This work provides a novel insight into integrating
physical laws with deep learning for spatiotemporal prediction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE TPAMI 2025. 18 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mixture of Experts in Image Classification: What's the Sweet Spot? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.18322v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.18322v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mathurin Videau, Alessandro Leite, Marc Schoenauer, Olivier Teytaud
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mixture-of-Experts (MoE) models have shown promising potential for
parameter-efficient scaling across domains. However, their application to image
classification remains limited, often requiring billion-scale datasets to be
competitive. In this work, we explore the integration of MoE layers into image
classification architectures using open datasets. We conduct a systematic
analysis across different MoE configurations and model scales. We find that
moderate parameter activation per sample provides the best trade-off between
performance and efficiency. However, as the number of activated parameters
increases, the benefits of MoE diminish. Our analysis yields several practical
insights for vision MoE design. First, MoE layers most effectively strengthen
tiny and mid-sized models, while gains taper off for large-capacity networks
and do not redefine state-of-the-art ImageNet performance. Second, a Last-2
placement heuristic offers the most robust cross-architecture choice, with
Every-2 slightly better for Vision Transform (ViT), and both remaining
effective as data and model scale increase. Third, larger datasets (e.g.,
ImageNet-21k) allow more experts, up to 16, for ConvNeXt to be utilized
effectively without changing placement, as increased data reduces overfitting
and promotes broader expert specialization. Finally, a simple linear router
performs best, suggesting that additional routing complexity yields no
consistent benefit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Transactions on Machine Learning Research</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CityAQVis: Integrated ML-Visualization Sandbox Tool for Pollutant
  Estimation in Urban Regions Using Multi-Source Data (Software Article) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.18878v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.18878v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brij Bidhin Desai, Yukta Arvind Rajapur, Aswathi Mundayatt, Jaya Sreevalsan-Nair
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Urban air pollution poses significant risks to public health, environmental
sustainability, and policy planning. Effective air quality management requires
predictive tools that can integrate diverse datasets and communicate complex
spatial and temporal pollution patterns. There is a gap in interactive tools
with seamless integration of forecasting and visualization of spatial
distributions of air pollutant concentrations. We present CityAQVis, an
interactive machine learning ML sandbox tool designed to predict and visualize
pollutant concentrations at the ground level using multi-source data, which
includes satellite observations, meteorological parameters, population density,
elevation, and nighttime lights. While traditional air quality visualization
tools often lack forecasting capabilities, CityAQVis enables users to build and
compare predictive models, visualizing the model outputs and offering insights
into pollution dynamics at the ground level. The pilot implementation of the
tool is tested through case studies predicting nitrogen dioxide (NO2)
concentrations in metropolitan regions, highlighting its adaptability to
various pollutants. Through an intuitive graphical user interface (GUI), the
user can perform comparative visualizations of the spatial distribution of
surface-level pollutant concentration in two different urban scenarios. Our
results highlight the potential of ML-driven visual analytics to improve
situational awareness and support data-driven decision-making in air quality
management.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 10 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fixed-Point RNNs: Interpolating from Diagonal to Dense <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.10799v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.10799v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sajad Movahedi, Felix Sarnthein, Nicola Muca Cirone, Antonio Orvieto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Linear recurrent neural networks (RNNs) and state-space models (SSMs) such as
Mamba have become promising alternatives to softmax-attention as sequence
mixing layers in Transformer architectures. Current models, however, do not
exhibit the full state-tracking expressivity of RNNs because they rely on
channel-wise (i.e. diagonal) sequence mixing. In this paper, we investigate
parameterizations of a large class of dense linear RNNs as fixed-points of
parallelizable diagonal linear RNNs. The resulting models can naturally trade
expressivity for efficiency at a fixed number of parameters and achieve
state-of-the-art results on the state-tracking benchmarks $A_5$ and $S_5$,
while matching performance on copying and other tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025 (Spotlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Convergence and Generalization of Anti-Regularization for Parametric
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.17412v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.17412v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongseok Kim, Wonjun Jeong, Gisung Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anti-regularization introduces a reward term with a reversed sign into the
loss function, deliberately amplifying model expressivity in small-sample
regimes while ensuring that the intervention gradually vanishes as the sample
size grows through a power-law decay schedule. We formalize spectral safety
conditions and trust-region constraints, and we design a lightweight safeguard
that combines a projection operator with gradient clipping to guarantee stable
intervention. Theoretical analysis extends to linear smoothers and the Neural
Tangent Kernel regime, providing practical guidance on the choice of decay
exponents through the balance between empirical risk and variance. Empirical
results show that Anti-regularization mitigates underfitting in both regression
and classification while preserving generalization and improving calibration.
Ablation studies confirm that the decay schedule and safeguards are essential
to avoiding overfitting and instability. As an alternative, we also propose a
degrees-of-freedom targeting schedule that maintains constant per-sample
complexity. Anti-regularization constitutes a simple and reproducible procedure
that integrates seamlessly into standard empirical risk minimization pipelines,
enabling robust learning under limited data and resource constraints by
intervening only when necessary and vanishing otherwise.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v3: Revised the paragraph under Theoretical Analysis (English
  translation and typo corrections)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BioReason: Incentivizing Multimodal Biological Reasoning within a
  DNA-LLM Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.23579v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.23579v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adibvafa Fallahpour, Andrew Magnuson, Purav Gupta, Shihao Ma, Jack Naimer, Arnav Shah, Haonan Duan, Omar Ibrahim, Hani Goodarzi, Chris J. Maddison, Bo Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unlocking deep and interpretable biological reasoning from complex genomic
data remains a major AI challenge limiting scientific progress. While current
DNA foundation models excel at representing sequences, they struggle with
multi-step reasoning and lack transparent, biologically meaningful
explanations. BioReason addresses this by tightly integrating a DNA foundation
model with a large language model (LLM), enabling the LLM to directly interpret
and reason over genomic information. Through supervised fine-tuning and
reinforcement learning, BioReason learns to produce logical, biologically
coherent deductions. It achieves major performance gains, boosting KEGG-based
disease pathway prediction accuracy from 86% to 98% and improving variant
effect prediction by an average of 15% over strong baselines. BioReason can
reason over unseen biological entities and explain its decisions step by step,
offering a transformative framework for interpretable, mechanistic AI in
biology. All data, code, and checkpoints are available at
https://github.com/bowang-lab/BioReason
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 4 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lightweight Facial Landmark Detection in Thermal Images via Multi-Level
  Cross-Modal Knowledge Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.11128v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.11128v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiyi Tong, Olivia Nocentini, Marta Lagomarsino, Kuanqi Cai, Marta Lorenzini, Arash Ajoudani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facial Landmark Detection (FLD) in thermal imagery is critical for
applications in challenging lighting conditions, but it is hampered by the lack
of rich visual cues. Conventional cross-modal solutions, like feature fusion or
image translation from RGB data, are often computationally expensive or
introduce structural artifacts, limiting their practical deployment. To address
this, we propose Multi-Level Cross-Modal Knowledge Distillation (MLCM-KD), a
novel framework that decouples high-fidelity RGB-to-thermal knowledge transfer
from model compression to create both accurate and efficient thermal FLD
models. A central challenge during knowledge transfer is the profound modality
gap between RGB and thermal data, where traditional unidirectional distillation
fails to enforce semantic consistency across disparate feature spaces. To
overcome this, we introduce Dual-Injected Knowledge Distillation (DIKD), a
bidirectional mechanism designed specifically for this task. DIKD establishes a
connection between modalities: it not only guides the thermal student with rich
RGB features but also validates the student's learned representations by
feeding them back into the frozen teacher's prediction head. This closed-loop
supervision forces the student to learn modality-invariant features that are
semantically aligned with the teacher, ensuring a robust and profound knowledge
transfer. Experiments show that our approach sets a new state-of-the-art on
public thermal FLD benchmarks, notably outperforming previous methods while
drastically reducing computational overhead.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MobileRL: Online Agentic Reinforcement Learning for Mobile GUI Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.18119v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.18119v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Xu, Xiao Liu, Xinghan Liu, Jiaqi Fu, Hanchen Zhang, Bohao Jing, Shudan Zhang, Yuting Wang, Wenyi Zhao, Yuxiao Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building general-purpose graphical user interface (GUI) agents has become
increasingly promising with the progress in vision language models. However,
developing effective mobile GUI agents with reinforcement learning (RL) remains
challenging due to the heavy-tailed distribution of task difficulty and the
inefficiency of large-scale environment sampling. We present an online agentic
reinforcement learning framework MobileRL to enhance GUI agents in mobile
environments. Its core component is the Difficulty-ADAptive GRPO (ADAGRPO)
algorithm. In ADAGRPO, we design difficulty-adaptive positive replay and
failure curriculum filtering to adapt the model to different task difficulties.
We introduce the shortest-path reward adjustment strategy to reshape rewards
concerning the task length in multi-turn agentic tasks. Those strategies
jointly stabilize RL training, improve sample efficiency, and generate strong
performance across diverse mobile apps and tasks. We apply MOBILERL to two open
models (Qwen2.5-VL-7B-Instruct and GLM-4.1V-9B-Base). The resultant MOBILERL-9B
model achieves state-of-the-art results in terms of success rates on both
AndroidWorld (80.2%) and AndroidLab (53.6%). The MOBILERL framework is
open-sourced at: https://github.com/THUDM/MobileRL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Transformer</span>-Gather, Fuzzy-Reconsider: A Scalable Hybrid Framework for
  Entity Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.17470v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.17470v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammadreza Sharifi, Danial Ahmadzadeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Entity resolution plays a significant role in enterprise systems where data
integrity must be rigorously maintained. Traditional methods often struggle
with handling noisy data or semantic understanding, while modern methods suffer
from computational costs or the excessive need for parallel computation. In
this study, we introduce a scalable hybrid framework, which is designed to
address several important problems, including scalability, noise robustness,
and reliable results. We utilized a pre-trained language model to encode each
structured data into corresponding semantic embedding vectors. Subsequently,
after retrieving a semantically relevant subset of candidates, we apply a
syntactic verification stage using fuzzy string matching techniques to refine
classification on the unlabeled data. This approach was applied to a real-world
entity resolution task, which exposed a linkage between a central user
management database and numerous shared hosting server records. Compared to
other methods, this approach exhibits an outstanding performance in terms of
both processing time and robustness, making it a reliable solution for a
server-side product. Crucially, this efficiency does not compromise results, as
the system maintains a high retrieval recall of approximately 0.97. The
scalability of the framework makes it deployable on standard CPU-based
infrastructure, offering a practical and effective solution for
enterprise-level data integrity auditing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICCKE 2025 Conference. 6 tables, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Alleviating Forgetfulness of Linear Attention by Hybrid Sparse Attention
  and Contextualized Learnable Token Eviction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.20787v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.20787v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mutian He, Philip N. Garner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Linear-attention models that compress the entire input sequence into a
fixed-size recurrent state offer an efficient alternative to Transformers, but
their finite memory induces forgetfulness that harms retrieval-intensive tasks.
To mitigate the issue, we explore a series of hybrid models that restore direct
access to past tokens. We interleave token mixers with intermediate time and
space complexity between linear and full attention, including sparse attention
with token eviction, and the query-aware native sparse attention. Particularly,
we propose a novel learnable token eviction approach. Combined with
sliding-window attention, an end-to-end trainable lightweight CNN aggregates
information from both past and future adjacent tokens to adaptively retain a
limited set of critical KV-pairs per head, maintaining linear attention's
constant time and space complexity. Efficient Triton kernels for the sparse
attention mechanisms are provided. Empirical evaluations on retrieval-intensive
benchmarks support the effectiveness of our approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Planning and Learning in Average Risk-aware MDPs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17629v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17629v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weikai Wang, Erick Delage
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For continuing tasks, average cost Markov decision processes have
well-documented value and can be solved using efficient algorithms. However, it
explicitly assumes that the agent is risk-neutral. In this work, we extend
risk-neutral algorithms to accommodate the more general class of dynamic risk
measures. Specifically, we propose a relative value iteration (RVI) algorithm
for planning and design two model-free Q-learning algorithms, namely a generic
algorithm based on the multi-level Monte Carlo (MLMC) method, and an off-policy
algorithm dedicated to utility-based shortfall risk measures. Both the RVI and
MLMC-based Q-learning algorithms are proven to converge to optimality.
Numerical experiments validate our analysis, confirm empirically the
convergence of the off-policy algorithm, and demonstrate that our approach
enables the identification of policies that are finely tuned to the intricate
risk-awareness of the agent that they serve.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Combinatorial Creativity: A New Frontier in Generalization Abilities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.21043v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.21043v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Schapiro, Sumuk Shashidhar, Alexi Gladstone, Jonah Black, Royce Moon, Dilek Hakkani-Tur, Lav R. Varshney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence (AI) systems, and Large Language Models (LLMs) in
particular, are increasingly employed for creative tasks like scientific idea
generation, constituting a form of generalization from training data
unaddressed by existing conceptual frameworks. Despite its similarities to
compositional generalization (CG), combinatorial creativity (CC) is an
open-ended ability. Instead of evaluating for accuracy or correctness against
fixed targets, which would contradict the open-ended nature of CC, we propose a
theoretical framework and algorithmic task for evaluating outputs by their
degrees of novelty and utility. From here, we make several important empirical
contributions: (1) We obtain the first insights into the scaling behavior of
creativity for LLMs. (2) We discover that, for fixed compute budgets, there
exist optimal model depths and widths for creative ability. (3) We find that
the ideation-execution gap, whereby LLMs excel at generating novel scientific
ideas but struggle to ensure their practical feasibility, may be explained by a
more fundamental novelty-utility tradeoff characteristic of creativity
algorithms in general. Importantly, this tradeoff remains persistent even at
scale, casting doubt on the long-term creative potential of LLMs in their
current form. Together, our conceptual framework and empirical findings provide
a foundation for understanding and improving creativity in modern AI models,
bridging the gap between human and machine intelligence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. The first two authors contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FORLA: Federated Object-centric Representation Learning with Slot
  Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.02964v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.02964v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guiqiu Liao, Matjaz Jogan, Eric Eaton, Daniel A. Hashimoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning efficient visual representations across heterogeneous unlabeled
datasets remains a central challenge in federated learning. Effective federated
representations require features that are jointly informative across clients
while disentangling domain-specific factors without supervision. We introduce
FORLA, a novel framework for federated object-centric representation learning
and feature adaptation across clients using unsupervised slot attention. At the
core of our method is a shared feature adapter, trained collaboratively across
clients to adapt features from foundation models, and a shared slot attention
module that learns to reconstruct the adapted features. To optimize this
adapter, we design a two-branch student-teacher architecture. In each client, a
student decoder learns to reconstruct full features from foundation models,
while a teacher decoder reconstructs their adapted, low-dimensional
counterpart. The shared slot attention module bridges cross-domain learning by
aligning object-level representations across clients. Experiments in multiple
real-world datasets show that our framework not only outperforms centralized
baselines on object discovery but also learns a compact, universal
representation that generalizes well across domains. This work highlights
federated slot attention as an effective tool for scalable, unsupervised visual
representation learning from cross-domain data with distributed concepts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Neurips2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Narrow Gate: Localized Image-Text Communication in Native Multimodal
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.06646v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.06646v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessandro Serra, Francesco Ortu, Emanuele Panizon, Lucrezia Valeriani, Lorenzo Basile, Alessio Ansuini, Diego Doimo, Alberto Cazzaniga
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in multimodal training have significantly improved the
integration of image understanding and generation within a unified model. This
study investigates how vision-language models (VLMs) handle image-understanding
tasks, focusing on how visual information is processed and transferred to the
textual domain. We compare native multimodal VLMs, models trained from scratch
on multimodal data to generate both text and images, and non-native multimodal
VLMs, models adapted from pre-trained large language models or capable of
generating only text, highlighting key differences in information flow. We find
that in native multimodal VLMs, image and text embeddings are more separated
within the residual stream. Moreover, VLMs differ in how visual information
reaches text: non-native multimodal VLMs exhibit a distributed communication
pattern, where information is exchanged through multiple image tokens, whereas
models trained natively for joint image and text generation tend to rely on a
single post-image token that acts as a narrow gate for visual information. We
show that ablating this single token significantly deteriorates
image-understanding performance, whereas targeted, token-level interventions
reliably steer image semantics and downstream text with fine-grained control.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ YTLive: A <span class="highlight-title">Dataset</span> of Real-World YouTube Live Streaming Sessions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2510.24769v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2510.24769v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mojtaba Mozhganfar, Pooya Jamshidi, Seyyed Ali Aghamiri, Mohsen Ghasemi, Mahdi Dolati, Farzad Tashtarian, Ahmad Khonsari, Christian Timmerer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Live streaming plays a major role in today's digital platforms, supporting
entertainment, education, social media, etc. However, research in this field is
limited by the lack of large, publicly available datasets that capture
real-time viewer behavior at scale. To address this gap, we introduce YTLive, a
public dataset focused on YouTube Live. Collected through the YouTube
Researcher Program over May and June 2024, YTLive includes more than 507000
records from 12156 live streams, tracking concurrent viewer counts at
five-minute intervals along with precise broadcast durations. We describe the
dataset design and collection process and present an initial analysis of
temporal viewing patterns. Results show that viewer counts are higher and more
stable on weekends, especially during afternoon hours. Shorter streams attract
larger and more consistent audiences, while longer streams tend to grow slowly
and exhibit greater variability. These insights have direct implications for
adaptive streaming, resource allocation, and Quality of Experience (QoE)
modeling. YTLive offers a timely, open resource to support reproducible
research and system-level innovation in live streaming. The dataset is publicly
available at github.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Seeing Sound, Hearing Sight: Uncovering Modality Bias and Conflict of AI
  models in Sound Localization <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.11217v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.11217v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanhao Jia, Ji Xie, S Jivaganesh, Hao Li, Xu Wu, Mengmi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Imagine hearing a dog bark and turning toward the sound only to see a parked
car, while the real, silent dog sits elsewhere. Such sensory conflicts test
perception, yet humans reliably resolve them by prioritizing sound over
misleading visuals. Despite advances in multimodal AI integrating vision and
audio, little is known about how these systems handle cross-modal conflicts or
whether they favor one modality. In this study, we systematically examine
modality bias and conflict resolution in AI sound localization. We assess
leading multimodal models and benchmark them against human performance in
psychophysics experiments across six audiovisual conditions, including
congruent, conflicting, and absent cues. Humans consistently outperform AI,
demonstrating superior resilience to conflicting or missing visuals by relying
on auditory information. In contrast, AI models often default to visual input,
degrading performance to near chance levels. To address this, we propose a
neuroscience-inspired model, EchoPin, which uses a stereo audio-image dataset
generated via 3D simulations. Even with limited training data, EchoPin
surpasses existing benchmarks. Notably, it also mirrors human-like horizontal
localization bias favoring left-right precision-likely due to the stereo audio
structure reflecting human ear placement. These findings underscore how sensory
input quality and system architecture shape multimodal representation accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2025, Spotlight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Save It for the "Hot" Day: An LLM-Empowered Visual Analytics System for
  Heat Risk Management 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.03317v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.03317v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haobo Li, Wong Kam-Kwai, Yan Luo, Juntong Chen, Chengzhong Liu, Yaxuan Zhang, Alexis Kai Hon Lau, Huamin Qu, Dongyu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The escalating frequency and intensity of heat-related climate events,
particularly heatwaves, emphasize the pressing need for advanced heat risk
management strategies. Current approaches, primarily relying on numerical
models, face challenges in spatial-temporal resolution and in capturing the
dynamic interplay of environmental, social, and behavioral factors affecting
heat risks. This has led to difficulties in translating risk assessments into
effective mitigation actions. Recognizing these problems, we introduce a novel
approach leveraging the burgeoning capabilities of Large Language Models (LLMs)
to extract rich and contextual insights from news reports. We hence propose an
LLM-empowered visual analytics system, Havior, that integrates the precise,
data-driven insights of numerical models with nuanced news report information.
This hybrid approach enables a more comprehensive assessment of heat risks and
better identification, assessment, and mitigation of heat-related threats. The
system incorporates novel visualization designs, such as "thermoglyph" and news
glyph, enhancing intuitive understanding and analysis of heat risks. The
integration of LLM-based techniques also enables advanced information retrieval
and semantic knowledge extraction that can be guided by experts' analytics
needs. Our case studies on two cities that faced significant heatwave events
and interviews with five experts have demonstrated the usefulness of our system
in providing in-depth and actionable insights for heat risk management.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2025-11-01T05:29:42.103489430Z">
            2025-11-01 05:29:42 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
